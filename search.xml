<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BERT</title>
    <url>/blog/2021/08/16/BERT/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来看读的是大名鼎鼎的 BERT，出自论文 Google 团队 2018 年的论文《BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding》。BERT（<strong>B</strong>idirectional
<strong>E</strong>ncoder Representations from
<strong>T</strong>ransformers）可谓是 NLP 历史上划时代的预训练模型，在 11 项自然语言处理任务上都取得了 state-of-the-art。并且，Google 将 BERT 的代码与预训练模型全部开源，便于大家使用。</p>
<span id="more"></span>
<h2 id="预训练语言模型">预训练（语言）模型</h2>
<p>预训练语言模型是自然语言处理中的重要部分，与计算机视觉中的预训练模型类似，也是为了从特定的下游任务中脱离出来，在大量数据上预训练可以解决通用任务的模型。这样以来，下流任务可以根据自己的任务特点进行微调，而不需要从头训练。这样做的好处非常明显：</p>
<ul>
<li>训练时间更短</li>
<li>数据要求更少</li>
</ul>
<p>在计算机视觉中，预训练模型例如 ImageNet 首先得到广泛应用，利用 ImageNet，你可以很快地构造一个特定任务的识别模型，而不需要从头训练，重复捕捉像物体边界等信息。在自然语言处理中，预训练模型应用就没有那么广泛。一个主要的原因就是多义词，例如 “苹果” 既可以代表电脑品牌、也可以代表水果，其具体的含义要根据具体的上下文才能推断出来。而像 word2vec/glove 这样的静态词向量算法，词向量一经训练得到就固定了，所以不能建模一词多义的现象。ELMO 于 2018 年 3 月提出，在双向 LSTM 上预训练语言模型，解决了静态词向量存在的问题，然而还没来得及大展拳脚，就被 2018 年 10 月的 BERT 拍死在了沙滩上。。。</p>
<p>预训练模型可以简单分为两类：</p>
<ul>
<li>基于特征的预训练模型，指利用语言模型的中间结果，作为额外的特征，引入到下游任务中。典型的就是 ELMO。特点：<strong>模型参数是固定的</strong>。</li>
<li>基于微调的预训练模型，指在语言模型的基础上，加入少量的特定任务参数（例如分类任务，加一层 softmax），再在任务数据上微调模型，典型的就是 GPT。特点：<strong>模型参数需要微调</strong>。</li>
</ul>
<p>在 BERT 之前的预训练模型，例如 ELMO 与 GPT，存在的一个重要问题是它们只从单向建模了序列。虽然 ELMO 是使用的双向 LSTM，也只是把双向的隐藏状态进行了拼接，双向的特征信息也没有很好地进行融合。形式化的来说，标准的语言模型就是单向的，当前词的选择只依赖于先前词，例如从左到右方向：
<span class="math display">\[
P_{l2r}(x_1,x_2,\dots,x_n)=\prod_{t=1}^nP(x_t|x_{&lt;t})
\]</span></p>
<h3 id="掩码语言模型masked-language-modelmlm">掩码语言模型（masked
language model，MLM）</h3>
<p>考虑到标准的语言模型所存在的问题，BERT 提出了一种掩码语言模型的任务，具体做法为：随机遮挡输入中的部分单词，目标是仅根据单词的上下文预测当前位置本来的单词（即遮掩前的单词）。与从左到右的语言模型不同，掩码语言模型能够融合单词的左右上下文。形式化而言，对于<span class="math inline"> \(x_t\)</span> 的概率分布，掩码语言模型计算的是<span class="math inline"> \(P(x_t|x_{&lt;t},x_{&gt;t})\)</span>，标准的语言模型计算的是<span class="math inline"> \(P(x_t|x_{&lt;t})\)</span>。但值得注意的是，掩码语言模型并不是传统的语言模型，一些论文中可能存在二者混用的情况。语言模型的定义可以看我之前的博客
<a href="https://tqnwhz.github.io/blog/2021/07/22/language-model/#more">语言模型
| 一隅</a>。</p>
<p>个人认为掩码语言模型的思路与 cbow 的思想是有些像的，根据一个单词的上下文来预测这个词。</p>
<h2 id="bert">BERT</h2>
<p>BERT 主要分为两个阶段：预训练和微调。两者流程如下所示：</p>
<p><img src="architecture.png"></p>
<p>可以看到，除了输出层，预训练和微调的架构基本上是完全一致的。预训练的权重将会作为下流微调任务的参数的初始权重。在序列中，BERT 加入了两个特殊标记：</p>
<ul>
<li>[CLS]：添加于序列起始位置，作为整个序列的表示，可以用于分类任务。</li>
<li>[SEP]：当输入为一对序列的时候（例如问答任务，一问一答），添加于两序列之中，作为分隔符。</li>
</ul>
<p>BERT 由若干个 <strong>Transformer 编码器</strong>（注意只有编码器）组成，在论文中，作者提出了两种规模的 BERT：</p>
<ul>
<li><span class="math inline">\(BERT_{BASE}\)</span>​：由 12 个 Transformer 编码器堆叠而成，隐节点大小为 768，自注意力机制有 12 个头，约 110M 参数。</li>
<li><span class="math inline">\(BERT_{LARGE}\)</span>​：由 24 个 Transformer 编码器堆叠而成，隐节点大小为 1024，自注意力机制有 16 个头，约 340M 参数。</li>
</ul>
<h3 id="输入输出表征">输入输出表征</h3>
<p>在输入表征上，BERT 使用了 WordPiece 的词嵌入，词表大小为 30000。WordPiece 是一种子词模型，token 粒度介于整个单词与字符之间，例如会将 “working” 这个单词拆分为 “work”、“ing” 两个 token 存储在词表中，"ing" 又可以与其他的动词结合，这样就可以用更小的词表存储更多的单词，也没有损失太多的语义信息。</p>
<p>BERT 的输入既可以是单个序列，也可以是一对序列（例如问答场景）。应用于一对序列时，需要插入 [SEP] 分隔符，并且要使用段嵌入向量。对于输入的每个 token，它的输入表征为以下三个向量的和：</p>
<ul>
<li>符号向量（token embedding），也就是我们传统意义上所说的词向量。</li>
<li>段向量（segement
embedding），用以区分一对序列中的两个不同的序列。</li>
<li>位置向量（position embedding），用以编码位置信息。</li>
</ul>
<p><img src="embedding.png"></p>
<p>值得注意的是，<strong>以上三种词向量全部通过学习得到</strong>，这与 Transformer 不同。Transformer 中的位置向量是通过三角函数计算得到，而 BERT 是通过学习得到的。就这一点作者似乎没有进行解释，而 Transformer 的作者在论文中实验结果是，学习得到与使用三角函数的位置向量效果相近，而三角函数更易扩展。一种说法是 BERT 使用的语料更大，可能可以学习到更好的位置向量。</p>
<h3 id="预训练">预训练</h3>
<h4 id="掩码语言模型">掩码语言模型</h4>
<p>在训练时，按一定比例（实验中为 15%）随机屏蔽输入序列中的部分单词（使用 [MASK] 替换），然后预测被屏蔽掉的单词，而不是重建整个输入序列。这个过程与完形填空类似，想象一个句子中有几个单词空缺，掩码语言模型的目标就是根据上下文成功预测空缺的单词。这与语言模型的训练方式还是有较大差别的。</p>
<p>上述方式虽然听上去很合理，但有一个问题，在下流任务的微调阶段，并不会出现 [MASK] 标记，这一定程度上导致了预训练与微调间的不匹配。为了解决这种情况，BERT 并不总是使用 [MASK] 替换掉需要屏蔽的单词，而是按照概率执行对应操作：</p>
<ul>
<li>80% 替换为 [MASK]</li>
<li>10% 替换为随机其他单词</li>
<li> 10% 保持不变</li>
</ul>
<p>之后，被屏蔽掉的单词会被用以预测原本的单词，换而言之，预测概率变为<span class="math inline"> \(P(x_t|x_{&lt;t},x_{mask},x_{&gt;t})\)</span>。这样能够缓解预训练与微调间的不匹配情况。直观来看，BERT 会参考被屏蔽掉单词，因为它有 10% 的概率就是真实的单词，但也不会完全依赖这个单词，因为 10% 的概率还是很小的。</p>
<h4 id="下句预测next-sentence-predictionnsp">下句预测（Next Sentence
Prediction，NSP）</h4>
<p>考虑到 NLP 中的很多任务例如问答、自然语言推理都基于句子间关系的理解，而这种句子间的关系不能被语言模型捕获，因此 BERT 提出了一个名为下句预测的预训练任务。顾名思义，这个任务的目标是判断两个句子是否构成上下句的关系。这个任务的数据非常容易获得，在大规模语料上获取连续的两句话，并以 50% 的概率替换真实的下句话，即可得到正负样本分布均匀的数据集。</p>
<h3 id="微调">微调</h3>
<p>在微调时，只需要将特定任务的输入输出放到 BERT 中，微调所有参数，输入的形式可以是单个句子或句子对，可以应用的任务举例如下：</p>
<ul>
<li>单个句子：文本分类、序列标注、情绪分析（利用 [CLS] 符号）</li>
<li>句子对：释义、问答等任务</li>
</ul>
<p>对于句子对，传统的模型往往将其拆分分开处理，而 BERT 将句子对同时投喂到模型中，能够更好地捕获句子间关系。</p>
<h2 id="实验">实验</h2>
<p>实验部分主要介绍了 BERT 微调之后是怎么横扫涵盖通用语言评理解评估等十一项任务的，基线模型有 biLSTM+elmo，GPT 等，这里就不多介绍了。有兴趣的可以去看看原文。</p>
<h2 id="消融实验">消融实验</h2>
<p>消融实验更像是控制变量法，对于模型中的多个改进，控制变量来分析哪个改进对于效果的提升是最大的，这就是消融实验。消融实验细节可以看原文，我在这里总结一下消融实验的结论：</p>
<h3 id="预训练任务">预训练任务</h3>
<ul>
<li>去除下句预测后的 BERT 模型在 QNLI,
MNLI 等涉及句子对的任务上的性能损失严重，证明下句预测对于句子间关系的捕获还是很有作用的。</li>
<li>仅使用从左到右的语言模型训练的 BERT 比使用掩码语言模型训练得到的 BERT 效果要差，证明掩码语言模型训练方式的有效性。</li>
</ul>
<h3 id="模型大小">模型大小</h3>
<ul>
<li>参数越多，各项任务上的效果越好，非常的真实。</li>
</ul>
<h3 id="训练步数">训练步数</h3>
<ul>
<li>BERT 真的需要在 128,000 字符 /batch
上训练 1,000,000 步才能达到这么好的效果，训练一百万步比五十万步的 BERT 在 MNLI 获得了 1% 的提升。</li>
<li>掩码语言模型的收敛慢于自左向右的语言模型，但就精度而言，掩码语言模型几乎在训练之初就强于语言模型。</li>
</ul>
<h3 id="基于特征的方法">基于特征的方法</h3>
<ul>
<li>BERT 对基于特征和微调的方法都是有效的。</li>
</ul>
<h2 id="结论">结论</h2>
<p>论文提出了一种全新的预训练任务 -- 掩码语言模型，并在该任务和下句预测任务上预训练了一个基于双向 Transformer 的深层模型 BERT。在十一项 NLP 任务上的微调实验结果表明，BERT 的效果优于现有的预训练模型。</p>
<p>BERT 的论文断断续续读了几天，读下来感觉醍醐灌顶，很多之前模棱两可的东西都真正了解了。果然读论文还是要读原文，别人的博客只是参考。后面有空了再读一读 BERT 的源码，又想去读 GPT 的论文，时间也太少了。</p>
<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/46833276">论文解读：BERT 模型及 fine-tuning
- 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>BERT</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>预训练模型</tag>
      </tags>
  </entry>
  <entry>
    <title>BART</title>
    <url>/blog/2021/08/29/BART/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>BART 是 Facebook AI 于 2019 年发表的《Denoising Sequence-to-Sequence
Pre-training for Natural Language Generation, Translation, and
Comprehension》论文中提出的预训练模型，论文收录于 2020 年 ACL。顾名思义，BART 是一个基于 seq2seq 的预训练模型，可以用于自然语言生成、翻译、理解等任务。论文中的 “Denoising” 直译为降噪，实际上是模型的预训练目标。</p>
<span id="more"></span>
<p>一个水逆的周末，博客更新不能停！</p>
<h2 id="模型">模型</h2>
<p>预训练模型之前已经介绍过了，参考 <a href="https://tqnwhz.github.io/blog/2021/08/16/BERT/#more">BERT</a>。这里只做简单的介绍。预训练模型的目的是在大量数据上预训练一个能够解决通用任务的模型，下游任务可以在预训练模型的基础上进行调整适配，无需从头训练。预训练模型往往有几个关键因素：</p>
<ul>
<li>模型架构。Transformer 是公认的特征抽取能力很强的架构，因此常见的预训练模型都是用的 Transformer 架构。</li>
<li>预训练目标。在 BERT 之前，预训练模型往往都是按照标准的语言模型进行训练，例如 ELMO。BERT 第一次提出了掩码语言模型这样的预训练任务，不仅能够更好地适配下游任务，而且取得了更优的效果。如何能够挑选一个更好的预训练目标来建模通用任务，也是预训练模型的关键。</li>
<li>适用任务及使用方法。虽然预训练模型是为建模通用任务而存在的，然而还是存在适用任务的限制，具体任务对使用方法也有要求。</li>
<li>数据集。要求很简单，大而全。大就不用说了，数据集最好能够涵盖多个领域的数据，这样适配下游任务也会更简单。</li>
<li>效果。事实上没有个 state-of-the-art 都不太可能发出来，相对没有那么重要。</li>
</ul>
<p>按照这个顺序，我们来介绍一下 BART 模型。</p>
<h3 id="架构">架构</h3>
<p>基于 Transformer 的 seq2seq 模型，与 GPT 和 BERT 一样，使用的激活函数是 gelu 而不是 relu。与 BERT 的区别在于：</p>
<ul>
<li>有解码器、在解码器的每一层，添加了对编码器最后一层输出的注意力，跟 seq2seq 的注意力一致。</li>
<li>BART 去掉了在词预测之前的前馈神经网络。</li>
</ul>
<p>总结一下就是个 Transformer。与 BERT 的主要区别在于有解码器，可以用于生成任务，与 GPT 的主要区别在于有编码器，可以更好地用于监督的生成任务，如下图所示。</p>
<p><img src="architecture.png"></p>
<h3 id="预训练目标">预训练目标</h3>
<p>BART 的预训练目标定义为：给定文档，使用噪声函数（符号遮挡、符号删除、符号填充、文档排列、文档旋转）对文档施加噪声，再进行文档重构。换而言之，输入为有噪声的文档，期望输出为没有噪声的文档，这正是论文名中的 “降噪” 的由来。
在实验过程中，噪声可能是以上噪声函数的组合。几种噪声函数的示例分别如下：</p>
<p><img src="noises.png"></p>
<h3 id="适用任务">适用任务</h3>
<p>BART 可适用于以下任务：</p>
<ul>
<li>句子分类，输入输出均为该序列，将解码器的最终隐藏状态拿去分类即可，类似 BERT 中的 [CLS]
token。</li>
<li>符号分类，输入输出均为该序列，将解码器每个位置的隐藏状态拿去分类即可。</li>
<li>序列生成，例如文本摘要、问答等任务，给定输入输出进行 fine-tune 即可。</li>
<li>目标语言为英语的机器翻译，这个任务其实也属于序列生成，不过有点不太一样。具体做法为，将 BART 的编码器随机初始化（就是丢弃本来的权重），然后冻结其他参数只更新编码器权重，后面再微调所有权重。这里限制为英语主要是 BART 本身在英文语料上训练的。</li>
</ul>
<h2 id="实验">实验</h2>
<h3 id="预训练目标比较">预训练目标比较</h3>
<p>为了评估各种预训练目标的有效性，BART 在尽量控制变量（分别调优，学习率、正则化可能有所差别）的前提下比较了如下几种预训练目标：</p>
<ul>
<li>语言模型，与 GPT 类似，从左向右的语言模型</li>
<li>置换语言模型，基于 XLNET，对 1/6 的符号进行采样，再进行自回归预测</li>
<li>掩码语言模型，与 BERT 类似</li>
<li>多任务掩码语言模型，与 uniLM 类似</li>
<li>掩码 seq2seq：掩码 50% 的序列，由 seq2seq 预测</li>
</ul>
<p>通过在问答、对话、摘要等多项任务上进行比较，论文得出以下结论：</p>
<ul>
<li>预训练目标的性能与下游任务有着很密切的关系，一个简单的语言模型可以在生成式问答上取得最优效果，在抽取式问答上效果确实最差的</li>
<li>符号遮挡是至关重要的，没有符号遮挡的文档旋转、句子重排的预训练目标表现较差</li>
<li>从左到右的语言模型预训练任务能改善生成任务，像掩码语言模型和置换语言模型不包含自回归语言模型训练任务，生成任务效果就会比较差</li>
<li>双向编码器对 SQuAD 数据集是非常重要的</li>
<li>预训练目标并非唯一重要的因素，任务性能也与模型结构等因素有很大关系</li>
<li>纯粹的语言模型在 ELI5 数据集上取得了最好的性能</li>
<li>使用文本填充预训练的 BART 取得了大部分数据集上的最优性能</li>
</ul>
<h3 id="大规模预训练">大规模预训练</h3>
<h4 id="判别任务">判别任务</h4>
<p>在两个数据集上进行实验：</p>
<ul>
<li>SQuAD（Stanford Question Answering
Dataset，斯坦福问答数据集）：给定一篇文章和一个问题，从原文中选取部分文字作为问题的答案（答案一定原文中）。虽然是问答任务，但是并不是生成式的任务，而是判别式的任务。</li>
<li>GLUE（General Language Understanding
Evaluation，通用语言理解评估）：由九个任务组成，每个任务都是句子或句子对的分类任务，例如 CoLA 对单句子是否符合文法进行评估，QQP 评估一对问题是否等价。</li>
</ul>
<p><img src="nlu.png"></p>
<p>两个数据集上的实验结果显示，BART 可以和 RoBERTa 打的有来有回。</p>
<h4 id="生成任务">生成任务</h4>
<p>分别在摘要生成、对话、问答生成、翻译多个任务上进行了实验，数据集分别介绍如下：</p>
<ul>
<li>CNN/DailyMail 和 XSum 是摘要生成的两个英文数据集。每个数据样本由文档与人工总结的摘要组成。与抽取式摘要任务不同，摘要中可以出现文档中未出现的单词或者句子。</li>
<li>CONVAI 是一个对话数据集，回复不仅取决于上下文，还取决于对话人的角色信息，换而言之，模型需要根据上下文和角色信息生成合适的回复。</li>
<li>ELI5 是一个生成式问答数据集，根据文档回答指定的问题。</li>
<li>WMT16 是一个翻译数据集，涵盖了多种语料到英语的翻译数据。</li>
</ul>
<p>在四项生成任务上的评估表明，BART 都取得了 SOTA 的性能。</p>
<h2 id="总结">总结</h2>
<p>BART 是一个基于 Transformer 架构的去噪 seq2seq 模型，通过破坏和重建原始文本进行预训练，在自然语言理解任务上与现有模型难分伯仲，但在自然语言生成任务上达到了 SOTA 的性能。</p>
<p>预训练模型简单分为三类：</p>
<ul>
<li>仅编码器，如 BERT，可以直接用于自然语言理解任务，或者加个解码器再用于生成任务。</li>
<li>仅解码器，如 GPT，可以直接用于自然语言生成任务，或者加个编码器可以用于条件生成任务。</li>
<li>编码器 + 解码器，如 BART，可以同时直接用于两种任务。</li>
</ul>
<p>后面可能会读一下 RoBERTa 的论文。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>BART</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>BART</tag>
      </tags>
  </entry>
  <entry>
    <title>ConKADI</title>
    <url>/blog/2021/07/20/ConKADI/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来看一篇对话系统的文章，收录于 2020 年的 ACL。这篇文章是常识对话模型（CCM）的后续工作，我之前粗读过一次，还有很多疑惑，今天正好借着这个机会细读一遍。在此之前，先梳理一下对话系统的发展历史。</p>
<span id="more"></span>
<h2 id="对话系统发展简介">对话系统发展简介</h2>
<p>对话系统，即能够与人进行对话的计算机系统，是自然语言处理中的一个重要方向。在前神经网络时期，对话系统主要基于模板生成回复。即使是现在，仍由一些场景下在使用基于模板的回复生成。在 2014 年，seq2seq（Sequence
to
sequence）模型被提出。seq2seq 提供了一种序列间进行转换映射的通用方法。此后，seq2seq 被广泛用于各类序列任务，包含对话系统。但是 seq2seq 应用于对话系统任务时会有以下问题：</p>
<ul>
<li>对同一输入，只能生成单一回复。而理想的对话系统间输入与回复间的关系应该是一对多的。</li>
<li>倾向于生成通用性回复（例如，我不知道）。</li>
</ul>
<p>此后，构建能够生成多样性回复的对话系统一直是研究人员研究的重点。2017 年，zhao 等人将条件变分自编码器（Condition
vae，CVAE）应用于对话生成，通过在隐变量分布中采样不同的隐变量，模型能够生成多样的回复。</p>
<p>另外，有研究指出，对话模型生成通用性回复的原因之一是语料中缺少人类拥有的知识背景，这使得模型无法学习知识进而理解对话。基于此，一部分工作开始探索在对话模型中引入外部知识。2018 年 zhou 等提出的常识对话模型（CCM）就是这类研究的典型代表。</p>
<p>常识对话模型 CCM 虽然比传统模型取得了更好的效果，但是 CCM 在检索知识实体相关知识事实时，没有考虑到实体单词所在的上下文，而复杂实体单词的具体含义往往是由其上下文决定的。这就来到了本文要介绍的 ConKADI。</p>
<p><img src="apple.png"></p>
<h2 id="研究方法">研究方法</h2>
<p>本文提出了：</p>
<ul>
<li>Felicitous Fact
mechanism（恰当事实机制）帮助模型关注在上下文高度相关的知识事实。</li>
<li>上下文知识融合以及灵活模式融合技术，促进知识的集成。</li>
</ul>
<p>ConKADI（Context Knowledge-Aware Diverse and Informative conversation
generation
model），别的不说，这个名字真的跟叠 buff 一样。。。模型的流程如下：</p>
<ol type="1">
<li>恰当事实机制根据知识实体词所在上下文计算得到知识事实的概率分布。（此过程中，使用真实回复作为后验来监督学习）。</li>
<li>上下文融合机制在解码之前将上下文与知识融合。</li>
<li>ConKADI 在灵活融合模式下生成三种类型的单词。</li>
</ol>
<h3 id="模型概览">模型概览</h3>
<p><img src="model.png"></p>
<p>主要由以下几部分组成：</p>
<ul>
<li>知识检索器（Knowledge Retriever）：给定输入<span class="math inline"> \(X\)</span>，对于每一个单词<span class="math inline"> \(x_i\)</span>，检索<span class="math inline"> \(x_i\)</span>​作为头实体或者尾实体的知识事实，若不为实体词，则返回一个空事实。</li>
<li>上下文编码器（Context
Encoder）：使用双向 GRU 进行编码，特殊的是，GRU 的输入加入了当前实体词的嵌入向量。</li>
<li>恰当知识识别器（Felicitous Fact Recognizer）：计算检索事实<span class="math inline"> \(F=\{f_1,f_2,\dots,f_n\}\)</span>​上的概率分布<span class="math inline"> \(z\)</span>，计算过程如下:</li>
</ul>
<p><span class="math display">\[
z_{post}=\eta(\phi(F\cdot
W_{ft})\cdot\phi([{h^x_n}^\intercal;{h^y_m}^\intercal]\cdot
W_{post}))^\intercal
\]</span></p>
<p><span class="math display">\[
z_{prior}=\eta(\phi(F\cdot W_{ft})\cdot\phi({h^x_n}^\intercal\cdot
W_{prior}))^\intercal
\]</span></p>
<p>其中，<span class="math inline">\(\eta\)</span>​​是 softmax 函数，<span class="math inline">\(\phi\)</span>​​是 tanh 激活函数，<span class="math inline">\(F\in
R^{l*(d_e+d_r+d_e)}\)</span>​​是知识事实矩阵，<span class="math inline">\(W_{ft},W_{post},W_{prior}\)</span>​​​​​是训练参数​。直观来看，上下文、知识事实都包含在公式中，但是也不好进一步解释公式的由来，更像是两部分拼凑在一起的。与 VAE 一样，在得到先后验分布后，使用 KL 散度作为损失函数<span class="math inline"> \(\mathcal
L_k\)</span>，达到逼近先后验分布的效果。</p>
<ul>
<li>上下文知识融合：为了增强解码器对知识背景的理解，将输入上下文与知识融合作为解码器的初始权重，即<span class="math inline"> \({h^y_0}^\intercal=tanh([{h^x_n}^\intercal;f_z^\intercal]\cdot
W_{init})\)</span>​</li>
</ul>
<p>此外，为了保证<span class="math inline"> \({h^x_n}^\intercal,f_z^\intercal\)</span> 是有意义的，模型中还引入了词袋损失（参考 CVAE）。为了监督<span class="math inline"> \(z_{post}\)</span>​的概率分布的计算，引入了监督的条件信号（参考 CCM），二者之和为损失函数<span class="math inline"> \(\mathcal L_f\)</span>。​</p>
<h3 id="知识解码器">知识解码器</h3>
<p>解码器同样是 GRU，在解码时，会从以下三种类型的单词中选择进行输出：</p>
<ul>
<li>词表单词</li>
<li>知识实体单词，计算过程如下：</li>
</ul>
<p><span class="math display">\[
z_{d,t}=\eta(\phi(F\cdot
W_{ft})\cdot\phi([{h^y_t}^\intercal;{u_{t-1}}^\intercal]\cdot
W_{d}))^\intercal
\]</span></p>
<p><span class="math display">\[
\gamma_t=sigmoid([{h^y_t}^\intercal;u_t^\intercal;c_t^\intercal]\cdot
W_{gate})\in R^1
\]</span></p>
<p><span class="math display">\[
p_{k,t}=\gamma_t*z+(1.0-\gamma_t)*z_d
\]</span></p>
<p>其中，<span class="math inline">\(c_t\)</span> 是注意力机制的结果，<span class="math inline">\(z_{d,t}\)</span> 也是同样方法计算得到的知识事实的概率分布，与<span class="math inline"> \(z\)</span> 相比，<span class="math inline">\(z_{d,t}\)</span> 是动态的，而<span class="math inline"> \(z\)</span> 是静态的，与 CCM 中的动 / 静态图注意力机制类似。之后，计算得到一个标量<span class="math inline"> \(\gamma_t\)</span> 作为二者的相对比例，求和得到最终的实体单词权重。</p>
<ul>
<li>复制单词。解码器可从输入中复制一个单词作为输出，计算过程如下：</li>
</ul>
<p><span class="math display">\[
p_{c,t}=\eta(\phi(H^x\cdot W_{cs})\cdot\phi({u_t^c}^\intercal\cdot
W_{ct})^\intercal)
\]</span></p>
<p><span class="math display">\[
{u^c_t}^\intercal=[{h^y_t}^\intercal,{u_{t-1}}^\intercal,{c_t}^\intercal]
\]</span></p>
<p>计算形式与前文知识事实概率分布的计算相似。</p>
<h3 id="灵活模式融合">灵活模式融合</h3>
<p>最终输出的概率分布为三种模式的加权和（其中，<span class="math inline">\((\gamma_{w,t},\gamma_{k,t},\gamma_{c,t})\)</span> 是由灵活模式融合计算得出的概率分布，即三者之和为 1。）：
<span class="math display">\[
p_{out,t}=\gamma_{w,t}*p_{w,t}+\gamma_{k,t}*p_{k,t}+\gamma_{c,t}*p_{c,t}
\]</span> 这一部分损失函数为<span class="math inline"> \(\mathcal
L_n\)</span>： <span class="math display">\[
-\sum_t\lambda_tlogp_{out,t}(y_t|y_{t-1:1},X,F)+\frac{\mathcal L_m}{2}
\]</span> 其中，<span class="math inline">\(\mathcal
L_m\)</span> 为解码器输出与真实回复间的交叉熵，<span class="math inline">\(\lambda_t\)</span>​为词表外单词（unk）的惩罚项权重：
<span class="math display">\[
\lambda_t=
\begin{cases}
\frac{1}{\#(unk\in Y)}^3,\ if\ y_t=unk\\
1,\ otherwise
\end{cases}
\]</span> 个人猜测思路是这样，如果<span class="math inline"> \(y_t\)</span> 为 unk，<span class="math inline">\(\lambda_t\)</span>​会更小，进而优化对应参数的速度会减慢。</p>
<h2 id="case-study">Case Study</h2>
<p>下文是论文中展示的回复样例，只看表格生成回复的效果还是不错的。</p>
<p><img src="result.png"></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话系统</category>
      </categories>
      <tags>
        <tag>常识对话</tag>
        <tag>CopyNet</tag>
      </tags>
  </entry>
  <entry>
    <title>DEM-VAE</title>
    <url>/blog/2021/07/30/DEM-VAE/</url>
    <content><![CDATA[<h2 id="序言">序言</h2>
<p>DEM-VAE 是字节跳动 AI LAB 团队于 2020 年发表的《Dispersed Exponential
Family Mixture VAEs for Interpretable Text
Generation》论文中提出的模型，论文收录在 ICML 中。论文名直译为 “用于<strong>可解释</strong>文本生成的<strong>分散指数族混合</strong>
VAE ”。</p>
<span id="more"></span>
<h2 id="题外话">题外话</h2>
<p>最近实习面试结束了，回来更新博客了。“黄色的树林里分出两条路，可惜我不能同时去涉足”，最近有些感慨。看到这篇博客的人，希望这篇博客能对你有所帮助，也希望你天天开心。</p>
<h2 id="简介">简介</h2>
<p>连续型 VAE 的隐变量难以解释分散属性，例如主题、对话动作等。这一点与 VQ-VAE 的动机相似。然而只使用分散隐变量的 VAE 的表达能力有限，隐变量<span class="math inline"> \(c\)</span>​只包含<span class="math inline"> \(log(\#c)\)</span>​位的信息，其中<span class="math inline"> \(\#c\)</span>​为<span class="math inline"> \(c\)</span>​​可选值的数量。（这里的意思应该是信息论中的 “信息量”，默认隐变量服从均匀分布，各值取得的概率相等，信息量<span class="math inline"> \(-log(1/\#c)=log(\#c)\)</span>​。）</p>
<p>混合高斯分布的 VAE（GM-VAE）提供了一种自然的想法，将分散隐变量与连续隐变量结合：每个高斯分布代表一个分散属性值，分量的值代表属性相同的句子。在理想情况下，不同高斯分布的均值与方差应该差别很大。然而 GM-VAE 容易出现模式崩溃问题，这使得不同高斯分布的均值与方差非常接近，GM-VAE 退化为只有一个高斯分量的普通 VAE。如下图所示：</p>
<p><img src="example.png"></p>
<p>在本文中，作者证明了模式崩溃问题不仅存在于 GM-VAE 中，而是具有指数族混合先验分布的 VAE（EM-VAE）的普遍问题，由证据下界中的一个分散项引起。进而，作者提出了一个船新的 DEM-VAE，在 EM-VAE 的目标函数里引入了额外一项分散项。按照论文的说法，DEM-VAE 虽然适度减小了句子的似然（由于引入了新的损失项），但是在 rPPL（reverse
perplexity）与 BLEU 得到了更好的结果，并且能够有效地避免模式崩溃问题。</p>
<h2 id="模式崩溃vs后验崩塌">模式崩溃 vs 后验崩塌</h2>
<p>普通的 VAE 会面临后验坍塌（KL 散度消失）的问题，具体而言，KL 损失项在训练之初迅速变为 0。而本文要解决的是模式崩溃问题，是指先验分布中的多个模式崩溃为一个模式。模式崩溃会后验坍塌之间无必然联系。在后验坍塌未出现时，也可能出现模式崩溃。</p>
<p>虽然本文采用的解决方案与之前的解决后验坍塌的方案有些相似：找到目标函数中导致问题的那一项并削弱它的影响。但是本文采用的解决方案只引入了一个启发式的分散项，而不是整个 KL 损失项。</p>
<h2 id="解决方法">解决方法</h2>
<h3 id="混合指数族vae">混合指数族 VAE</h3>
<p>混合指数族 VAE 是指使用混合指数族分布作为先验分布的 VAE（<a href="https://en.wikipedia.org/wiki/Exponential_family">Exponential
family -
Wikipedia</a>），最为常见的就是混合高斯分布的 VAE，GM-VAE，它的先验分布为混合的高斯分布。GM-VAE 使用一个分散变量<span class="math inline"> \(c\)</span> 代表不同的高斯成分，连续隐变量<span class="math inline"> \(z\)</span> 依赖于<span class="math inline"> \(c\)</span>。如下图所示：</p>
<p><img src="gm-vae.png"></p>
<p>其中，实现为依赖关系，虚线为变分后验。其中，<span class="math inline">\(p(c)\)</span>​可以近似为均匀分布，<span class="math inline">\(p_\eta(z|c)\)</span>​为指数族分布，例如高斯分布。</p>
<p>测试时：从先验分布<span class="math inline"> \(p(c)\)</span> 中采样一个<span class="math inline"> \(c\)</span>，然后从<span class="math inline"> \(c\)</span> 对应的高斯分布中采样隐变量<span class="math inline"> \(p(z|c)\)</span>，接着投喂到解码器<span class="math inline"> \(p(x|z)\)</span> 中。</p>
<p>训练时：通过最大化边际似然<span class="math inline"> \(\int\sum_cp_\eta(z,c)p_\theta(x|z)dz\)</span>​​进行训练是不可行的。与 VAE 一样，使用近似后验分布<span class="math inline"> \(q_\phi(z,c|x)=q_\phi(z|x)q_\phi(c|x)\)</span>​作为<span class="math inline"> \(p(z,c|x)\)</span>​​​的估计，进一步改为优化如下所示的证据下界：</p>
<p><img src="elbo.png"></p>
<h3 id="模式崩溃问题">模式崩溃问题</h3>
<p>作者通过研究 ELBO 目标函数，将导致模式崩溃的原因定位到<span class="math inline"> \(\mathcal R_c\)</span> 与<span class="math inline"> \(\mathcal
R_z\)</span> 中。作者从指数族分布的参数化定义出发，将损失项<span class="math inline"> \(\mathcal R_z,\mathcal
R_c\)</span>​重写为 KL 平均正则项与分散项<span class="math inline"> \(\mathcal L_d\)</span>​​。 <span class="math display">\[
\mathcal L_d=\mathbb E_{q_\phi(c|x)}A(\eta_c)-A(\mathbb
E_{q_\phi(c|x)}\eta_c)&gt;=0
\]</span> 作者得出结论，最小化分散项<span class="math inline"> \(\mathcal
L_d\)</span>​使得先验分布的加权方差（即模式崩溃趋势）。这一部分的数学推导较为复杂，有兴趣的可以去看看原文。因此，作者提出在损失函数中加入一项正的分散项来抵消这一趋势，最终损失函数如下所示：
<span class="math display">\[
L(\theta;x)=ELBO+\beta \cdot \mathcal L_d
\]</span> 其中，<span class="math inline">\(\beta\)</span>​是一个超参数，通过调整<span class="math inline"> \(\beta\)</span>​​来达到平衡模式崩溃与正常训练。</p>
<h3 id="dem-vae">DEM-VAE</h3>
<p>在上述方法基础上，作者发现，使用额外的互信息项能够进一步优化可解释性，这一部分可以在实验结果中看到。互信息项在之前的工作中用于缓解 KL 散度消失的问题，定义如下：
<span class="math display">\[
\mathcal L_{mi}=\mathcal H(c)-\mathcal H(c|x)=\mathbb E_x\mathbb
E_{q_\phi(c|x)}(logq_\phi(c|x)-logq_\phi(c))
\]</span>
公式部分介绍完毕。在模型结构上，编码器为 GRU 等循环单元、解码器为一个语言模型。</p>
<h2 id="实验">实验</h2>
<h3 id="模式崩溃实验结果">模式崩溃实验结果</h3>
<p><img src="mode-collapse-result.png"></p>
<p>可以看出，同时引入互信息项和分散项的 VAE（DGM-VAE，DEM-VAE）的各个分量分布有着较为明显的分类边界，没有出现模式崩溃问题。</p>
<h3 id="文本生成">文本生成</h3>
<p>作者使用四个指标：逆困惑度、BLEU、词级 KL 散度、负对数似然来评估文本生成的质量。其中，逆困惑度是指一个 LSTM 语言模型，从 VAE 的先验分布中采样的数据上进行训练，再在测试集上进行评估。实验结果如下：</p>
<p><img src="lg-result.png"></p>
<p>可以看到，正如前文作者所说，由于引入了额外的分散项，使得 NLL（负对数似然）相较基线模型更大，但是 rPPL，BLEU 等指标上取得了更好的结果。</p>
<h2 id="总结">总结</h2>
<p>这篇论文也是离散 VAE 的一种尝试，在混合高斯分布的基础上，引入额外的分散项来解决模式崩溃问题。这使得模型的解释性更强。与之前介绍过得 EQ-VAE 相比，隐变量可以表征更多信息。感觉还是很有意义的工作，就是有点难懂。。。</p>
<h2 id="参考">参考</h2>
<p><a href="https://www.iczhiku.com/hotspotDetail/q7K2UUl4a6Isl4ZlEzOrgg==">ICML
2021 | DEM-VAE：一类新的可解释文本生成模型 - IC 智库
(iczhiku.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>文本生成</category>
      </categories>
      <tags>
        <tag>VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>对话系统综述</title>
    <url>/blog/2022/02/08/DialogueSystem-Survey/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来读一篇对话系统综述。《Recent Advances in Deep Learning Based
Dialogue Systems: A Systematic
Survey》是由南洋理工大学于 2021 年发表的论文，目前收录在 arxiv 中。该文调研了对话系统中的历史研究工作，并从模型和系统两个方面进行分析，确定了可能的未来研究方向。此外，本文还涵盖了相关框架、数据集的介绍。</p>
<span id="more"></span>
<p>具体而言：</p>
<ul>
<li>模型类型层面，分析广泛应用于对话系统的不同模型的原理、特点和应用。</li>
<li>系统类型层面，讨论对比面向任务的对话系统和开放域对话系统。</li>
</ul>
<h2 id="背景">背景</h2>
<p>按照应用，对话系统通常可以分为两类：</p>
<p><strong>面向任务的对话系统（Task-oriented dialogue
systems）</strong>，用于解决特定领域的特定问题，例如机票预订等。传统的面向任务的对话系统往往以流水线结构组织，包含四个基本模块：自然语言理解、对话状态跟踪、策略学习和自然语言生成。近期工作探索使用端到端的方法，以实现更好的效果。</p>
<p><strong>开放域对话系统（Open-domain dialogue
systems）</strong>，旨在不受域的限制地与用户聊天，通常完全由数据驱动。开放域对话系统通常可以分为三类：</p>
<ul>
<li><strong>生成系统。</strong>使用 Seq2Seq 模型将用户输入和历史消息生成回复，可以灵活生成上下文相关的回复，但有时缺乏连贯性并且乏味（例如我不知道）。</li>
<li><strong>基于检索的系统。</strong>根据用户输入从语料库中检索回复，受有限语料库的影响，有时检索到的回复与对话上下文相关性较弱。</li>
<li><strong>集成系统。</strong>结合生成和检索的方法，从二者中选出最优的。</li>
</ul>
<p>此外，生成系统还可以用于改进检索得到的回复。</p>
<p>传统的对话系统大多基于有限状态的、基于统计学习和机器学习方法。基于有限状态的系统易于实现并且可以生成自然的回复，应用于早期的一些场景固定、对话流程确定的产品中。基于统计学习与机器学习的方法通常使用模板填充来处理任务，相较基于有限状态的系统，要更为灵活。但由于模板固定，应用场景和回复的多样性也受到限制。</p>
<p>深度学习的发展则提高了对话系统的性能，其广泛应用于各种 NLP 任务中，也是近些年的研究热点。</p>
<h2 id="神经模型">神经模型</h2>
<p>本节讨论的模型包括：卷积神经网络 (CNNs)、循环神经网络 (RNNs)、Vanilla
序列到序列模型、分层循环编码器 - 解码器
(HRED)、记忆网络、注意力网络、Transformer、指针网络和
CopyNet、深度强化学习模型、生成对抗网络 (GAN)、知识图增强神经网络。</p>
<h3 id="cnn">CNN</h3>
<p><strong>卷积神经网络</strong>（<strong>Convolutional Neural
Networks，CNN</strong>）包含卷积层，池化层和前向层，架构如下图所示。输入为长度为 7，特征维度为 5 的序列，使用 6 个卷积核得到 6 个特征图，池化后拼接得到 6 维向量，接一个全连接层后进行分类。</p>
<p><img src="CNN.png"></p>
<p>卷积核的操作如下。其中，m 和 n 代表结果矩阵行列的索引，f 是输入矩阵，h 是卷积核。滑动窗口使得卷积层能够捕获局部特征，池化层则用于扫描产生全局特征，赋予了 CNN 局部和全局感知能力。而参数共享机制可以降低模型的复杂度。
<span class="math display">\[
G(m,n)=(f*h)(m,n)=\sum_j\sum_k h(j,k)f(m-j,n-k)
\]</span>
由于这些特性，CNN 广泛应用于 CV 中，在 NLP 中也得到了一定应用。CNN 是很好的文本特征提取器，但它们可能不是理想的序列编码器。虽然一些对话系统直接使用 CNN 作为编码器编码语言和知识，但是大多数历史最佳对话系统选择在编码文本信息之后，使用 CNN 作为层次特征抽取器。这是由于 CNN 的固定输入长度（输入尺寸改变，全连接层参数也需要改变）和有限的卷积跨度（难以捕捉长距离依赖）。</p>
<p>通常，CNN 用于处理编码信息的对话系统主要有两种情况：</p>
<ul>
<li>应用 CNN 基于来自编码器的特征向量提取特征。从 character-level
embeddings 中抽取特征，证明了 CNN 的层级抽取能力。</li>
<li>在响应检索任务中提取特征图。使用单独的编码器对对话上下文和候选响应进行编码，使用
CNN
作为从编码对话上下文和候选响应计算的相似度矩阵的提取器。实验表明，该方法可以在响应检索任务中取得良好的性能。</li>
</ul>
<p>近几年较新的工作不选择 CNN 作为对话编码器的主要原因是，它们无法连续、灵活地跨时间序列提取信息。</p>
<h3 id="rnn">RNN</h3>
<p>标准神经网络和 CNN 的两个限制在于：假设数据不同位置相互独立；输入为固定长度。对于存在依赖、长度可变的输入序列，网络性能受到限制。循环神经网络（RNN）应运而生。公式的介绍可以参考我的博客<a href="https://tqnwhz.github.io/blog/2021/07/22/rnns/#more">循环神经网络 RNN 及其变体 GRU、LSTM
| 一隅 (tqnwhz.github.io)</a>，这里就不再赘述。</p>
<p>在对话系统中，RNN 多用于编码器和解码器。编码器用于编码对话上下文、对话状态、对话历史以及外部知识等信息。解码器通过 greedy
search 或者 beam search 进行解码。</p>
<h3 id="hred">HRED</h3>
<p><strong>HRED（Hierarchical Recurrent
Encoder-Decoder）</strong>是一个上下文感知的 Seq2Seq 模型，使用两个层次的 RNN 对 token-level 和 turn-level 的序列进行建模，架构如下图所示。token-level
RNN 编码器序列得到序列表征（同时也是解码器），turn-level
RNN 编码对话历史的序列表征，得到当前的上下文向量，并作为解码器的初始状态。</p>
<p><img src="HRED.png"></p>
<p>HRED 还有一些变体。例如 VHRED 在解码阶段引入隐变量，以建模更复杂的序列依赖关系。最近在对话相关任务中的许多工作都应用了基于
HRED
的框架来捕获分层对话特征。其中有一些改进工作，例如参考 Transformer 引入自注意力，引入新的层次捕获全局知识、主题信息等。</p>
<h3 id="memory-network">Memory Network</h3>
<p>记忆网络（Memory
Network）是解决有关外部知识问题的重要网络。诸如 LSTM 类的模型，虽然有记忆功能，但记忆模块太小（只有一个固定维度的向量），无法精确地记录全部内容。因此 Weston
et al. (2014) 提出了记忆网络，包含四个模块：</p>
<ul>
<li>记忆模块：存储记忆事实表示</li>
<li> I（Input）模块：将输入的记忆事实映射到嵌入式表示</li>
<li> G（Generalization）模块：决定记忆模块的更新，简单起见可以直接将新记忆插入，不进行遗忘和更新</li>
<li> O（Output）模块：根据输入和现有的记忆状态输出</li>
<li> R（Response）模块：根据 O 模块输出产生最终响应</li>
</ul>
<p>端到端的记忆网络架构如下，流程分为三个阶段：</p>
<ul>
<li><strong>权重计算：</strong>使用模型 A、B 分别将输入记忆和输入查询映射为向量，计算权重<span class="math inline"> \(p_i=Softmax(u^Tm_i)\)</span> 代表查询<span class="math inline"> \(u\)</span> 与记忆<span class="math inline"> \(m_i\)</span> 的关系。</li>
<li><strong>记忆选择：</strong>使用模型 C 将输入记忆编码为嵌入向量<span class="math inline"> \(c_i\)</span>，并计算加权和<span class="math inline"> \(o=\sum
p_ic_i\)</span>。这实际上跟 Attention 类似，是一种 soft-alignment。</li>
<li><strong>最终预测：</strong>将记忆和查询的和映射为概率向量，即<span class="math inline"> \(\hat \alpha=Softmax(W(o+u))\)</span>。</li>
</ul>
<p><img src="MemoryNetwork.png"></p>
<p>记忆网络广泛用于 QA、面向任务的对话系统中，记忆模块用于存储外部知识等信息。记忆网络的思想也在很多模型中得到了体现。</p>
<h3 id="注意力-transformer">注意力 &amp; Transformer</h3>
<p>为解决 Seq2Seq 将输入序列编码为固定维度向量带来的信息损失，Bahdanau et
al. (2014)
提出注意力机制用于机器翻译任务。其思想是解码的每一步与整个输入序列关联，公式描述如下：
<span class="math display">\[
P(y_i|y_{t&lt;i})=g(y_{i-1},s_i,c_i)\\
s_i=f(s_{i-1},y_{i-1},c_i)\\
c_i=\sum_{j=1}^{T_x}\alpha_{ij}h_j
\]</span> 其中，h 和 s 分别为编码器和解码器的隐藏状态，i 代表时间步，<span class="math inline">\(y\)</span> 代表输出的 token。g 和 f 为计算输出和更新隐藏状态的函数，由 RNN 的种类决定。<span class="math inline">\(c_i\)</span> 为解码的第 i 步与整个输入序列的注意力结果，<span class="math inline">\(\alpha_{ij}\)</span> 为注意力分数，计算公式为：
<span class="math display">\[
\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})} \\
e_{ij}=a(s_{i-1},h_j)\\
\]</span> a 是相关性度量函数，根据注意力的类型有不同的形式，<span class="math inline">\(\alpha_{ij}\)</span> 即是相关性上进行归一化（Softmax）。注意力的图示如下：</p>
<p><img src="Attention.png"></p>
<p>此后，注意力机制大行其道，基本成为 Seq2Seq 的标配。但是由于 RNN 的不可并行性、难以捕捉长距离依赖等缺点，研究者提出了 Transformer，详细的介绍可参考我的博客 <a href="https://tqnwhz.github.io/blog/2021/08/14/Transformer/">Transformer
| 一隅</a>。</p>
<p>... 待补充</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://blog.csdn.net/u014577702/article/details/117825782">Recent
Advances in Deep Learning-based Dialogue
Systems_北风吹过的秋 - CSDN 博客</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/31170263">论文笔记 - HRED 与
VHRED - 知乎 (zhihu.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32257642">论文笔记 - Memory
Networks 系列 - 知乎 (zhihu.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话系统</category>
        <category>综述</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>对话系统</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT 三部曲之三</title>
    <url>/blog/2021/08/18/GPT-3/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>GPT 系列是 OpenAI 推出的预训练模型，时至今日已经包含了三个模型，今天我来读的是 GPT 系列第三部，出自 2020 年发表在 NeurIPS 上的论文《Language
Models are Few-Shot
Learners》。秉着最新的成果往往更重要的原则，GPT 系列我打算倒着读。从名字可以看出，GPT-3 关注点在于少样本学习，虽然预训练模型在下游任务微调上取得了很好的成果，但是下游任务的微调往往也需要一定规模的数据集。GPT-3 希望能够用更大的模型（1750 亿）来将微调任务转变为少样本学习任务。</p>
<span id="more"></span>
<h3 id="预训练模型">预训练模型</h3>
<p>像 BERT、GPT 此类的预训练模型，虽然经过微调后能够很好地应用于各种下游任务。但是微调任务往往也需要数万规模的标注数据集。而对于人类来说，并不需要大规模的数据集才能学习到特定任务，只需要一个简单的描述（例如，请告诉我这些句子的情绪是开心的还是低落的）或者很少的数据（这是两个勇敢的人的例子，请再给出一个勇敢的例子）。因此，如何让预训练模型能够像人一样灵活、通用地解决问题，成了研究者追求的目标。</p>
<p>解决上述问题的想法之一是通过元学习（meta-learning）。元学习的。在语言模型中，元学习意味着模型在训练阶段能够拥有识别通用模式的能力，并在推理阶段快速识别并适应特定任务，如下图所示。</p>
<p><img src="meta-learning.png"></p>
<p>GPT-2 就是通过上下文学习来达到上述效果，具体来说，GPT-2 会将任务嵌入到预料中，例如一个英语到法语的翻译任务的语料为：“”I’m
not the cleverest man in the world, but like they say <strong>in
French</strong>: Je ne suis pas un imbecile [I’m not a
fool].”，进而避免了显式的任务枚举、编码等操作。在这样的语料上训练语言模型，来达到多任务学习的效果。但是这样的一个问题就是模型（虽然有十几亿参数）可能很难学习到这样复杂的依赖关系。虽然 GPT-2 取得了一些初步的结果，但是效果仍远不如微调。</p>
<p><img src="learning-type.png"></p>
<h3 id="参数膨胀">参数膨胀</h3>
<p>自从 NLP 中预训练模型提出以来，参数越多，性能越好基本成为了大家的共识。参数膨胀也成为了预训练模型更新换代的趋势。从 2018 年 BERT 的 3 亿参数，到 GPT-2 的 15 亿参数，再到 2020 年 GPT-3 的 1750 亿参数，每次模型增大都带来了下游任务的改进。其算力的要求也让预训练模型成为大公司垄断的研究方向。</p>
<p>在论文中，作者还实验了从 1.25 亿参数到 130 亿参数间的一系列小模型，并对其在零样本、单样本、少样本实验上的性能进行了评估，结果如下图所示：</p>
<p><img src="model-size.png"></p>
<p>可以看到，三种任务上的性能都随参数的增加而得到提升。</p>
<h3 id="数据污染">数据污染</h3>
<p>在使用像 Common
Crawl 这样大规模数据集的时候，可能会出现数据污染问题：由于数据规模过大，测试集的一些样本出现在训练集之中，使得评估不准确。GPT-3 使用的是来自互联网上的文本数据，更有可能出现这样的问题。因此，作者开发了工具来量化数据污染对实验的影响，并对受较大影响的数据集进行了标注。</p>
<h2 id="实验方法">实验方法</h2>
<p>实验的设置与 GPT-2 类似，不同的是，GPT-3 对比了上下文学习的不同设置，包含以下四种方法（单样本与零样本学习区分开来的原因在于，在某些任务例如与人类对话中，单样本学习更匹配）：</p>
<table>
<colgroup>
<col style="width: 6%">
<col style="width: 39%">
<col style="width: 30%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>方式</th>
<th>特点</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>微调</td>
<td>在下游任务的监督数据集上更新权重</td>
<td>性能好</td>
<td>每个任务都需要较大规模的监督数据</td>
</tr>
<tr class="even">
<td>少样本学习</td>
<td>在下游任务推理时提供 K 个样本（10-100）作为演示，不更新权重</td>
<td>减少了对监督数据规模的要求</td>
<td>效果比微调差的多</td>
</tr>
<tr class="odd">
<td>单样本学习</td>
<td>除了任务的自然语言描述，只有一个演示样本。</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>无样本学习</td>
<td>只接受任务的自然语言描述，无演示样本。</td>
<td>使用起来最便利<br>最接近人类执行任务的方式</td>
<td>挑战性最强，在某些情况下非常困难</td>
</tr>
</tbody>
</table>
<p>四种方式介绍如下图所示：</p>
<p><img src="learning-type.png"></p>
<h3 id="模型">模型</h3>
<p>"GPT-3 的模型与 GPT-2 类似，除了 GPT-3 是交替使用密集 Transformer 与局部带状稀疏注意力机制的 Transformer"。这基本就是论文中对 GPT-3 模型的全部介绍了。我翻到 GPT-2 的论文，"GPT-2 是基于 Transformer 的语言模型架构，模型细节大体与 GPT 类似，除了将层标准化提前到子块的输入位置，并在最后一个自注意力机制块后加入层标准化"。套娃现象属实有点严重。</p>
<p>下面是 GPT-1 的结构，看起来就是个 12 层的 Transformer。上面提到的那些局部带状系数注意力机制的 Transformer 等到后面再补充吧。</p>
<p><img src="gpt-1.png"></p>
<h2 id="总结">总结</h2>
<p>GPT 系列的模型结构变化不大，重要的一直是实验部分，从普通的预训练语言模型到试图通过少样本学习解决各种下游任务的庞然大物。整个 GPT-3 的论文，只有十页左右在介绍非实验部分，剩下的几十页都是实验。这次因为时间原因先介绍到这里，实验部分等后续有时间再补充吧。</p>
<p>这篇论文一个最大的写作特点在于引用非常的奇怪。可能是我见识比较少，但是 word2vec，glove 这种写出来非常直观的方法，后面加个引用也非常清晰，论文中却不提缩写，只使用的作者姓首字母加年份的引用，像 word2vec、glove 的引用名分别为 MCCD13、PSM14。只看这个引用让人不知所云，还要点超链接浪费时间。</p>
<h2 id="参考">参考</h2>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>GPT</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>谷歌 LaMDA：高达 137B 参数的 “全能型” 聊天机器人</title>
    <url>/blog/2022/03/19/LaMDA/</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>《LaMDA: Language Models for Dialog
Applications》是谷歌于 2022 年发表的论文，收录在 arxiv 中。论文提出了一个名为 LaMDA（<strong>La</strong>nguage
<strong>M</strong>odels for <strong>D</strong>ialog
<strong>A</strong>pplication）的对话模型，拥有 137B 参数，在 1.56T 公开对话数据和网页上预训练。实验证明，虽然模型扩展能够提升对话质量，但是在安全性和事实性方面的改进很小。而监督数据上的微调能够帮助模型利用外部知识源进行回复，显著改进了安全性和事实性两个指标。</p>
<span id="more"></span>
<p>LaMDA 构建了一个工具集（TS，Tool
set），包含：信息检索系统、计算器、翻译器。通过监督数据微调，LaMDA 能够利用这些工具来回答问题，这使得模型能够根据已知知识来源做出响应，减少了幻觉现象。</p>
<h2 id="简介">简介</h2>
<p>LaMDA
使用单个 Transformer 模型来执行多项任务：它生成潜在响应，然后出于安全考虑对其进行过滤、基于外部知识源并重新排序以找到最高质量的响应。LaMDA 的参数范围从
2B 到 137B
参数，在以上指标上进行测试。结果如下图所示。左侧为质量分数，右侧为安全性和事实性分数。</p>
<p><img src="overview.png"></p>
<p>可以观察到：</p>
<ul>
<li>单独的模型缩放提高了质量，但它在安全性和接地性方面的改进远远落后于人类表现</li>
<li>结合缩放和微调在所有指标上显着提高了
LaMDA，尽管模型的性能仍然低于人类水平在安全性和接地性的水平</li>
</ul>
<h2 id="预训练">预训练</h2>
<p>预训练模型架构都差不多，关键参数如下：</p>
<ul>
<li>参数规模：137B</li>
<li> 数据集：预训练（2.97B 文档 + 1.12B 对话）</li>
<li>架构 &amp; 训练目标：Transformer-Decoder，语言模型（预测下一个 token），如下图所示</li>
<li>训练成本：1024 TPU-v3 * 57.7 天</li>
</ul>
<p><img src="pretraining.png"></p>
<h2 id="评价指标">评价指标</h2>
<h3 id="ssi">SSI</h3>
<p><strong>SSI</strong> 是合理性、特异性、趣味性三项指标（Sensibleness,
Specificity,
Interestingnes）的平均值，是谷歌在 Menna 中提出的 SSA（合理性、特异性两项平均）的改进。各项指标具体含义如下：</p>
<ul>
<li><strong>合理性</strong>：衡量模型的回复在上下文和不要与前面所说的任何内容相矛盾。然而，通用和无聊的回复，例如 “我不知道” 的合理性分数可能很高。因此只有这一项指标是远远不够的。</li>
<li><strong>特异性</strong>：衡量模型的回复是否特定于上下文。例如，如果用户说 “我爱欧洲电视网”，而模型回答 “我也是”，那么它的特异性得分为
0。因为这种句式适用于很多上下文。</li>
<li><strong>趣味性</strong>：衡量模型的回复是否有趣。例如，对 “我如何扔球？” 的回应可能是 “你可以先捡起然后扔球来扔球”，这是有道理的，并且是针对问题的。另一个更深层次和更令人满意的答案可能是 “扔球的一种方法是用双手牢牢握住它，然后再向下摆动你的手臂，伸展你的肘部，然后向上释放球”。</li>
</ul>
<p>每项指标对应一个 0/1 标签，正例为 1，负例为 0，算术平均后就是 SSI 的值。</p>
<h3 id="角色特定指标">角色特定指标</h3>
<p><strong>有用性</strong>：如果模型的响应包含基于用户使用信息检索系统进行的独立研究的正确信息，并且用户认为它们有帮助，则它们被标记为有用。<strong>有用的响应是信息性响应的子集</strong>，由用户判断为正确且有用。</p>
<p><strong>角色一致性</strong>：如果模型的响应看起来像执行目标角色的代理会说的话，则它们被标记为角色一致。这个与合理性中的一致性不同，这里的一致性是指，例如让模型扮演珠穆朗玛峰，模型的回复中的语气词、设定等都要以珠穆朗玛峰为准。</p>
<h3 id="其他">其他</h3>
<p><strong>安全性</strong>是根据 Google
人工智能原则设定的，用以以避免造成伤害风险的意外结果，并避免产生或加强不公平的偏见，这个对应很多条规则，比较复杂，就略过了。</p>
<p><strong>事实性</strong>：包含外部世界声明的回复中，可由权威外部来源支持的回复的百分比。</p>
<p><strong>信息性</strong>：在所有回复中，包含已知来源支持的外部世界信息的回复所占的百分比。信息性与事实性仅在限定词上有所不同。因此，像 “这是一个好主意” 这样的回答，如果不包含任何外部世界的信息，就不会影响其事实，但会影响其信息性。</p>
<p><strong>引用准确度</strong>：引用其来源 URL 的模型回应在所有明确声称外部世界的回应中所占的百分比，不包括众所周知的事实（如 “马有四条腿”）。</p>
<h2 id="微调">微调</h2>
<h3 id="判别式生成式微调">判别式 / 生成式微调</h3>
<p>为了提高质量，谷歌团队收集了众包人员与 LaMDA 就任何主题交谈的 6400 次对话，每个对话包含 14-30 轮。对于每个模型回复，众包人员为其评估每个质量标签。如果回复不合理（合理性 = 0），不会收集特异性和趣味性标签。同样，如果回复不具体（特异性 = 0），不会收集趣味性标签。</p>
<p>LaMDA 生成回复的时候按照 <code>&lt;上下文&gt;&lt;哨兵&gt;&lt;回复&gt;</code> 的模板进行生成，按照 <code>&lt;上下文&gt;&lt;哨兵&gt;&lt;回复&gt;&lt;属性&gt;&lt;分数&gt;</code> 的模板进行判别式微调，例如 "What’s
up? RESPONSE not much. SENSIBLE 1"。</p>
<p>得到众包数据后，按照上述模板进行微调。这样在生成时，就能够预测出对应属性值作为筛选的辅助信息。将生成的候选序列按照<span class="math inline"> \(3 *P(sensible) + P(specific) +
P(interesting)\)</span> 进行排名，选择排名靠前的候选序列作为下一个响应。</p>
<p>可以看到，经过微调 + 生成筛选，LaMDA 的安全性和质量都有了较高提升。论文贴心地给出了两种类型的图。可以看到经过微调的 LaMDA 比仅预训练的基础模型在各项指标上都有提升，在对话质量上已经接近甚至超过了人类的水平，安全性上也十分接近人类的水平。不过事实性和信息性还有一定的差距。</p>
<p><img src="finetune-line.png"></p>
<p><img src="finetune.png"></p>
<h3 id="调用外部信息系统">调用外部信息系统</h3>
<p>为了避免幻觉，谷歌团队构建了一个工具集（Toolset，TS），包含：信息检索系统、计算器、翻译器。TS 接收一个字符串作为输入，输出一个字符串的列表。例如，计算器接收 135+7721”，返回 [“7856”]。类似地，翻译器可以接收 “hello
in French” 并输出 [“Bonjour”]。信息检索系统可以接收 “Howold is Rafael
Nadal?”，并输出 [“Rafael Nadal / Age /
35”]。如果一个工具无法解析输入（例如，计算器无法解析 “Rafael Nadal
几岁？”），它将返回一个空的结果列表，因此不会对最终输出列表做出贡献。</p>
<p>团队收集了 40k 监督的对话数据用于生成，9k 条 LaMDA 的生成候选数据（标记为正确 / 不正确）用于判别排名。这些数据同样是众包人员与 LaMDA 间通过交互式和静态方法收集得到。</p>
<p>微调分为两部分：</p>
<ul>
<li>根据上下文和基础模型响应，获取 TS 查询字符串。例如，Rafael Nadal
几岁？：上下文 + 基础→“TS，Rafael Nadal 的年龄”</li>
<li> 根据基础回复和 TS 返回结果，预测事实版本回复：例如，“他现在 31
岁”+“Rafael Nadal / Age / 35”。然后它预测事实版本：上下文 + 基础 + 查询
+ 片段 →“用户，他现在 35 岁”。</li>
</ul>
<p>实际交互的例子如下：</p>
<p><img src="toolset.png"></p>
<h2 id="总结">总结</h2>
<p>这篇论文还是挺有意思的，这种生成式 + 判别式微调的方式还是第一次见，而且也可以辅助结果搜索。而且，少量的微调数据就可以取得非常好的效果。引用原文中的一句话：使用适量的人工注释微调数据（不到
0.001%
的预训练数据），可以在更好质量和更安全的对话模型方面取得重大进展。</p>
<p>虽然，说是少量，也有几千上万条数据，而且就论文中的标注复杂度，标注成本也是蛮高的。不过相较于预训练数据的海量数据，微调数据可以说是少的多的多了。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话生成</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>对话生成</tag>
        <tag>LaMDA</tag>
      </tags>
  </entry>
  <entry>
    <title>基于知识库的问答综述（KBQA）</title>
    <url>/blog/2021/10/25/KBQA-survey/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来读一篇 2021 年的知识库问答的综述，《A Survey on Complex Knowledge
Base Question Answering:Methods, Challenges and
Solutions》，论文收录在 IJCAI
2021 中。这篇文章主要介绍了知识库问答的背景与挑战，并总结介绍了两大主流方法：基于语义解析（semantic
parsing-based，SP-based）和基于信息检索（information
retrivel-based，IR-based）。 <span id="more"></span></p>
<h2 id="知识库问答">知识库问答</h2>
<p>知识库是一个结构化的数据库，它与知识图谱类似，由（主体 Subject，关系 Relation，客体 Object）三元组组成，例如（JK 罗琳，出生地，英国），常见的知识库有 Freebase 等。这个三元组可以用来回答 “JK 罗琳出生在哪里？” 的问题。与简单的、答案与主体直接相连的简单 QA 不同，复杂 QA 查询任务涉及多跳推理甚至一些聚合关系。例如下图知识库，“谁是 The
Jeff Probst Show 提名的 TV
Producer 的第一任妻子” 问题的答案，包含多个实体和多跳处理逻辑。 <img src="multi-hop.png"></p>
<p>KBQA 的第一步是识别问题中的主体并链接到知识库中的实体，然后根据实体的邻域推导问题答案。这里分为两种方法基于语义解析和基于信息检索的两种方法。语义解析的思想是将自然语言问题表示为可以在知识库中进行查询的符号化的逻辑形式，然后再用逻辑语言进行查询（例如 SQL）。基于信息检索的方法思想是构建一个问题特定的知识图包含了相关的所有信息，然后将所有实体按相关性进行排序。然而，这些方法会面临以下挑战：</p>
<ul>
<li>基于语义解析的方法很难覆盖复杂的查询（多跳推理、约束关系、数值计算等）。类似的，基于信息检索的方法也很难回答复杂的问题，检索的实体范围可能太小，而且解释性差。</li>
<li>复杂的实体和关系会使得搜索空间过大（逻辑形式、候选结果等），搜索开销过大。</li>
<li>两种方法将问题理解看作重要的步骤，当问题的语法和语义复杂时，模型需要有很强的自然语言理解和生成能力。</li>
<li>弱监督问题。问答数据集中往往只存在问题和答案，缺少推理路径，而标注这样的推理路径成本过于高昂。弱监督问题给两种方法都带来了困难。</li>
</ul>
<p>评估指标上，KBQA 往往是从答案集合上选出置信度最高的，常见的评估指标由 F1、准确率、召回率、Hits@1 等。</p>
<h2 id="主流方法">主流方法</h2>
<p>流程图如下图所示。 <img src="methods.png"></p>
<h3 id="基于语义解析的方法">基于语义解析的方法</h3>
<p>旨在将自然语言问题解析为逻辑形式，按照以下步骤：</p>
<ol type="1">
<li>问题编码</li>
<li>逻辑解析</li>
<li>逻辑验证</li>
<li>逻辑执行</li>
</ol>
<p>优点：解释性强</p>
<p>缺点：严重依赖逻辑形式和解析算法的设计，成为性能提升的瓶颈</p>
<h3 id="基于信息检索的方法">基于信息检索的方法</h3>
<p>旨在根据问题检索候选答案集合并对其进行排序，按照以下步骤：</p>
<ol type="1">
<li>确定中心实体，提取问题特定的部分知识子图</li>
<li>问题编码</li>
<li>图推理，沿着相邻实体关系进行语义匹配，传播和聚合信息</li>
<li>按照置信度进行排序</li>
</ol>
<p>优点：端到端训练</p>
<p>缺点：解释性差</p>
<h2 id="挑战与解决方案">挑战与解决方案</h2>
<p>论文总结了两种方法面临的挑战和解决方案，汇总成下面的表格。</p>
<p><img src="summary.png"></p>
<h2 id="总结和展望">总结和展望</h2>
<p>论文主要介绍了两种典型的知识库问答方法，总结了它们面临的挑战及解决方案，并指出复杂 KBQA 未来的可能研究方向：</p>
<ul>
<li>在线学习。已有的复杂 KBQA 系统都是离线学习、在线部署。然而用户的反馈可能是改进 KBQA 的方法。基于此，一些工作开始利用用户反馈去纠正 KBQA 的回答。还有一些工作直接让用户参与到了问题解析过程中。</li>
<li>鲁棒性和可解释性。已有的方法虽然可以在基准数据集上可以取得很好的结果，但是遇到分布之外的情况可能无法很好处理。有研究人员开始提出相关的数据集来解决该问题。</li>
<li>更通用的知识库。知识库往往是不完整的，一些工作开始引入文本、图像来进行更复杂的 KBQA 推理。关于知识库更普遍的定义和更灵活的使用方法能让 KBQA 发展的更好。</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>问答检索</category>
      </categories>
      <tags>
        <tag>问答</tag>
        <tag>知识库</tag>
        <tag>KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>预训练模型综述</title>
    <url>/blog/2022/01/22/PTM-Study/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>《Pre-Trained Models: Past, Present and
Future》是由来自清华、人大、复旦等高校的多位学者合作完成的预训练模型（Pre-Trained
Models,
PTM）综述。顾名思义，该文主要在讨论预训练模型的过去、现状和未来。
<span id="more"></span></p>
<p>深度神经网络（CNN、RNN 等）通过自动化学习特征，解决了传统机器学习面临的复杂特征工程问题。但是由于特征学习过程缺少人为干预，只通过数据学习，严重依赖于有监督数据的质量和数量。而高昂的人工标注费用使得这一问题更为严峻。</p>
<p>迁移学习的思想缓解了这一问题。借鉴于人类可以利用已学到的知识解决新的问题，迁移学习旨在通过预训练 - 微调两阶段的方法，在预训练阶段大规模数据上学习后，只需较少的样本即可在下游任务上微调并取得较好效果。这降低了模型训练和试错的开销。广泛应用于计算机视觉的各项任务：图像分类、目标检测等。</p>
<p>这一想法也被应用于自然语言处理领域。在 GPT 之前，这一思想代表为 Word2Vec、Glove 为代表的静态词向量。通过在连续词袋等任务上的预训练，模型能够学习到有意义的词向量（平移不变性），进而可以应用于各类任务中。然而，受限于静态词向量的表征能力，面临一词多义问题时，这些词向量往往不能表现出很好的效果。虽然也有一些工作试图将每个词的不同含义在空间中区分开，但并没有根本性地解决这个问题。</p>
<p>在 Transformer 提出后的次年，基于该架构的 GPT 和 BERT 预训练模型问世。它们证实了，当预训练模型的规模变得更大，具有数亿个参数时，预训练模型可以捕获多义消歧、词汇和句法结构，在下游任务上表现出出色的性能，取得甚至比人类更优的结果。随着更大算力的投入，近些年来，预训练模型的规模呈几何翻倍式增长。GPT-3 具有数千亿参数，表现出了类似人类的少样本学习能力，如下图所示。</p>
<p><img src="gpt3-fsl.png"></p>
<p>但是，预训练模型的理论尚不成熟，大规模参数的本质难以理解，巨大的计算成本也让人望而却步。预训练模型已经将研究人员推在一个选择的十字路口中，本文就是在这样的背景下，总结预训练模型取得的成果，并讨论其发展和未来。</p>
<h2 id="背景">背景</h2>
<h3 id="迁移学习">迁移学习</h3>
<p>迁移学习的概念可以追溯到 1998 年，迁移学习旨在从多个源任务中获取重要的<strong>知识</strong>，然后将这些知识应用于目标任务。源任务和目标任务的数据格式、任务目标可能不同，但解决任务所需的知识是一致的。因此，NLP 里出现很多不同的预训练方法，本质上是希望能从多个维度去学习无监督语料中的知识。当预训练任务和下游任务关系密切时，模型更有可能取得好效果。</p>
<p>迁移学习包含两种方法：特征迁移和参数迁移。特征迁移方法预训练有效的特征表示以预编码跨领域和任务的知识。通过将这些预训练的表示注入目标任务，可以显着提高目标任务的模型性能。典型的代表就是 NLP 中的静态词向量 Word2Vec。参数迁移方法遵循一个直观的假设，即源任务和目标任务可以共享模型参数或超参数的先验分布。因此，这些方法将知识预编码为共享模型参数。然后将知识通过精细转换使用目标任务的数据调整预训练参数。参数迁移常见于 CNN。ELMO 和 BERT 分别是特征迁移和参数迁移的两种代表。</p>
<p>在迁移学习思想指导下、高质量数据集 ImageNet 的驱动下（覆盖数千个类别的百万张图片）、正则化方法（残差连接）的加持下，在 CV 领域诞生了 ResNet 这样的预训练模型，可用于图像分类、目标检测、图像分割等多项任务。NLP 领域也进行了尝试，典型代表是 CoVE，在 LSTM 上预训练机器翻译任务后，其编码器可以用于下游任务。</p>
<h3 id="自监督学习">自监督学习</h3>
<p>监督学习、无监督学习、子监督学习的关系如下图所示。</p>
<p><img src="methods.png"></p>
<p>自监督学习利用输入数据本身作为监督，从大规模未标记数据中提取知识，这与无监督学习类似。区别在于无监督学习主要侧重于检测数据模式，通过聚类、异常检测等方法，而自监督学习是通过无监督数据构造出了有监督的数据，使用有监督的方法进行训练。近些年来，NLP 里的预训练任务都是自监督学习。在 BERT 的启发下，研究者们设计出了各类的预训练任务。预训练模型成功的关键就是自监督学习和
Transformer。</p>
<h2 id="transformer">Transformer</h2>
<p>在 Transformer 之前，RNN 一直是处理序列任务的标准方法。RNN 顺序读取 token 并更新状态，这使得它可以处理任意长度的序列，但也限制了它的并行能力。Transformer 结构如下图所示。</p>
<p><img src="transformer-gpt-bert.png"></p>
<p>Transformer 是一种基于 Seq2Seq 的架构，由编码器和解码器组成。编码器和解码器都由几个相同的模块堆叠而成。每个模块都包含多头注意力机制、残差连接和层标准化。详细的介绍可以看我的这篇博客 <a href="https://tqnwhz.github.io/blog/2021/08/14/Transformer/">Transformer
| 一隅</a>
。正则化方法使得训练更深的网络成为可能。Transformer 使用的 Attention 包含以下三种：</p>
<ul>
<li>自注意力。用于编码阶段的多头注意力机制，将输入序列中的每个单词和其他单词计算注意力分数，进而得到该单词的向量表征。这也是 Word2Vec 的思想。</li>
<li>掩码自注意力。用于解码阶段生成阶段。Transformer 解码器还是按照自回归的方式进行解码，因此计算注意力的时候需要注意不能泄露数据，只能计算每个单词左侧的注意力，右侧掩码。</li>
<li>交叉注意力。用于解码阶段。Query 为前一个解码器模块的输出，Key 和 Value 为编码网络的输出。类似 Seq2Seq 中的注意力。</li>
</ul>
<p>由于强大的特征抽取能力，Transformer 逐渐成为 NLP 中的标准。GPT、BERT 等 PTM 由此诞生。</p>
<h3 id="gptbert">GPT&amp;BERT</h3>
<p>GPT 使用 Transformer 的解码器来训练语言模型。BERT 使用 Transformer 的编码器训练掩码语言模型，二者的区别如下图所示。GPT 只有掩码自注意力机制，使用自回归的方法建模语言模型，通过极大似然的方法训练，主要用于自然语言生成（NLG）任务。BERT 中只有自注意力机制，使用双向的注意力机制建模掩码语言模型和下句预测任务，主要用于自然语言理解任务（NLU）。掩码语言模型参考了 “完形填空” 的思路，即从句子中随机挡住一个符号（标记为 [MASK]），由模型来根据其他部分预测这个符号，即可认为是这个单词的表示。</p>
<p><img src="gpt-bert.png"></p>
<h3 id="gptbert之后">GPT&amp;BERT 之后</h3>
<p>在 GPT 和 BERT 之后，研究者们提出了一些改进工作，例如 RoBERTa 和 ALBERT。</p>
<p>RoBERTa 是 BERT 的成功变体之一，主要有四个简单有效的变化：（1）去除
NSP 任务； (2) 更多的训练步骤，更大的 batch size 和更多的数据； (3)
更长的训练期限；
(4) 动态改变 [MASK] 模式。这些改进使得 RoBERTa 在一些任务上取得优于 BERT 的效果。</p>
<p>ALBERT 是 BERT
的另一个重要变体，它致力于减少 BERT 的参数。首先，它将输入的词嵌入矩阵分解为两个较小的矩阵。其次，它强制所有 Transformer 层之间的参数共享以显着减少参数。第三，它提出了句子顺序预测（SOP）任务来替代
BERT 的 NSP 任务。作为对其空间效率的牺牲，ALBERT
的微调和推理速度较慢。如图 9 所示，除了 RoBERTa 和
ALBERT，近年来还提出了各种
PTM，以更好地从未标记的数据中捕获知识。一些工作改进了模型架构并探索了新的预训练任务，例如
XLNet、UniLM，MASS 等。此外，整合丰富的数据源也是一个重要的方向，例如利用多语言语料库、知识图谱和图像。还有一些工作致力于则增大模型规模，例如 GPT-3。</p>
<p>参考下面这张预训练模型全家福。</p>
<p><img src="ptm-family.png"></p>
<h2 id="架构设计">架构设计</h2>
<p>通常，所有用于语言预训练的 BERT 之后的 Transformer
架构都可以根据两个动机进行分类：统一序列建模和认知启发式架构。</p>
<h3 id="统一序列建模">统一序列建模</h3>
<p>NLP 具有挑战性的原因之一，就是因为它有很多下游任务，通常可以分为三类：</p>
<ul>
<li>自然语言理解（NLU）：单词 / 段落 / 句子分类、语法 / 句法分析、知识推理等</li>
<li>开放域自然语言生成：对话生成、文本生成、故事生成等</li>
<li>非开放域自然语言生成：机器翻译、摘要等</li>
</ul>
<p>这些任务虽然多样，但所需的能力却是通用的，无外乎语言理解和语言生成能力。正如费曼所说，我不能创造的，我也不理解（What
I cannot create, I do not
understand）。语言理解任务可以转化为生成任务。因此 GPT 此类生成模型也可用于理解任务，甚至一些研究表明，与
BERT 相比，GPT 在理解基准方面可以达到相似甚至更好的性能（Liu
等人，2021b）。因此研究者们开始寻求一种统一的方式，建模所有的任务。</p>
<h4 id="结合自回归和自编码">结合自回归和自编码</h4>
<p>XLNet 首先将 GPT 式的单向生成和 BERT
式的双向理解统一起来，结合自回归和自编码的思想
，提出了置换语言模型的预训练任务。XLNet 通过在预训练中排列 token 的顺序，然后应用自回归预测范式来解决这个问题，这赋予了 XLNet 理解和生成的能力。UniLM 提出联合训练不同的语言建模目标，包括单向、双向和
seq2seq 目标。这可以通过更改 Transformers 中的注意力掩码来实现。 UniLM
在生成式问答和抽象摘要方面表现出色。</p>
<p>最近，GLM (Du et al., 2021)
提出了一种更优雅的方法来结合自回归和自编码。给定一个可变长度的掩码跨度，而不是像
BERT
一样，一个 token 对应一个 [MASK] 标记。GLM 会自回归地生成 [MASK] 对应的 token。GLM
是第一个在包括自然语言理解、条件生成和无条件生成在内的所有类型任务上同时实现最佳性能的模型。</p>
<h4 id="应用广义的编码-解码器">应用广义的编码 - 解码器</h4>
<p>在 GLM 之前，无论是编码器架构的模型（例如 BERT）还是解码器架构的模型（例如 GPT），都无法解决在句子的多个空白处添加任意个符号的问题。BERT 只能将 MASK 替换为一个单词，而 GPT 只能在句子末尾添加任意个单词。因此，很自然的想法是转向类似机器翻译 Transformer 的 encoder-decoder 架构。</p>
<p>这一类型的先驱是 MASS（Song et al.,
2019），它将掩码预测策略引入到编码器 - 解码器结构中。然而，MASS
并没有涉及填充可变长度空白的问题。 T5 (Raffel et al., 2020)
通过仅用一个掩码标记掩码文本中可变长度的跨度来解决该问题，并要求解码器恢复整个掩码序列。BART
(Lewis et al., 2020a)
引入了一个有趣的想法，即通过截断、删除、替换、改组和掩码等多种操作来破坏源序列，而不仅仅是掩码。然而，encoder-decoder 架构的预训练模型存在以下问题：</p>
<ol type="1">
<li>参数更多、参数效率低下</li>
<li>在 NLU 方面表现不佳</li>
</ol>
<h3 id="认知启发式架构">认知启发式架构</h3>
<p>虽然 Transformer 很强大，但与人类的认知系统还是有较大差距。注意力机制借鉴自人类的感知功能，然而人类还具有决策、逻辑推断、工作记忆等功能。</p>
<h4 id="可维护的工作记忆">可维护的工作记忆</h4>
<p>Transformer 的一个问题在于固定的序列长度和<span class="math inline"> \(O(n^2)\)</span> 的复杂度，即每个 token 都要与其他 token 计算注意力。这严重阻碍了它在长文档理解中的生成和应用。然而，人类也不具备很好的长程注意力机制。认知科学家们发现人类可以保持一种工作记忆，这种记忆不仅可以组织和记忆，还可以忘却。传统的 LSTM 正是这种思想的实践。</p>
<p>基于 Transformer 的架构中，Transformer-XL 是第一个引入段级递归和相对位置编码来实现这一目标的。它将源序列分割为若干个段，上一个段的状态会被缓存下来，在当前片段计算注意力时使用（但不更新梯度），然后缓存新的状态，重复此过程，也就是段级递归名称的来源。这个过程隐含着工作记忆的思想。CogQA 提出在多跳阅读中保持认知图。它通过 PTM（BERT）和 GNN 两个系统建模认知图。先通过 BERT 提取相关实体，再通过 GNN 构造认知图谱，进行推理和计算。CogQA 的一个局限是 PTM 仍使用了固定的窗口大小。CogLTX 将长文本切分为若干个 block，利用记忆回想模块给 block 打分，选择相关性更高的 block 进行使用。</p>
<h4 id="可持续的长期记忆">可持续的长期记忆</h4>
<p>GPT-3 的成功揭示了 Transformer 具有记忆功能。在此之前， Lample et al.
(2019) 等人发现，将 Transformer 的前馈神经网络替换为大型键值记忆网络，仍可以工作的很好。这在某种程度上表明 Transformer 中的前馈神经网络等价于记忆网络。然而，其记忆内存容量非常有限。REALM
(Guu et
al.,2020) 探索了如何为 Transformer 构建外部存储的先驱。它通过将整个维基百科的文本向量化，使用掩码语言模型预训练知识检索器。在开放域问答上取得了 SOTA 效果。</p>
<p>除了张量文本语料库外，(Vergaet al., 2020; Févry et al., 2020)
提出将知识库中的实体和关系向量化，并将上下文中 token
embedding 替换为对应的 entity embedding。(Dhingra et al., 2020; Sun et
al.,
2021) 从零开始维护一个虚拟知识，并提出了一个可微分的训练目标。所有这些方法在很多开放域问答基准上取得了一些改进。</p>
<h4 id="其他变种">其他变种</h4>
<p>此外，还有一些工作致力于修改 BERT 的结构 / 预训练目标，达到更好的 NLU 能力。Span-BERT 证实了使用跨边界目标
(SBO) 掩盖连续随机长度的 token 可以提高 BERT
的性能。ELECTRA 将掩码语言模型替换为替换符号检测任务，生成器会替换原始序列中的 token，而判别器检测 token 是否被替换。</p>
<h2 id="使用多源数据">使用多源数据</h2>
<p>一些预训练模型使用多源异构数据，例如多模态、多语言的 PTM、知识增强的 PTM。</p>
<h3 id="多语言预训练">多语言预训练</h3>
<p>在单语言语料库（如英语）上预训练的模型在许多基准测试中取得了巨大成功。但是我们生活的世界是多语言的，为每种语言都训练和维护一个单独的模型并不是一个合理的方案，尤其是涉及到机器翻译的场景。事实上，虽然人们使用的语言不尽相同，但是他们可以表达相同的意思。这表明语义是独立于语言的。一些研究人员发现，使用多语言训练模型，效果要优于几种单语言模型。因此，相较于训练很多个单语言模型，训练多语言模型可能是个更好的方法。</p>
<p>在 BERT 之前，已经有研究者探索多语言表征，主要有两种方法。第一种是参数共享，例如使用多种语言对训练多语言 LSTM，实现多语言翻译。另一种是学习与语言无关的约束，例如使用 WGAN 框架将语言表示解耦为与语言无关的表示。这两种方式都能应用于多语言场景，但仅限特定的任务。换而言之，上述两种方法都是用同一个特定的任务训练的，不能推广到其他任务。</p>
<p>BERT 的出现表明，使用自监督的方法进行预训练，对特定任务进行微调的方法是可行的。这促使研究人员设计任务预训练通用的多语言模型。多语言任务同样可以分为 NLU 和 NLG 两类。</p>
<p>一些 NLU 任务首先在非并行多语言语料上训练多语言的 PTM。例如，Devlin
等人提出的多语言的 BERT（mBERT），使用维基百科上 104 种语言的非并行语料库建模多语言掩码语言模型（MMLM）任务。（吐槽，维基百科不同语言的内容还是有较大差别的）。研究表明，mBERT 具有在零样本场景中泛化跨语言知识的能力。这表明即使使用相同的
BERT
结构，使用多语言数据也可以使模型学习跨语言表示。XLM-R 构建了一个名为 CC-100 的非并行多语言数据集，规模远大于 mBERT 使用的维基百科语料，尤其是对于那些语料相对匮乏的语言。XLM-R 在 CC-100 上进行预训练，在多项基准测试中获得优于 mBERT 的性能，这表明更大规模的多语言语料库可以带来更好的性能。</p>
<p>然而，多语言掩码语言模型无法很好的利用并行语料。而并行语料对于一些 NLP 任务，例如机器翻译来说是至关重要的。并且直觉来说，并行语料能够让模型更快更好地学习到意义相同的跨语言表征。从这一点出发，XLM 使用双语句子执行翻译语言模型（TLM）的任务。具体做法是将双语语料拼接成一个句子，在两个部分中分别随机掩码。与 MLM 相比，TLM 需要模型从不同语料中获取和对齐语义信息，并进行预测。</p>
<p>除了 TLM，还有一些其他的方法从并行语料中学习跨语言表征。Unicoder
提供了两个基于平行语料的新预训练任务：跨语言单词恢复（CLWR）和跨语言释义分类（CLPC）。CLWR 使用目标语言 embedding 和注意力机制恢复源语言 embedding，类似机器翻译。CLPC 将对齐的语料拼接作为正样本，未对齐的作为负样本，进行句子级别的分类。ALM
(Yang et al., 2020) 自动从并行句子生成代码转换序列并对其执行
MLM，这迫使模型仅基于其他语言的上下文进行预测。InfoXLM (Chi et al.,
2020b) 从信息论的角度分析了 MMLM 和
TLM，鼓励模型在对比学习的框架下区分对齐的句子对和未对齐的负例。HICTL
(Wei et al., 2021)
扩展了使用对比学习来学习句子级和单词级跨语言表示的想法。 ERNIE-M (Ouyang
et al., 2020)
提出了反向翻译掩码语言建模（BTMLM），并通过反向翻译机制扩大了并行语料库的规模。反向翻译是一种数据增强的方法，例如将语言 X 翻译为语言 Y 的任务，可以将语言 Y 翻译回语言 X‘，然后比较 X 与 X’是否相同，若不同的话可以将 X' 也加入数据集。
这些工作表明，利用平行语料库可以为学习跨语言表示带来很大帮助。</p>
<h3 id="多模态预训练">多模态预训练</h3>
<p>人类所面临的世界是多模态的，包含视觉、听觉、语言等多种模态。模态指的是事情是如何发生和经历的。近年来，研究者们对多模态研究热情高涨，这些跨模态的工作大部分都归类于视觉和语言（V&amp;L）的交叉，例如视频和文本、图像和文本的交叉。V&amp;L 预训练的工作主要集中在改进模型架构、利用更多数据以及设计更好的预训练任务上。</p>
<p>对于基于图像文本的 PTM，目前大多数工作都是基于视觉语言 BERT
的架构。主要挑战在于统一语义空间中视觉和文本内容的对齐（即 V&amp;L
基础）。为此，主要有两种模型架构设计：双流和单流。双流模型，例如 ViLBERT，使用两个独立的流处理图像和文本，并将它们通过 Transformer 注意力模块融合。单流模型，例如 Visu-alBERT，图像区域特征和词嵌入通常被拼接送入单个 Transformer 中。考虑到简单性和效率，目前工作主要使用单流模型。</p>
<p>在预训练任务的选择上，V&amp;L 的理解任务广泛使用 MLM、句子 - 图像对齐（SIA）、遮挡区域分类（MRC），遮挡区域特征回归（MRFR）和直接合并下游任务。其中，MLM
旨在借助视觉和文本上下文恢复字幕中的掩码标记。 SIA
旨在判断图像 - 文本对是否匹配。
MRC 可以被认为是视觉 MLM，需要 V&amp;L 模型来预测被掩蔽对象的类别。MRFR 进一步需要 V&amp;L 模型来恢复被掩蔽对象区域的视觉特征。
也有模型在预训练阶段直接进行下游 V&amp;L 理解任务。</p>
<p>上述提到的预训练任务专用于 V&amp;L 理解或者字幕生成，不能用于图像生成任务。最近提出的 DALLE 是第一个基于 Transformer 的文本到图像的 PTM，可用于条件图像生成，它显示了多模态 PTM 在联系文本描述和图像生成之间的潜力，尤其是组合不同对象的出色能力。</p>
<p>除了图像 - 文本 PTM，还有其他形式的 PTM，例如视频和音频。 VideoBERT
(Sun et al., 2019a) 对 Cooking312K 视频数据集 (Sun et al.,2019a)
进行预训练，并在零镜头动作分类任务和视频字幕任务上验证模型。SpeechBERT
(Chuang et al.,
2019）首先将连续音频信号编码成几个语音语义词嵌入，然后使用 MLMon
文本和音频模态作为预训练任务。
预训练后，使用口语问答（SQA）任务进行评估。</p>
<h3 id="知识增强预训练">知识增强预训练</h3>
<p>PTM 可以从大量数据中获取统计信息，而外部知识是统计建模的优秀先验。外部知识可以分为结构化知识（如知识图谱）和非结构化知识（维基百科文本）。一些工作试图通过整合实体和关系嵌入来增强 PTM，或者是它们与文本的对齐方式。Wang 等人（2021b）基于维基数据实体的描述预训练模型，通过将语言模型损失和知识嵌入损失结合在一起以获得知识增强表示。</p>
<h2 id="提高计算效率">提高计算效率</h2>
<p>PTM 的趋势是模型越来越大，因此提升计算效率以满足日益增加的内存与计算需求非常关键，可以分为以下三种方法。</p>
<h3 id="系统级优化">系统级优化</h3>
<p>通常与具体模型无关，可以分为单设备优化和多设备优化。</p>
<p><strong>单设备优化</strong>，一个典型的例子是浮点数精度优化。现代深度学习系统主要基于单精度浮点数（FP32），然而权重往往落在一个有限的区间里，
可以考虑使用半精度格式（FP16）完成大部分计算，而几乎没有精度损失。但是在某些情况下，也可能会出现浮点截断和溢出，为解决这个问题，研究者们提出了混合精度训练，它在
FP32
中保留一些临界权重以避免浮点溢出，并使用动态损失缩放操作来摆脱浮点截断。充分的实验表明，混合精度训练方法比
FP16
中直接训练模型更稳定。尽管混合精度训练方法可以显着减少训练时间和内存使用量，但它们仍然面临一些挑战。当模型参数没有很好地初始化时，混合精度方法仍然可能导致训练不稳定。这些挑战仍有待进一步探索。此外，还可以通过舍弃 Transformer 中的部分隐藏状态、利用 CPU 存储模型参数再通过精细的策略完成 CPU 和 GPU 内存交换，来降低模型内存开销。</p>
<p><strong>多设备优化</strong>。预训练模型往往使用分布式的方法进行训练，使用多个节点中的多个 GPU 来加速计算，并行方法可以分为数据并行、模型并行。</p>
<p><strong>数据并行</strong>是一种简单有效的加速模型的方法，如下图所示。使用数据并行时，大 Batch 数据被划分到不同的节点，可以并行化前向传播。
在反向传播时，不同节点上的梯度应该通过 all-reduce
操作进行聚合，以保证参数优化的一致性，这可能会引入额外的通信开销。容易看出，这相当于每个 GPU 上都保存了一份模型参数。</p>
<p>当单个模型的参数达到十亿或更多时，模型参数无法容纳在同一个 GPU 上（即使是半精度或者混合精度训练），这使得数据并行无法进行。<strong>模型并行</strong>则可以解决这个问题，通过将矩阵运算分块，分布在不同的 GPU 上，再通过节点间的通信操作保证前向 / 反向传播的正确性。但是，模型并行需要在前向 / 反向传播过程中插入通信操作，无法与计算重叠。对比之下数据并行的 all-reduce 操作通常可以被反向计算重叠。因此，数据并行是首选，只要它可以克服内存容量的过度需求。</p>
<p>下面这张图和模型并行和数据并行的示例。</p>
<p><img src="data-parallel.png"></p>
<p>模型并行还存在另一种流水线并行的方法。将模型划分为很多层，不同层分布在不同的节点，前一层的输出作为后一层的输入。流水线并行只需要在执行管道相邻阶段的节点之间传递中间激活状态，通信成本较小。但是，流水线并行以一个 batch 的前向和反向传播为完整周期，会有流水线气泡产生。</p>
<h3 id="高效预训练">高效预训练</h3>
<p>除了一些系统级的优化方法外，研究人员还致力于探索更有效的预训练方法，以便能够以较低成本的解决方案对大规模
PTM 进行预训练。</p>
<p><strong>高效的训练方法。</strong>传统的预训练任务可能样本效率低下。以 MLM 为例，需要模型根据上下文来预测掩码标记。掩码标记通常是输入标记的子集（通常为
15%），即模型只能从一小组输入标记中学习。为了解决这个问题，ELECTRA
(Clarket al., 2020)
提出了替换令牌检测任务。此任务强制模型区分输入标记是否被生成器替换。此任务可以利用来自每个样本的更多监督信息，因为需要区分所有输入标记。实验证明，ELECTRA
仅需少得多的预训练步骤，就可以达到与 MLM 相似的性能。另外，传统 MLM 随机掩盖文档中的标记以进行预测。由于预测不同标记的难度差异很大，随机掩码策略使训练过程变得漫无目的且效率低下。一些工作根据 token 的重要性或者梯度，加速模型训练。</p>
<p>除了预训练任务外，当前的预训练动态也是次优的。最近的大规模 PTM 都要求大的 batch
size，因为研究指出这有利于模型收敛。但在一项早期工作中（Goyal 等人，2017
年），研究人员发现简单地增加 batch
size 可能会导致<strong>优化困难</strong>。因此，他们提出了一种预热策略（即 warm
up），在训练开始时线性增加学习率。这种策略通常用于最近的大规模
PTM。此外，研究者发现在 Transformer 不同层间自适应地使用不同的学习率也可以在 batch
size 较大时加快收敛速度。</p>
<p><strong>高效的模型架构。</strong>除了高效的预训练方法，更多的模型架构变体也可以降低计算复杂度，提高训练效率。正如之前提到的，基于 Transformer 的 PTM 面临长输入序列时会存在<span class="math inline"> \(O(n^2)\)</span> 序列长度复杂度的问题。一些工作致力于降低 Transformer 的复杂度。</p>
<h3 id="模型压缩">模型压缩</h3>
<p>另一个提高 PTM
效率的重要方法是模型压缩。通过将大型模型压缩为小型模型，以满足资源受限设备上更快推理和部署的需求。</p>
<p><strong>参数共享。</strong>PTM
可以通过在相似单元之间共享参数进行压缩。AL-BERT (Lan et al., 2019)
使用分解嵌入参数和跨层参数共享来减少 PTM 的参数，在所有 Transformer
层上使用相同的权重。ALBERT 在 BERT
模型的基础上实现了显着的参数减少，同时具有相同甚至更好的性能。这表明 PTM
可能极度过度参数化。</p>
<p><strong>模型剪枝。</strong>为了更好地利用当前 PTM
的过度参数化特性，另一种减少模型参数的方法是模型剪枝，它在 PTM
中剪掉一些无用的部分，以在保持性能的同时实现加速。研究人员研究了
Transformers
中注意力头的冗余，发现只有一小部分就足以获得良好的性能。这些头中的大部分都可以移除，而对准确性的影响很小。</p>
<p><strong>知识蒸馏。</strong>虽然 ALBERT 减少了 PTM 的大小，但并没有减少推理时间，因为模型计算复杂度并没有减小。知识蒸馏旨在训练一个小模型以复现大模型的行为。有一些典型的工作将知识蒸馏用于
PTM，例如 DistillBERT (Sanhet al., 2019)、TinyBERT (Jiao et al.,
2019)、BERT-PKD (Sun et al., 2019b) 和 MiniLM (Wang et al., .,2020d)。
但是，知识蒸馏方法需要用于预训练教师模型的数据，考虑到数据版权和隐私，这些数据通常不会发布。而且，教师模型需要对整个预训练数据进行转发，以产生对数或中间表示进行知识蒸馏，导致训练时间更长。</p>
<p><strong>模型量化。</strong>模型量化是指将高精度浮点参数压缩为低精度浮点参数。
模型量化这个词听上去不是很好理解，更像是一种参数压缩方法。 传统的 PTM
通常用 32 位或 16 位浮点数表示参数，而量化后的模型可以用 8 位甚至 1 或 2
位表示。一种量化方法可以使用 k-means 对参数进行聚类，让相近的值落在同一个聚类中心，进而复用同一个值。对于最近的基于
Transformer 的模型，8
位量化已在 Q8BERT 中被证明是有效的，对模型性能的影响很小。为了减轻性能下降，也可以采用其他保持精度的方法。
Q-BERT (Shen et al.,2020a) 使用混合比特量化，其中 Hessian
谱较高的参数需要更高的精度，而 Hessian 谱较低的参数需要较低的精度。</p>
<h2 id="可解释性理论">可解释性 &amp; 理论</h2>
<p>鉴于 PTM 在多项任务上取得的卓越性能，研究者试图解释 PTM 的行为，包括其如何工作和捕获到了怎样的模式。这些工作涵盖了
PTM 的几个重要属性：知识、鲁棒性和结构稀疏性 / 模块化。 此外，在构建 PTM
的理论分析方面也有一些开创性的工作。</p>
<h3 id="知识">知识</h3>
<p>PTM 捕获的隐性知识大致可以分为两大类：语言知识和世界知识。</p>
<p><strong>语言知识</strong>包含了句子的语法、语义、词义等信息。与传统的神经模型如
CNN 和 RNN 相比，大规模 PTM
可以从海量的预训练数据中学习到丰富的语言知识。为了研究 PTM
的语言知识，研究人员设计了几种方法：表征分类（利用隐藏状态对句子 / 单词进行分类）、表征分析（利用隐藏状态计算统计信息，例如相似度、距离）、注意力分析（利用注意力矩阵，发现文本的层次结构）、生成分析（语言模型计算概率）。</p>
<p><strong>世界知识</strong>主要包括常识知识和事实知识。Davison
等人提出将关系三元组转化为掩码句子，根据 PTM 给出的互信息对句子进行排序，证明在其表示空间中学习了各种常识特征。Petroni
等人（2019）提出将关系知识生成表述为填空语句的完成。根据实验结果，他们发现在没有任何微调的情况下，PTM
在这项任务上明显优于以前的监督基线，证实 PTM 学习到了事实知识。但是，这些填空语句的构造并非易事。</p>
<h3 id="鲁棒性">鲁棒性</h3>
<p>近期工作通过使用对抗性样本证明 PTM 存在严重的鲁棒性问题。对抗性攻击旨在通过对原始输入的小扰动来生成被模型错误分类的新样本。
例如，PTM
很容易被同义词替换所愚弄。事实上，这个问题在 word2vec 时代就存在了。由于同义词、反义词所在上下文相似，它们的表征也近似。同时，不相关的格式词也会误导 PTM 做出错误的预测。但是高质量对抗样本的获取也面临挑战，目前的工作主要利用模型的预测概率和模型梯度来搜索对抗性样本。最近，人在回路（Human-in-the-loop）方法（Wallace
等人，2019b；Nie
等人，2020）已被应用于生成更自然、有效和多样化的对抗样本，这带来了更大的挑战和经验。总而言之，当人们为实际应用部署
PTM 时，PTM 的鲁棒性已成为严重的安全威胁。</p>
<h3 id="结构稀疏性">结构稀疏性</h3>
<p>正如前文提到的，Transformer
具有过度参数化的问题。研究人员表明，多头注意力结构在机器翻译 (Michel et
al., 2019)、抽象摘要 (Baan et al., 2019) 和语言理解 (Kovaleva et al.,
2019)
的任务中是多余的，即当去除部分注意力头，可以获得更好的性能。这种现象与
(Clark et al., 2019)
中的观察结果一致，他们发现同一层中的大多数头部具有相似的自我注意模式。他们的研究结果表明，不同头部的注意力行为可以归类为一组有限的模式。除了多头注意力之外，其他几项工作也在探索识别参数的稀疏性。
Gordon 等人 (2020) 表明，低水平的剪枝 (30-40%)
根本不会影响预训练损失或下游任务的性能。</p>
<h3 id="ptm理论">PTM 理论</h3>
<p>由于预训练在深度学习方面取得了巨大成功，研究人员试图研究预训练的工作原理，尤其是无监督预训练。在深度学习的早期，人们发现通过贪婪的逐层无监督预训练和监督微调来训练深度贝叶斯网络是有效的（Hin-ton
et al.,
2006）。最近，基于包括语言建模在内的对比学习的预训练已经成为主流方法。</p>
<p>Erhan et al.
(2010) 提出了两个假设来解释预训练的效果：（1）更好的优化和（2）更好的正则化。在更好的优化方面，与随机初始化的模型相比，预训练的网络更接近全局最小值。在更好的正则化方面，PTM 的训练误差不一定比随机模型好，而 PTM 的测试误差更好，这意味着更好的泛化能力。</p>
<p>对于预训练目标的最新发展。Saunshi, et
al（2019）对对比无监督表示学习进行了理论分析。对比学习将出现在相同上下文中的文本 / 图像对视为语义相似对，将随机采样的对视为语义不相似对。然后，相似对之间的距离应该很近，不同点之间的距离应该很远。在语言建模的预测过程中，上下文和目标词是相似对，其他词是负样本（Kong
et al., 2020）。Saunshi
等人（2019）首先提供了一个新的概念框架来弥合预训练和微调之间的差距。具体来说，他们引入了潜在类的概念，语义相似的对来自同一个潜在类。例如，潜在类可以是 “快乐” 以包括所有文本，包括快乐的情绪。潜在类涵盖所有可能的类，下游任务定义的类来自潜在类集合。然后，他们证明了对比学习的损失是下游损失的上限。因此，在优化预训练损失时，我们可以预期下游任务的损失会更低。</p>
<h2 id="未来发展方向">未来发展方向</h2>
<h3 id="架构和预训练方法">架构和预训练方法</h3>
<p>值得探索的问题有：</p>
<ul>
<li><strong>新架构。</strong>Transformer 饱受诟病的计算复杂度，需要更有效的模型捕获更长范围的依赖信息。另外，也需要根据下游任务设计特定架构，例如 NLU 使用 Transformer
Encoder，NLG 使用 Transformer Decoder。</li>
<li><strong>新的预训练任务。</strong>如何设计有效、高效的自监督任务，类似 ELECTRA。</li>
<li>不止微调。微调是将 PTM
的知识转移到下游任务的主要方法，但一个缺点是其参数效率低下：每个下游任务都有自己的微调参数。NLU 最近盛行的 Prompt 就是对微调的改进。</li>
<li><strong>可靠性。</strong>提高 PTM 的鲁棒性，免受对抗攻击。</li>
</ul>
<h3 id="多语言多模态预训练">多语言、多模态预训练</h3>
<ul>
<li>更多的模态。除了图像和文本，还可以利用视频和音频进行多模态预训练。主要挑战在于如何对这两种模态中涉及的时间上下文进行建模。</li>
<li><strong>更深刻的解释。</strong>将视觉和语言联系起来的原因仍然没有定论，只是一些经验性的感觉，没有脑科学或者深度学习理论的支撑。另外，多模态训练是否会对单模态造成损失，如何克服？这些都是悬而未决的问题。</li>
<li><strong>更多的下游任务。</strong>虽然多模态预训练可以应用于图文检索、图文生成、图文生成等下游任务。
然而，为多模态预训练找到一个 “真正的” 真实世界应用场景仍然具有挑战性。</li>
<li><strong>迁移学习。</strong>多语言模型应当灵活适配新的语言。另外，目前的多模态多语言模型无法处理音频数据，不同语言的音频需要转换为文本再翻译。</li>
</ul>
<h3 id="计算复杂度">计算复杂度</h3>
<p>大规模深度学习模型的新需求给现有的深度学习框架带来了严峻的挑战。为了开发更有效的框架，可以探索以下方向：</p>
<ul>
<li><strong>数据转移。</strong>设计精细的、定义良好的数据调度和计算策略，最小化通信成本、最大化计算和内存资源以及优化计算 - 通信重叠。</li>
<li>并行策略。从数据并行、模型并行、流水线并行以及各种混合并行方法可以根据神经网络的结构和硬件配置找到它们的最佳使用方式。在当前实践中，用户必须全面考虑给定深度学习模型的网络结构和设备间通信带宽，以决定最合适的并行策略或在不同策略之间切换（Shazeer
等，2018）。</li>
<li><strong>大规模预训练。</strong>鉴于现有深度学习框架对模型并行和流水线并行的支持不佳，一些新兴的开源项目开发了用于大规模训练的专用框架。由于应用案例优先以及存在的兼容性问题，这些方法无法共同构成完整的解决方案。</li>
<li><strong>包装器和插件</strong>。由框架提供插件或者包装器自动管理通信操作，避免用户手动编程通信的复杂过程。</li>
</ul>
<h3 id="理论基础">理论基础</h3>
<p>目前的 PTM 理论存在以下问题：</p>
<ul>
<li><strong>不确定性。</strong>PTM（以及其他深度神经网络）的一个未得到解决的问题是它们通常对预测过于自信，即这些模型不知道他们不知道什么。你问 GPT-3
“我的脚有几只眼睛”，GPT-3
肯定会给出 “你的脚有两只眼睛” 这样的答案，这看起来违反直觉。在机器学习中处理这种分布外（OOD）数据通常是一项具有挑战性的任务。为了应对上述挑战，一个有希望的方向是采用贝叶斯方法，探索概率工具来捕获数据和模型的不确定性（也分别称为任意不确定性和认知不确定性）。当然，提高贝叶斯深度学习的计算效率是解决上述挑战的关键因素。</li>
<li><strong>泛化和鲁棒性。</strong>PTM
的另一个重要问题是泛化。从理论上理解预训练在提高下游任务泛化方面的作用很重要。有没有有效的方法来探索
PTM 作为额外的数据资源来提高下游任务的鲁棒性？此外，如前所述，PTM
本身的鲁棒性是一个未解决的问题。</li>
</ul>
<h3 id="模型边缘学习">模型边缘学习</h3>
<p>模型边缘是指存储在模型中的知识。给定三元组 &lt;h,r,t&gt;，我们很容易知道头部实体 h 和尾部实体具有关系 r，但是 PTM 中的表征的意义却不明晰。越来越多的研究人员探索了
PTM
从数据中学到了哪些知识，以及为什么它们在下游任务中表现如此出色？这些模型边缘如何存储和管理？是否有可能建立一个通用连续知识库（UCKB）来存储来自各种
PTM 的模型边缘？这些都是有希望的研究方向。</p>
<h3 id="认知和知识学习">认知和知识学习</h3>
<p>让 PTM 更有知识是 PTM 未来的一个重要主题。可以将知识型 PTM
的未来发展分为以下三种方法：</p>
<ul>
<li><strong>知识增强。</strong>对于输入文本和外部知识，关键的问题是弥合文本表示和知识表示（包括符号或向量）之间的差距，并统一使用它们的信息作为输入。这个问题的解决需要统一的模型架构和知识引导的预训练目标。</li>
<li><strong>知识支持。</strong>根据输入的先验知识，设计不同的模块处理不同类型的输入，类似人脑不同区域对应不同的活动功能，加快训练和推理进程。</li>
<li><strong>知识监督。</strong>通过从知识库和大规模语料库中学习，与仅使用纯文本相比，PTM
可以具有更好的语言理解和生成能力。通过改进认知架构、明确推理、知识交互这三个方向，未来的
PTM 有望于能够轻松理解文字之外的含义。</li>
</ul>
<h3 id="应用">应用</h3>
<p>在具体应用中，PTM 还存在一些问题：</p>
<ul>
<li><strong>对话系统。</strong>虽然基于 Transformer 的开放域对话系统显示出优秀的与人对话的能力，但是在对话领域，缺少特定的预训练任务。</li>
<li><strong>特定领域的 PTM。</strong>当大规模的特定领域语料库可以廉价获得时，可以在这些数据上训练特定领域的
PTM。这种领域专业知识通常被认为对于解决许多特定领域的问题很重要。</li>
<li><strong>领域适应和任务适应。</strong>大规模 PTM
的简单微调对于特定领域的应用是不够的（Gururangan 等人，2020；Ke
等人，2020）。最根本的原因是分布变化：特定域中的数据分布可能与一般预训练文本中的<strong>数据分布</strong>有很大不同。如何弥合预训练和特定任务微调之间的差距变得至关重要。</li>
</ul>
<h2 id="总结">总结</h2>
<p>这篇综述回顾了预训练模型的发展历史、分析了其核心问题并指明了一些改进的方向。这篇论文中的一些工作我也没有接触过，像多模态、Transformer-XL 等。因此读起来也有一些一知半解。建议有余力的读者去看原文。毕竟综述类文章本身就是知识的压缩，很难在博客中再进行压缩了。这也是我为什么这篇博客完全按照原论文格式排版。。。</p>
<p>最后，新年快乐！</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://www.jiqizhixin.com/articles/2021-09-07-3">国内数十位 NLP 大佬合作，综述预训练模型的过去、现在与未来
| 机器之心 (jiqizhixin.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/84159401">Transformer-XL 介绍 -
知乎 (zhihu.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/132561405#:~:text=模型量化在最初的定义里是为了压缩模型参数，比如韩松在ICLR2016上获得best%20paper的论文，首次提出了参数量化方法。.,其使用k-mean聚类，让相近的数值聚类到同一个聚类中心，复用同一个数值，从而达到用更少的数值表示更多的数，这是量化操作的一种方案。.%20反过来，从量化数变到原始数的过程，称之为反量化，反量化操作完之后，模型就可以按照原来的方式进行正常的计算。.%20我们认为绝大部分的模型量化算法都能压缩参数，因此压缩参数的实用性不存在问题。.">模型量化了解一下？
- 知乎 (zhihu.com)</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2016-10-18-7">人类智慧与机器学习结合──微软的「Human-in-the-Loop」
| 机器之心 (jiqizhixin.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>综述</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>预训练模型</tag>
        <tag>BART</tag>
        <tag>自然语言处理</tag>
        <tag>综述</tag>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>PLATO-XL</title>
    <url>/blog/2022/02/19/PLATO-XL/</url>
    <content><![CDATA[<h2 id="前言">前言</h2>
<p>PLATO-XL 是百度于 2021 年发布的论文《PLATO-XL: Exploring the Large-scale
Pre-training of Dialogue
Generation》中提出的模型，旨在探索大规模预训练对话生成任务的效果，在中英文的多项对话任务上取得了 SOTA。目前，百度提供了<code>百度PLATO</code> 微信公众号服务，可供试用。经笔者测试，PLATO 的效果要远优于微软小冰。因此今天来读下这篇论文。</p>
<span id="more"></span>
<p>下图是微信公众号二维码，大家可以自行体验。</p>
<p><img src="QR.png"></p>
<h2 id="简介">简介</h2>
<p>近些年来，预训练模型的共识是更大的模型 + 更好的数据 = 更好的效果。但是，在对话预训练领域，DialoGPT 的 345M 版本要优于 762M，Blender 的 2.7B 版本要优于 9.4B 的版本，这些反常的现象让人怀疑模型的规模和生成效果是否有清晰的结论关系。本论文指出，对话质量可能还是受益于模型规模，前提是合适的预训练设计。</p>
<p>PLATO-XL 使用 unified
Transformer 的架构进行训以提高参数效率，并实行了” 多角色意识预训练 “用于区分人物信息。据论文所述，多角色意识预训练能够显著减少多轮对话中的不一致性。论文的英文模型和推理脚本开源在了 <a href="https://github.com/PaddlePaddle/Knover/tree/develop/projects/PLATO-XL">github</a> 上，没有开源训练脚本、中文模型等。这很百度。</p>
<p>实验结果表明，PLATO-XL 不仅在开放域闲聊任务取得了 SOTA，也可用于微调后应用于任务型对话、知识增强对话场景，同样取得了 SOTA 效果。</p>
<h2 id="plato-xl">PLATO-XL</h2>
<h3 id="网络结构">网络结构</h3>
<p>PLATO-XL 的架构如下图所示，使用 Unified
Transformer 的 Seq2Seq 的训练方法。将输入和输出以 [SEP] 间隔，输入内部计算双向 self-attention，输入 - 输出间存在 cross-attention，输出间为单侧的 mask-attention。这种做法的优点是参数利用率高，同一套参数即用来编码又用来解码，得到的模型的泛用性也强。</p>
<p><img src="architecture.png"></p>
<h3 id="多角色意识预训练">多角色意识预训练</h3>
<p>这个名字听着很玄乎，其实思想很简单。从社交媒体中搜集的数据一般如下图所示。多个用户的连续回帖构成了多轮对话。但是由于每个人的性格、观念等的不同，直接拿去训练多轮对话容易产生不一致性。因此 PLATO 引入了角色嵌入（role
embedding）来解决这个问题，将角色嵌入和句子向量相加即可。如上面的图所示。</p>
<p><img src="multiparty.png"></p>
<h3 id="预训练设置">预训练设置</h3>
<p>数据集：</p>
<ul>
<li>英文：来自 Reddit，由第三方收集公开于 pushshift.io 上，使用 PLATO-2 的精细清洗流程。训练集为 2005 年到 2019 年的 811M 个样本。词汇表包含 8k
BPE token，使用 SentencePiece 构建。</li>
<li>中文：数据集来自社交媒体，清洗后包含 1.2B 训练样本，词表包含 30k BPE
token。</li>
</ul>
<p>PLATO-XL
拥有 11B 参数，使用了 72 个 Transformer 和 32 个注意力头，嵌入维度为 3072，前馈层的隐藏状态为 18432。为了训练的稳定性，PLATO-XL 参考 GPT2 将 Layer
Normalization 提前到块开始，并对残差层初始参数进行缩放<span class="math inline"> \(*1/\sqrt N\)</span>，N 为残差层数量。</p>
<p>PLAOT-XL 基于飞桨实现，使用了 256 块 32G
V100 进行训练。受限于显存，11B 模型无法容纳在单张卡中，标准的数据并行无法进行。因此将优化器状态、梯度、参数分别保存在不同设备，以减少通信并提高计算效率。为了提高 batch
size，论文还使用了梯度检查点，即不保存一部分前向过程中的激活值，在反向传播时重新计算，用时间换空间。</p>
<h2 id="实验">实验</h2>
<h3 id="基线">基线</h3>
<ul>
<li>DialoGPT：Reddit 评论， 345M 参数。</li>
<li>Blender：Reddit 评论预训练 + 人工注释对话数据（BST）微调，2.7B 参数。</li>
<li>PLATO-2：使用课程学习方法训练。英文：Reddit 评论 + BST 微调，1.6B 参数。中文：1.2B 社交媒体数据集，336M 参数。</li>
<li>CDial-GPT：LCCC 数据集，95.5M 参数。</li>
<li>ProphetNet-X：豆瓣数据集，379M 参数。</li>
<li>EVA：1.4B 数据，2.8B 参数。</li>
</ul>
<p>此外，论文还与公开的中文聊天机器人：微软小冰、图灵机器人、天猫精灵、小爱同学进行了对比。</p>
<h3 id="评估指标">评估指标</h3>
<p>在开放域对话中，自动化指标和人工平板的相关性很小，因此论文主要通过众包平台进行人工评估。</p>
<ul>
<li>连贯性（Coherence）：话语级指标，回复是否与上下文相关和一致。</li>
<li>信息性（Informativeness）：话语级指标，给定上下文的情况下，回复是否有信息。</li>
<li>参与度（Engagingness）：会话级指标，用户是否愿意与机器人持续聊天。</li>
</ul>
<p>上述三个指标取值为 [0,1,2]，分数越高越好。为了进一步分析对话质量，还有两个更细粒度的评估指标：</p>
<ul>
<li>不一致性（Inconsistency）：细粒度评估连贯性，回复是否与上下文冲突。</li>
<li>错觉（Hallucination
）：细粒度评估信息性，回复是否存在事实性错误。</li>
</ul>
<p>这两项指标取值为 [0,1]，越小越好。</p>
<h3 id="实验结果">实验结果</h3>
<p>实验结果如下图所示。可以看到 PLATO-XL 在所有指标上都是最优的。但是仅看中文指标，PLATO-XL 和 PLATO-2 的差别却不大，数据规模均为 1.2B，二者可能使用的是同一套数据集。二者的主要差别在不一致性和错觉这两项指标上，其他三项指标差距均不足 0.1，但模型规模差了 30 倍。</p>
<p>另一个角度看，PLATO-2 是 2020.1 登录 arxiv，EVA 是 2021.10 登录 arxiv，相差近两年，EVA 的模型和数据集的规模都大于 PLATO-2，在论文中表现却全线弱于 PLATO-2。不过 EVA 论文中也未与 PLATO-2 直接比较。这就比较微妙了，要么是 PLATO 的 UniLM 框架要优于 EVA 的 Seq2Seq，要么就是 PLATO 的数据集质量更好。</p>
<p><img src="experiment-result.png"></p>
<p><img src="chat-result.png"></p>
<h3 id="case-study">Case Study</h3>
<p>论文给出了一个对话样例。大家也可以自行去微信公众号体验。</p>
<p><img src="case-study.png"></p>
<p>下面这张图是网上的体验图，可以看到基本可以达到以假乱真的地步。我自己试用的过程中，也激发了 PLATO 的四川话属性。相较于微软小冰，对话质量和体验提升可以说是巨大的。</p>
<p><img src="demo.png"></p>
<h3 id="其他对话任务">其他对话任务</h3>
<p>将 PLATO-XL 微调后用于任务型对话和知识增强对话，在三个数据集上达到了 SOTA 效果，证实了 PLATO-XL 作为对话 AI 的基础模型的潜力。</p>
<p><img src="other-task.png"></p>
<h2 id="总结">总结</h2>
<p>PLATO-XL 的效果确实是非常惊艳，但是代码、数据、模型全部闭源，就 emmmm。模型出于商业角度不开源，数据出于知识产权角度不开源，这篇论文最大的影响就是证实了中文闲聊场景大模型的上限是很高的，但是怎么达到这个上限，我不告诉你。</p>
<p>英文模型反倒可以开源，毕竟百度也不做国外的生意，开源英文模型和推理脚本，论文应该才更容易接收。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://ai.baidu.com/support/news?action=detail&amp;id=2630&amp;hmpl=yunying=10.22">「比人还会聊天」百度 PLATO 对话机器人开放体验
- https://ai.baidu.com/</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话生成</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>对话生成</tag>
        <tag>闲聊</tag>
        <tag>Transformer</tag>
        <tag>PLATO</tag>
      </tags>
  </entry>
  <entry>
    <title>复杂知识库问答（KBQA）中的问题迁移</title>
    <url>/blog/2022/03/13/Program-Transfer/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>《Program Transfer for Answering Complex Questionsover Knowledge
Bases》是由清华大学和华为合作完成的工作，收录于 ACL
2022 长文中。该文试图通过一个两阶段的方法来完成 KBQA 任务：解析器将 query 解析为知识库操作序列（无参数），参数分析器为每个操作填入参数。通过在复资源知识库预训练，显著提升了在低资源知识库上的 KBQA 性能。
<span id="more"></span></p>
<p>本文所讲的方法是语义解析的思想，将 query 先转化为知识库的操作序列，如下图所示。分析出 query 对应的操作序列（Find，Relate..），再为每个操作填入参数（实体、关系等），执行操作即可得到结果。</p>
<p><img src="example.png"></p>
<h2 id="背景">背景</h2>
<p>知识库问答，即 KBQA，旨在利用结构化的知识库来解答 query。在<a href="https://tqnwhz.github.io/blog/2021/10/25/KBQA-survey/#more">基于知识库的问答综述（KBQA）</a>中提到了，KBQA 有两种主流方法，<strong>语义解析</strong>的方法将 query 转化为 SQL 语句，执行即可得到结果；信息检索的方法在特定子图中将实体按相关性排序，最终得到结果。复杂 KBQA 是指需要处理多跳逻辑的 KBQA 问题。这种情况下，KBQA 所面临的推理路径的监督数据缺失、语义理解能力不足、检索空间过大等问题会更为突出。</p>
<p>程序规约是指将 KBQA 问题规约为某个可执行的程序，也就是语义解析的思想。近些年来，一些知识库提供了监督的程序规约信号，在这些知识库上的程序规约问答取得了大幅性能提升。如何使用这些监督信号，提升低资源知识库上的性能呢？本文将其定义为程序迁移任务，该任务面临以下挑战：</p>
<ul>
<li><strong>域异构</strong>：由于语言和知识的多样性，源知识库和目标知识库上的知识、问题的表现形式可能相去甚远。</li>
<li><strong>未知的元素</strong>：源知识库的知识覆盖率往往相当有限，例如 KBQA
Pro 数据集只覆盖了 3.9% 的关系和 0.24% 的维基百科概念。</li>
<li><strong>过大的搜索空间</strong>：每个知识库操作可选的参数很多，搜索知识库和操作和可选参数不现实。</li>
</ul>
<p>本文的贡献包括：</p>
<ul>
<li>首次提出复杂 KBQA 的程序迁移方法</li>
<li>为程序迁移提出一个两阶段解析框架和本体剪枝策略</li>
<li>通过扩展实验和消融实验证实了程序迁移的有效性</li>
</ul>
<h2 id="框架结构">框架结构</h2>
<p>这一部分其实非常简单，对着下面这张图就能解释清楚，不需要任何数学公式。</p>
<p><img src="framework.png"></p>
<ol type="1">
<li>BERT+GRU 将 query 映射为无参数操作序列（与知识库无关）</li>
<li>利用上一步每个操作的 hidden
state 投影后在可选参数空间做 softmax，选择参数</li>
<li>参数剪枝：根据 query 维护可能的域、关系、实体集合，并随着参数序列的自回归过程迭代更新。</li>
</ol>
<h3 id="源域预训练">源域预训练</h3>
<p>损失函数包含两部分：操作序列的交叉熵和参数位置的交叉熵，即两个分类任务损失之和。
<span class="math display">\[
\mathcal {L}^{pretrain}=-\sum_{(x^S,y^S)\in
D^S}(logp(y^S_s|x^S)+\sum_{t=1}^{|y_s|}logp(arg_t^S|x^S,o_t^S,\mathcal
P))
\]</span></p>
<h3 id="目标域微调">目标域微调</h3>
<p>由于目标域缺少源域的监督程序归纳信号，因此需要通过强化学习 / EM 算法来微调。主要步骤是先搜索得到若干个可能的程序，然后执行程序根据结果更新参数。</p>
<h2 id="实验">实验</h2>
<h3 id="数据集">数据集</h3>
<ul>
<li>源域：KBQA Pro</li>
<li> 目标域：WebQuestionSP，ComplexWebQuestions (CWQ)</li>
<li> 知识库：Freebase</li>
</ul>
<h3 id="基线">基线</h3>
<ul>
<li>语义解析：TEX-TRAY，QGG，TeacherNet</li>
<li> 信息检索：Graft-Net，PullNet</li>
<li> 消融实验基线：<span class="math inline">\(Ours_{-f}\)</span>（未微调），<span class="math inline">\(Ours_{-p}\)</span>（未预训练），<span class="math inline">\(Ours_{-pa}\)</span>（未预训练参数解析器），<span class="math inline">\(Ours_{-o}\)</span>（缺少本体剪枝）</li>
</ul>
<h3 id="评估指标">评估指标</h3>
<ul>
<li>F1</li>
<li>Hits@1：Hits@n 是指正例中处在前 n 个结果中的比例，Hits@1 也就是模型结果第一名为正确结果的比例</li>
</ul>
<p>由于数据集中的问题有多个答案，F1 更好地反映了答案的覆盖程度。</p>
<h3 id="实验结果">实验结果</h3>
<p><img src="exper-result.png"></p>
<p>值得关注的是<span class="math inline"> \(Ours_{-p}\)</span> 极差的结果和<span class="math inline"> \(Ours_{-f}\)</span> 的一般表现，表明了预训练的重要性。并且移去预训练参数解析器和本体剪枝策略都会对模型效果有较大影响。</p>
<h2 id="总结">总结</h2>
<p>这篇论文还是蛮有意思的，用一个比较贴近人认知的框架，把迁移学习应用到 KBQA 中并证实了其有效性。</p>
<p>近期还会从各高校 ACL2022 录取宣传中找一些值得读的论文。等到 ACL
2022 公布 accepted list 了就不用这么折腾了。</p>
<p>碎碎念：我也好想发一篇论文啊。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>问答</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>问答</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>RoBERTa</title>
    <url>/blog/2021/09/03/RoBERTa/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>RoBERTa 是华盛顿大学和 FaceBook 在论文《RoBERTa: A Robustly Optimized
BERT Pretraining
Approach》提出的预训练模型，论文似乎仅存在 arxiv 版本。RoBERTa 本质上是 BERT 的一个改进版本。论文发现 BERT 是未充分训练的，改进训练之后的 RoBERTa 在 GLUE、RACE、SQuAD 数据集上达到了 SOTA。代码和模型公开在了 <a href="https://github.com/pytorch/fairseq">github</a> 上。</p>
<span id="more"></span>
<p>相对于 BERT 的修改主要有以下方面：</p>
<ul>
<li>训练时间更长、数据更大（提出了一个新的数据集 CC-News）、batch 更大（有论文指出更大的 batch 模型训练结果越好）</li>
<li>移除下句预测预训练任务</li>
<li>训练序列更长</li>
<li>动态改变数据的 MASK，而 BERT 的 MASK 是固定的</li>
</ul>
<p>在不使用额外的训练数据的情况下，RoBERTa 在 GLUE 和 SQuAD 数据集上取得了优于 BERT 的性能。引入额外的训练数据后，RoBERTa 在 GLUE 中的四项任务、SQuAD、RACE 数据集上达到了 SOTA。</p>
<h2 id="实验">实验</h2>
<h3 id="数据集">数据集</h3>
<ul>
<li>GLUE（ General Language Understanding
Evaluation，通用语言理解评估）：由 9 个句子或句子对的分类任务组成。</li>
<li>SQuAD（ Stanford Question Answering
Dataset，斯坦福问答数据集）：抽取式问答任务。给出文档和问题，从文档中选择部分文本作为问题答案。</li>
<li>RACE（ ReAding Comprehension from
Examinations，考试阅读理解数据集）：顾名思义，数据集来自考试题目，是一个分类任务。单个样本由文档、问题和若干个候选答案组成。正确答案不一定直接体现在文章中，需要深层理解文章并进行推断。</li>
</ul>
<h3 id="静态掩码vs动态掩码">静态掩码 vs 动态掩码</h3>
<p>论文将 RoBERTa 与参数量相近的<span class="math inline"> \(BERT_{BASE}\)</span> 进行了比较，结果如下所示。可以看出，动态掩码的效果与静态掩码持平或者略优于。个人猜测原因是动态掩码虽然能够使得模型模型接触到更多数据、更加鲁棒，但频繁的动态掩码会使得某些样本无法得到充足的训练。</p>
<p><img src="mask-result.png"></p>
<h3 id="下句预测">下句预测</h3>
<p>下句预测任务是 BERT 中提出的预训练任务，用于判断两句话是否构成连续上下句的关系。BERT 论文中认为下句预测任务是非常重要的，它提升了 QNLI、MNLI、SQuAD 数据集的性能。然而，一些工作开始质疑下句预测任务的有效性。RoBERTa 论文中比较了以下几种训练方法：</p>
<ul>
<li>句子段（连续多个句子）对 + 下句预测，也就是原版 BERT 的训练方法。</li>
<li>句子对 + 下句预测。</li>
<li>跨文档完整句子，将多篇文档拼接在一起，从中连续采样句子，可能跨文档也可能来自同一篇文档。</li>
<li>单文档句子，从单个文档中连续采样句子。</li>
</ul>
<p>实验结果如下：</p>
<p><img src="nsp.png"></p>
<p>前两种训练方法比较，前者优于后者，说明独立的句子会损害下流任务的性能。接下来比较有无 NSP 任务的训练方法，分析后可以看出，完整句子移除了 NSP 任务，与拥有 NSP 任务的性能基本持平，在某些任务上还略胜一筹。而单文档句子任务甚至优于跨文档完整句子。</p>
<h3 id="更大的batch">更大的 Batch</h3>
<p>机器翻译上的部分工作证实了大 batch-size 能够同时提高优化速度和任务性能，近期工作证实这同样适用于 BERT，论文在<span class="math inline"> \(BERT_{BASE}\)</span> 上进行了 Batch-size 的实验，结果如下：</p>
<p><img src="batch-size.png"></p>
<p>可以看出，2k 的 batch size
确实要优于 256，但 8k 却差于 2k。论文中也没有进行解释，迷惑。</p>
<h3 id="文本编码">文本编码</h3>
<p>字节对编码（Byte-Pair
Encoding）是一种字词模型，BERT 使用它来构建词表。然而当语料规模很大时，unicode 字符会占据词表中相当大部分。2019 年 GPT2 论文指出，可以使用 unicode 字节而非 unicode 字符来作为基本字词单元，然而这种方法可能会有轻微的性能损失（毕竟破坏了字符的完整结构），但是由于其能减小词表规模，RoBERTa 还是基于此进行的词表构建。</p>
<h2 id="roberta">RoBERTa</h2>
<p>RoBERTa=BERT + 动态掩码 + 跨文档完整句子 + 更大 batch
size + 字节编码 + 更大数据 + 更长训练时间</p>
<p>实验结果如下：</p>
<p><img src="roberta.png"></p>
<p>控制训练数据时，RoBERTa 已经优于<span class="math inline"> \(BERT_{LARGE}\)</span> 了（但在 SQuAD 上逊于 XLNET），在增加数据和训练更长时间后，三个数据集上全面超越 XLNET。</p>
<p>后面就是 GLUE、SQuAD 上各项指标的实验和比较了，基本 RoBERTa 也是最优的，这里就略去了。</p>
<h2 id="总结">总结</h2>
<p>RoBERTa 可以看作是 BERT 真正的完全体吧，弥补了原生 BERT 的缺陷。可能是因为创新性不足？没有被会议接受。看来预训练模型也还是很卷的。。。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>BERT</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>预训练模型</tag>
        <tag>RoBERTa</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer-code</title>
    <url>/blog/2021/09/05/Transformer-code/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来读一下《<a href="https://arxiv.org/abs/1706.03762">Attention is
All You Need</a>》的代码，也就是 Transformer。pytorch 代码<a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">地址</a>)。</p>
<span id="more"></span>
<h2 id="目录结构">目录结构</h2>
<p>Transformer 文件夹下有以下文件：</p>
<ul>
<li>Constants.py：模型常量定义</li>
<li> Layers.py：编码器和解码器层定义</li>
<li> Models.py：模型定义</li>
<li> Modules.py：工具模块</li>
<li> Optim.py：优化模块</li>
<li> SubLayer.py：多头注意力机制等子层</li>
<li> Translator.py：翻译 beam search</li>
</ul>
<p>顶层目录下有以下文件：</p>
<ul>
<li>train.py：训练入口，实例化模型、优化器等，进行优化迭代</li>
<li> learn_bpe.py：bpe 学习词表</li>
<li> apply_bpe.py：使用 bpe 得到的词表将文本进行编码</li>
<li> translate.py：加载模型进行翻译</li>
</ul>
<p>下面逐一进行分析模型相关文件，剩下的文件等到下次再读吧。</p>
<h2 id="模型解析">模型解析</h2>
<p>下面按照依赖顺序对各文件进行解析。</p>
<h3 id="constants.py">Constants.py</h3>
<p>文件内容非常简单，分别为填充、未知、起始、终止四种 token 的定义。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">PAD_WORD = <span class="string">'&lt;blank&gt;'</span> <span class="comment"># 填充token</span></span><br><span class="line">UNK_WORD = <span class="string">'&lt;unk&gt;'</span> <span class="comment"># 未知token</span></span><br><span class="line">BOS_WORD = <span class="string">'&lt;s&gt;'</span> <span class="comment"># 起始token</span></span><br><span class="line">EOS_WORD = <span class="string">'&lt;/s&gt;'</span> <span class="comment"># 结束token</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="modules.py">Modules.py</h3>
<p>定义了标量化点乘注意力机制类。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledDotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Scaled Dot-Product Attention '''</span></span><br><span class="line">	</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, temperature, attn_dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.temperature = temperature <span class="comment"># softmax的温度系数，论文中为\sqrt d_k</span></span><br><span class="line">        self.dropout = nn.Dropout(attn_dropout) <span class="comment"># 原论文dropout比例即为0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">		<span class="string">'''</span></span><br><span class="line"><span class="string">		q,k:  bsz x n_head x lq  x d_k</span></span><br><span class="line"><span class="string">		v: bsz x n_head x lq x d_v</span></span><br><span class="line"><span class="string">		'''</span></span><br><span class="line">        attn = torch.matmul(q / self.temperature, k.transpose(<span class="number">2</span>, <span class="number">3</span>)) </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn = attn.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>) <span class="comment"># 填充位置的注意力掩码，避免信息泄露等问题</span></span><br><span class="line"></span><br><span class="line">        attn = self.dropout(F.softmax(attn, dim=-<span class="number">1</span>)) <span class="comment"># 注意力的dropout</span></span><br><span class="line">        output = torch.matmul(attn, v) </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br></pre></td></tr></tbody></table></figure>
<h3 id="sublayers.py">SubLayers.py</h3>
<p>多头注意力机制定义：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Multi-Head Attention module '''</span></span><br><span class="line">	<span class="string">'''</span></span><br><span class="line"><span class="string">	n_head：注意力头数</span></span><br><span class="line"><span class="string">	d_model：词嵌入向量维度</span></span><br><span class="line"><span class="string">	d_k：query,key向量维度</span></span><br><span class="line"><span class="string">	d_v：value向量维度，d_v*n_head即为注意力输出的维度</span></span><br><span class="line"><span class="string">	'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_head, d_model, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_k = d_k</span><br><span class="line">        self.d_v = d_v</span><br><span class="line">		<span class="comment"># query key value 矩阵</span></span><br><span class="line">        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=<span class="literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(n_head * d_v, d_model, bias=<span class="literal">False</span>) <span class="comment">#全连接层</span></span><br><span class="line"></span><br><span class="line">        self.attention = ScaledDotProductAttention(temperature=d_k ** <span class="number">0.5</span>) <span class="comment">#注意力计算</span></span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>) <span class="comment"># 层标准化</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head</span><br><span class="line">        sz_b, len_q, len_k, len_v = q.size(<span class="number">0</span>), q.size(<span class="number">1</span>), k.size(<span class="number">1</span>), v.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        residual = q</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pass through the pre-attention projection: b x lq x (n*dv)</span></span><br><span class="line">        <span class="comment"># Separate different heads: b x lq x n x dv</span></span><br><span class="line">        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)</span><br><span class="line">        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)</span><br><span class="line">        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transpose for attention dot product: b x n x lq x dv</span></span><br><span class="line">        q, k, v = q.transpose(<span class="number">1</span>, <span class="number">2</span>), k.transpose(<span class="number">1</span>, <span class="number">2</span>), v.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)   <span class="comment"># For head axis broadcasting.</span></span><br><span class="line">	    <span class="comment"># q为注意力的输出</span></span><br><span class="line">        q, attn = self.attention(q, k, v, mask=mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transpose to move the head dimension back: b x lq x n x dv</span></span><br><span class="line">        <span class="comment"># Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)</span></span><br><span class="line">        q = q.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(sz_b, len_q, -<span class="number">1</span>)</span><br><span class="line">        q = self.dropout(self.fc(q))</span><br><span class="line">        q += residual</span><br><span class="line"></span><br><span class="line">        q = self.layer_norm(q)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> q, attn</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>前馈神经网络子层、残差层定义：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A two-feed-forward-layer module '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_in, d_hid, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_in, d_hid) <span class="comment"># position-wise</span></span><br><span class="line">        self.w_2 = nn.Linear(d_hid, d_in) <span class="comment"># position-wise</span></span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_in, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line"></span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        x = self.w_2(F.relu(self.w_1(x)))</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x += residual <span class="comment"># 残差计算</span></span><br><span class="line"></span><br><span class="line">        x = self.layer_norm(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="layers.py">Layers.py</h3>
<p>编码器层类（非常简单，将多头注意力机制与前馈神经网络拼接起来即可）：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">''' Define the Layers '''</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformer.SubLayers <span class="keyword">import</span> MultiHeadAttention, PositionwiseFeedForward</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Compose with two layers '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_input, slf_attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        enc_output, enc_slf_attn = self.slf_attn(</span><br><span class="line">            enc_input, enc_input, enc_input, mask=slf_attn_mask)</span><br><span class="line">        enc_output = self.pos_ffn(enc_output)</span><br><span class="line">        <span class="keyword">return</span> enc_output, enc_slf_attn</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>解码器层类：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Compose with three layers '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</span><br><span class="line">        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout) <span class="comment"># 解码器对编码器输出的注意力</span></span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, dec_input, enc_output,</span></span></span><br><span class="line"><span class="params"><span class="function">            slf_attn_mask=<span class="literal">None</span>, dec_enc_attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        dec_output, dec_slf_attn = self.slf_attn(</span><br><span class="line">            dec_input, dec_input, dec_input, mask=slf_attn_mask) <span class="comment"># 解码器的自注意力</span></span><br><span class="line">        dec_output, dec_enc_attn = self.enc_attn(</span><br><span class="line">            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask) <span class="comment"># 解码器对编码器输出的注意力</span></span><br><span class="line">        dec_output = self.pos_ffn(dec_output) <span class="comment"># 前馈神经网络</span></span><br><span class="line">        <span class="keyword">return</span> dec_output, dec_slf_attn, dec_enc_attn</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="models.py">Models.py</h3>
<p>Models.py 是模型定义核心文件。</p>
<p>工具函数：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pad_mask</span>(<span class="params">seq, pad_idx</span>):</span> <span class="comment"># 获取序列的MASK（填充位置为0）</span></span><br><span class="line">    <span class="keyword">return</span> (seq != pad_idx).unsqueeze(-<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_subsequent_mask</span>(<span class="params">seq</span>):</span> <span class="comment"># 获取序列的所有MASK（长度为1,2,...n）</span></span><br><span class="line">    <span class="string">''' For masking out the subsequent info. '''</span></span><br><span class="line">    sz_b, len_s = seq.size()</span><br><span class="line">    subsequent_mask = (<span class="number">1</span> - torch.triu(</span><br><span class="line">        torch.ones((<span class="number">1</span>, len_s, len_s), device=seq.device), diagonal=<span class="number">1</span>)).<span class="built_in">bool</span>()</span><br><span class="line">    <span class="keyword">return</span> subsequent_mask</span><br></pre></td></tr></tbody></table></figure>
<p>位置编码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_hid, n_position=<span class="number">200</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Not a parameter</span></span><br><span class="line">        <span class="comment"># 成员变量无法保存在模型参数中且无法通过.cuda()转移到gpu上，register_buffer注册后则可以</span></span><br><span class="line">        self.register_buffer(<span class="string">'pos_table'</span>, self._get_sinusoid_encoding_table(n_position, d_hid))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_sinusoid_encoding_table</span>(<span class="params">self, n_position, d_hid</span>):</span></span><br><span class="line">        <span class="string">''' Sinusoid position encoding table '''</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> make it with torch instead of numpy</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_position_angle_vec</span>(<span class="params">position</span>):</span></span><br><span class="line">            <span class="keyword">return</span> [position / np.power(<span class="number">10000</span>, <span class="number">2</span> * (hid_j // <span class="number">2</span>) / d_hid) <span class="keyword">for</span> hid_j <span class="keyword">in</span> <span class="built_in">range</span>(d_hid)]</span><br><span class="line"></span><br><span class="line">        sinusoid_table = np.array([get_position_angle_vec(pos_i) <span class="keyword">for</span> pos_i <span class="keyword">in</span> <span class="built_in">range</span>(n_position)])</span><br><span class="line">        sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i</span></span><br><span class="line">        sinusoid_table[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(sinusoid_table[:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.FloatTensor(sinusoid_table).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x + self.pos_table[:, :x.size(<span class="number">1</span>)].clone().detach()</span><br></pre></td></tr></tbody></table></figure>
<p>Transformer 编码器：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A encoder model with self attention mechanism. '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v,</span></span></span><br><span class="line"><span class="params"><span class="function">            d_model, d_inner, pad_idx, dropout=<span class="number">0.1</span>, n_position=<span class="number">200</span>, scale_emb=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx) <span class="comment"># 词嵌入表</span></span><br><span class="line">        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.layer_stack = nn.ModuleList([</span><br><span class="line">            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout) <span class="comment"># 编码网络</span></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.scale_emb = scale_emb</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src_seq, src_mask, return_attns=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        enc_slf_attn_list = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -- Forward</span></span><br><span class="line">        enc_output = self.src_word_emb(src_seq)</span><br><span class="line">        <span class="keyword">if</span> self.scale_emb:</span><br><span class="line">            enc_output *= self.d_model ** <span class="number">0.5</span></span><br><span class="line">        enc_output = self.dropout(self.position_enc(enc_output))</span><br><span class="line">        enc_output = self.layer_norm(enc_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> enc_layer <span class="keyword">in</span> self.layer_stack: <span class="comment"># 编码网络</span></span><br><span class="line">            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)</span><br><span class="line">            enc_slf_attn_list += [enc_slf_attn] <span class="keyword">if</span> return_attns <span class="keyword">else</span> []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> return_attns:</span><br><span class="line">            <span class="keyword">return</span> enc_output, enc_slf_attn_list</span><br><span class="line">        <span class="keyword">return</span> enc_output,</span><br></pre></td></tr></tbody></table></figure>
<p>Transformer 解码器：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A decoder model with self attention mechanism. '''</span></span><br><span class="line">	<span class="string">'''</span></span><br><span class="line"><span class="string">		与编码器类似</span></span><br><span class="line"><span class="string">	'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,</span></span></span><br><span class="line"><span class="params"><span class="function">            d_model, d_inner, pad_idx, n_position=<span class="number">200</span>, dropout=<span class="number">0.1</span>, scale_emb=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)</span><br><span class="line">        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.layer_stack = nn.ModuleList([</span><br><span class="line">            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.scale_emb = scale_emb</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, trg_seq, trg_mask, enc_output, src_mask, return_attns=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        dec_slf_attn_list, dec_enc_attn_list = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -- Forward</span></span><br><span class="line">        dec_output = self.trg_word_emb(trg_seq)</span><br><span class="line">        <span class="keyword">if</span> self.scale_emb:</span><br><span class="line">            dec_output *= self.d_model ** <span class="number">0.5</span> <span class="comment"># Transformer论文中的trick，对词嵌入向量进行放大</span></span><br><span class="line">        dec_output = self.dropout(self.position_enc(dec_output))</span><br><span class="line">        dec_output = self.layer_norm(dec_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> dec_layer <span class="keyword">in</span> self.layer_stack:</span><br><span class="line">            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(</span><br><span class="line">                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)</span><br><span class="line">            dec_slf_attn_list += [dec_slf_attn] <span class="keyword">if</span> return_attns <span class="keyword">else</span> []</span><br><span class="line">            dec_enc_attn_list += [dec_enc_attn] <span class="keyword">if</span> return_attns <span class="keyword">else</span> []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> return_attns:</span><br><span class="line">            <span class="keyword">return</span> dec_output, dec_slf_attn_list, dec_enc_attn_list</span><br><span class="line">        <span class="keyword">return</span> dec_output,</span><br></pre></td></tr></tbody></table></figure>
<p>Transformer：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A sequence to sequence model with attention mechanism. '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,</span></span></span><br><span class="line"><span class="params"><span class="function">            d_word_vec=<span class="number">512</span>, d_model=<span class="number">512</span>, d_inner=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            n_layers=<span class="number">6</span>, n_head=<span class="number">8</span>, d_k=<span class="number">64</span>, d_v=<span class="number">64</span>, dropout=<span class="number">0.1</span>, n_position=<span class="number">200</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            trg_emb_prj_weight_sharing=<span class="literal">True</span>, emb_src_trg_weight_sharing=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            scale_emb_or_prj=<span class="string">'prj'</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx</span><br><span class="line"></span><br><span class="line">        <span class="comment"># In section 3.4 of paper "Attention Is All You Need", there is such detail:</span></span><br><span class="line">        <span class="comment"># "In our model, we share the same weight matrix between the two</span></span><br><span class="line">        <span class="comment"># embedding layers and the pre-softmax linear transformation...</span></span><br><span class="line">        <span class="comment"># In the embedding layers, we multiply those weights by \sqrt{d_model}".</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Options here:</span></span><br><span class="line">        <span class="comment">#   'emb': multiply \sqrt{d_model} to embedding output</span></span><br><span class="line">        <span class="comment">#   'prj': multiply (\sqrt{d_model} ^ -1) to linear projection output</span></span><br><span class="line">        <span class="comment">#   'none': no multiplication</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> scale_emb_or_prj <span class="keyword">in</span> [<span class="string">'emb'</span>, <span class="string">'prj'</span>, <span class="string">'none'</span>]</span><br><span class="line">        scale_emb = (scale_emb_or_prj == <span class="string">'emb'</span>) <span class="keyword">if</span> trg_emb_prj_weight_sharing <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">        self.scale_prj = (scale_emb_or_prj == <span class="string">'prj'</span>) <span class="keyword">if</span> trg_emb_prj_weight_sharing <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">        self.encoder = Encoder(</span><br><span class="line">            n_src_vocab=n_src_vocab, n_position=n_position,</span><br><span class="line">            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,</span><br><span class="line">            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,</span><br><span class="line">            pad_idx=src_pad_idx, dropout=dropout, scale_emb=scale_emb)</span><br><span class="line"></span><br><span class="line">        self.decoder = Decoder(</span><br><span class="line">            n_trg_vocab=n_trg_vocab, n_position=n_position,</span><br><span class="line">            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,</span><br><span class="line">            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,</span><br><span class="line">            pad_idx=trg_pad_idx, dropout=dropout, scale_emb=scale_emb)</span><br><span class="line"></span><br><span class="line">        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=<span class="literal">False</span>) <span class="comment"># 投影到词表</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters():</span><br><span class="line">            <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                nn.init.xavier_uniform_(p) <span class="comment"># xavier 初始化权重</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> d_model == d_word_vec, \</span><br><span class="line">        <span class="string">'To facilitate the residual connections, \</span></span><br><span class="line"><span class="string">         the dimensions of all module outputs shall be the same.'</span></span><br><span class="line">		<span class="comment"># 嵌入层与投影线性层权重共享</span></span><br><span class="line">        <span class="keyword">if</span> trg_emb_prj_weight_sharing:</span><br><span class="line">            <span class="comment"># Share the weight between target word embedding &amp; last dense layer</span></span><br><span class="line">            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> emb_src_trg_weight_sharing:</span><br><span class="line">            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src_seq, trg_seq</span>):</span></span><br><span class="line"></span><br><span class="line">        src_mask = get_pad_mask(src_seq, self.src_pad_idx)</span><br><span class="line">        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) &amp; get_subsequent_mask(trg_seq)</span><br><span class="line"></span><br><span class="line">        enc_output, *_ = self.encoder(src_seq, src_mask)</span><br><span class="line">        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)</span><br><span class="line">        seq_logit = self.trg_word_prj(dec_output)</span><br><span class="line">        <span class="keyword">if</span> self.scale_prj:</span><br><span class="line">            seq_logit *= self.d_model ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> seq_logit.view(-<span class="number">1</span>, seq_logit.size(<span class="number">2</span>))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="optim.py">Optim.py</h3>
<p>一个简单封装的优化器类，用以动态调整学习率。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">'''A wrapper class for scheduled optimizer '''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScheduledOptim</span>():</span></span><br><span class="line">    <span class="string">'''A simple wrapper class for learning rate scheduling'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, optimizer, lr_mul, d_model, n_warmup_steps</span>):</span></span><br><span class="line">        self._optimizer = optimizer</span><br><span class="line">        self.lr_mul = lr_mul</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_warmup_steps = n_warmup_steps</span><br><span class="line">        self.n_steps = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step_and_update_lr</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">"Step with the inner optimizer"</span></span><br><span class="line">        self._update_learning_rate()</span><br><span class="line">        self._optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">"Zero out the gradients with the inner optimizer"</span></span><br><span class="line">        self._optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_lr_scale</span>(<span class="params">self</span>):</span></span><br><span class="line">        d_model = self.d_model</span><br><span class="line">        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps</span><br><span class="line">        <span class="keyword">return</span> (d_model ** -<span class="number">0.5</span>) * <span class="built_in">min</span>(n_steps ** (-<span class="number">0.5</span>), n_steps * n_warmup_steps ** (-<span class="number">1.5</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_learning_rate</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">''' Learning rate scheduling per step '''</span></span><br><span class="line"></span><br><span class="line">        self.n_steps += <span class="number">1</span></span><br><span class="line">        lr = self.lr_mul * self._get_lr_scale()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> self._optimizer.param_groups:</span><br><span class="line">            param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="总结">总结</h2>
<p>读一遍代码发现了论文中忽视的好几个点，例如嵌入向量放缩、dropout 等，读代码还是很有必要的。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/blog/2021/08/14/Transformer/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天读的是大名鼎鼎的 BERT------- 的组件之一 Transformer，出自论文 Google 团队 2017 年的论文《Attention
Is All You
Need》。与传统的 GRU、LSTM 等相比，Transformer 只使用注意力机制来建模输入与输出间的依赖关系，并支持并行化。论文在机器翻译上进行了实验，Transfomer 达到了更好的效果，因此自提出以来，就得到了极为广泛的关注。</p>
<span id="more"></span>
<h2 id="题外话">题外话</h2>
<p>之前由于换电脑的原因，断更了一段时间。BERT 与 Transformer 的论文之前也粗读过一两次，还是有些一知半解，正好趁这个周末再复习总结一下，记录在博客里。希望我的博客能对你有所帮助。</p>
<h2 id="模型结构">模型结构</h2>
<p>Transformer 使用的仍然是 encoder-decoder 架构，但与 RNN 等自回归模型不同，Transformer 使用的是堆叠的多头注意力机制，全连接层等，其模型结构如下所示：</p>
<p><img src="architecture.png"></p>
<p>左侧的为单个编码器的结构，第一层为多头注意力、残差层与层标准化，第二层是前馈神经网络。编码网络是由若干个编码器堆叠而成的，原论文中 N=6，嵌入向量维度为 512。</p>
<p>右侧为单个解码器的结构，主要在编码器的基础上，加入了一个 Masked 的多头注意力机制，用以保证每个时间步的输出只已知已经输出的信息。同样的，解码网络也有 6 个解码器组成。</p>
<h3 id="注意力机制">注意力机制</h3>
<p>多头注意力机制可谓是 Transformer 的核心，详细过程可以参考<a href="https://blog.csdn.net/longxinchen_ml/article/details/86533005">图解 Transformer 完整版</a>。这里只做核心部分介绍，单头计算过程为：
<span class="math display">\[
Attention(Q,K,V)=softmax(\frac{QK^\intercal}{\sqrt{d_k}})V
\]</span>
Q，K，V 分别为查询、键、值矩阵，由词嵌入向量矩阵映射得出。多头注意力机制使用点乘计算相似度，不同的是，这里除以了一个标量<span class="math inline"> \(\sqrt{d_k}\)</span>​​​。这个标量是 softmax 的温度系数，由于点积结果方差可能很大，可能会存在梯度过小无法正常更新的情况。除以一个标量能够使得概率分布更加均匀。这一部分可以参考学习下 softmax 的温度系数。</p>
<p>作者发现，相较于仅使用一个注意力机制，使用多个注意力机制并将其拼接能够拥有更好的效果。在论文中，作者使用 8 个注意力机制，每个注意力机制的输出为 512/8=64 维嵌入向量。</p>
<h3 id="注意力机制的使用">注意力机制的使用</h3>
<p>多头注意力机制以三种方式在模型中使用：</p>
<ul>
<li>编码器与解码器间的注意力：查询 q 来自解码器，键 K 与值 V 来自编码器。这里的注意力机制用以在输出的每一步关注在输入序列的不同部分，与 seq2seq 的注意力机制相似、</li>
<li>编码器内的自注意力：查询、键、值均来自编码器。输入序列的每个位置可以得到到整个输入序列的信息。</li>
<li>解码器内的掩码自注意力：查询、键、值均来自解码器。为了保证解码器只能获得已输出的部分序列的信息，将当前位置之后位置的标量化点积设置为<span class="math inline"> \(-\infty\)</span>​，进而经过 softmax 后概率值为 0。</li>
</ul>
<h3 id="前馈神经网络">前馈神经网络</h3>
<p>在编码器和解码器中的前馈神经网络，搭配 relu 激活函数来为模型构造非线性计算。计算过程如下所示：
<span class="math display">\[
FFN(x)=\max(0,xW_1+b_1)W2+b_2
\]</span>
其中，输入和输出的维度均为 512，隐藏层维度为 1024。另外，前馈神经网络在每个层内不共享参数，换而言之，它们的参数是独立的。</p>
<h3 id="位置编码">位置编码</h3>
<p>由于 Transformer 中不存在 RNN 中的自回归结构，输入序列的不同位置是等价的。为了编码位置信息，作者引入了位置编码，使用 sin 与 cos 函数：
<span class="math display">\[
PE_{(pos,2i)}=\sin(pos/10000^{2i/d_{model}})
\]</span></p>
<p><span class="math display">\[
PE_{(pos,2i+1)}=\cos(pos/10000^{2i/d_{model}})
\]</span></p>
<p>其中，pos 为位置，i 为向量维度。作者称选取三角函数的原因是假设这样可以更好地使模型学到相对位置关系，对于任意固定的偏移 k，<span class="math inline">\(PE_{pos+k}\)</span> 可以表示为<span class="math inline"> \(PE_{pos}\)</span> 的线性函数。另外，作者还尝试了学习位置编码的方式，实验对比显示，二者结果差别不大。因此作者最终选择了上述编码方式，因为它可以处理更长的序列。</p>
<h2 id="为什么使用自注意力机制">为什么使用自注意力机制</h2>
<p>论文从计算时间复杂度、序列操作数、最长路径长度三方面对比了自注意力机制、RNN、CNN 以及受限的自注意力机制，结果如下：</p>
<p><img src="comparison.png"></p>
<p>这一部分计算过程论文没有给出，我参考了 <a href="https://zhuanlan.zhihu.com/p/264749298">Transformer/CNN/RNN 的对比（时间复杂度，序列操作数，最大路径长度）</a>，这里简单介绍一下。</p>
<h3 id="计算时间复杂度">计算（时间）复杂度</h3>
<p>计算复杂度主要取决于计算的规模，以矩阵乘法为例，形状为 NxM 的矩阵与形状为 MxP 的矩阵相乘，得到一个 NxP 的矩阵。结果矩阵中的每个元素为 M 维向量内积的结果，进行 M 次乘法，并求和。所以整个矩阵乘法的复杂度为<span class="math inline"> \(O(NMP)\)</span>。</p>
<h4 id="自注意力机制">自注意力机制</h4>
<p><span class="math display">\[
Attention(Q,K,V)=softmax(\frac{QK^\intercal}{\sqrt{d_k}})V
\]</span></p>
<p>其中，Q，K 分别为 nxd 与 dxn 的矩阵，<span class="math inline">\(QK^\intercal\)</span>​的复杂度为<span class="math inline"> \(O(n^2d)\)</span>​​，softmax 的复杂度为<span class="math inline"> \(O(n^2)\)</span>，加权求和的矩阵形状分别为 nxn 与 nxd，复杂度为<span class="math inline"> \(O(n^2d)\)</span>，因此总复杂度为<span class="math inline"> \(O(n^2d)\)</span>​。受限自注意力机制与之同理，区别在于它只使用查询最近的 k 个</p>
<h4 id="rnn">RNN</h4>
<p><span class="math display">\[
h_t=f(Ux_t+Wh_{t-1})
\]</span></p>
<p>其中，U 与 x 的形状分别为 dxd 与 dx1（假设隐藏状态与输入维度均为 d），复杂度为<span class="math inline"> \(O(d^2)\)</span>​，W 与 h 的形状分别为 dxd 与 dx1，复杂度同样为<span class="math inline"> \(O(d^2)\)</span>。对于长度为 n 的序列，总复杂度为<span class="math inline"> \(O(nd^2)\)</span>。</p>
<h4 id="cnn">CNN</h4>
<p>将输入序列进行 padding 后，总共需要 n 次卷积，每次卷积计算量为 kxd，假设步长为 1，单个卷积核复杂度为<span class="math inline"> \(O(nkd)\)</span>​。为了保证维度相同，需要使用 d 个卷积核，总复杂度为<span class="math inline"> \(O(nkd^2)\)</span></p>
<h3 id="序列操作数">序列操作数</h3>
<p>序列操作数主要衡量了并行化的支持情况，只有 RNN 需要串行地完成 n 次序列操作，其他模型均支持并行化。</p>
<h3 id="最长路径长度">最长路径长度</h3>
<p>最长路径为序列中首尾 token 在模型中的路径，其长度越长，依赖越不容易被捕捉到。对于自注意力机制，序列中的任意两个元素均可以看作直接相连，路径长度为<span class="math inline"> \(O(1)\)</span>。而 RNN 中，第一个 token 的信息需要进行 n 次迭代才能到达最后一个 token，最大路径长度即为<span class="math inline"> \(O(n)\)</span>。CNN 中，通过若干个卷积层来获取不同位置的信息，每个卷积层（论文中使用的是空洞卷积）相当于让序列信息浓缩了 k 倍（卷积层的输出中的每个位置都有输入中 k 个位置的信息），最大路径长度为<span class="math inline"> \(O(log_kn)\)</span>​。受限的自注意力机制与连续卷积类似，每次卷积相当于可以获取连续 k 个位置的信息，最大路径长度为<span class="math inline"> \(O(n/k)\)</span>。</p>
<p>这就基本解释了这个突兀的表格是怎么计算得来的了。那么可以总结自注意力机制的优点是：</p>
<ul>
<li>单层计算量更少。在实际应用中，序列长度 n 往往小于表征维度 d，因此，自注意力机制的单层计算量相较于 RNN 与 CNN 都要更小。</li>
<li>支持并行化。这个就不说了，全世界都在针对 RNN。</li>
<li>能够更好地捕捉长距离依赖。相较于 CNN 与 RNN，自注意力机制的最长路径最短。</li>
<li>可解释性更强。作者将注意力机制的概率分布展示如下，证明多头注意力的多个头完成了与句子语义与结构相关不同的工作。</li>
</ul>
<p><img src="example.png"></p>
<p>上图是作者给出的多头注意力的例子，使用了两个头。对于 its 这个单词，得到了非常尖锐的概率分布，its 主要与 law 与 application 相关联，一个头捕获到了 its 指代的主体 law，一个头捕获到了 its 的目标 application。个人感觉这个效果也太过于理想了。。。可能这就是 Transformer 吧。</p>
<h2 id="总结">总结</h2>
<p>这篇论文提出了完全依赖于注意力机制的序列转换模型 Transformer，相较于 RNN，它有着可并行化、解释性更强、单层参数更少等优点。在机器翻译上取得了 state-of-the-art，在英语成分分析上也取得了比 RNN 更优的结果。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://www.cnblogs.com/sandwichnlp/p/11612596.html#nlp界cnn模型的进化史">三大特征提取器（RNN/CNN/Transformer）
- 西多士 NLP - 博客园 (cnblogs.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/264749298">Transformer/CNN/RNN 的对比（时间复杂度，序列操作数，最大路径长度）</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>BERT</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>Transformer</tag>
        <tag>多头注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title>变分自编码器 VAE</title>
    <url>/blog/2021/10/07/VAE/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来回顾一下变分自编码器（Variational
Autoencoder，VAE），这是 2013 年提出的一种生成模型，时至今日，它的各类变体还活跃在各类会议上。之前我读过它的离散变体 VQ-VAE，这里再回顾一下原本的 VAE。
<span id="more"></span></p>
<h2 id="数学知识">数学知识</h2>
<p>理解 VAE 需要一些信息论和概率论的知识，这里总结一下。</p>
<h3 id="概率统计">概率统计</h3>
<h4 id="数值计算-vs-采样计算">数值计算 vs 采样计算</h4>
<p>对于一个随机变量 X，如果我们想知道 X 的期望<span class="math inline"> \(E(X)\)</span>。如果我们已知 X 的分布函数，很容易可以计算出准确的期望<span class="math inline"> \(E(X)=\sum
p(x)x\)</span>（连续型变量替换为积分即可），这当然是最好的。然而很多情况下，我们无法得知准确的分布函数，那么我们可以采用统计量进行估计，对于 n 个随机样本<span class="math inline"> \(x_1,x_2,\dots,x_n\)</span>，<span class="math inline">\(\overline X=\frac{1}{n}\sum
x_i\)</span> 就是期望<span class="math inline"> \(E(X)\)</span> 的无偏估计。</p>
<h3 id="信息论">信息论</h3>
<h4 id="信息熵">信息熵</h4>
<p>在信息论中，信息熵衡量了信息的不确定性，公式为<span class="math inline"> \(H(X)=-\sum_{x\in
X}p(x)logp(x)\)</span>。以单个事件 x 为例，概率越小的事件的信息熵越大。当一个事件必定会发生时（<span class="math inline">\(p(x)=1\)</span>），其信息熵为 0，没有任何不确定性。对随机变量 X 而说，其信息熵就是<span class="math inline"> \(-logp(x)\)</span> 的期望，熵越大代表随机变量越不确定，很自然可以想到，分布越均匀，变量的状态越不容易确定，其熵越大。</p>
<p>在通信领域，信息熵可以看作对随机变量 X 进行编码所需的最短期望位数，这也被称为编码定理。在通信编码问题中，将随机变量 X 的每个值编码为一个二进制序列，使得序列长度期望最短。同时为了避免混乱，一个序列不能是其他序列的延申。这时编码位数的最短期望位数就是信息熵，有兴趣的同学可以去看看证明。</p>
<h4 id="交叉熵">交叉熵</h4>
<p>对于随机变量<span class="math inline"> \(X\)</span> 的真实分布<span class="math inline"> \(p(x)\)</span>，有时是未知的，我们只有它的近似分布<span class="math inline"> \(q(x)\)</span>，如果按照<span class="math inline"> \(q(x)\)</span> 对变量 X 进行编码，得到的编码长度的期望称为交叉熵，记为<span class="math inline"> \(H(p,q)=-\sum_x
p(x)logq(x)\)</span>。容易知道交叉熵是大于等于信息熵的，因为信息熵是最短编码长度。</p>
<h4 id="相对熵kl散度">相对熵（KL 散度）</h4>
<p>对于真实分布<span class="math inline"> \(p\)</span> 和近似分布<span class="math inline"> \(q\)</span>，相对熵为使用近似分布编码得到的编码长度与最短编码长度的差，即交叉熵与信息熵的差，定义为<span class="math inline"> \(D(p||q)=H(p,q)-H(p)\)</span>。KL 散度衡量了两个分布之间的差异，两个分布差异越大，KL 散度越大。不过 KL 散度并不是距离，因为它不是对称的。因此，KL 散度可以用于分类任务中计算真实概率分布与预测的概率分布之间的差异。事实上，<strong>由于这时信息熵为常数，往往将其略去使用交叉熵作为损失函数</strong>。</p>
<h2 id="变分自编码器">变分自编码器</h2>
<p>对于数据集<span class="math inline"> \(D={x_1,x_2,\dots,x_n}\)</span>（假设是很多张猫的图片，每个样本<span class="math inline"> \(x\)</span> 是像素矩阵）。所有可能的猫图构成数据总体<span class="math inline"> \(X\)</span>，即<span class="math inline"> \(x_i\in
D\subseteq X\)</span>。<span class="math inline">\(X\)</span> 上存在一个概率分布<span class="math inline"> \(p(x)\)</span>，当我们随机采样一张猫的图片时，这张图片有<span class="math inline"> \(p(x)\)</span> 的概率被采样到。对于非猫图<span class="math inline"> \(y,\ p(y)=0\)</span>。我们希望的是能够找到<span class="math inline"> \(p(x)=p(x^{(1)},x^{(2)},\dots)\)</span> 的准确数学形式，其中<span class="math inline"> \(x^{(1)}\)</span> 代表 x 一维展开后的第 1 个像素，依次类推。如果这个目标能够实现，我们就能分析出<span class="math inline"> \(p(x)\)</span> 这个概率函数，对哪些输入<span class="math inline"> \(x\)</span> 能够取到非 0 概率值（即猫图），进而能够随机采样猫图和非猫图。</p>
<p>如果上述描述还不好理解的话，可以想象一个二维坐标系，以原点为圆心的单位圆上的均匀分布。所有猫图都满足<span class="math inline"> \(x^2+y^2\le1\)</span>，在这个单位圆内外分别随机采样，即可得到猫图和非猫图。</p>
<p>但是事实上，这样的目标是很难实现的。在数理统计里，这是个典型的非参数估计问题，在未知分布形式的情况下，没有办法对分布形式和分布参数进行估计。而且很容易想到，这也绝对是一个非常复杂的概率分布。因此直接对<span class="math inline"> \(p(x)\)</span> 建模是不现实的。我们可以曲线救国。假设存在一个隐变量<span class="math inline"> \(z\)</span> 控制着数据<span class="math inline"> \(x\)</span> 的生成。那么根据<span class="math inline"> \(p(x)=\int p(z)p(x|z)dz\)</span> 可以计算得到<span class="math inline"> \(p(x)\)</span>。然而这个边界似然是不可解的，每一项都不知道具体的数学形式，更不要说还要积分。</p>
<p>那么求其次，我们可以用一个分布<span class="math inline"> \(q(x,z)\)</span> 近似联合概率分布<span class="math inline"> \(p(x,z)\)</span>，那么我们的优化目标就是<span class="math inline"> \(KL(p||q)\)</span> 最小。 <span class="math display">\[
\begin{align}
KL(p||q)&amp;=\int\int p(x,z)log\frac{p(x,z)}{q(x,z)}dzdx \\
&amp;=\int p(x)\int p(z|x)log\frac{p(x,z)}{q(x,z)}dzdx \\
&amp;=E_{x\sim p(x)}\int p(z|x)log\frac{p(x)p(z|x)}{q(x,z)}dz \\
&amp;=E_{x\sim p(x)}\int p(z|x)(log\frac{p(z|x)}{q(x,z)}+logp(x))dz
\end{align}
\]</span> 而 <span class="math display">\[
\begin{align}
E_{x\sim p(x)}\int q(z|x)logp(x)dz&amp;=E_{x\sim p(x)}logp(x)\int
q(z|x)dz\\
&amp;=E_{x\sim p(x)}logp(x)
\end{align}
\]</span> 为一个常数，可以略去。令 <span class="math display">\[
\begin{align}
\mathcal{L}&amp;=E_{x\sim p(x)}\int p(z|x)log\frac{p(z|x)}{q(x,z)}dz\\
&amp;=E_{x\sim p(x)}\int p(z|x)log\frac{p(z|x)}{q(z)q(x|z)}dz\\
&amp;=E_{x\sim p(x)}[\int -p(z|x)logq(x|z)  dz+ \int
p(z|x)log\frac{p(z|x)}{q(z)}]dz\\
&amp;=E_{x\sim p(x)}[E_{z\sim p(z|x)}(-log(q(x|z)))+KL(p(z|x)||q(z))]
\end{align}
\]</span> 最小化 KL 与最小化<span class="math inline"> \(\mathcal{L}\)</span> 等价。进而得到了 VAE 的损失函数。只不过与原论文中的符号有些出入，将最后的 KL 项的 p 与 q 调换，即得到了论文中 VAE 的损失函数：
<span class="math display">\[
\mathcal{L}=E_{x\sim p(x)}[E_{z\sim
p(z|x)}(-log(q(x|z)))+KL(q(z|x)||p(z))]
\]</span> 符号的差别是由于论文直接引入的<span class="math inline"> \(q(z|x)\)</span>，而这里引入的是联合概率分布<span class="math inline"> \(q(x,z)\)</span>。</p>
<h3 id="说明">说明</h3>
<p>值得注意的是，VAE 的两个损失函数项并不是割裂的。换而言之，VAE 并不是独立地优化每项损失。这很容易理解，如果 VAE 独立优化第二项损失至最小，<span class="math inline">\(p(z)=q(z|x)\)</span>，那么<span class="math inline"> \(q(z|x)\)</span> 将不具备任何<span class="math inline"> \(x\)</span> 的信息，这显然会使得第一项重构损失很大。同理，如果第一项重构损失很小，就意味着<span class="math inline"> \(q(z|x)\)</span> 包含了过多<span class="math inline"> \(x\)</span> 的信息，与<span class="math inline"> \(p(z)\)</span> 的差异（即 KL 项）就会很大。因此，VAE 是在两项损失的相互作用下，取得一个最优解。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://kexue.fm/archives/5253">变分自编码器（一）：原来是这么一回事
- 科学空间</a></li>
<li><a href="https://kexue.fm/archives/5343">变分自编码器（二）：从贝叶斯观点出发
- 科学空间</a></li>
<li><a href="https://kexue.fm/archives/5383">变分自编码器（三）：这样做为什么能成？
- 科学空间</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>生成模型</category>
      </categories>
      <tags>
        <tag>VAE</tag>
        <tag>生成模型</tag>
      </tags>
  </entry>
  <entry>
    <title>对抗自编码器 AAE</title>
    <url>/blog/2021/10/05/aae/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>对抗自编码器（Adversarial
Autoencoders，AAE）出自 2015 年的《Adversarial
Autoencoders》，其核心想法是将 VAE 与 GAN 结合，来得到一个更好的生成模型。论文首次提出了聚合后验分布（aggregated
posterior），并使用它来优化 VAE。事实上这篇论文是在 CV 上生成的，这里只是简单分析下其提出的聚合后验分布及与 VAE 的比较。
<span id="more"></span></p>
<h2 id="对抗自编码器">对抗自编码器</h2>
<p>符号定义：</p>
<ul>
<li><span class="math inline">\(p(z)\)</span>：隐变量<span class="math inline"> \(z\)</span> 的先验分布</li>
<li><span class="math inline"> \(q(z|x)\)</span>：编码器分布</li>
<li><span class="math inline"> \(p(x|z)\)</span>：解码器分布</li>
<li><span class="math inline"> \(p_d(x)\)</span>：数据<span class="math inline"> \(x\)</span> 分布</li>
<li><span class="math inline"> \(p(x)\)</span>：模型的分布</li>
</ul>
<p>定义聚合分布<span class="math inline"> \(q(z)\)</span> 为<span class="math inline"> \(q(z)=\int_xq(z|x)p_d(x)dx\)</span>，AAE 添加了<span class="math inline"> \(q(z)\)</span> 与<span class="math inline"> \(p(z)\)</span> 进行匹配的正则化项，模型结构如下：</p>
<p><img src="architecture.png"></p>
<p>可以看到与 VAE 最大的区别是，VAE 是从后验分布<span class="math inline"> \(q(z|x)\)</span> 中采样、使用 KL 散度逼近后验分布与先验分布，AAE 是使用对抗网络来逼近<span class="math inline"> \(q(z)\)</span> 与<span class="math inline"> \(p(z)\)</span>。在此基础上，引入了一个对抗网络判断隐藏状态来自随机先验分布<span class="math inline"> \(p(z)\)</span> 的采样，还是来自<span class="math inline"> \(q(z)\)</span>。在编码器<span class="math inline"> \(q(z|x)\)</span> 的选择上，论文提供了三种候选：</p>
<ul>
<li>确定性函数，<span class="math inline">\(z\)</span> 仅与<span class="math inline"> \(x\)</span> 相关。</li>
<li>高斯分布，与分布参数和随机性相关。</li>
<li>通用近似后验，<span class="math inline">\(z\)</span> 与<span class="math inline"> \(x\)</span> 和随机噪声<span class="math inline"> \(\eta\)</span> 相关。通用近似是一种近似分布的方法，确定性函数<span class="math inline"> \(f(x,\eta)\)</span>，<span class="math inline">\(\eta\)</span> 是一个随机噪声，因而其可以看作分布的通用近似。</li>
</ul>
<p>论文中最终使用了确定性函数，个人猜测是更好收敛一些。</p>
<h2 id="section"></h2>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>生成模型</category>
      </categories>
      <tags>
        <tag>VAE</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>EA-VQ-VAE 代码学习（1）</title>
    <url>/blog/2021/07/17/ea-vq-vae-code/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>之前学习 EA-VQ-VAE 的时候发现只读论文本身还是有很多细节问题不太懂，而 EA-VQ-VAE 的代码开源在 <a href="https://github.com/microsoft/EA-VQ-VAE">github</a> 上。今天正好通过学习代码更深层地理解一下这个模型以及基础的 VQ-VAE 模型。</p>
<span id="more"></span>
<h2 id="目录结构">目录结构</h2>
<p>代码的目录结构如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">│  README.md</span><br><span class="line">|  LICENSE</span><br><span class="line">├─data</span><br><span class="line">│      get_atomic_data.sh</span><br><span class="line">│      get_event2mind_data.sh</span><br><span class="line">│      preprocess-atomic.py</span><br><span class="line">│      preprocess-event2mind.py</span><br><span class="line">├─estimator</span><br><span class="line">│      model.py</span><br><span class="line">│      run.py</span><br><span class="line">├─generator</span><br><span class="line">│      beam.py</span><br><span class="line">│      model.py</span><br><span class="line">│      run.py</span><br><span class="line">└─vq-vae</span><br><span class="line">        gpt2.py</span><br><span class="line">        model.py</span><br><span class="line">        run.py</span><br></pre></td></tr></tbody></table></figure>
<p>可以看到，整个代码目录结构还是比较清晰的四部分：</p>
<ul>
<li>data/：用以数据的获取和预处理</li>
<li> estimator/：估计先验分布的模型</li>
<li> generator/：推理阶段生成推理文本（光束搜索等）</li>
<li>vq-vae/：vq-vae 的模型定义：包含 codebook、编码器、解码器等</li>
</ul>
<p>这次先介绍最为核心的 vq-vae 模型，处在 vq-vae/model.py。剩下的部分后续有时间再进行分享。</p>
<h2 id="vq-vae">VQ-VAE</h2>
<h3 id="model.py">model.py</h3>
<p>首先是 CodeBook。codebook 在 EA-VQ-VAE 充当了隐变量表的角色，保存了一张由 K 个 D 维隐变量组成的<span class="math inline"> \(R^{K*D}\)</span>。CodeBook 类代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CodeBook</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_embeddings, embedding_dim, commitment_cost</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CodeBook, self).__init__()  </span><br><span class="line">        self._embedding_dim = embedding_dim</span><br><span class="line">        self._num_embeddings = num_embeddings     </span><br><span class="line">        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)      </span><br><span class="line">        self._commitment_cost = commitment_cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="comment"># Calculate distances</span></span><br><span class="line">        distances = (torch.<span class="built_in">sum</span>(inputs**<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">                    + torch.<span class="built_in">sum</span>(self._embedding.weight**<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">                    - <span class="number">2</span> * torch.matmul(inputs, self._embedding.weight.t()))</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Encoding</span></span><br><span class="line">        encoding_indices = torch.argmin(distances, dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        encodings = torch.zeros(encoding_indices.shape[<span class="number">0</span>], self._num_embeddings).cuda()</span><br><span class="line">        encodings.scatter_(<span class="number">1</span>, encoding_indices, <span class="number">1</span>) <span class="comment"># 离散隐变量索引 [batch_size,num_embeddings]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Quantize and unflatten</span></span><br><span class="line">        quantized = torch.matmul(encodings, self._embedding.weight) <span class="comment">## 乘法获得隐变量</span></span><br><span class="line">        <span class="comment"># 整个隐变量的获取方法有点复杂了，argmin之后直接查询embedding即可，无需手动操作。这里这样处理是为了后续</span></span><br><span class="line">        <span class="comment"># 还要计算perplexity</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loss</span></span><br><span class="line">        <span class="comment"># detach()从计算图中脱离，达到stop gradient的目的</span></span><br><span class="line">        e_latent_loss = torch.mean((quantized.detach() - inputs)**<span class="number">2</span>) </span><br><span class="line">        q_latent_loss = torch.mean((quantized - inputs.detach())**<span class="number">2</span>)</span><br><span class="line">        loss = q_latent_loss + self._commitment_cost * e_latent_loss</span><br><span class="line">        </span><br><span class="line">        quantized = inputs + (quantized - inputs).detach()</span><br><span class="line">        avg_probs = torch.mean(encodings, dim=<span class="number">0</span>)</span><br><span class="line">        perplexity = torch.exp(-torch.<span class="built_in">sum</span>(avg_probs * torch.log(avg_probs + <span class="number">1e-10</span>)))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># convert quantized from BHWC -&gt; BCHW</span></span><br><span class="line">        <span class="keyword">return</span> loss, quantized, perplexity, encodings</span><br></pre></td></tr></tbody></table></figure>
<p>整个代码是比较清晰的。在初始化中根据传入参数初始化嵌入空间，并保存了 commitment
cost。commitment cost 指的是 VQ-VAE 损失函数的第三项的权重<span class="math inline"> \(\beta\)</span>。由论文可知，CodeBook 的前向过程应该是输入编码器输出<span class="math inline"> \(z_e(x)\)</span>，输出最近的隐变量<span class="math inline"> \(z\)</span>。那么代码中 inputs 的 shape 应该为 [batch_size，embedding_dim]，进而距离的计算过程就很自然了。其他见代码的注释部分。</p>
<p>接下来是 seq2seq 模型：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        Build Seqence-to-Sequence.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * `encoder`- encoder of seq2seq model. e.g. 2-layer transformer</span></span><br><span class="line"><span class="string">        * `decoder`- decoder of seq2seq model. e.g. GPT2</span></span><br><span class="line"><span class="string">        * `config`- configuration of encoder model. </span></span><br><span class="line"><span class="string">        * `args`- arguments.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder,decoder,config,args</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder=decoder</span><br><span class="line">        self.config=config</span><br><span class="line">        self.args=args</span><br><span class="line">        </span><br><span class="line">        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=<span class="literal">False</span>)      </span><br><span class="line">        self.codebook = CodeBook(args.z_size, config.n_embd,<span class="number">0.25</span>)  </span><br><span class="line">        self.codebook._embedding.weight.data.normal_(mean=<span class="number">0</span>,std=<span class="number">0.1</span>)</span><br><span class="line">        self.lsm = nn.LogSoftmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.lm_head.weight=self.decoder.wte.weight     </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, event_ids,target_ids</span>):</span>   </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            Forward the VQ-VAE model.</span></span><br><span class="line"><span class="string">            Parameters:</span></span><br><span class="line"><span class="string">            * `event_ids`- event ids of examples</span></span><br><span class="line"><span class="string">            * `target_ids`- target ids of examples</span></span><br><span class="line"><span class="string">        """</span>  </span><br><span class="line">        input_ids=torch.cat((event_ids,target_ids),-<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#obtain hidden of event+target by encoder</span></span><br><span class="line">        hidden_xy=self.encoder(input_ids,special=<span class="literal">True</span>)[<span class="number">0</span>][:,-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#obtain latent variable z by coodebook</span></span><br><span class="line">        vae_loss, z, perplexity, encoding=self.codebook(hidden_xy)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#obtain hiddens of target </span></span><br><span class="line">        transformer_outputs=self.decoder(input_ids,z=z)</span><br><span class="line">        hidden_states = transformer_outputs[<span class="number">0</span>][:,-target_ids.size(<span class="number">1</span>):]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#calculate loss</span></span><br><span class="line">        lm_logits = self.lm_head(hidden_states+z[:,<span class="literal">None</span>,:])</span><br><span class="line">        <span class="comment"># Shift so that tokens &lt; n predict n</span></span><br><span class="line">        active_loss = target_ids[..., <span class="number">1</span>:].ne(<span class="number">0</span>).view(-<span class="number">1</span>) == <span class="number">1</span> <span class="comment"># 将推理文本展平并得到非0位置的索引，用以计算loss</span></span><br><span class="line">        shift_logits = lm_logits[..., :-<span class="number">1</span>, :].contiguous() <span class="comment"># 去除末尾的EOS</span></span><br><span class="line">        shift_labels = target_ids[..., <span class="number">1</span>:].contiguous() <span class="comment"># </span></span><br><span class="line">        <span class="comment"># Flatten the tokens</span></span><br><span class="line">        loss_fct = CrossEntropyLoss(ignore_index=-<span class="number">1</span>)</span><br><span class="line">        loss = loss_fct(shift_logits.view(-<span class="number">1</span>, shift_logits.size(-<span class="number">1</span>))[active_loss],</span><br><span class="line">                        shift_labels.view(-<span class="number">1</span>)[active_loss])</span><br><span class="line"></span><br><span class="line">        outputs = (loss,vae_loss,perplexity),loss*active_loss.<span class="built_in">sum</span>(),active_loss.<span class="built_in">sum</span>(),encoding</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></tbody></table></figure>
<p>init 方法比较简单，只是保存参数和新建 codebook。前向过程也比较简单：训练阶段，seq2seq 的输入是事件和推理文本的拼接，然后进行编码和解码（这里编码器为 2 层 Transformer，解码器为预训练的 GPT 模型）。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>文本生成</category>
      </categories>
      <tags>
        <tag>VQ-VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>推理文本生成 | EA-VQ-VAE</title>
    <url>/blog/2021/07/16/ea-vq-vae/</url>
    <content><![CDATA[<h2 id="题外话">题外话</h2>
<p>今天是我的生日，希望能看到这篇博客的你天天开心。如果今天还恰好是你的生日，希望你生日快乐！</p>
<h2 id="简介">简介</h2>
<p>EA-VQ-VAE 是微软团队于 2020 年发表的《Evidence-Aware Inferential Text
Generation with Vector Quantised Variational
AutoEncoder》中提出的模型，该文发表在 ACL 上。该文的主要工作是利用 VQ-VAE 进行推理文本生成。推理文本生成定义为，给定一个事件（例如 “A 偷看了 B 的日记”），从多个维度对该事件进行推断（“A 的心理状态”，“A 的目的”）。而 EA-VQ-VAE 中的 EA（Evidence-Aware）指的是利用证据来进行推理文本生成。</p>
<span id="more"></span>
<h2 id="实现方法">实现方法</h2>
<p>下图展示了整个模型的流程：给定事件<span class="math inline"> \(x\)</span> 后，经过 VQ-VAE 将其映射为离散的隐变量<span class="math inline"> \(z\)</span>，根据事件<span class="math inline"> \(x\)</span> 从文本语料中检索证据，再一起投喂给解码器输出最终的推理文本<span class="math inline"> \(y\)</span>。下面逐项介绍模型的细节。</p>
<p><img src="model.png"></p>
<h3 id="vq-vae">VQ-VAE</h3>
<p>VQ-VAE 的详细介绍可以看我的上一篇博客。论文使用的 VQ-VAE 与标准的 VQ-VAE 最主要的区别在于，普通的 VQ-VAE 生成是数据<span class="math inline"> \(x\)</span>，而在推理文本生成任务中，生成的是以符合事件<span class="math inline"> \(x\)</span> 的推理文本<span class="math inline"> \(y\)</span>，换而言之，这是一个<strong>条件模型</strong>，叫它 VQ-CVAE 可能更恰当一点。基于此，下面所述的后验分布<span class="math inline"> \(q_\phi(z|x,y)\)</span> 与先验分布<span class="math inline"> \(p_\theta(z|x)\)</span> 均与标准的 VQ-VAE 有所不同。</p>
<p>本文使用的 VQ-VAE 分为以下三个部分：</p>
<ul>
<li>codebook：对应 VQ-VAE 中的隐变量嵌入空间，只是换了个名字，同样是一张<span class="math inline"> \(R^{k*d}\)</span> 的表，由<span class="math inline"> \(k\)</span> 个维度为<span class="math inline"> \(d\)</span> 的隐变量组成</li>
<li>后验分布<span class="math inline"> \(q_\phi(z|x,y)\)</span>：同样是一个独热分布，使用最近邻算法将编码器输出<span class="math inline"> \(h_(x,y)\)</span> 映射到最近的隐变量<span class="math inline"> \(z'\)</span></li>
<li> 先验分布<span class="math inline"> \(p_\theta(z|x)\)</span>：先利用预训练的语言模型（例如 RoBERTa）将事件编码为隐藏状态<span class="math inline"> \(h\)</span>，，再将其映射为 k 个类别，即<span class="math inline"> \(p_\theta(z|x)=softmax(hW_k)\)</span></li>
</ul>
<h3 id="证据的检索与选择">证据的检索与选择</h3>
<p>去除事件中的停用词后，在大规模文本语料中使用 Elastic
Search 引擎检索事件，并选取前 K 个得分最高的句子。论文使用的语料库基于 BookCorpus，由一万多篇故事书组成，因为作者认为故事中会对事件的起因和结果介绍地较为清晰。</p>
<p>证据的选择与隐变量类似，在训练阶段和推理阶段有着不同的逻辑。在训练阶段，事件<span class="math inline"> \(x\)</span> 与推理文本<span class="math inline"> \(y\)</span> 均已知，例如给定事件 “A 读了 B 的日记”，与推理文本 “A 感到很愧疚”，那么证据 “A 偷了 B 的日记” 就比 “B 把日记给 A 看” 更合理，此时我们想要建模的就是<span class="math inline"> \(q(c|x,y)\)</span>（c 代表事件上下文，即证据）与<span class="math inline"> \(p(c|x)\)</span>。考虑到已经有一个后验分布<span class="math inline"> \(q_\phi(z|x,y)\)</span>，那么我们可以直接利用隐变量来完成证据的选择，即建模<span class="math inline"> \(p(c|z)\)</span>, 而不是再引入一个复杂的神经网络。对于一组证据（<span class="math inline">\(c_\phi\)</span> 代表填充的空证据）<span class="math inline">\(\{c_1,c_2,\dots,c_K,c_\phi\}\)</span>，使用 Transformer 将其编码为向量<span class="math inline"> \(\{h_{c_1},h_{c_2},\dots,h_{c_K},h_{c_\phi}\}\)</span>。<span class="math inline">\(p_s(c|z)\)</span> 与<span class="math inline"> \(q_\phi(z|x,y)\)</span> 类似，也是一个独热分布，再通过最近邻算法选取最近的证据，即：
<span class="math display">\[
p_s(c_k|z)=
\begin{cases}
1 &amp;if\ k=\arg\min_j||h_{c_j}-z||_2 \\
0 &amp;otherwise
\end{cases}
\]</span></p>
<p><span class="math display">\[
c_z=c_k\ where\ k=\arg\min_j||h_{c_j}-z||_2
\]</span></p>
<p>值得注意的是，作者没有使用注意力机制得到的 “软” 分布，而是借鉴 VQ-VAE，采用了一种独热分布将<span class="math inline"> \(z\)</span> 映射到最近的<span class="math inline"> \(c\)</span>。这样的优点是一定程度上降低了学习的难度，由于<span class="math inline"> \(z\)</span> 与<span class="math inline"> \(c\)</span> 处在同一个语义空间，解码器利用起来的效率会更高。而且这样做也更为统一。但我总觉得注意力机制得到的结果会更好一点，论文里没有进行比较属实有点伤。</p>
<h3 id="解码器">解码器</h3>
<p>解码器使用的是预训练的 GPT-2，是一个基于 Transformer 的语言模型。这里就不多赘述了，有兴趣的小伙伴可以去了解一下 GPT 家族。</p>
<h2 id="训练过程">训练过程</h2>
<p>首先单独训练 VQ-VAE 与 codebook，再训练基于后验分布<span class="math inline"> \(q_\phi(z|x,y)\)</span> 的证据感知解码器。</p>
<h3 id="vq-vae-1">VQ-VAE</h3>
<p>首先只根据隐变量<span class="math inline"> \(z\)</span> 重构推理文本<span class="math inline"> \(y\)</span>，损失函数与 VQ-VAE 损失函数类似： <span class="math display">\[
loss_{rec}=-logp(y|x,h_{(x,y)}+sg[z-h_{(x,y)}])+||sg[h_{(x,y)}]-z||_2^2+\beta||h_{(x,y)}-sg[z]||_2^2
\]</span></p>
<p>真实的先验分布可以使用频率近似（<span class="math inline">\(N_{(x)}\)</span> 代表包含<span class="math inline"> \(x\)</span> 事件的数据数量）： <span class="math display">\[
p(z|x)=\sum_{(x,y_i)\in D}\frac{q_\phi(z|x,y_i)}{N_{(x)}}
\]</span> 通过 KL 散度来优化先验分布<span class="math inline"> \(p_\theta(z|x)\)</span>: <span class="math display">\[
loss_{prior}=KL(p(z|x)||p_\theta(z|x))
\]</span></p>
<p>不过这里为什么不像 CVAE 一样，直接优化后验分布与先验分布间的 KL 散度，暂时还不是很理解。</p>
<h3 id="证据感知解码器">证据感知解码器</h3>
<p>这一部分通过最大化边际似然进行训练： <span class="math display">\[
\begin{align}
logp(y|x)&amp;=E_{z\sim q_\phi}[\sum_{c\in C}logp_m(y|x,c)p_s(c|z)]\\
&amp;=log(p_m(y|x,c_{z'}))+logp_s(c_{z'}|z')
\end{align}
\]</span>
然而，由于真实的证据是未知的，直接优化上述似然函数可能得不到正确结果。具体而言，与<span class="math inline"> \(z'\)</span> 最近的<span class="math inline"> \(c_{z'}\)</span> 不一定就是真实有用的证据，如果我们已知真实的证据标签<span class="math inline"> \(c\)</span>，损失函数中应该还有一项是<span class="math inline"> \(||c-c_{z'}||_2\)</span>。为解决这个问题，原论文采取了强化学习的方法：
<span class="math display">\[
R=\delta(p_m(y|x,c_{z'})-p_m(y|x,c_r))
\]</span></p>
<p><span class="math display">\[
\begin{align}
logp(y|x)&amp;=logp_m(y|x,c_{z'})+Rlogp_s(c_{z'}|z')\\
&amp;=logp_m(y|x,c_{z'})-R|||h_{c_{z'}}-z'||_2^2
\end{align}
\]</span> 其中，<span class="math inline">\(\delta(x)\)</span> 当 x 大于 0 时为 1，否则为 - 1。<span class="math inline">\(c_r\)</span> 为随机选取的与<span class="math inline"> \(c_{z'}\)</span> 不同的证据。这样设计的原因是，正确的证据相较于其他证据应该能够提高生成正确推理文本的概率。当<span class="math inline"> \(R\)</span> 为正时，<span class="math inline">\(logp(y|x)\)</span> 会更大，进而激励模型选择正确的证据。</p>
<h2 id="个案研究">个案研究</h2>
<p><img src="case.png"></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>文本生成</category>
      </categories>
      <tags>
        <tag>VQ-VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP 必备网站 Hugging Face</title>
    <url>/blog/2021/09/27/huggingface/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来分享一个网站吧，<a href="https://huggingface.co/">Hugging
Face</a>，最大的 NLP 社区，提供了数以千计的预训练模型，涵盖一百余种语言、覆盖几乎所有常见的 NLP 任务。其提供的 transformers 框架提供了简洁高效的 api，可以在无需处理模型细节的前提下，快速进行推理、微调。Hugging
Face 至今在 github 上已有超过 5 万个 star，可见其影响力。</p>
<span id="more"></span>
<h2 id="为什么需要hugging-face">为什么需要 Hugging Face</h2>
<p><img src="intro.png"></p>
<p>Hugging
Face 不仅仅是若干数据集、预训练模型的资源整合，在此基础上，它还拥有如下特性：</p>
<ul>
<li>开箱即用：对于常见的 NLP 任务，很容易找到对应的预训练模型并进行实验，无需过度关注模型的细节。</li>
<li>多后端支持：Transformers 支持 Pytorch、Jax、Tensorflow 三种框架，无需再为框架微调苦恼。</li>
<li>可定制性：高效封装的同时，Transformers 支持魔改定制模型，模型文件可以单独使用，方便快速实验。</li>
</ul>
<p>鉴于现在 NLP 方向的研究、工程基本都是大规模预训练模型相关，Hugging
Face 的重要性就一目了然了。如果你是学生党，Hugging
Face 能让你在各类 NLP 比赛中快速使用预训练模型进行实验。如果你已经工作，Hugging
Face 也能帮你减少业务问题上的试错成本，快速把任务跑起来。</p>
<h2 id="有用的链接">有用的链接</h2>
<ol type="1">
<li><a href="https://github.com/huggingface/transformers">github 链接</a>，可以对其使用方法、支持的模型有个快速的认识。</li>
<li><a href="https://huggingface.co/">Hugging Face
官网</a>，试试推理 api、看一看文档。</li>
<li><a href="https://huggingface.co/course/chapter0?fw=pt">Hugging Face
Course</a>，Hugging
Face 出品的官方课程，目前更新了前四章，基本上是 step-by-step 的教你从推理到微调任务如何构建和完成。</li>
</ol>
<h2 id="总结">总结</h2>
<p>没错，这么短的一篇博客还有总结。今天刚刚看完 Hugging Face
的前四章课程，感觉学到了很多。早点知道也不会走一些弯路了，一起加油吧！</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>Hugging Face</tag>
        <tag>工程</tag>
      </tags>
  </entry>
  <entry>
    <title>语言模型</title>
    <url>/blog/2021/07/22/language-model/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>语言模型是自然语言处理的一个重要部分，在很多生成式任务中，都离不开语言模型。今天我想梳理一下我所理解的语言模型及其发展。</p>
<span id="more"></span>
<h2 id="定义">定义</h2>
<p>对于一串语言序列<span class="math inline"> \(w_1w_2\dots
w_n\)</span>，语言模型试图分析其出现的概率，即<span class="math inline"> \(P(w_1,w_2,\dots,w_n)\)</span>​​​​。进而，可以通过概率大小判断文本是否合理。例如句子 “学生们打开了书” 的概率应该比 “学生们打开了玛卡巴卡” 高得多，即更像是人说的话。</p>
<p>在之前的 VQ-VAE 中，我们提到过自回归模型，按照自回归的思路，如果第 n 个单词只与前 n-1 个单词相关，那么句子的概率可以转化为如下形式：
<span class="math display">\[
P(w_1,w_2,\dots,w_n)=\prod_t^nP(w_t|w_{t-1:1})
\]</span> 那么怎么求解右侧的式子呢？</p>
<h2 id="n-gram">N-gram</h2>
<p>首先要提到的是 N-gram 模型。为了解决上面这个问题，N-gram 模型引入了马尔科夫假设，认为某一个词只与它之前的<span class="math inline"> \(N-1\)</span> 个词有关。以 4-gram 为例，即每个词只与其之前的 3 个词有关，即：
<span class="math display">\[
P(w_n|w_{n-1:1})=P(w_n|w_{n-1},w_{n-2},w_{n-3})
\]</span>
换而言之，只要在大规模语料中进行频数的统计，那么就可以得到上述概率的估计：
<span class="math display">\[
P(w_n|w_{n-1},w_{n-2},w_{n-3})=\frac{C(w_n,w_{n-1},w_{n-2},w_{n-3})}{C(w_{n-1},w_{n-2},w_{n-3})}
\]</span> 进而整个句子的概率可以计算如下： <span class="math display">\[
P(w_1,w_2,\dots,w_n)=\prod_t^nP(w_t|w_{t-1},w_{t-2},w_{t-3})
\]</span> 上述的方法虽然简单直接，但是有以下缺点：</p>
<ul>
<li>稀疏问题：一些片段可能没有在语料中出现，计数为 0，在概率连乘之下整句概率变为 0。</li>
<li>存储问题：随着 n 的增大，存储量指数级上升，而 n 过小时模型性能又会很差。</li>
</ul>
<h2 id="基于窗口的神经网络">基于窗口的神经网络</h2>
<p>在 N-gram 的基础上，使用神经网络来计算条件概率。同样以 4-gram 为例，计算用公式表达如下:
<span class="math display">\[
\begin{align}
P(w_n|w_{n-1:1})&amp;=P(w_n|w_{n-1},w_{n-2},w_{n-3})\\
&amp;=softmax(W[w_{n-1};w_{n-2};w_{n-3}])
\end{align}
\]</span>
思路非常简单，即将前 N-1 个词输入到神经网络，由神经网络计算得到第 N 个词的概率分布。这样做解决了 N-gram 的稀疏问题与存储问题，但也存在一些问题：</p>
<ul>
<li>缺少参数共享：以上述公式为例，<span class="math inline">\(W\)</span>​​​中可以分为三部分，分别处理前三个词。然而，词向量的处理逻辑应该是相似的（因为他们都是同样的方法训练出来的）。</li>
<li>需要变化窗口大小时，矩阵 W 的形状也需要变化，进而需要重新训练。</li>
</ul>
<h2 id="循环神经网络rnn">循环神经网络 RNN</h2>
<p>RNN 的思路同样也很直接，使用同一个矩阵<span class="math inline"> \(W\)</span> 来处理词向量，并使用一个隐藏状态来记录已处理的信息（换而言之，就无需马尔科夫假设）。RNN 的公式如下:
<span class="math display">\[
h_t=\sigma(W^{(hh)}h_{t-1}+W^{(hx)}x_t)
\]</span></p>
<p><span class="math display">\[
\hat{y_t}=softmax(W^{(S)}h_t)
\]</span></p>
<p>其中，<span class="math inline">\(\sigma\)</span> 是激活函数，<span class="math inline">\(h_t\)</span> 是 t 时刻的隐藏状态，<span class="math inline">\(W\)</span> 是参数矩阵。</p>
<p>RNN 有以下优点：</p>
<ul>
<li>能够处理任意长度的序列。</li>
<li>没有进行马尔科夫假设，理论上每一时刻模型都知道之前时刻的全部信息。</li>
</ul>
<p>但也有以下缺点：</p>
<ul>
<li>会出现梯度消失和梯度爆炸问题。</li>
<li>不支持并行化，计算较慢。（可以说是自回归模型的通病）</li>
</ul>
<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/63397627">CS224N 笔记 (六)：语言模型与 RNN
- 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语言模型</category>
      </categories>
      <tags>
        <tag>语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title>循环神经网络 RNN 及其变体 GRU、LSTM</title>
    <url>/blog/2021/07/22/rnns/</url>
    <content><![CDATA[<h2 id="序言">序言</h2>
<p>同样，借着复习面试，把 RNN 家族再梳理回顾一下，包含 RNN、GRU、LSTM。
<span id="more"></span></p>
<h2 id="循环神经网络rnn">循环神经网络 RNN</h2>
<h3 id="模型结构">模型结构</h3>
<p><img src="rnn.png"></p>
<p>RNN 的结构如上图所示，其核心思想是使用同一套参数来更新状态<span class="math inline"> \(s\)</span> 与计算输出<span class="math inline"> \(o\)</span>，箭头右侧是按时序展开的模型结构图。可以看到，RNN 仅使用了一个状态<span class="math inline"> \(s\)</span>​来保存序列信息，共有三个参数矩阵。这一部分公式化描述如下:
<span class="math display">\[
s_t=f(Ux_t+Ws_{t-1})
\]</span></p>
<p><span class="math display">\[
o_t=g(Vs_t)
\]</span></p>
<p>其中，<span class="math inline">\(f\)</span> 与<span class="math inline"> \(g\)</span>​均为激活函数，激活函数可选的有 sigmoid，tanh，relu 等（下面会分析）。</p>
<p>RNN 有以下缺陷：</p>
<ul>
<li>容易发生梯度消失和梯度爆炸现象（由于导数连乘）。</li>
<li>难以捕捉长距离的依赖。</li>
</ul>
<p>在其中，梯度消失相对于梯度爆炸要更为严重，因为梯度爆炸是可以观测到的（NAN），梯度消失则难以直接观测。梯度爆炸问题很容易解决，可以通过梯度裁剪的方法进行解决。</p>
<h3 id="梯度消失和梯度爆炸">梯度消失和梯度爆炸</h3>
<p>梯度消失和爆炸的解决方法：</p>
<ul>
<li>梯度的剪切以及正则化（常见的是 l1 正则和 l2 正则）。</li>
<li>relu、elu 等激活函数。（梯度消失）</li>
<li>批标准化（<strong>Batch Normalization</strong>）。</li>
<li>残差结构（将映射 F (x) 改为 F (x)+x，使用 relu 激活函数的 F 在 x&lt;0 时能够无损传播梯度，保证了深层网络的性能）。</li>
<li>LSTM、GRU 等结构。</li>
</ul>
<h4 id="批标准化-batch-normalization">批标准化 Batch Normalization</h4>
<p>Batch
Normalization 是一种常用于 CNN 的正则化方法，可以分为两个步骤：</p>
<p>（1）标准化：对 batch 的数据求均值与标准差，将数据标准化到标准正态分布</p>
<p>（2）进行放缩与平移</p>
<p>整个过程类似于 VAE 的重参数化，先获得一个正态分布的变量，再进行放缩平移，达到从任意正态分布中取样的效果。</p>
<p>也就是说，batch normlization
假设每个 batch 的数据服从一个正态分布（参数 γ 和 β 学习得来，即通过 batch 数据计算得来），先将数据标准化，再放缩与平移，使得数据 “看起来” 是从这个正态分布中取样而来的。</p>
<p>在预测阶段，所有参数的取值是固定的，对 BN 层而言，意味着 μ、σ、γ、β 都是固定值。γ 和 β 比较好理解，随着训练结束，两者最终收敛，预测阶段使用训练结束时的值即可。</p>
<p>对于 μ 和 σ，在训练阶段，它们为当前 mini
batch 的统计量，随着输入 batch 的不同，μ 和 σ 一直在变化。在预测阶段，输入数据可能只有 1 条，该使用哪个 μ 和 σ，或者说，每个 BN 层的 μ 和 σ 该如何取值？可以采用训练收敛最后几批 mini
batch 的 μ 和 σ 的期望，作为预测阶段的 μ 和 σ。</p>
<h3 id="层标准化-layer-normalization">层标准化 Layer Normalization</h3>
<p>Batch
Normalization 是在 Batch 的方向上进行 Normalization。这种方法在 NLP 中不是很适合。由于文本序列的长度可变性，一个 batch 中的数据往往长度不同，进而对每个位置进行标准化不是很合适。</p>
<p>而 Layer
Normalization 则是在序列的方向上进行 Normalization。这使得它可以处理变长序列。</p>
<h3 id="激活函数">激活函数</h3>
<p>对于激活函数而言，sigmoid 的最大梯度为 0.25，因此很容易发生梯度消失现象，而 tanh 虽然最大梯度为 1，但也只有 0 处取得，也容易
发生梯度消失。因此 RNN 常使用 relu 作为激活函数。relu 的梯度非 0 即 1，这能够缓解梯度消失现象，但也有一定的问题：1.
容易发生梯度爆炸。（梯度恒为 1 时）2.
负数部分梯度恒为 0，部分神经元无法激活。elu 能够缓解 relu 的 0 梯度的问题，但是由于加入了幂运算，会更慢一点。</p>
<h2 id="门控循环单元gru">门控循环单元 GRU</h2>
<h3 id="模型结构-1">模型结构</h3>
<p>GRU 的思想是在 RNN 的基础上，引入门控信号来缓解 RNN 存在的梯度消失问题。模型结构如下：</p>
<p><img src="gru.png"></p>
<p>公式化描述如下（公式中的<span class="math inline"> \(\odot\)</span> 代表哈达玛积，即同型矩阵间逐元素乘法）：</p>
<p>首先根据输入<span class="math inline"> \(x_t\)</span> 与上一时刻隐藏状态<span class="math inline"> \(h_{t-1}\)</span> 计算得到两个门控状态<span class="math inline"> \(z_t\)</span> 与<span class="math inline"> \(r_t\)</span>​，假设<span class="math inline"> \(h_t\in \mathbb R^H\)</span>： <span class="math display">\[
z_t=sigmoid(W_zx_t+U_zh_{t-1})\in \mathbb R^{H}
\]</span></p>
<p><span class="math display">\[
r_t=sigmoid(W_rx_t+U_rh_{t-1})\in \mathbb R^{H}
\]</span></p>
<p>之后，使用重置门计算得到一个新的隐藏状态（即图中的<span class="math inline"> \(h’\)</span>）： <span class="math display">\[
\tilde h_t=tanh(Wx_t+U(r_t\odot h_{t-1}))\in \mathbb R^{H}
\]</span> 再使用更新门<span class="math inline"> \(z_t\)</span> 更新隐藏状态： <span class="math display">\[
h_t=(1-z)\odot h_{t-1}+z\odot \tilde h_t\in \mathbb R^{H}
\]</span></p>
<h2 id="长短期记忆网络lstm">长短期记忆网络 LSTM</h2>
<h3 id="模型结构-2">模型结构</h3>
<p>LSTM 的思想是在 RNN 的基础上，加入一个不易被改变的新状态<span class="math inline"> \(c_t\)</span>​​，代表的是 0-t 时刻的全局信息。而<span class="math inline"> \(h_t\)</span>​代表的是在 0~t-1 时刻全局信息的影响下，<span class="math inline">\(t\)</span> 时刻的信息。换而言之，<span class="math inline">\(c_t\)</span> 变化的很慢，而<span class="math inline"> \(h_t\)</span> 变化的很快。</p>
<p><img src="lstm.png"></p>
<p>公式化描述如下：</p>
<p>首先计算得到三个门控状态（分别对应图中的<span class="math inline"> \(z^i,z^f,z^o\)</span>）： <span class="math display">\[
i_t=sigmoid(W_ix_t+U_ih_{t-1})\in \mathbb R^{H}
\]</span></p>
<p><span class="math display">\[
f_t=sigmoid(W_fx_t+U_fh_{t-1})\in \mathbb R^{H}
\]</span></p>
<p><span class="math display">\[
o_t=sigmoid(W_ox_t+U_oh_{t-1})\in \mathbb R^{H}
\]</span></p>
<p>以及一个与当前输入密切相关的向量（对应图中的<span class="math inline"> \(z\)</span>） <span class="math display">\[
\tilde c_t=tanh(W_zx_t+U_zh_{t-1})
\]</span> 接着，更新两种状态： <span class="math display">\[
c_t=f_t\odot c_{t-1}+i_t\odot \tilde c_t
\]</span></p>
<p><span class="math display">\[
h_t=o_t\odot tanh(c_t)
\]</span></p>
<p>其中，<span class="math inline">\(i_t.f_t,o_t\)</span> 分别代表信息、遗忘、输出门控。信息和遗忘门控负责 cell
state 的更新，输出门控负责 hidden
state 的更新。具体而言，LSTM 可以简单分为以下三个阶段：</p>
<ul>
<li>遗忘阶段，根据遗忘门控，忘记上一个 cell state 的部分信息。</li>
<li>记忆阶段，根据信息门控，将输入信息进行选择记忆。</li>
<li>输出阶段，根据输出门控，输出最终的状态。</li>
</ul>
<h3 id="lstm-vs-gru">LSTM VS GRU</h3>
<p>本质上，LSTM 和 GRU 都是通过引入门控信号来解决 RNN 的梯度消失问题。在实现方法上，GRU 相对于 LSTM 要更为简单。GRU 抛弃了 LSTM 中的 hidden
state（GRU 中的 hidden state 实际上是 LSTM 中的 cell
state），因为 LSTM 中的<span class="math inline"> \(h_t\)</span> 只是想保存当前时刻的信息，这一部分已经包含到 GRU 中的<span class="math inline"> \(\tilde h_t\)</span> 中了。cell
state 中的之前的全局信息与当前时刻的信息应当是一个此消彼长的状态，GRU 因此直接使用一个门控信号<span class="math inline"> \(z_t\)</span> 同时控制了遗忘和更新。</p>
<p>在参数上，GRU 有着比 LSTM 更少的参数，收敛速度更快，并且与 LSTM 有着差不多的性能表现，因此实际工程中多使用 GRU。</p>
<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/68579467">深度学习之 3—— 梯度爆炸与梯度消失
- 知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/32481747">人人都能看懂的 GRU -
知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/32085405">人人都能看懂的 LSTM -
知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/55386469#:~:text=这里归纳一下%20LSTM%20与%20GRU%20的区别：%20首先，,LSTM%20选择暴露部分信息（%20才是真正的输出，%20只是作为信息载体，并不输出">RNN
vs LSTM vs GRU -- 该选哪个？ - 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>循环神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>GRU</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>序列到序列模型</title>
    <url>/blog/2021/07/22/seq2seq/</url>
    <content><![CDATA[<h2 id="序言">序言</h2>
<p>序列到序列模型（sequence to sequence,
seq2seq）是自然语言处理中的一个重要模型，在机器翻译、对话系统等多个领域都有着广泛的应用。今天借着准备面试的机会，将这一部分知识整理出一篇博客。</p>
<span id="more"></span>
<h2 id="seq2seq">seq2seq</h2>
<p><img src="basemodel.png"></p>
<p>seq2seq 模型常用语序列间的转化任务，其结构如上图所示，主要由两部分组成：</p>
<ul>
<li>编码器，常见为循环神经网络，用以将输入序列编码为固定维度的向量（即最后时刻编码器的隐藏状态），进而投喂给解码器进行解码。</li>
<li>解码器，同样常见为循环神经网络，用以根据向量输出最终序列。可以看做一个条件语言模型，“条件” 即为输入序列。因此，可以使用预训练的语言模型初始化权重，再进行 fine-tune。</li>
</ul>
<p>在训练阶段，解码器的输出仅用于计算损失，解码器的输入是编码器得到的上下文状态向量 (最后一个时间步的隐藏状态) 和目标序列当前的单词。换而言之，训练时，解码器的输出一定是与目标序列等长的。</p>
<p>在推理阶段，解码器每一个时间步的输出是下一个时间步的输入。可以通过限制输出序列的最大长度或者在输出结束标志后停止。对于 batch 的数据，往往使用限制最大长度，再删去结束标志之后的部分。</p>
<p>seq2seq 虽然简单有效，但存在以下的缺点：</p>
<ul>
<li>输入序列过长时，固定长度的向量无法存储全部的信息，进而造成信息丢失。</li>
<li>贪婪解码问题，下面会提到。</li>
</ul>
<h2 id="贪婪解码问题">贪婪解码问题</h2>
<p>对于 seq2seq 模型，我们希望得到概率最大的输出序列，即建模的是<span class="math inline"> \(\arg\max_YP(Y|X)\)</span>（<span class="math inline">\(X\)</span> 为输入序列，<span class="math inline">\(Y\)</span> 为输出序列）。然而事实上，解码器每一步求解的是<span class="math inline"> \(\arg\max_{y_t}P(y_t|y_{t-1:1},X)\)</span>​，即当前时间步概率最大的单词。这样以来，整个解码的过程就是贪婪的，每一步的单词概率最大并不意味着整个句子的概率最大。</p>
<p>怎么解决这个问题呢？解决方法是 beam
search（光束搜索）。核心思想是，在推理阶段（训练时不需要，因为知道 ground
truth），</p>
<p>保留 k 个可能性最大的序列（可能性以概率相乘的对数作为分数，即<span class="math inline"> \(\sum_{i=1}^tlogP_{I,M}(y_i|y_{i-1:1},X)\)</span>​）。</p>
<p>当某个序列输出终止符号时，可以认作该序列已经结束，继续维护其他序列。</p>
<p>搜索的终止条件可以根据任务具体选择，例如：</p>
<ul>
<li>最多搜索多长时间步（例如 30 步）。</li>
<li>至少拥有多少个候选序列（例如 10 个）。</li>
</ul>
<p>在搜索结束，得到若干个候选序列后，将序列分数标准化后，即<span class="math inline"> \(\frac{1}{t}\sum_{i=1}^tlogP_{I,M}(y_i|y_{i-1:1},X)\)</span>​作为最终的分数。这样是为了避免短序列概率更大（概率连乘的数量小），然后选择概率最大的序列。</p>
<p>在搜索时，分数不需要进行标准化，因为搜索时处理的序列总是等长的。</p>
<h2 id="注意力机制">注意力机制</h2>
<p>为了解决 seq2seq 在面对长序列时的信息丢失问题，研究者们在 seq2seq 中引入了注意力（Attention）机制。借助于注意力机制，解码器能够在解码时与输入序列直接相连，还可以关注到输入序列的不同部分。公式化描述如下：</p>
<p>首先根据编码器状态<span class="math inline"> \({h_1,\dots,h_N}\)</span> 与当前解码器状态<span class="math inline"> \(s_t\)</span> 点乘计算分数 <span class="math display">\[
e^t=[s_t^\intercal h_1,\dots,s_t^\intercal h_N]\in \mathbb R^N
\]</span> 将分数归一化后作为输入序列与当前位置相关性的概率分布： <span class="math display">\[
\alpha^t=softmax(e^t)\in \mathbb R ^N
\]</span> 加权求和后作为最终的注意力结果： <span class="math display">\[
\alpha_t=\sum_{i=1}^N\alpha_i^th_i\in \mathbb R^h
\]</span> 将注意力结果与解码器隐藏状态拼接后计算新的隐藏状态<span class="math inline"> \(\hat s_t\)</span>，再计算输出。 <span class="math display">\[
[\alpha_t;s_t]\in \mathbb R^{2h}
\]</span> 带有 Attention 的 seq2seq 的简单示意图如下：</p>
<p><img src="model.png"></p>
<h3 id="广义的attention机制">广义的 Attention 机制</h3>
<p>广义的 attention 定义如下：给定一组向量 values，一个向量 query，attention 是 value 的加权和，权重是某个相似性度量函数，例如点积、加性注意力等。</p>
<p>度量函数可以为：</p>
<ul>
<li><p>点乘：<span class="math inline">\(e_i=s^\intercal h_i\in \mathbb
R\)</span></p></li>
<li><p> 乘法注意力：<span class="math inline">\(e_i=s^\intercal Wh_i \in
\mathbb R\)</span>​（其中<span class="math inline"> \(W\in \mathbb
R^{d_2*d_1}\)</span> 为权重矩阵）</p></li>
<li><p>加法注意力：<span class="math inline">\(e_i=v^\intercal
tanh(W_1h_i+W_2s)\in \mathbb R\)</span>​（其中<span class="math inline"> \(W_1,W_2\)</span> 为权重矩阵，<span class="math inline">\(v\)</span> 是权重向量）</p></li>
</ul>
<p>对应到 seq2seq 的 Attention 机制中，query 向量为解码器隐藏状态，values 为编码器的全部隐藏状态，度量函数为点乘。</p>
<p>联想一下 BERT 的自注意力机制：</p>
<p>对于每个单词向量，通过 Query、Key、Value 三个参数矩阵计算得到三个向量：q,k,v。在每个位置，使用当前位置的 query 向量与每个位置的 key 做点乘，作为相似性度量，再对 value 矩阵加权求和。公式如下：
<span class="math display">\[
Attention(Q,K,V)=softmax(\frac{QK^\intercal}{\sqrt{d_k}})V
\]</span></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>seq2seq</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>量子变分自编码器 VQ-VAE</title>
    <url>/blog/2021/07/15/vq-vae/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>VQ-VAE（Vector Quantised - Variational
AutoEncoder，量子变分自编码器）出自 2017 年 Google 团队的论文 Neural Discrete
Representation Learning。顾名思义，VQ-VAE 是 VAE（ Variational
AutoEncoder，变分自编码器）的变种。主要是为了解决 VAE 所存在的” 后验坍塌 “问题。VQ-VAE 与 VAE 的主要区别在于：</p>
<ul>
<li>隐变量是离散的，而非连续的</li>
<li>先验分布是学习得来的，而非固定不变的</li>
</ul>
<span id="more"></span>
<h2 id="研究动机与背景">研究动机与背景</h2>
<h3 id="离散型隐变量">离散型隐变量</h3>
<p>离散型隐变量对于某些任务是更为自然与恰当的，例如语言是由离散的字符组成的，图像的像素是 0-255 的自然数。然而，离散 VAE 往往难以训练，现有的训练方法无法弥补其与连续型 VAE 存在的性能上的差距。尽管连续型 VAE 会存在后验坍塌问题，但是由于从高斯分布中使用重参数化技巧采样隐变量，连续型 VAE 中能够获得方差更小，即更稳定的参数梯度。</p>
<h3 id="自回归模型">自回归模型</h3>
<p>自回归模型（<strong>A</strong>uto<strong>r</strong>egressive
model）是一种处理时间序列的方法，使用<span class="math inline"> \(x_1,x_2,\dots,x_{t-1}\)</span> 来预测<span class="math inline"> \(x_t\)</span>，并假设它们是线性关系。由于其使用<span class="math inline"> \(x\)</span> 本身来预测<span class="math inline"> \(x\)</span>，因而得名为自回归模型。形式化来讲，自回归模型定义如下：
<span class="math display">\[
X_t=c+\sum_{i=1}^p\phi_iX_{t-i}+\epsilon_t
\]</span> 其中，<span class="math inline">\(c\)</span> 是常数项，<span class="math inline">\(\epsilon_t\)</span> 假设为一个均值为 0，标准差为<span class="math inline"> \(\sigma\)</span> 的随机误差。</p>
<p>典型的自回归模型有循环神经网络（Recurrent Neural Network,
RNN），PixelCNN 等。下面以文中提到的 PixelCNN 为例进行介绍。</p>
<p>PixelCNN 是虽然是 CNN，但它与传统的 CNN 不同，而是参考了 RNN 的思路，将图片扁平化为一维后，将其看成时间序列进行逐像素的生成。即：
<span class="math display">\[
\begin{align}
p(x)&amp;=p(x_1,x_2,\dots,x_t)\\
&amp;=p(x_1)p(x_2|x_1)\dots p(x_t|x_1,x_2,\dots,x_{t-1})
\end{align}
\]</span> 可以看到，符合上述的自回归模型的定义（令<span class="math inline"> \(X_t=p(x_1,x_2,\dots,x_t)\)</span>）。</p>
<h3 id="变分自编码器">变分自编码器</h3>
<p>变分自编码器（Variational
AutoEncoder，VAE）是一类重要的生成模型。由于篇幅原因这里只做简单介绍，后面可能会单独出一篇博客介绍。VAE 假设存在一个无法观测的隐变量<span class="math inline"> \(z\)</span> 控制数据<span class="math inline"> \(x\)</span> 的生成，它主要由以下几部分组成：</p>
<ul>
<li>编码网络，拟合后验分布<span class="math inline"> \(q(z|x)\)</span>
，将数据<span class="math inline"> \(x\)</span> 映射到连续隐变量<span class="math inline"> \(z\)</span></li>
<li> 生成网络，拟合分布<span class="math inline"> \(p(x|z)\)</span></li>
<li> 隐变量的先验分布<span class="math inline"> \(p(z)\)</span></li>
</ul>
<p>在训练过程中，从<span class="math inline"> \(q(z|x)\)</span> 中采样隐变量<span class="math inline"> \(z\)</span> 来重构数据。在推理过程中，从<span class="math inline"> \(p(z)\)</span> 中采样隐变量来生成数据。</p>
<h2 id="模型细节">模型细节</h2>
<p>整体结构如下图所示：</p>
<p><img src="model.png"></p>
<h3 id="离散隐变量">离散隐变量</h3>
<p>模型定义了一个<span class="math inline"> \(K*D\)</span> 的隐变量嵌入空间，其中<span class="math inline"> \(K\)</span> 为空间大小，<span class="math inline">\(D\)</span> 为隐变量向量的维度。在得到编码网络的输出<span class="math inline"> \(z_e(x)\)</span> 后，通过<strong>最近邻算法</strong>将其映射为隐变量嵌入空间中的某个隐变量<span class="math inline"> \(e_k\)</span>（简记为<span class="math inline"> \(z\)</span>），投喂到解码器。后验分布<span class="math inline"> \(q(z|x)\)</span> 定义为如下的独热分布： <span class="math display">\[
q(z=k|x) = \begin{cases}
1 &amp;if\ k=\arg\min_j||z_e(x)-e_j|| , \\
0 &amp; otherwise.
\end{cases}
\]</span> 进而： <span class="math display">\[
z_q(x)=e_k, where\ k=\arg\min_j||z_e(x)-e_j||
\]</span></p>
<h3 id="梯度计算">梯度计算</h3>
<p>注意到上述公式中的<span class="math inline"> \(\arg\min\)</span> 操作是无法求梯度的，这使得模型无法进行反向传播。VQ-VAE 采取直通估计（straight-through
estimator
）来解决这个问题。原论文中具体做法描述为 <strong>” 将解码器输入<span class="math inline"> \(z_q(x)\)</span> 的梯度复制到解码器的输出<span class="math inline"> \(z_e(x)\)</span>“</strong>。对应上述结构图中的红线。</p>
<h3 id="损失函数">损失函数</h3>
<p>损失函数表示如下： <span class="math display">\[
L=logp(x|z_q(x))+||sg[z_e(x)]-e||_2^2+\beta||z_e(x)-sg[e]||^2_2
\]</span> 其中，<span class="math inline">\(sg\)</span> 代表停止梯度，即反向传播时不再向前计算梯度。这个符号的含义我个人感觉论文解释的有点不清楚，可能需要对照代码进一步看一下。我目前的理解是，在前向传播的时候，sg 是恒等式，即被忽略掉了，此时计算得到的 loss 是真正的 loss。在反向传播时，sg 部分的计算图相当于断开了，以<span class="math inline"> \(||sg[z_e(x)]-e||_2^2\)</span> 为例，前项传播时等价于<span class="math inline"> \(||z_e(x)-e||_2^2\)</span>。反向传播时等价于<span class="math inline"> \(||const-e||_2^2\)</span>，即将<span class="math inline"> \(z_e(x)\)</span> 看做常数，不对其进行优化。</p>
<p>损失函数的各项含义解释如下：</p>
<ul>
<li>第一项为重构损失，用以训练编码器和解码器，个人感觉这里是不是少了个负号，这一部分是似然函数，按理说应该是要最大化的。</li>
<li>第二项为 L2 范数损失函数。通过矢量量化（Vector
Quantisation，VQ）学习嵌入空间的字典，即希望编码器的输出<span class="math inline"> \(z_e(x)\)</span> 与最近邻算法得到的<span class="math inline"> \(e\)</span> 距离越近越好，用以优化嵌入空间。</li>
<li>第三项为 L2 范数损失函数。与第二项的区别在于优化的是编码器。原论文中的说法是，由于嵌入空间是无量纲的，当仅存在第二项时，若<span class="math inline"> \(e\)</span> 的参数训练速度慢于编码器参数，会使得<span class="math inline"> \(e\)</span> 的参数向任意方向增长。</li>
</ul>
<p>第二项和第三项本质上都是希望编码器的输出<span class="math inline"> \(z_e(x)\)</span> 与离散化隐变量<span class="math inline"> \(e\)</span> 相互接近，相较于<span class="math inline"> \(||z_e(x)-e||_2^2\)</span>，个人理解这里的设计是为了控制二者的优化速度。如果希望编码器输出相对稳定，则调小<span class="math inline"> \(\beta\)</span>，让嵌入空间更多地靠近编码器的输出，也可以反之。</p>
<p>论文中实验发现<span class="math inline"> \(\beta\)</span> 从 0.1-2.0 都是非常鲁棒的，实验设置<span class="math inline"> \(\beta=0.25\)</span>，可能意味着二者靠近的速度影响不大（这也更符合直观认知）。</p>
<h3 id="先验分布pz">先验分布<span class="math inline"> \(p(z)\)</span></h3>
<p>先验分布<span class="math inline"> \(p(z)\)</span> 是个分类分布（categotical
distribution），在训练过程中保持不变。在训练结束后，在隐变量<span class="math inline"> \(z\)</span> 上拟合一个自回归分布，即<span class="math inline"> \(p(z)\)</span>，进而通过祖先采样（ancestral
sampling）来生成<span class="math inline"> \(x\)</span>。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://zh.wikipedia.org/wiki/自迴歸模型">自回归模型 -
维基百科，自由的百科全书 (wikipedia.org)</a></li>
</ul>
]]></content>
      <categories>
        <category>生成模型</category>
        <category>VAE</category>
      </categories>
      <tags>
        <tag>VQ-VAE</tag>
        <tag>论文</tag>
      </tags>
  </entry>
</search>
