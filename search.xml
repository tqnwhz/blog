<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>浅谈 Golang 的优缺点</title>
    <url>/blog/2023/03/05/golang-talk/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>最近学习完了《Go 程序设计语言》一书，感觉 Golang 作为一门现代语言，优缺点都非常的明显，也在网上看了很多吐槽或者赞扬 Golang 的说法，也是各有各的理。本文从个人的主观视角，聊一下我认为 Golang 设计及使用上的一些优缺点。</p>
<span id="more"></span>

<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><h3 id="容易入门"><a href="#容易入门" class="headerlink" title="容易入门"></a>容易入门</h3><p>Go 最为人知的一个特点就是 “大道至简”，被誉为 21 世纪的 C 语言。Go 的一大特点是上手简单容易，虽然语法规则与 C、Java 等有较大差异（例如变量声明、无类型隐式转换等），但是还是能比较快地适应。</p>
<p>Go 设计的并不复杂，容器只有数组、切片、map；并发只有 goroutine、channel；锁只有互斥锁、读写锁；编程风格只有接口和接口实现，这大大地减少了入门的学习成本和系统的复杂性。当然这种特性也可以称之为 “简陋”，也是我认为 Go 的缺点之一，后面会再提到。</p>
<h3 id="默认初始化"><a href="#默认初始化" class="headerlink" title="默认初始化"></a>默认初始化</h3><p>C 语言的一大特点就是随机初始化，未经显式初始化的局部变量的值均为随机数。在 Go 中，所有的类型都有对应的零值，例如 bool 类型默认是 false，int 默认是 0，指针默认是 nil，struct 会递归地为每个字段赋零值。这样虽然可能会多余一些初始化的开销，但是代码会更加健壮。</p>
<p>包括像 map 访问不存在的元素时，也会默认返回零值，而不是异常。</p>
<h3 id="多返回值"><a href="#多返回值" class="headerlink" title="多返回值"></a>多返回值</h3><p>C 语言一个典型的问题是不支持多返回值，这意味着要么使用特殊值做异常处理（例如 - 1，null），要么就得新建一个结构体对结果再次包装。Java 提供了 Optional 这一包装类来解决这个问题，虽然解决了这个问题，但还是无法覆盖其他多返回值需要的场景。Python 支持多返回值作为一个 tuple 返回，C++ 可以通过 pair 来部分支持。</p>
<p>Go 支持多返回值并且提供了一种良好的编程风格，即一个函数总是应该返回两个参数，结果和对应的解释：</p>
<ul>
<li>如果函数只可能存在一种失败情况，解释应该是一个 bool，标识这次操作是否成功。例如 map 查找，失败的唯一原因就是 key 不存在。</li>
<li>如果函数存在多种失败情况，解释应该是一个 error，标识操作的错误类型。</li>
</ul>
<p>这使得 Go 能够比较灵活地处理函数执行过程中可能出现的各种错误，当然这也是有代价的。</p>
<h3 id="goroutine"><a href="#goroutine" class="headerlink" title="goroutine"></a>goroutine</h3><p>Go 最大、最广为人知的优点之一，可能就是 goroutine 了。goroutine 是一种用户态线程，也叫协程，由运行时去进行 m:n 调度，即 m 个操作系统线程负责 n 个 goroutine 的执行。作为用户态线程，goroutine 的切换没有系统线程切换那么大的开销，需要陷入内核、保存状态等，而且不受操作系统线程数量的限制。goroutine 的数量可以达到成千上万，而 cpu 的线程一般只有几十个。这使得 goroutine 支持相当大的并发，非常适合后端服务器的场景。</p>
<p>在 C++ 20，Java 19 中，引入了对协程的支持，Java 中的叫虚拟线程。可惜这一块蛋糕可能已经被 G 抢完了，而且多少公司还在用 c++ 11 和 java 8 呢。</p>
<h3 id="编译速度快"><a href="#编译速度快" class="headerlink" title="编译速度快"></a>编译速度快</h3><p>相较于 C++，Go 的一个显著优点是编译速度更快，Go 的 import 支持按需导入，而且能够能够分析文件依赖，利用编译缓存加快编译速度。</p>
<h3 id="支持垃圾回收"><a href="#支持垃圾回收" class="headerlink" title="支持垃圾回收"></a>支持垃圾回收</h3><p>Go 是支持垃圾回收的，有着与 Java 类似的垃圾回收机制，通过可达性分析对堆上内存进行分析回收，有时需要 stop the world，虽然略微影响性能，但是避免了指针乱飞的内存泄漏，还是值得的。C++ 虽然没有垃圾回收，但是有智能指针，使用得当也可以避免内存泄漏。</p>
<h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><h3 id="err泛滥"><a href="#err泛滥" class="headerlink" title="err泛滥"></a>err 泛滥</h3><p>写过 Go 代码的人可能都对 err 深恶痛绝，据说 Go 业务代码中可能有 50% 的代码都是：</p>
<figure class="highlight go"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>有人把这做成了一个表情包，来调侃 err 的滥用以及引起的代码冗余。当然，我也好想有一个按下 enter 就能输入这个 snippet 的键盘（狗头。</p>
<p><img src="https://pic1.zhimg.com/80/v2-fc238ce2a5d51a01929fc54c8406e7bc_720w.webp?source=1940ef5c" alt="img"></p>
<p>这种代码泛滥的原因在于，复杂业务的每一环可能都会出错的。参数校验会出错、序列化反序列化会出错、rpc 调用会出错等等，任何一个环节出错都需要终止流程。而多返回值的风格广受认可，就导致每个函数调用都需要去判错处理，使得代码复杂而冗余。</p>
<p>Go 社区对这个问题意见很大，提出了 try 的语法糖希望简化这一判断，参见 <a href="https://github.com/golang/go/issues/56165">proposal: Go 2: error handling: try statement with handler · Issue #56165 · golang/go (github.com)</a>，不知道能否被采纳。之前 19 年关于 try 的提案是被拒绝了。</p>
<h3 id="不支持面向对象"><a href="#不支持面向对象" class="headerlink" title="不支持面向对象"></a>不支持面向对象</h3><p>虽然 Go 中存在接口，但是并不支持面向对象。严格意义上，Go 的风格可能更适合称为面向抽象编程，而非面向对象。Go 中只存在接口和接口的实现，没有父类、继承这些概念。Go 提倡使用组合来结合功能，并提供了匿名嵌入的方法， 使得可以便捷调用嵌入结构体的方法。这也提供了一种编程风格，但是面向对象的缺失还是使得可选的代码风格减少。</p>
<h3 id="不支持函数重载"><a href="#不支持函数重载" class="headerlink" title="不支持函数重载"></a>不支持函数重载</h3><p>作为一门现代语言，Go 不支持重载是难以置信的。Go 有很多功能相同只是参数略有差异的函数名，例如 <code>slices.ContainsInt64,slices.ContainsString </code>分别用于判断切片中是否存在 int64 和字符串。当然这也与 Go 1.18 之前不支持泛型有关。</p>
<h3 id="内置的slice和map"><a href="#内置的slice和map" class="headerlink" title="内置的slice和map"></a>内置的 slice 和 map</h3><p>这也是 Go 的设计失误之一。slice、map 不由标准库提供，而是内置在语言核心。这导致了一些非常奇怪、反人类的链式反应：</p>
<ul>
<li>没有好用的容器标准库，只在语言核心提供了 slice 和 map</li>
<li> 占用 map、append、len、cap 等关键词</li>
<li>难以扩展，没有通用的接口来实现新的容器类以使得支持 range、len 等</li>
<li>不满足 duck type，容器没有任何方法绑定，不能链式调用，必须使用额外的工具包函数层层套括号</li>
<li>给语言迭代更新增加了新的困难</li>
</ul>
<p>C++、Java，分别提供了 stl 库和 collection 抽象，实现了包括不限于动态数组、链表、队列堆栈、树等各种复杂的数据结构，大大减少了使用过程中造轮子的过程。作为面向对象的代表，Java 提供了丰富完备的容器接口，可以针对不同场景实现对应的容器，例如并发安全的容器。Python 虽然内置 list 和 dict，但也是以面向对象的方式提供，抽象了 len、iter 等接口，支持自定义容器类。</p>
<p>众所周知，Go 使用的是 duck type，即鸭子类型：一个东西不需要说它自己是鸭子，只要看起来像鸭子，走路、叫像鸭子，它就是鸭子。这种理念下，结构体不需要显式声明实现的接口，由编译器分析是否实现了某个接口。而 Go 语言层面的 slice 和 map 却是不支持鸭子类型的，这两个类型没有方法绑定，只能依赖语言层面提供的其他关键字，例如 len、append 等完成功能。</p>
<h3 id="简陋而固执"><a href="#简陋而固执" class="headerlink" title="简陋而固执"></a>简陋而固执</h3><p>简单过了头，就是简陋。<strong>Go 目前还不支持可重入锁，直到 1.18 前，不支持泛型、TryLock 等操作</strong>。难以想象这是一门现代的编程语言。Google 团队有一种自负在里面，不愿意为了用户考虑，他们的大道至简只是去简化编译器的工作，把大部分工作量留给程序员，这是很反人类的。</p>
<p>早在十几年前，Google 团队声明可重入锁是不好的设计，滋生 bug，参见 <a href="https://groups.google.com/g/golang-nuts/c/XqW1qcuZgKg/m/Ui3nQkeLV80J">groups.google.com</a>，于是直到现在 Go 仍不支持可重入锁。但是他们也承认了 TryLock 操作是最终会需要的，过了十多年的 Go1.18 版本才正式支持了 TryLock。</p>
<p>我部分理解他们的声明，他们认为可重入锁会需要额外的工作去管理 goroutine 持有的锁，相当于 goroutine 需要有 local storage 存储这些信息，类似 Java 中的 ThreadLocal。这也是他们在避免的，于是 Go 也不支持这些，只能通过 context 层层透传，用于 goroutine 存储。</p>
<p>给人的感觉就是 Go 不像是一个产品，产品是从用户需要角度出发，采取各语言的精华，去做出一个易用、好用的产品。可重入锁的优点是很明显的，各函数可以独立工作，也可以协同工作，不需要担心死锁；不支持可重入锁就需要把每个函数写加锁和不加锁两个版本，避免调错函数死锁。他们不仅是认为实现可重入锁是不必要的，甚至拒绝相关的 PR。一他们固执地坚持着自己的观点，把握着 Go 的代码准入权，直至 2021 年的 Go 1.18 讨论上，Go 语言之父还在反对加入泛型，认为这会导致过大的工作量，而且容易出错。</p>
<p>类似的观点还有，Go 的泛型中使用了方括号，而不是尖括号。Go 的 map、slice 使用的也全部是方括号，如果一个函数有泛型、有 map、有 slice，看起来会比较费劲。他们拒绝尖括号的原因是，可能会导致歧义并且他们不愿意通过更改编译器前端解决。可以凝练为，<strong>他们不愿意去顺应用户习惯，而是强迫用户按自己的想法使用并培养习惯</strong>。这可能就是大公司的傲慢。</p>
<h3 id="不支持枚举"><a href="#不支持枚举" class="headerlink" title="不支持枚举"></a>不支持枚举</h3><p>作为一个强类型语言，去除了所有的隐式转换保证代码健壮的语言，居然不支持枚举类型。枚举类型是一个不难实现且可以显著提升代码可读性的特性，迟迟不实现实属不应该。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从我两个多月的 Go 体验下来，感觉 Go 的优点和缺点都非常突出。Go 按 Google 团队的意志提供了一种几乎固定的编码规范，一定程度会方便标准化，但是并不是一门易用、好用、优雅的语言。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><p><a href="https://www.zhihu.com/question/563395289">为啥网上这么多人 diss Golang？ - 知乎 (zhihu.com)</a></p>
</li>
<li><p><a href="https://www.zhihu.com/question/490457306">Go 为什么设计地这么简陋，是为了推广还是在编译器上难以实现？- 知乎 (zhihu.com)</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 集群模式</title>
    <url>/blog/2023/02/15/redis-cluster/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本篇文章主要梳理一下 Redis 的集群模式，明白它是如何做到高可用、支持高并发的。</p>
<span id="more"></span>

<h2 id="Copy-On-Write"><a href="#Copy-On-Write" class="headerlink" title="Copy On Write"></a>Copy On Write</h2><p>首先要了解一下 copy on write 机制，才能理解 Redis 做备份时如何保持一致。copy on write（简写为 cow），中文名为写时复制，是计算机领域非常经典的优化思想。可以先从 linux 的 fork 函数了解下 cow 的原理。已经理解这部分知识的读者可以直接跳到下一章节。</p>
<p>fork 是 unix 系统上用于创建进程的函数，例如下面的 C 代码，调用 fork 后，会创建一个新的进程，从 fork 处接着向后执行。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span>&nbsp;<span class="meta-string">&lt;unistd.h&gt;</span>&nbsp;&nbsp;</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span>&nbsp;<span class="meta-string">&lt;stdio.h&gt;</span>&nbsp;&nbsp;</span></span><br><span class="line">&nbsp;</span><br><span class="line"><span class="function"><span class="keyword">int</span>&nbsp;<span class="title">main</span>&nbsp;<span class="params">()</span>&nbsp;&nbsp;&nbsp;</span></span><br><span class="line"><span class="function"></span>{&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">pid_t</span>&nbsp;fpid;&nbsp;<span class="comment">//fpid表示fork函数返回的值&nbsp;&nbsp;</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">int</span>&nbsp;count=<span class="number">0</span>;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 调用fork，创建出子进程&nbsp;&nbsp;</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;fpid=fork();</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 所以下面的代码有两个进程执行！</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">if</span>&nbsp;(fpid&nbsp;&lt;&nbsp;<span class="number">0</span>)&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="built_in">printf</span>(<span class="string">"创建进程失败!/n"</span>);&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">else</span>&nbsp;<span class="keyword">if</span>&nbsp;(fpid&nbsp;==&nbsp;<span class="number">0</span>)&nbsp;{&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="built_in">printf</span>(<span class="string">"我是子进程，由父进程fork出来/n"</span>);&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;count++;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">else</span>&nbsp;{&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="built_in">printf</span>(<span class="string">"我是父进程/n"</span>);&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;count++;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="built_in">printf</span>(<span class="string">"统计结果是:&nbsp;%d/n"</span>,count);&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">return</span>&nbsp;<span class="number">0</span>;&nbsp;&nbsp;</span><br><span class="line">}&nbsp;&nbsp;</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>fork 函数的特点是，它会在父子进程各返回一次，<strong>将子进程的 pid（进程 id）返回给父进程，将 0 返回给子进程</strong>，因此可以通过 pid 判断当前进程是父进程还是子进程。上述代码的结果是</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">我是子进程，由父进程fork出来</span><br><span class="line"></span><br><span class="line">统计结果是: 1</span><br><span class="line"></span><br><span class="line">我是父进程</span><br><span class="line"></span><br><span class="line">统计结果是: 1</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>在 fork 之后，出现了一个与父进程的副本，从相同的位置向后执行。副本意味着从操作系统的角度，父子进程的地址空间是相同的（除了 fork 返回值），这样两个进程才能共享 fork 前的状态，同样地向后执行。如何实现这个副本呢？一种简单的想法是，直接申请相同大小的物理页，逐页面地拷贝数据，再更新页表即可。这样就可以<strong>保证两个进程的初始状态相同，且后续的修改不相互影响</strong>。如上所示，<code>count</code> 的值在每个进程中都是 1，而不是 2。</p>
<p>但是这样做的开销是非常大的，子进程可能只需要对很少的变量进行修改，拷贝整个地址空间显得既多余又严重影响速度，启动时间非常长。copy on write 就可以完美解决这个问题。在这种思想下，<strong>在 fork 后，两个进程是共享地址空间的，并把所有页面标记为 READ_ONLY，即只可读不可写，也就是 fork 只是把父进程的页表做了一次拷贝，非常快</strong>。当父 / 子进程需要对地址进行修改时：</p>
<ol>
<li>发现页面不可修改，触发<strong>页异常中断 page-fault</strong>，申请新的页面，将旧页面数据拷贝至新页面</li>
<li>在新页面做修改，并更改页表映射关系</li>
</ol>
<p>这也就是 copy on write 名字的由来，在需要修改（写入）的时候，对页面进行复制。这样<strong>既达到了非常快的启动时间，也实现了按需申请资源</strong>。当然缺点也是有的，如果父 / 子进程需要修改大量的数据，反复的异常中断也会影响性能。</p>
<h2 id="复制与分片"><a href="#复制与分片" class="headerlink" title="复制与分片"></a>复制与分片</h2><p>数据库的集群模式中，按思想可分为复制和分片两类，这对 MySQL、Redis 都适用。这里先介绍下这两种思想，方便后续理解。需要注意的是，这两种方法应用于 MySQL 时都会有一致性的问题。</p>
<h3 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h3><p>在复制的概念中，数据库分为两类，一类是主数据库（master），另一类是从数据库（slave）。主数据库可以进行读写操作，当<strong>写操作导致数据变化时会自动将数据同步给从数据库</strong>。而<strong>从数据库一般是只读的</strong>，并接受主数据库同步过来的数据。一个主数据库可以拥有多个从数据库，而一个从数据库只能拥有一个主数据库。</p>
<p>引入复制的目的主要有：</p>
<ul>
<li>读取高效：读取的吞吐量成倍提升，选择延迟更低的节点</li>
<li>多点备份，容灾恢复</li>
</ul>
<p>复制的缺点也很明显，会有一致性的问题：写操作无法及时同步到主节点，导致读取数据不一致，对于 MySQL 这种需要保证一致性的数据库问题尤为严重。</p>
<p>复制的过程中需要同步写操作：</p>
<ul>
<li>对于新启动的节点，最省事的方法当然是把整个数据库文件传过去</li>
<li>对于运行中的节点，只需要同步本次写的操作即可，MySQL 可以使用 binlog，Redis 可以使用 aof 日志</li>
</ul>
<h3 id="分片"><a href="#分片" class="headerlink" title="分片"></a>分片</h3><p>在分片的概念中，需要按一定的规则，将数据分为若干个分区（partition），分布于各个节点上。分配规则可以是：</p>
<ul>
<li>MySQL<ul>
<li> 行分片：常用的按主键范围划分，例如 1-1w 在一个节点，1w-2w 在一个节点</li>
<li>列分片：也就是分表，将数据按数据库范式拆分，存储在不同节点</li>
</ul>
</li>
<li> Redis：使用 hash 将 key 转化为整数，再做范围划分</li>
</ul>
<p>切片的优势在于：</p>
<ul>
<li>写入高效，多个写操作命中不同的节点，互不影响，提高并发</li>
<li>读取昂贵：MySQL 中的联合、全表扫描操作效率降低，可能需要对多个节点加锁<ul>
<li>对 Redis 还好</li>
</ul>
</li>
</ul>
<h2 id="Redis-集群模式"><a href="#Redis-集群模式" class="headerlink" title="Redis 集群模式"></a>Redis 集群模式</h2><h3 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a>主从复制</h3><p>主从复制、读写分离是存储中常用的提高性能、可用性的方法。Redis 也不例外。最直觉的集群模式，就是设置很多的从节点复制，写操作在主节点完成，主从之间通过 aof 日志 /rdb 文件同步。整个架构如下所示，一个主节点可能有多个从节点，从节点可能再会有自己的从节点。</p>
<p><img src="/blog/ms-mode.png"></p>
<p>主从复制的具体流程如下：</p>
<p><img src="/blog/ms-pipeline.png"></p>
<p>流程如下从数据库启动成功后，连接主数据库，发送 SYNC 命令；</p>
<ul>
<li>主数据库接收到 SYNC 命令后，<strong>开始执行 BGSAVE 命令生成 RDB 文件并使用缓冲区记录此后执行的所有写命令</strong>；</li>
<li>主数据库 BGSAVE 执行完后，向所有从数据库发送快照文件，并在发送期间继续记录被执行的写命令；</li>
<li>从数据库收到快照文件后丢弃所有旧数据，载入收到的快照；</li>
<li>主数据库快照发送完毕后开始向从数据库发送缓冲区中的写命令；</li>
<li>从数据库完成对快照的载入，开始接收命令请求，并执行来自主数据库缓冲区的写命令；（<strong>从数据库初始化完成</strong>）</li>
<li>主数据库每执行一个写命令就会向从数据库发送相同的写命令，从数据库接收并执行收到的写命令（<strong>从数据库初始化完成后的操作</strong>）</li>
<li>出现断开重连后，2.8 之后的版本会将断线期间的命令传给重数据库，增量复制。</li>
<li>主从刚刚连接的时候，进行全量同步；全同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。Redis 的策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。</li>
</ul>
<p>其他步骤都比较好理解，最开始困扰我的是这个 bgsave，它生成的 RDB 文件为什么能够记录某个时间数据库的准确状态？如果不是准确状态，写命令执行两次不是会有幂等性问题？后面了解才知道，<strong>bgsave 是通过 fork 与 copy on write 实现的</strong>。</p>
<p>主从复制的优点基本就是复制操作的优点，就不赘述了，它的缺点在于：</p>
<ul>
<li>Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者<strong>手动切换前端的 IP 才能恢复</strong>（<strong>也就是要人工介入</strong>）；</li>
<li>主机宕机，宕机前有部分数据未能及时同步到从机，切换 IP 后还会引入数据不一致的问题，降低了系统的可用性；</li>
<li>如果多个 Slave 断线了，需要重启的时候，尽量不要在同一时间段进行重启。<strong>因为只要 Slave 启动，就会发送 sync 请求和主机全量同步，当多个 Slave 重启的时候，可能会导致 Master IO 剧增从而宕机</strong>。</li>
<li>Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂；</li>
</ul>
<p>需要注意的是，新启动 / 重启的节点都需要发送 sync 请求和主机全量同步，会影响主服务的稳定性。</p>
<h3 id="Sentinel（哨兵）模式"><a href="#Sentinel（哨兵）模式" class="headerlink" title="Sentinel（哨兵）模式"></a>Sentinel（哨兵）模式</h3><p>第一种主从同步 / 复制的模式，当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑哨兵模式。消息队列 Kafka 使用的就是哨兵模式。</p>
<p>哨兵模式可以看成主从模式的扩展。首先 Redis 提供了哨兵的命令，<strong>哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵通过发送命令，等待 Redis 服务器响应，从而监控运行的多个 Redis 实例</strong>。</p>
<p><img src="/blog/single-sentinel.png"></p>
<p><strong>哨兵模式的作用</strong></p>
<ul>
<li>通过发送命令，让 Redis 服务器返回监控其运行状态，包括主服务器和从服务器；</li>
<li>当哨兵监测到 master 宕机，会自动将 slave 切换成 master ，然后通过<strong>发布订阅模式</strong>通知其他的从服务器，修改配置文件，让它们切换主机；</li>
</ul>
<p>然而一个哨兵进程对 Redis 服务器进行监控，也可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。</p>
<p><img src="/blog/multi-sentinel.png"></p>
<h4 id="故障切换的过程"><a href="#故障切换的过程" class="headerlink" title="故障切换的过程"></a><strong>故障切换的过程</strong></h4><p>假设主服务器宕机，哨兵 1 先检测到这个结果，系统并不会马上进行 failover 过程，仅仅是哨兵 1 主观的认为主服务器不可用，这个现象成为<strong>主观下线</strong>。当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行 failover 操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为<strong>客观下线</strong>。这样对于客户端而言，一切都是透明的。</p>
<h4 id="哨兵模式的工作方式："><a href="#哨兵模式的工作方式：" class="headerlink" title="哨兵模式的工作方式："></a>哨兵模式的工作方式：</h4><ul>
<li>每个 Sentinel（哨兵）进程以每秒钟一次的频率向整个集群中的 Master 主服务器，Slave 从服务器以及其他 Sentinel（哨兵）进程发送一个 PING 命令。</li>
<li>如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel（哨兵）进程标记为主观下线（SDOWN）</li>
<li>如果一个 Master 主服务器被标记为主观下线（SDOWN），则正在监视这个 Master 主服务器的所有 Sentinel（哨兵）进程要以每秒一次的频率确认 Master 主服务器的确进入了主观下线状态</li>
<li>当有足够数量的 Sentinel（哨兵）进程（大于等于配置文件指定的值）在指定的时间范围内确认 Master 主服务器进入了主观下线状态（SDOWN）， 则 Master 主服务器会被标记为客观下线（ODOWN）</li>
<li>在一般情况下， 每个 Sentinel（哨兵）进程会以每 10 秒一次的频率向集群中的所有 Master 主服务器、Slave 从服务器发送 INFO 命令。</li>
<li>当 Master 主服务器被 Sentinel（哨兵）进程标记为客观下线（ODOWN）时，Sentinel（哨兵）进程向下线的 Master 主服务器的所有 Slave 从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。</li>
<li>若没有足够数量的 Sentinel（哨兵）进程同意 Master 主服务器下线， Master 主服务器的客观下线状态就会被移除。若 Master 主服务器重新向 Sentinel（哨兵）进程发送 PING 命令返回有效回复，Master 主服务器的主观下线状态就会被移除。</li>
</ul>
<h4 id="哨兵模式的优缺点"><a href="#哨兵模式的优缺点" class="headerlink" title="哨兵模式的优缺点"></a>哨兵模式的优缺点</h4><p><strong>优点：</strong></p>
<ul>
<li>哨兵模式是基于主从模式的，所有主从的优点，哨兵模式都具有。</li>
<li>主从可以自动切换，系统更健壮，可用性更高 (<strong>可以看作自动版的主从复制</strong>)。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。</li>
</ul>
<h3 id="Cluster-集群模式（Redis官方）"><a href="#Cluster-集群模式（Redis官方）" class="headerlink" title="Cluster 集群模式（Redis官方）"></a>Cluster 集群模式（Redis 官方）</h3><p>Redis Cluster 是一种服务器 Sharding 技术，3.0 版本开始正式提供。</p>
<p>Redis 的哨兵模式基本已经可以实现高可用，读写分离 ，但是在这种模式下每台 Redis 服务器都存储相同的数据，很浪费内存，所以在 redis3.0 上加入了 Cluster 集群模式，实现了 Redis 的分布式存储，<strong>也就是说每台 Redis 节点上存储不同的内容</strong>。</p>
<p><img src="/blog/cluster.png"></p>
<p>在这个图中，每一个蓝色的圈都代表着一个 redis 的服务器节点。它们任何两个节点之间都是相互连通的。客户端可以与任何一个节点相连接，然后就可以访问集群中的任何一个节点。对其进行存取和其他操作。</p>
<h4 id="集群的数据分片"><a href="#集群的数据分片" class="headerlink" title="集群的数据分片"></a><strong>集群的数据分片</strong></h4><p>Redis 集群没有使用一致性 hash，而是引入了哈希槽【hash slot】的概念。</p>
<p>Redis 集群有 16384 个哈希槽，每个 key 通过 CRC16 校验后对 16384 取模来决定放置哪个槽。集群的每个节点负责一部分 hash 槽，举个例子，比如当前集群有 3 个节点，那么：</p>
<ul>
<li>节点 A 包含 0 到 5460 号哈希槽</li>
<li>节点 B 包含 5461 到 10922 号哈希槽</li>
<li>节点 C 包含 10923 到 16383 号哈希槽</li>
</ul>
<p>这种结构很容易添加或者删除节点。比如如果我想新添加个节点 D ， 我需要从节点 A， B， C 中得部分槽到 D 上。如果我想移除节点 A ，需要将 A 中的槽移到 B 和 C 节点上，然后将没有任何槽的 A 节点从集群中移除即可。由于从一个节点将哈希槽移动到另一个节点并不会停止服务，所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态。</p>
<p>在 Redis 的每一个节点上，都有这么两个东西，一个是插槽（slot），它的的取值范围是：0-16383。还有一个就是 cluster，可以理解为是一个集群管理的插件。当我们的存取的 Key 到达的时候，Redis 会根据 CRC16 的算法得出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。</p>
<h4 id="Redis-集群的主从复制模型"><a href="#Redis-集群的主从复制模型" class="headerlink" title="Redis 集群的主从复制模型"></a>Redis 集群的主从复制模型</h4><p><strong>为了保证高可用，redis-cluster 集群引入了主从复制模型，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点</strong>。当其它主节点 ping 一个主节点 A 时，如果半数以上的主节点与 A 通信超时，那么认为主节点 A 宕机了。如果主节点 A 和它的从节点 A1 都宕机了，那么该集群就无法再提供服务了。</p>
<h4 id="集群的特点"><a href="#集群的特点" class="headerlink" title="集群的特点"></a><strong>集群的特点</strong></h4><ul>
<li>所有的 redis 节点彼此互联 (PING-PONG 机制)，内部使用二进制协议优化传输速度和带宽。</li>
<li>节点的 fail 是通过集群中超过半数的节点检测失效时才生效。</li>
<li>客户端与 Redis 节点直连，不需要中间代理层。客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://segmentfault.com/a/1190000022808576">Redis 你了解 Redis 的三种集群模式吗？ - 个人文章 - SegmentFault 思否</a></li>
<li><a href="https://www.cnblogs.com/L-Test/p/11626124.html">Redis ==&gt; 集群的三种模式 - 破解孤独 - 博客园 (cnblogs.com)</a></li>
<li><a href="https://www.cnblogs.com/xiaolei2017/p/15713194.html">Redis 中 bgsave 方式持久化的细节问题 - 百合叶 - 博客园 (cnblogs.com)</a></li>
<li><a href="https://zhou-yuxin.github.io/articles/2017/fork(">fork () 后 copy on write 的一些特性 (zhou-yuxin.github.io)</a> 后 copy on write 的一些特性 /index.html)</li>
<li><a href="https://www.cnblogs.com/Java3y/p/9884583.html">COW 奶牛！Copy On Write 机制了解一下 - Java3y - 博客园 (cnblogs.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>copy on write</tag>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 6 实验报告</title>
    <url>/blog/2022/12/02/MIT-6-830-6/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Lab 6 要求实现数据库的恢复功能。在真实场景下，数据库可能因为种种原因宕机崩溃，需要保证此时的数据不会丢失，保证数据库事务的 ACID 性质。这往往要通过日志来实现。</p>
<span id="more"></span>

<h2 id="理论知识"><a href="#理论知识" class="headerlink" title="理论知识"></a>理论知识</h2><p>前面提到了，事务需要具备 ACID 性质，其中有两点需要关注：</p>
<ul>
<li>原子性：事务可能会中止，需要回滚</li>
<li>持续性：如果 DBMS 崩溃了怎么办</li>
</ul>
<p>事务中止的原因可能有很多，例如执行出错、出现死锁等。DBMS 崩溃的原因也有很多，例如磁盘空间不足、管理员误操作等。如何在这些场景下保证数据库的一致性、可靠性等，是一个很重要的问题。看过之前博客的同学可能记得，在 Lab4 中，实现了 NO STEAL 的模式。其思想是将事务的脏页面在 BufferPool 中置顶，不将其逐出，直到事务提交后再逐出。这在一定程度上可以解决原子性的问题，但是 BufferPool 一旦满了，DBMS 就直接崩溃了。</p>
<h3 id="WAL"><a href="#WAL" class="headerlink" title="WAL"></a>WAL</h3><p>数据库常使用先写日志（Write-Ahead Logging，WAL）与 ARIES 算法解决原子性和持续性的问题。WAL 的思想是，先写日志，再写数据，遵循下面的协议</p>
<ol>
<li>在更新数据到磁盘前，必须强制保存日志</li>
<li>在事务提交前，必须强制保存所有日志</li>
</ol>
<p>日志是一个多元组，例如 &lt;XID, pageID, offset, length, old data, new data&gt;，包含事务 ID、页面 ID、偏移、修改前后的数据等信息。有了这些信息，就可以解决 ACID 的两个问题：</p>
<ul>
<li>如何回滚事务：根据日志，回滚该事务，用旧值覆盖新值</li>
<li>如果 DBMS 崩溃了怎么恢复：根据日志，重做部分事务，用新值替换旧值</li>
</ul>
<p>可以看到，在 WAL 的写日志协议中，第一点包含了 UNDO 的信息，保证了写到磁盘的数据必定可以通过日志回滚，确保了原子性，第二点包含了重做的信息，保证了已经提交的事务，必定可以通过日志重新复现其结果，确保了持续性。</p>
<p>WAL 容易混淆的一点是，因为写日志先于写磁盘，所以：</p>
<ul>
<li>日志中出现了 UPDATE 记录，<strong>并不意味更新在磁盘上生效了</strong></li>
<li>日志中出现了 COMMIT 记录，<strong>并不意味着事务在磁盘上提交到 DBMS 了</strong></li>
</ul>
<p>所以，对于日志中的 COMMIT 的事务，也需要对其重做，因为它的结果可能并没有保存到磁盘中。WAL 确保了可以通过日志撤销和重做操作，与我们平时的使用日志习惯：“先出现现象，再记录现象”，是相反的。这点需要深刻理解。</p>
<h3 id="ARISE"><a href="#ARISE" class="headerlink" title="ARISE"></a>ARISE</h3><p>ARISE 算法就使用这种 WAL 的日志，在数据库崩溃时，分三阶段处理：</p>
<ol>
<li>分析：分析最新的检查点到崩溃中间的日志，分析崩溃时的状态<ol>
<li> BufferPool 中有哪些脏页面</li>
<li>还有哪些事务正在运行（即崩溃中止了）</li>
</ol>
</li>
<li>重做：从脏页面对应的日志中的最早修改记录开始，逐个地重做结果</li>
<li>撤销：从崩溃位置开始，到崩溃中止事务的最早日志，倒序地撤销结果</li>
</ol>
<p>如下图所示（来自伯克利 CS186），A、R、U 分别对应阶段 1、2、3。需要注意的是，重做的时候重做了所有的操作，包含最后中止的事务操作。这样的优点是确保不会出错，能够完美复现数据库崩溃时的状态，而且扩展性更好。撤销的时候需要倒序撤销，因为同一个页面可能被更新多次，撤销的最终结果的修改的最初结果。</p>
<p><img src="/blog/ARIES.png"></p>
<h2 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h2><h3 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h3><p>首先介绍下 SimpleDB 的日志模块，根据 <code>LogFile</code> 可知，其日志的格式如下：</p>
<ol>
<li>文件最开始的 8 个字节，标识日志文件中最新的检查点在文件中的偏移</li>
<li>之后每条记录的格式为 <code>&lt;int Type, long tid, [additional info], offset&gt;</code>，其中<ol>
<li> Type 标识了日志的类型，有事务开始、更新数据、创建检查点、事务提交、事务中止五种</li>
<li> tid 标识了事务的 ID</li>
<li>additional info 记录了额外的数据，只有更新数据、创建检查点时存在。<ol>
<li>更新数据时，保存修改前后的页面数据 </li>
<li>创建检查点时，保存此时所有未结束的事务，及其在日志中的第一条日志的偏移</li>
</ol>
</li>
<li> offset 记录了该条日志的起始在文件中的偏移</li>
</ol>
</li>
</ol>
<p>从代码注释可以看出，在事务中止，释放其锁之前，会调用 <code>logAbort() -&gt; rollback()</code> 方法，撤销该事务的操作，并在日志中留下一条 ABORT 记录。这是需要注意的。</p>
<h3 id="Rollback"><a href="#Rollback" class="headerlink" title="Rollback"></a>Rollback</h3><p>首先要实现的 <code>rollback(tid)</code> 方法，回滚被中止掉的事务的操作。前面提到，回滚需要倒序扫描日志。不过实验报告中描述的是从事务开始扫描，并提供了 <code>tidToFirstLogRecord</code> 的 Map 来把事务映射到日志第一条记录。所以我按从头开始扫描实现的。从头扫描要注意不要重复地回滚页面，需要维护一个 Set，记录哪些页面已经被回滚过了，再次遇到就直接跳过。</p>
<h3 id="Recover"><a href="#Recover" class="headerlink" title="Recover"></a>Recover</h3><p>故障恢复则相对复杂一些。它包含三个步骤</p>
<ol>
<li>重建检查点（如果存在检查点的话），重建运行中的事务，日志偏移的 Map 映射，即 <code>tidToFirstLogRecord</code></li>
<li>从检查点（或文件开始），逐个地重做日志操作</li>
<li>到日志结束到达崩溃点时，回滚此时还在运行的事务</li>
</ol>
<p>首先，为什么可以安全地从检查点加载开始呢？因为在记录检查点的 <code>logCheckpoint</code> 方法中，调用 <code>flushAllPages()</code> 将 BufferPool 中的脏页面全部写回了磁盘。所以检查点对应的 BufferPool 中是没有脏页面的，可以安全地作为起始点。</p>
<p>其次，在重做日志中，需要注意以下事项：</p>
<ul>
<li>维护 <code>tidToFirstLogRecord</code>，因为它会影响最后回滚日志的逻辑，需要在事务开始、提交、中止时维护其状态</li>
<li><strong>需要重做 ABORT</strong>。即对于日志中的 <code>ABORT tid</code> 的记录，需要重做，即将其回滚。</li>
</ul>
<p>重做 ABORT 听上去与之前的 ARISE 算法有些矛盾。因为 ARISE 是先重做再回滚的，这里为什么在重做的中间就回滚了事务呢？事实上，这里需要区分两种要回滚的事务：</p>
<ol>
<li>日志中正常输出 ABORT 的事务：可能是因为死锁等原因中止。SimpleDB 强制这些事务中止前调用了 <code>logAbort</code> 方法，回滚操作且写到日志中。</li>
<li>崩溃时意外中止的事务：这些事务在日志中只有开始记录，没有 COMMIT 或者 ABORT 记录</li>
</ol>
<p>理解了这两者的区别，就会发现真正需要在 ARISE 第三阶段回滚的，是第二类的事务。第一类的事务是正常的回滚，在重做时，需要同样回滚才能保持后续的状态一致。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>数据库的恢复是个非常复杂的过程，本人上述的理论知识也省去了一些篇幅。SimpleDB 也对恢复过程做了简化，但这两个 Exercise 也还是 “五脏俱全” 的，包含了各种日志类型、检查点、重做和撤销等核心要点。</p>
<p>到这里，SimpleDB 的 6 个 Lab 就正式结束了。这个实验课程的质量真的很高，从底层的磁盘文件逐步地构建一个数据库系统，过程中做了很多简化以避免过于复杂的无聊操作，保留了精华的重点难点。独立地完成这个实验课，真的能让人学到很多。</p>
]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>恢复</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 5 实验报告</title>
    <url>/blog/2022/12/01/MIT-6-830-5/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Lab 5 要求实现 B + 树索引的相关逻辑，包含查找、插入、删除等，过程中需要维护 B + 树的阶性质。索引是一种数据结构，用于实现在某个字段上快速地查找和修改数据记录。对于常访问的字段，构建索引是很有必要的，B+ 树是最为广泛使用的数据库索引。</p>
<span id="more"></span>

<h2 id="理论知识"><a href="#理论知识" class="headerlink" title="理论知识"></a>理论知识</h2><p>索引用于加快高频字段的查找效率，例如用户 ID 这种字段，基本哪里都要用，每次都做全表扫描是不现实的。索引和记录间形成了一个一对一（多）的关系，取决于索引的键。下面先从简单的搜索树开始，对索引进行介绍。</p>
<h3 id="高扇出搜索树"><a href="#高扇出搜索树" class="headerlink" title="高扇出搜索树"></a>高扇出搜索树</h3><p>先考虑简单的情况，在磁盘上按某个字段对表进行了排序。这种情况下不需要记录前后指针，因为物理上这些页面都是按序排列的。然后需要构建索引。为什么不用二分查找呢，因为二分查找也会导致 $log_2N$ 的页面 IO，是比较低效的。<br>可以构建一个 <code>&lt;key, Record&gt;</code> 的索引，由于 record 可能很大，而 key 很小，这种存储也很低效。因此可以使用指向 Page 的指针，<code>&lt;key, PageId&gt;</code>，构建这样的一个搜索文件，在页面内完成二分查找。<br>这种算法是复杂度与之前的二分查找类似，只是<strong>常数更小，因为每个页面能存储更多的索引</strong>。这个过程可以递归完成，直到最顶层的索引只需要记录在一个页面中，变成了一个多叉的搜索树。<br>在该树下进行二分查找时，会在每个节点完成二分查找，然后定位下一层的节点，直到找到页面内。复杂度为 $log_F (#Pages)$，F 为节点的扇出（相较于 2 有了很大改进）。<br>分析该算法可知，该算法具有以下特点：</p>
<ol>
<li>支持连续扫描。因为数据还是连续存储的</li>
<li>高扇出。因为索引记录比记录小很多，一个页面可以存储很多条索引。</li>
<li>不支持插入。当需要插入时，可能会导致页面溢出，这种情况需要后接页面形成链表。当插入过多时，就退化成了线性扫描。</li>
</ol>
<p>上述这种算法称为 ISAM（Indexed Sequential Access Method），上世纪由 IBM 提出。</p>
<h3 id="B-树"><a href="#B-树" class="headerlink" title="B+树"></a>B + 树</h3><p>B + 树与上面的搜索树相似，也是一种多叉搜索树，但它支持动态插入，而且总是平衡的。B + 树的阶记为 d，每个内部节点（根节点除外）的子节点数量需要处在 [d,2d] 之间，每个节点的最大扇出是 2d+1，即 2d 个子节点划分得到 2d+1 个区间。B + 树的叶子节点上保存了左右兄弟的指针，可用于线性扫描。</p>
<p><img src="/blog/btree.png"></p>
<p>B + 树与 ISAM 的区别在于，<strong>在底层的叶子页面上，不需要严格按顺序排列</strong>，如上图所示。Page 3 和 Page 5 是邻居的关系，在磁盘上中间还隔着 Page 4。但是叶子节点间的前后指针使得可以遍历叶子页面。这也允许了动态地插入和删除。</p>
<p>典型设置，B + 树的阶为 1600，fill-factor 为 67%（叶子页面的记录占比）</p>
<ul>
<li>平均扇出为 2144</li>
<li> 假设是 128KB 的页面，每条记录 40B</li>
<li> 高度为 1 的树，$2144^2=4,596,736$ records</li>
<li> 高度为 2 的树，$2144^3=9,855,401,984$ records 高</li>
</ul>
<p>高度为 2 的 B + 树就可以容纳近 10 亿的数据，B + 树会非常的矮，进而提高查找效率。B + 树的高度很少超过 3 和 4。</p>
<p><strong>查找</strong>：B + 树的查找与 ISAM 类似，从根节点起，在内部节点内做二分查找定位子节点，直到定位至叶子页面，再在页面内做二分查找。</p>
<p><strong>插入</strong>：</p>
<ul>
<li>当要插入的页面还有空间时，可以直接在页面内插入并排序。</li>
<li>当没有空间时，需要新建页面，并将一半的数据转移到新页面，将新页面插入到父节点中<ul>
<li>如果父节点也满了，递归向上</li>
</ul>
</li>
</ul>
<p><strong>删除</strong>：删去元组后，可能会导致页面不满足阶约束，可以选择：</p>
<ul>
<li>直接忽略：数据库场景中，一般插入比删除多，因此删除多出来的空间可以保留，等待后续插入即可</li>
<li>维护约束：<ul>
<li>当页面不满足约束时，从兄弟页面匀一些多余的元组过来</li>
<li>如果兄弟页面也没有多余的，就需要合并两个页面，递归向上删除节点</li>
</ul>
</li>
</ul>
<h2 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h2><h3 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h3><p>首先，先介绍下 SimpleDB 的 B + 树是怎么设计的。B + 树的页面被分为四种：</p>
<ul>
<li>BTreeHeaderPage：保存 B + 树索引文件的首部信息</li>
<li> BTreeRootPtrPage：用于保存 B + 树根节点的指向，类似一个假根节点，避免插入过程中根节点变化</li>
<li> BTreeInternalPage：非叶子节点，保存 m 个分界点以及 m+1 个子节点</li>
<li> BTreeLeafPage：叶子结点，保存元组数据，以及左右兄弟指针</li>
</ul>
<p>重点需要打交道的是最后两种，它们继承了抽象类 <code>BtreePage</code>，每个页面内包含一个父指针，用于处理递归向上的逻辑。<code>BTreeInternalPage</code> 暴露了一个 <code>Iterator&lt;BTreeEntry&gt;</code>，其中 <code>BTreeEntry</code> 包含以下属性：</p>
<ul>
<li>key：一个值，代表分界点</li>
<li> leftChildId：左孩子的页面 ID</li>
<li>rightChildId：右孩子的页面 ID</li>
</ul>
<p>一个 <code>BTreeInternalPage</code> 保存 m 个分界点，即 m+1 个子节点。</p>
<h3 id="Search"><a href="#Search" class="headerlink" title="Search"></a>Search</h3><p>首先，要实现的是查找方法，<code>findLeafPage()</code>，该方法接收一个页面和 <code>Field</code>，返回这个值对应的叶子页面。上面提到，正常在节点内应该是使用二分查找找到对应的孩子节点。由于 <code>BTreeInternalPage</code> 只对外暴露了<code> Iterator&lt;BTreeEntry&gt; iterator()</code> 方法，这里只能使用遍历的方法，找到孩子节点对应的值区间。再递归查找直到找到叶子页面。</p>
<h3 id="Insert-（Split）"><a href="#Insert-（Split）" class="headerlink" title="Insert （Split）"></a>Insert （Split）</h3><p>然后来到了重头戏，B + 树的插入。Lab 里将拆分节点分为了两个方法，需要分别实现：</p>
<ul>
<li><code>splitLeafPage()</code>：拆分叶子结点，可能需要递归调用父节点的拆分</li>
<li><code>splitInernalPage()</code>：拆分非叶子节点，可能需要递归调用父节点的拆分</li>
</ul>
<p>拆分节点的逻辑可以分为下面几步：</p>
<ol>
<li>新建空白页面，转移一半的数据到新页面，一般是把值较大的一半转移过去</li>
<li>判断父节点是否有空槽，没有的话递归拆父节点</li>
<li>将新页面关联到父页面<ul>
<li>维护两个页面和父页面间的指向</li>
<li>在父页面新建 entry，key 为大页面的最小值， 左右孩子分别为原页面和新页面</li>
</ul>
</li>
<li>对于叶子结点，维护左右兄弟指针</li>
</ol>
<p>这里需要注意的是，要把修改后的页面更新在 <code>dirtypages</code> 中，否则会出现读取不一致的现象。Lab 提供了递归的拆分辅助方法，名为 <code>getParentWithEmptySlots</code>，其会判断父页面中是否有空槽，没有的话调用 <code>splitInternalPage</code> 将其拆分，返回一个带有空槽的父页面。</p>
<h3 id="Delete-Steal"><a href="#Delete-Steal" class="headerlink" title="Delete (Steal)"></a>Delete (Steal)</h3><p>这个 exercise 要求实现带 “偷取” 逻辑。当一个页面不满足半满约束时，需要从它的兄弟页面中匀一些多余的数据过来。根据页面、兄弟的类型，这部分逻辑被分散在三个方法中：</p>
<ul>
<li><code>stealFromLeafPage</code>：叶子结点间的偷取，<code>isRightSibling</code> 参数标识是否是右侧兄弟</li>
<li><code>stealFromLeftInternalPage</code>：从左侧内部节点偷取</li>
<li><code>stealFromRightInternalPage</code>：从右侧内部节点偷取</li>
</ul>
<p>偷取的逻辑可以分为下面几步：</p>
<ol>
<li>均匀地把数据匀过去，对于每条数据<ol>
<li>如果是叶子页面的元组记录，转移即可</li>
<li>如果是内部节点的 key 和 child 记录，需要新建 Entry 插入</li>
</ol>
</li>
<li>更新父节点中 Entry 的 key 值，需要根据左右关系，选择大页面中的最小值作为新的 key 值</li>
<li>更新父节点指向关系</li>
</ol>
<h3 id="Delete（Merge）"><a href="#Delete（Merge）" class="headerlink" title="Delete（Merge）"></a>Delete（Merge）</h3><p>当偷取已经不能满足需要的时候，需要合并两个均达不到半满的节点。同样的，根据节点类型，可以分为：</p>
<ul>
<li><code>mergeLeafPages</code>：合并两个叶子结点</li>
<li><code>mergeInternalPages</code>：合并内部节点</li>
</ul>
<p>合并可以看成偷取的一种极端情况，将两个节点所有的数据都匀到一个节点中，然后删除掉空页面以及父节点中的对应 entry。这个过程中也可能会导致递归向上删除。Lab 提供了 <code>deleteParentEntry</code> 的工具方法来处理删除父节点的 entry 后不满足半满约束的情况，会调用 <code>handleMinOccupancyPage</code> 根据情况调用偷取和合并方法。</p>
<p>当四个 exercise 做完，理论上已经可以通过这四个 exercise 的所有 Test。由于 SimpleDB 本身是使用的页面级别的锁，不存在意向锁的 phantom 问题，如果锁实现正确的话，应该也可以通过 BTreeNextKeyLockingTest 和 BTreeDeadlockTest。如果到目前为止一切都正确，应该可以通过一个很难的 <code>BTreeTest</code>。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本次 Lab 主要完成了 B + 树的增删查逻辑。Lab 提供了 B + 树索引的整体框架，只需要实现核心的增删查逻辑即可。B + 树还是一个非常复杂的数据结构，自己从头实现的话估计得很久。不过经过这个 Lab，对 B + 树的操作有了更深的认识。</p>
]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>B+树</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 4 实验报告</title>
    <url>/blog/2022/11/29/MIT-6-830-4/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本次 Lab 主要实现的数据库的事务功能，包含并发控制、死锁检测等。本 Lab 要求实现一个页面粒度的锁管理器，支持多事务的并发，且使用等待图完成死锁的检测。</p>
<span id="more"></span>

<h2 id="理论知识"><a href="#理论知识" class="headerlink" title="理论知识"></a>理论知识</h2><p>事务是一个包含多个操作的序列，应当被原子化地执行，其具有以下 ACID 特性：</p>
<ul>
<li>原子性（Atomicity:）：事务中的操作要么都被执行，要么都不被执行。</li>
<li>一致性（Consistency）：数据库是执行事务前是一致的，在执行事务后依然是一致的。</li>
<li>隔离性（Isolation）：每个事务的执行与其他事务隔离。实际上 DBMS （数据库管理系统）会在多个事务间交叉执行，并不会按顺序一个个执行，但 DBMS 会保证每个事务看起来是隔离执行的，隐藏了并发的细节。</li>
<li>持久性（Durability）：如果事务提交到数据库，结果会持久存在，即使刚提交 DBMS 就崩溃了也一样。</li>
</ul>
<p>事务管理器包含以下两个模块：</p>
<ul>
<li>锁管理器：使用共享锁、排它锁、意向锁等实现事务间访问的并发控制</li>
<li>日志和恢复：当事务需要回滚时，逐步地撤销事务已完成的操作</li>
</ul>
<h3 id="严格两阶段锁"><a href="#严格两阶段锁" class="headerlink" title="严格两阶段锁"></a>严格两阶段锁</h3><p>为了保证事务的 ACID 特性，DBMS 使用严格两阶段锁（Strict Two Phase Locking）来完成事务的锁管理，该方法具有以下特点：</p>
<ul>
<li>每个资源有一个共享锁（S，Shared）和排它锁（Exclusive，X）。<ul>
<li>至多一个事务可以持有该资源的排它锁，但是很多其他事务可以持有其的共享锁。</li>
</ul>
</li>
<li>事务在读之前必须获得排它锁，在写之前必须获得共享锁</li>
<li><strong>事务在释放任何锁之后都无法获得新锁</strong>。这是保证可串行性的关键。</li>
<li><strong>严格两阶段锁特性</strong>：事务在结束前统一释放锁，而非在过程中逐个释放锁。</li>
</ul>
<p>最后一点是为了避免级联回滚的问题。级联回滚意味着回滚一个事务会要求回滚另一个事务。考虑下面的场景</p>
<ol>
<li>T1 先读、写了 R，但是还没有提交到 DBMS</li>
<li>T2 后读、写了 R</li>
</ol>
<p>这时，要回滚 T1，必须回滚 T2，因为 T2 读的是 T1 写后的数据，如果 T1 回滚了，就不存在 T2 读的数据了，即脏数据。</p>
<h3 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h3><p>当事务间存在资源的循环等待，满足死锁条件时，就会发生死锁。在操作系统上，通常有以下三种方法处理死锁：</p>
<ul>
<li>预防。按照指定的顺序请求资源，例如静态指定顺序，打破循环等待条件。</li>
<li>避免。分配时预测是否可能出现死锁。</li>
<li>检测和处理。周期性检测是否有死锁，并处理</li>
</ul>
<p>也有一些数据库系统，直接不作处理。如果事务超时了就猜测发生了死锁，直接中止回滚。但这也会误伤真的需要很久才能计算出结果的事务，不提倡。</p>
<p>DBMS 广泛使用的是基于等待图的死锁检测。一个后台进程周期性地构建事务间的依赖图，当依赖图存在环时，就出现了死锁的循环等待条件，即出现了死锁。这种情况下就需要中止某个环内的事务打破循环，可以根据某些指标确定优先级，例如事务的持续时间、持有锁的数量等。</p>
<h3 id="意向锁"><a href="#意向锁" class="headerlink" title="意向锁"></a>意向锁</h3><p>锁的粒度需要同时考虑并发性能和管理成本。考虑极端情况：</p>
<ul>
<li>只使用一个锁，锁住整个数据库，很简单，但是不能并行</li>
<li>对每个元组上锁，可以很好的并发，但是锁太多，需要庞大的内存开销，锁管理的负载也会很大</li>
</ul>
<p>因此，需要折中：</p>
<ul>
<li>细粒度的锁有利于并发</li>
<li>少数的锁方便管理</li>
</ul>
<p>多粒度加锁可以较好地折中，其思想是：</p>
<ul>
<li>不应该为所有的事务设置相同的锁粒度</li>
<li>允许数据可变规模</li>
<li>定义一个加锁的层级，高层包含低层<ul>
<li>可以表示为一棵树</li>
</ul>
</li>
</ul>
<p>当对底层的元组或者页面加共享锁 / 排他锁时，需要相应的为高层页面加意向共享锁和意向排他锁。意向锁的设计允许高层节点进行 S 和 X 加锁时，无需检查所有的底层节点。如果没有意向锁，需要遍历所有底层节点才能知道能否给这个节点加锁。</p>
<h2 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h2><h3 id="LockManager"><a href="#LockManager" class="headerlink" title="LockManager"></a>LockManager</h3><p>该 Exercise 要求实现 <code>BufferPool</code> 中的读页面请求锁、释放锁等功能，指导书要求在页面层级管理锁，不能直接在表层级加锁。指导书建议使用一个 LockManager 类来管理所有的锁，我先介绍下我对这个类的设计。先从简单的资源抽象开始，一个资源应该由一个唯一键标识，例如 Page 有 PageId，Tuple 有 RecordId。因此，LockManager 实际是维护了一个资源 - 锁的 Map，资源是任意类型的 Object。而锁对象（<code>LockItem</code>），应该包含以下属性：持有锁的事务，当前锁的类型。当一个加锁请求（资源，锁类型，事务）到达锁管理器时：</p>
<ol>
<li>先获取资源的 <code>LockItem</code></li>
<li><code>LockItem</code> 内，同步地判断能否完成这次加锁<ul>
<li>若可以（锁无事务持有，或类型兼容，或可以直接升级锁），则加锁返回</li>
<li>否则，阻塞在 <code>LockItem</code> 对象上，等待其他事务释放锁后将其唤醒</li>
</ul>
</li>
</ol>
<p>当释放锁请求到达锁管理器时：</p>
<ol>
<li>先获取资源的 <code>LockItem</code></li>
<li><code>LockItem</code> 内，同步地释放锁，并 <code>notifyAll</code> 其他阻塞线程</li>
</ol>
<p>在设计上，<code>LockManager</code> 应当遵循单例模式。我查阅了 JAVA 实现单例模式的几种方法，最优雅的就是使用枚举类。JAVA 保证了枚举类型的每个值都有唯一实例，进而可以简洁优雅、线程安全的实现单例模式。</p>
<h3 id="Lock-Lifetime"><a href="#Lock-Lifetime" class="headerlink" title="Lock Lifetime"></a>Lock Lifetime</h3><p>该 Exercise 要实现严格两阶段的锁生命周期：逐个地申请锁，只有在事务结束时才可以统一释放锁。锁的申请上，可以直接在 <code>BufferPool.getPage</code> 中进行申请，遵循一个即用即申的模式。</p>
<p>锁的释放上，正常应该在调用 <code>BufferPool.transactionComplete</code> 方法时，统一地释放所有锁。但会有些特殊情况，例如在插入元组的时候，扫到了一个页面没有空槽，这种情况下其实可以释放共享锁了。这看起来跟严格两阶段锁是违背的，但是实际上并不影响，因为插入元组这个操作中，只会对有空槽的页面造成影响。把这些锁及时释放可以允许更多的并行。</p>
<h3 id="NO-STEAL"><a href="#NO-STEAL" class="headerlink" title="NO STEAL"></a>NO STEAL</h3><p>由于种种原因，事务可能中止，这时需要逐个地回滚事务已经完成的操作。在本次 Lab 中，由于还没有日志模块记录已经完成的操作，要使用 NO STEAL 的模式。该模式的思想是，只有当事务提交到 DBMS 后，才将其脏页面写回磁盘。在页面逐出时不能逐出脏页面。这种思想非常的简单有效，只有事务提交后，脏页面写回磁盘，结果才真正生效。当事务中止时，直接将脏页面丢弃即可，无需额外回滚。但是这种方法也有问题，最大的问题是，BufferPool 中只有脏页面的时候，DBMS 就直接崩溃了。不过这种思想还是有好处的，事实上，页面逐出时，逐出脏页面会导致额外的写回开销，有一种策略就是优先逐出非脏页面，减少 IO。</p>
<p>这个 Exercise 在页面逐出时，按照优先级（例如 LRU）找到第一个非脏页面逐出即可。若页面全脏则抛出异常。</p>
<h3 id="Transactions"><a href="#Transactions" class="headerlink" title="Transactions"></a>Transactions</h3><p>根据上面已经实现的逻辑，实现 <code>transactionComplete(tid, abort)</code> 方法。当事务中止结束时，丢弃脏页面；提交结束后，写回脏页面。无论哪种结束，事务都需要释放持有的所有锁。</p>
<h3 id="Deadlocks-and-Aborts"><a href="#Deadlocks-and-Aborts" class="headerlink" title="Deadlocks and Aborts"></a>Deadlocks and Aborts</h3><p>最后来到了本 Lab 的重头戏，死锁的检测。正如前文所说，避免死锁的方法有简单的超时中止，还有基于检测的等待图。指导书要求不能使用简单的超时中止策略，建议在每次分配锁前构建等待图，判断是否会发生死锁。这时，申请加锁就变成了下面的流程：</p>
<ol>
<li>判断此次加锁是否会引入新的等待关系<ul>
<li>会，构建等待图，判断是否有死锁<ul>
<li>有，抛出异常</li>
<li>没有，跳出</li>
</ul>
</li>
<li>不会，跳出</li>
</ul>
</li>
<li>正常申请锁</li>
</ol>
<p>可以看到，最重要的是多了一个<code>等待图</code>对象，需要保证它线程安全。可以直接使用一个 <code>HashMap&lt;TransacationId, LockItem&gt;</code>，<code>HashMap</code> 本身不是线程安全的，需要读写时加 <code>synchornized</code>，也可以使用线程安全的、并发性能更好的 <code>ConcurrentHashMap</code>。它将 <code>HashMap</code> 按键分成了若干个段，读写时只对对应的段加锁，提升了并发性能。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本次 Lab 主要实现了事务的并发控制和死锁检测。正如指导书里面提到的，对并发进行 debug 是非常困难的事情。我本人花了一天时间才找到最后一个 exercise 的代码里的 bug 在哪里。当时还看了很多开源的代码时间，发现全是朴素地超时中止策略，没有使用等待图的。自己实现一遍还是收获颇丰。</p>
]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 3 实验报告</title>
    <url>/blog/2022/11/26/MIT-6-830-3/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Lab 3 要实现的是查询优化模块。在数据库中，查询优化主要在查询被解析为抽象语法树后被调用，用于为指定的查询找到 “最优的” 执行计划。在 SimpleDb 中，这部分定义在 Optimizer 模块中，主要对联合操作进行优化。</p>
<span id="more"></span>

<h2 id="理论知识"><a href="#理论知识" class="headerlink" title="理论知识"></a>理论知识</h2><p>查询优化模块的作用是为指定的查询找到高效的执行方案。由于 SQL 是声明式语言，查询只声明了想要的结果，并没有规定执行的具体方案。这些结果等价的方案存在于一个计划空间中，包含物理上等价或者关系代数等价的方案。</p>
<ul>
<li>物理等价是指操作的不同物理实现，比如 Join 是使用嵌套循环、Sort-Merge 还是其他，Scan 是使用 Heap Scan 还是 Index Scan，等等。</li>
<li>关系代数等价是指，关系代数上等价的关系。例如一些操作的顺序是可交换的（例如 projection），多种同种操作是可串联的（例如 filter），等等。</li>
</ul>
<p>查询优化的终极目的，是在这个庞大的空间中找到实际上 “最优的” 方案，它使用以下子模块来完成：</p>
<ul>
<li>计划生成器：生成新的、结果等价的计划</li>
<li>成本估计器：估计一个计划的成本，包含 IO 和 CPU，其中 IO 占主要部分</li>
<li>搜索策略：如何在计划空间中行走，常使用动态规划策略</li>
</ul>
<p>为了对搜索空间进行剪枝，查询优化的开山之作，System R 提出只考虑左深树，即执行树只有左侧是深的。这种树可以较好地流水线化。值得注意的是，由于各种原因，查询优化实际上经常找不到最优的实际执行方案，例如，成本估计器的误差，成本估计器基于选择度去计算操作前后的表规模，进而估算 IO 和 CPU cost，每一步都会有误差。因此，实际上，查询优化器是去除一些看起来特别差的执行计划，与理想最优还有差距。</p>
<p>查询优化经常遵循一些启发式的策略，例如：</p>
<ul>
<li>尽早完成列和行的筛选，减小数据规模</li>
<li>避免表间的笛卡尔积</li>
<li>… 等等</li>
</ul>
<h2 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h2><h3 id="IntHistogram"><a href="#IntHistogram" class="headerlink" title="IntHistogram"></a>IntHistogram</h3><p>Histogram，即直方图，用于记录字段的统计信息，即字段值的分布，进而用于估计不同操作的选择度。这在成本估计器中会用到。这类直方图在构建时常用若干个等宽的桶，对应值区间，然后在这些桶间构建分布。IntHistogram 是对整形字段进行统计的类，它的构造函数如下：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">IntHistogram</span><span class="params">(<span class="keyword">int</span> buckets, <span class="keyword">int</span> min, <span class="keyword">int</span> max)</span> </span>{</span><br><span class="line">    <span class="comment">// some code goes here</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>即根据桶的数量、最小值、最大值初始化一个直方图，然后通过 <code>addValue</code> 方法逐个将值加入，最后通过 <code>estimateSelectivity</code> 等方法，估计操作符的选择度。实现逻辑就是将值域等距地划分为指定数量的桶，在添加值时对应桶计数 + 1,。在估计选择度时，要按照两步走的方法，例如 &lt;=，先估计 &lt; 的比率，在桶内再根据均匀分布，估计 = 的比率，求和即可。</p>
<p>值得注意的是，StringHistogram 是通过将字符串映射到整形，再通过 IntHistogram 实现的。但是这两个类并没有实现同样的接口，这给后面带来了麻烦。</p>
<h3 id="TableStats"><a href="#TableStats" class="headerlink" title="TableStats"></a>TableStats</h3><p>TableStats 记录了整张表的统计信息，包含每个字段的直方图，以及表的基数。正常应该是保存一个 <code>List&lt;Histogram&gt;</code>，用于保存每个字段的直方图。但并没有 <code>Histogram</code> 这样的接口，而重构代码还需要去修改测试代码，也并不是一种好的做法。我这里使用了一个 <code>List&lt;Object&gt;</code> 加强制类型转换完成，也不是一种值得提倡的做法。不过可以先达到效果。</p>
<p>在确定了如何保存直方图，还有一个问题是如何获得字段的最大最小值。因为直方图的构建需要指定最大最小值，之后才能一个个地添加值。这个过程不是一次遍历能够完成的。因此，需要先遍历一次，记录每个字段的最大最小值，构建直方图，再遍历一次逐个添加值。</p>
<h3 id="Join-Cardinality"><a href="#Join-Cardinality" class="headerlink" title="Join Cardinality"></a>Join Cardinality</h3><p>这个 exercise 要求去根据公式估计 join 操作结果的基数和操作成本。操作成本非常简单，根据在 Lab 2 中实现的 Join 策略计算即可。我 Lab 2 是使用的粗暴的嵌套循环的方法，将 IO 成本与 CPU 计算成本相加即可。join 操作的基数则相对难以估计。如果是 equijoin 且某侧字段为主键，那么基数一定不会超过另一张表的基数。这很容易理解，因为左侧主键是不重复的，右侧表中与主键相同的记录可以保留，其他则不会。如果不存在这样的条件，就需要一些策略去估计了：</p>
<ul>
<li>简单启发估计：如果是 equijoin，可以使用较大表的基数作为估计。如果是 range-join，可以按照一定的比例 * 笛卡尔积的基数，作为估计。</li>
<li>基于直方图：要获得准确一点的结果，需要根据两侧的直方图进行估计。例如 equijoin，对于每一个值，去估计在另一个直方图中的数量，然后两个数量相乘最后求和即可。range-join 则要在前一步的基础上，考虑两侧的比率。</li>
</ul>
<h3 id="Join-Ordering"><a href="#Join-Ordering" class="headerlink" title="Join Ordering"></a>Join Ordering</h3><p>最后一个 Exercise 要求实现找出多个 join 的最优执行方案。Join 操作的被封装在了 <code>LogicalJoinNode</code> 这个类中，包含 Join 的信息，例如两侧表、别名、Join 条件等。目标就是接收一个 <code>List&lt;LogicalJoinNode&gt;</code> 作为输入，找到最优的左深树联合顺序 <code>List&lt;LogicalJoinNode&gt;</code>。伪代码如下所示：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1. j = set of join nodes</span><br><span class="line">2. for (i in 1...|j|):</span><br><span class="line">3.     for s in {all length i subsets of j}</span><br><span class="line">4.       bestPlan = {}</span><br><span class="line">5.       for s' in {all length d-1 subsets of s}</span><br><span class="line">6.            subplan = optjoin(s')</span><br><span class="line">7.            plan = best way to join (s-s') to subplan</span><br><span class="line">8.            if (cost(plan) &lt; cost(bestPlan))</span><br><span class="line">9.               bestPlan = plan</span><br><span class="line">10.      optjoin(s) = bestPlan</span><br><span class="line">11. return optjoin(j)</span><br></pre></td></tr></tbody></table></figure>

<p>遵循动态规划的思想，按照规模从小到大的顺序建动态规划表。对于给定的规模 i，需要枚举所有该规模的子集，根据小规模的最优结果计算该子集的最优结果。这个伪代码看起来很简单，但是有以下几个难点：</p>
<ul>
<li>如何枚举指定规模的所有子集</li>
<li>如何计算指定 join 顺序的成本，如果子集的最优 join 顺序不支持新元素插入在最后面怎么办</li>
</ul>
<p>实验中给定了一些辅助方法，帮助我们解决了这些问题，虽然并不优雅：</p>
<ul>
<li><code>enumerateSubsets</code> 方法从规模为 1 开始，构建规模为 iS 支持，则直接跳过</li>
</ul>
<p>其中，<code>enumerateSubsets</code> 是一种非常低效的枚举策略，一次性地创建规模为 i 的 Set，会有较大的内存开销。正常的策略是遵循迭代器的模式，每次返回一个新的子集。join 顺序不支持直接跳过也是一种妥协，在动态规划的设定下。</p>
<p>这个 Exercise 就是在上面辅助函数的帮助下，实现上述伪代码。这里我写了一个迭代器类，替代一次返回所有结果的枚举函数。这个枚举函数实际上是在计算组合方案，相对容易改造成迭代器，维护多个指针，然后每次获取新元素时更新指针位置即可。如果是要计算排列方案的话，就要用那个基于左右箭头的算法了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>查询优化这个 Lab 本身难度不大，属于麻雀虽小五脏俱全。对于成本估计、方案生成、动态规划每部分都涉及到了，但都不深。查询优化的理论知识要更为复杂，只是获得一个搜索空间就要考虑各种关系代数的等价性、Join 交换时的条件问题等等。对于非左深树，这个空间会更大，剪枝和搜索也会更难。动态规划搜索理论上还要考虑每种方案的物理执行类型，是否有可用的排序结果等等。真正完成这些实践就属于重复造轮子了，过于复杂且没有必要。</p>
]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 2 实验报告</title>
    <url>/blog/2022/11/20/MIT-6-830-2/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>书接上回，本次 lab 的实验目标是实现数据库的各种操作符，包含 filter、join、aggregate、insert、delete 等。此外，还需要实现第一节没有实现的页面调度算法，处理 <code>BufferPool</code> 满时的页面调度，此外还有脏页面写回等操作。</p>
<span id="more"></span>

<p>下面将按 Exercise 的顺序一个个进行介绍。</p>
<h2 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h2><h3 id="OpIterator-amp-Operator"><a href="#OpIterator-amp-Operator" class="headerlink" title="OpIterator &amp; Operator"></a>OpIterator &amp; Operator</h3><p>在实现之前，先介绍下操作符的规范接口。基础接口是 <code>OpIterator</code>，它定义了以下方法，本质上是保存了一张临时的表，可以通过 <code>getTupleDesc()</code> 获取表的描述符，通过反复调用 <code>next()</code> 遍历表的每一行。<code>SeqScan</code> 操作符直接实现了 <code>OpIteraotr</code> 接口，根据表的 id 创建这样的迭代器，作为后续操作符的参数。</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">open</span><span class="params">()</span></span>; <span class="comment">// 开启迭代器</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span></span>; <span class="comment">// 是否有下一个元素</span></span><br><span class="line"><span class="function">Tuple <span class="title">next</span><span class="params">()</span></span>; <span class="comment">// 获取下一个元组</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">rewind</span><span class="params">()</span></span>; <span class="comment">// 迭代器指针重置</span></span><br><span class="line"><span class="function">TupleDesc <span class="title">getTupleDesc</span><span class="params">()</span></span>; <span class="comment">// 获取表的描述符</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>; <span class="comment">// 关闭迭代器</span></span><br></pre></td></tr></tbody></table></figure>
<p>操作符规范是一个抽象类 <code>Operator</code>，实现了 <code>OpIterator</code> 的 <code>open</code>、<code>close</code>、<code>hasnext</code>、<code>next</code> 方法，避免逻辑冗余。抽象类中包含以下为实现的方法：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">abstract</span> Tuple <span class="title">fetchNext</span><span class="params">()</span> <span class="comment">// 获取下一个元组，不存在则返回null</span></span></span><br><span class="line"><span class="function">TupleDesc <span class="title">getTupleDesc</span><span class="params">()</span></span>; <span class="comment">// 获取表的描述符</span></span><br><span class="line">OpIterator[] getChildren(); <span class="comment">// 获取操作符的参数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setChildren</span><span class="params">(OpIterator[] children)</span></span>; <span class="comment">// 设置操作符的参数</span></span><br></pre></td></tr></tbody></table></figure>
<p>可以看出，一个操作符持有一个或多个的临时表 <code>OpIterator</code> 参数，本身的 <code>fetchNext</code> 则是用于迭代该操作符的结果。换而言之，<code>Operator</code> 是持有 <code>OpIteraotr[]</code> 的 <code>OpIterator</code>，进而实现了操作符的嵌套，操作符一般只接收一个表参数，除了 <code>join</code>。</p>
<h3 id="Filter-amp-Join"><a href="#Filter-amp-Join" class="headerlink" title="Filter &amp; Join"></a>Filter &amp; Join</h3><p>首先要实现的是 filter 和 join 操作，均继承实现了 <code>Operator</code> 的抽象类，分别对应 SQL 语法中的 where 从句和 join 从句。filter 操作通过 <code>Predicate</code> 对象去判断每行是否存在于最终的结果中，内部持有一个 <code>OpIteraotr</code> 参数代表要遍历的表。要做的事情也很简单，<code>fetchNext</code> 的时候一直迭代表直到找到一条满足条件的记录，返回即可。</p>
<p>Join 操作与之类似，需要先实现 <code>JoinPredicate</code> 用于匹配一对记录是否存在于最终的结果中，<code>fetchNext</code> 操作则相对复杂。<code>join</code> 遍历的实际是两张表的笛卡尔积，需要内部去维护更新这个笛卡尔积的索引，找到符合条件的一对记录后，还要将它们拼接返回。</p>
<h3 id="Aggregate"><a href="#Aggregate" class="headerlink" title="Aggregate"></a>Aggregate</h3><p>在 SimpleDB 中，仅考虑对单个字段分组、聚合。根据聚合字段在类型，可以分为整形聚合和字符串型聚合两类。整形聚合支持 max、min、avg 等数值操作，而字符串型聚合只支持 count 操作。两种聚合分别定义在 <code>IntegerAggregator</code> 和 <code>StringAggregator</code> 操作符中，实现了抽象的 <code>Aggregator</code> 接口（而不是 <code>OpIteraotr</code>），包含以下方法：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mergeTupleIntoGroup</span><span class="params">(Tuple tup)</span></span>; <span class="comment">// 将新的元组加入到分组结果中</span></span><br><span class="line"><span class="function">OpIterator <span class="title">iterator</span><span class="params">()</span></span>; <span class="comment">// 返回聚合结果的迭代器</span></span><br></pre></td></tr></tbody></table></figure>

<p>可以看到，聚合类是通过 <code>mergeTupleIntoGroup</code> 方法逐条记录地去进行分组，再通过 <code>iterator()</code> 返回结果迭代器，而不是像其他的操作符那样，初始化时就具有表作为输入。我的理解是，由于聚合操作的特点，本身就是要先读取完整个表才能得到分组和聚合结果。如果聚合操作实现操作符接口，也只能是内部再维护一个 <code>OpIterator</code> 结果，所以不如直接解耦这部分逻辑。</p>
<p>两个聚合类的内部实现其实很简单，依赖一个 &lt;GroupValue, AggregateValue&gt; 的 HashMap，对于新的元组，如果不存在分组值，就新建这个分组。这个 HashMap 可以转换成一个 <code>Iterator&lt;Tuple&gt;</code>，核心问题是怎么把这个转成 <code>OpIterator</code>。因为 Java 自带的 <code>Iterator</code> 和 <code>Stream</code> 均不支持重用，我个人构建了一个 <code>Supplier&lt;Stream&lt;Tuple&gt;&gt;</code>，在每次 rewind 的时候获取一个新的 <code>Iterator&lt;Tuple&gt;</code>，这个 <code>Supplier</code> 再通过一个适配器转换成 <code>OpIterator</code>。</p>
<h3 id="HeapFile"><a href="#HeapFile" class="headerlink" title="HeapFile"></a>HeapFile</h3><p>这里需要支持 <code>HeapFile</code> 的修改表操作，即增删元组，需要从 <code>HeapPage, HeapFile, BufferPool</code> 三层由底向上地支持增删功能。之前说过，<code>HeapFile</code> 由很多个 <code>HeapPage</code> 组成，通过 <code>BufferPool</code> 统一读取页面进行缓存。在增删元组的时候，<code>HeapFile</code> 需要找到最后一个有空 slot 的页面，插入元组并更新其 RecordId，还需要将页面标记为 “脏” 的，因为页面发生了改变，丢弃前必须写回磁盘。删除元组时，<code>HeapFile</code> 根据其 RecordId 直接定位页面，将对应 slot 标记为空，将页面标记为脏即可。而 <code>HeapPage</code> 则负责在当前页面完成增删。<code>BufferPool</code> 的插入和删除元组操作，则要先通过 <code>Catalog</code> 定位到 <code>HeapFile</code>，在获得修改后脏页面后需要更新缓存中的页面，如果之前没有标记脏位，这里也需要标记。</p>
<p>上面是简单的逻辑介绍，在实现的时候还有些细节可以处理。在插入页面时，找到有空 slot 的页面后，需要具体找到 slot 的位置。由于 bitmap 是以 <code>byte[]</code> 存储的，可以先找到不为 - 1 的 byte，再在 byte 内找到标识位不为 1 的索引。-1 代表着一个全为 1 的有符号数，对于任意长度的有符号数均是如此。因为在负数的补码实现下，最高位的权重是负的，其余位的权重是正的，$-2^n+\sum_{i=0}^{n-1} 2^i=-1$。在插入页面时，若不存在有空 slot 的页面，需要新建页面，完成插入后最好将页面写回文件，不然 <code>numPages()</code> 方法的返回结果会错误。</p>
<h3 id="Insert-amp-delete"><a href="#Insert-amp-delete" class="headerlink" title="Insert &amp; delete"></a>Insert &amp; delete</h3><p>在实现了 <code>HeapFile</code> 的插入删除操作后，这两个操作符也不难了。值得注意的是，这两个操作只需要返回一个整数元组，代表受影响的元组数量。因此 <code>fetchNext</code> 方法只应该返回一个 tuple，之后就返回 null。</p>
<h3 id="Page-eviction"><a href="#Page-eviction" class="headerlink" title="Page eviction"></a>Page eviction</h3><p>最后，需要完成 <code>BufferPool</code> 的页面调度算法，也就是页面逐出的逻辑。操作系统上的页面调度算法有很多，先进先出，时钟算法，LRU（最近未使用）等。最常用的算法就是 LRU，因为它的性能是最好的。LRU 主要包含两个操作，获取页面和逐出页面。在获取页面时，先判断页面是否在缓存区中，命中则直接返回，未命中则加载至缓存区中。逐出页面时，删除最近最少使用的页面。为了达到 $O (1)$ 的获取页面和逐出页面的复杂度，需要使用一个 HashMap 和一个链表，HashMap 存储链表中的指针，用于查询是否在缓存区中。链表按最近使用的顺序存储页面，在页面使用后或者需要删除时完成高效地移动和删除。具体可以参考 <a href="https://leetcode.cn/problems/lru-cache/">146. LRU 缓存 - 力扣（LeetCode）</a>。</p>
<p>除了页面逐出操作外，还需要实现 <code>flushPage</code> 方法将脏页面写回磁盘，但不逐出，以及 <code>discardPage</code> 将页面丢弃，不将页面写回磁盘。通过调用 <code>HeapFile.writePage</code> 以及直接操作 <code>BufferPool</code> 即可实现这两种操作。</p>
]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT 6.830 数据库实验 Lab 1 实验报告</title>
    <url>/blog/2022/11/16/MIT-6-830-1/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>最近开始学数据库了，找到了 MIT 6.830 的 Lab，感觉质量还挺高的。打算从实现上了解下数据库的细节。MIT 6.830 的实验要求使用 JAVA 语言实现一个简易的关系数据库，支持常用的增删改查操作、事务、B + 树索引、恢复等功能。这次我分享下 Lab 1 的一些总结。我把课程资料也附在了后面，有兴趣一起学习的一起来学习讨论～</p>
<span id="more"></span>

<h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>代码主要分为以下几个目录：</p>
<ul>
<li>common: 定义公用的类：Database、Catalog，其中 Database 提供了静态方法访问 Bufferpool、Catalog 的单例</li>
<li> execution: 定义了数据库操作符的 <code>OpItertor</code> 接口，支持 scan、filter、aggregate、join 等操作</li>
<li> index: B + 树索引</li>
<li> optimizer: 查询优化</li>
<li> storage: 数据库的存储，定义了 Tuple、Record、Field 等基础概念类，以及 Heap File 存储</li>
<li> transaction: 事务</li>
</ul>
<p>后面的 6 个 lab 会逐渐地实现这些功能，在第一个 lab，只需要实现一些存储逻辑。</p>
<h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2><h3 id="Field-amp-Tuple"><a href="#Field-amp-Tuple" class="headerlink" title="Field &amp; Tuple"></a>Field &amp; Tuple</h3><p>这部分要求实现基础的 Field、Tuple 类的功能，具体包含 <code>TupleDesc</code> 和 <code>Tuple</code> 两个类。</p>
<p>先介绍一下 <code>Field</code>，SimpleDB 支持两种数据类型：整数与定长字符串，两种类型定义在枚举类 <code>Type</code> 中，对应的 <code>IntField</code> 和 <code>StringField</code> 实现了 <code>Field</code> 接口，可以同种 Field 相互比较。仅有定长类型简化了数据库模型。</p>
<p><code>TupleDesc</code> 是每个数据库表的元组描述，包含若干个字段，简单来说，就是一个 (Type, Name) 的数组，记录了每个字段的类型和可为字段的域名。这个类中主要要实现构造函数和一些 <code>get</code> 方法，比较简单。值得注意的是这里要实现一个静态的 <code>Merge</code> 方法，用于将两个 <code>TupleDesc</code> 对象合并，在后面的 <code>join</code> 操作时会用到。</p>
<p><code>Tuple</code> 是一条元组，由一个 <code>TupleDesc</code> 构造而成，包含了一个 <code>Field</code> 的数组存储各个字段的值。其中，每个 <code>Tuple</code> 关联一个 <code>RecordId</code>，为该元组在表的索引 id，由页号 + 页内偏移完成，这里只需要实现 get、set 方法，recordId 后面会用到。</p>
<h3 id="Catalog"><a href="#Catalog" class="headerlink" title="Catalog"></a>Catalog</h3><p><code>Catalog</code> 用于追踪数据库中的所有表，提供了方法用于新建和删除表。正常来说，<code>Catalog</code> 应该从磁盘加载，但是在这个 lab 不需要考虑序列化和加载的逻辑，只需要实现启动后的数据库表的管理逻辑。每个数据库表包含以下几个信息：</p>
<ul>
<li>表名：一个字符串</li>
<li>文件：一个实现了 <code>DbFile</code> 的文件对象</li>
<li>表的 id：由文件对象唯一对应，一般是绝对路径的 <code>hashcode</code></li>
<li>主键：一个字符串，SimpleDB 只考虑单一主键</li>
<li>元组描述，一个 <code>TupleDesc</code> 对象</li>
</ul>
<p><code>Catalog</code> 类还提供了一个已经实现的 <code>loadSchema</code> 方法，用于从文件中读取数据库的模式并新建表。</p>
<h3 id="BufferPool"><a href="#BufferPool" class="headerlink" title="BufferPool"></a>BufferPool</h3><p><code>BufferPool</code> 类定义了一个统一的、具有缓存的页面读取方法。一般来说，操作一个页面需要先将其从磁盘加载到内存中，然后才能操作。<code>BufferPool</code> 定义了一个缓存区，当页面已经被加载到内存后，就无需再次加载了。这里后续需要实现一个类似 LRU 的页面调度算法，在本次 lab 中尚不需要。</p>
<p>本 lab 中，BufferPool 需要实现一个 get 页面的方法，当缓存区满时，只需要抛出 <code>DbException</code> 异常即可。这里需要根据 <code>PageId</code> 获取 table id，进而通过 Catalog 获取表进而读取。</p>
<h3 id="HeapPage"><a href="#HeapPage" class="headerlink" title="HeapPage"></a>HeapPage</h3><p>这个 Exercise 要实现 HeapFile 的读取方法，包含 <code>HeapPage, HeapPageId, RecordId</code> 三个类。其中，<code>RecordId</code> 类的功能已经在上面介绍，是每个元组在表中的唯一的记录 id，包含 <code>PageId</code> 和在页内的编号。<code>HeapPageId</code> 实现了 <code>PageId</code> 接口，包含一个表 id 以及页面号，与 <code>RecordId</code> 类似。这两个类都比较简单，只需要实现一些 get、equals, hashcode 方法即可。</p>
<p><code>HeapPage</code> 是 <code>HeapFile</code> 的页面类，实现了 <code>Page</code> 接口，需要支持获取页面数据、脏位标记等功能。这里先简单介绍下 <code>HeapFile</code>，这是一种无序的存储格式，记录以乱序的方式存储在页面中。每个页面由若干个插槽（slot）组成，每个 slot 可用于存储一条记录，页面首部包含一个 bitmap 记录每个 slot 是否包含记录。在插入记录的时候，找到一个空的插槽插入即可。</p>
<p>对于定长记录的表，这里就已经很圆满了。而对于非定长的表，页面内的 slot 数量是可变的，slot 的大小也是可变的。这时，需要在页面尾部记录插槽的位置、大小、数量，还要保存已经有的记录的位置和大小。放在尾部，是因为这样可以方便地增长 slot 数量，当尾部和正向的记录重合时，就意味着页面满了。这种页面还需要周期性地重排一下，避免过多的碎片。当然，这些是题外话，SimpleDB 中不需要考虑这种复杂情况。</p>
<p>本次 lab 中，<code>HeapPage</code> 类需要实现查询槽位状态、计算空槽数量等功能，还要实现一个元组的迭代器。在按上面捋顺了 HeapFile 的原理之后，就很简单了。</p>
<h3 id="HeapFile"><a href="#HeapFile" class="headerlink" title="HeapFile"></a>HeapFile</h3><p>这个类主要实现两个功能：</p>
<ul>
<li>读取指定页面。值得注意的是，数据库文件可能很大，因此不能直接把整个文件加载到内存中，需要使用支持随机读写的 <code>RandomAccessFile</code>，移动指针到指定页面的位置，再进行读取。</li>
<li>返回一个可以迭代访问数据库所有元组的 <code>DbFileIterator</code>。值得注意的是，这个 <code>DbFileIterator</code> 需要支持重用，因此其不是一个传统的 <code>Iterator</code>，需要额外的代码来实现一个 <code>DbFileIterator</code>。我建议新建一个类继承自 <code>AbstractDbFileIterator</code>，其对接口的逻辑进行了一定简化。更好的方式是实现一个通用的适配器，把传统的 <code>Iterator</code> 转换成需要的格式。</li>
</ul>
<h3 id="SeqScan"><a href="#SeqScan" class="headerlink" title="SeqScan"></a>SeqScan</h3><p>最后一个 Exercise，实现表的遍历操作。这里包装一下上面的 <code>HeapFile</code> 的 <code>Iterator</code> 逻辑，实现 <code>OpIterator</code> 接口即可。这个新的 <code>OpIterator</code> 也需要支持重用，后面也会比较麻烦。这里还不需要担心，写完，通过单元测试就收工啦！</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个 lab 主要是先熟悉了数据库的整体架构以及相互间的关联。之后，在底层实现 <code>HeapFile</code> 这一经典的文件存储方法，对于页面的读取和元组的存储有了更深的理解。最后，实现了一个数据库的遍历操作，简单了解数据库操作符的规范和接口，剩下的操作符需要在后面实现。</p>
<h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><p>我学习的是 2021 年春季的版本，相关链接如下：</p>
<ul>
<li>课程官网: <a href="http://db.lcs.mit.edu/6.5830/2021/assign.php">http://db.lcs.mit.edu/6.5830/2021/assign.php</a>, 里面有讲义、PPT，没有视频</li>
<li>代码：<a href="https://github.com/MIT-DB-Class/simple-db-hw-2021">https://github.com/MIT-DB-Class/simple-db-hw-2021</a></li>
</ul>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
      <categories>
        <category>后端</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MIT 6.830</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title>DeBERTa: 注意力解纠缠和解码加强版的 BERT</title>
    <url>/blog/2022/09/04/DEBERTA/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>DeBERTa 是微软于去年在《DEBERTA: DECODING-ENHANCED BERT WITH DIS-ENTANGLED ATTENTION》中提出的预训练模型，<br>论文收录于 ICLR 2021 中。DeBERTa (<strong>De</strong>coding-enhanced <strong>BERT</strong> with disentangled <strong>a</strong>ttention)，<br>顾名思义，相较于普通的 BERT，DeBERTa 加强了其解码能力，解耦了注意力。DeBERTa 第一次在 SuperGLEU 基准上超越了人类，<br>在 MNLI、SQuAD、RACE 数据集上相较于 RoBERTa 也有较大的提升（0.9%-3.6%）。</p>
<span id="more"></span>

<p>DeBERTa 的模型和代码开源在 <a href="https://github.com/microsoft/DeBERTa1">github</a>。<br>现在 SuperGLEU 上的第一已经是 ST-MoE-32B 了，不过排在 DeBERTa 之前的模型参数量似乎都是 DeBERTa 的数倍，<br>这说明 DeBERTa 还是很有读一下的必要的。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>DeBERTa 对于 BERT 共两个改进点，注意力解耦与增强掩码解码。具体体现在：</p>
<p><strong>注意力解纠缠。</strong>BERT 中，不同类型的 embedding 求和作为最终的表征（例如 token embedding、position embedding）。<br>DeBERTa 对每个单词使用两个向量进行表示，分别对内容、位置进行编码。单词间的注意力权重由解纠缠矩阵分别根据其内容和相对位置计算。</p>
<p><strong>增强的掩码解码器。</strong>DeBETa 也是使用掩码语言模型进行预训练，与 BERT 类似。在这个任务设定下，被掩码词的绝对位置有时是很重要的。<br>例如 “a new <strong>store</strong> opened beside the new <strong>mall</strong>.” 这句话，如果对”store” 和”wall” 进行掩码，两者的上下文、语义都接近，但是句法作用是不同的（例如谁是主语）。<br>这些细微差距很大程度上与绝对位置相关，因此 DeBERTa 在 softmax 层前合并了绝对位置嵌入。<br>在该层中，模型基于聚合上下文和绝对位置预测被掩码的单词。</p>
<p>另外，论文还提出了一种新的虚拟对抗训练方法，用于微调 plm 到下游的 NLP 任务。该方法有效地提高了模型的泛化能力。</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="解耦注意力"><a href="#解耦注意力" class="headerlink" title="解耦注意力"></a>解耦注意力</h3><p>对于位置 $i$ 的 token，论文使用两个向量 ${H_i},{P_{i|j}}$ 分别代表它的内容嵌入和与位置 $j$token 的相对距离。位置 $i$ 与位置 $j$ 的 token 间的注意力分数可以分解为下面四个部分：<br>$$<br>\begin{aligned}<br>A_{i,j}&amp;={H_i,P_{i|j}}\times{H_j,P_{j|i}}^T\<br>&amp;=H_iH_i^T+H_iP_{j|i}^T+P_{i|j}H_j^T+P_{i|j}P_{j|i}^T<br>\end{aligned}<br>$$<br>也就是说，一对 token 间的注意力权重可以用内容到内容、内容到位置、位置到内容、位置到位置四个注意力的解纠缠矩阵进行计算。</p>
<p>传统的相对位置嵌入方法只使用一个单独的嵌入矩阵，计算相对位置偏差，再用于计算注意力分数。这相当于只使用了上述公式中的 “位置到位置” 注意力。论文认为位置到内容也很重要，建模了位置和内容间的交叉关系。此外，由于 DeBERTa 本身使用的就是相对位置嵌入，所以直接将上述公式中的位置到位置注意力部分删除了。</p>
<p>接下来就是使用两套 Query、Key 矩阵，分别将内容、位置嵌入映射到 query、key，然后求解注意力分数了。</p>
<h3 id="掩码解码增强"><a href="#掩码解码增强" class="headerlink" title="掩码解码增强"></a>掩码解码增强</h3><p>为了将绝对位置信息融入到 MLM 预测中，DeBERTa 在在解码掩码字时只使用绝对位置作为补充信息，在 Transformer 层中使用相对位置信息。因此，论文称 DeBERTa 的解码组件为增强掩码解码器（Enhanced Mask Decoder，EMD），其结构如下所示，由 n 个共享权重的块堆叠而成。$H$ 代表前一层 Transformer 输出的隐藏状态，$I$ 代表解码所须的信息，例如绝对位置嵌入或前一层 EMD 的输出。第一层 EMD 的 $I$ 是绝对位置嵌入信息，后续层的 $I$ 是前一层 EMD 的输出。</p>
<p><img src="/blog/decoder-layer.png"></p>
<p>在实证研究中，论文比较了这两种使用绝对位置的方法，发现 EMD 的效果要好得多。论文推测，BERT 使用的较早的合并绝对位置可能会妨碍模型学习足够的相对位置信息。而且 EMD 的共享参数也没有过多增加模型参数量。</p>
<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><p>DeBERTa 使用了一种虚拟对抗训练算法，Scale-invariant-Fine-Tuning (SiFT)，是 Miyato 等人 (2018) 中描述的算法的变体。</p>
<p>虚拟对抗训练是一种提高模型泛化能力的正则化方法。通过改进模型对对抗样本的鲁棒性实现。对抗样本是通过对输入进行小的扰动来创建的。在 NLP 中，是在单词嵌入进行扰动，而非原始单词序列。然而，嵌入向量的取值范围在不同的词和模型中是不同的。对于具有数十亿个参数的大型模型，方差会变得更大，导致对抗训练的一些不稳定性。</p>
<p>论文受层标准化启发，提出了 SiFT 算法，通过对归一化的词嵌入应用扰动来提高训练的稳定性。具体来说，将 DeBERTa 微调到下游的 NLP 任务时，SiFT 首先将词嵌入向量归一化为随机向量，然后对归一化后的嵌入向量进行扰动。实验发现，规范化大大提高了微调模型的性能。这种改进在较大的 DeBERTa 模型中更为突出。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>论文在数个 NLU 基准上测试了 DeBERTa。</p>
<h3 id="大模型"><a href="#大模型" class="headerlink" title="大模型"></a>大模型</h3><p>与 BERT 相同的设定预训练 DeBERTa，区别在于使用的是 BPE 子词模型。预训练使用了 96 个 V100，20 天。下面是 GLEU 榜单的结果。</p>
<p><img src="/blog/GLEU.png"></p>
<p>下面是 MNLI、SQuAD 等基准上的结果。</p>
<p><img src="/blog/SQuADs.png"></p>
<p>可以看出 DeBERTa 已经超过了自回归的 XLNET，可见其性能。</p>
<h3 id="小模型"><a href="#小模型" class="headerlink" title="小模型"></a>小模型</h3><p>base 模型的设置与 BERT-base 一致，12 层的架构。结果如下，可以看出相较于 XLNET 和 RoBERTa 是有明显改进的。</p>
<p><img src="/blog/base.png"></p>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>为了验证结果，论文首先重新训练了一个 RoBERTa-base，作为基准。</p>
<p>消融实验主要测试了三种变体：</p>
<ul>
<li>-EMD：没有增强掩码解码器，也就是跟 BERT 一样直接 softmax</li>
<li>-C2P：没有内容 - 位置间的注意力</li>
<li> -P2C：没有位置 - 内容间的注意力。由于 XLNET 也使用了相对位置偏差，这种设置接近接近于 XLNET+EMD</li>
</ul>
<p><img src="/blog/ablation.png"></p>
<p>从表格可以看出，删除 DeBERTa 中的任何一个组件都会导致性能的大幅下降，尤其是在 SQuAD 数据集上。</p>
<h3 id="15亿"><a href="#15亿" class="headerlink" title="15亿"></a>15 亿</h3><p>为了测试 DeBERTa 的上限，论文还构建了一个 48 层，共计 15 亿参数的 DeBERTa，首次以单个模型在 SuperGLUE 上超过了人类，集成性能也取得了第一名，具体分数如下。</p>
<p><img src="/blog/1.5B.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文其实回答了一个蛮重要的问题，相信 NLP 新人刚了解 BERT 的时候，或多或少都会对 embedding 直接相加有一些疑问。解纠缠和绝对位置的融合，同时兼顾了绝对位置的句法信息，和相对位置的邻接关系。解纠缠也使得各个 embedding 更为灵活结合。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://aestheticisma.github.io/2021/11/24/deberta/">DeBERTa 论文解读 - Fan’s Blog (aestheticisma.github.io)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>DEBERTA</tag>
      </tags>
  </entry>
  <entry>
    <title>XMTC: 极端多标签文本分类相关工作</title>
    <url>/blog/2022/08/27/XMTC/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本博客主要总结一下近期看到的一些关于极端多标签文本分类（Extreme Multi-Label Text Classification，XMTC）的相关工作。<br>之前尝试研究了一个月，发现 SOTA 复现不太出来，只能放弃了换方向了。希望这篇博客能帮到有缘人。</p>
<span id="more"></span>

<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>极端多标签文本分类（Extreme Multi-Label Text Classification，XMTC）。给定一段文本，和极多（百、千、万乃至更多）的标签，判断文本所属的标签子集。例如，维基百科有百万级别的标签，每个百科文本都对应若干个标签。亚马逊的商品，有着复杂的垂直标签结构，标签数量也是百万级的。</p>
<h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><p>XMTC 主要面临以下挑战：</p>
<ul>
<li>稀疏性：大量标签都只有少数样本与之关联，典型的长尾分布</li>
<li>计算成本高：标签数量过多，为每个标签独立训练分类器成本过高</li>
<li>可扩展性：对新增标签的可扩展性</li>
</ul>
<p><img src="/blog/Untitled.png" alt="两个XMTC数据集上，只有一半或更少的标签关联至少5个实例"></p>
<p>两个 XMTC 数据集上，只有一半或更少的标签关联至少 5 个实例</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>XMTC 的方法可以分为以下四类：</p>
<p>一对多分类：为每个标签构建 OVR（one vs rest）分类器，可以实现较高精度。但当标签数量过多时，训练、预测开销过大。</p>
<p><strong>目标嵌入的方法</strong>：标签矩阵 $N\times L$ 中，N，L 分别为样本、标签数量。这个矩阵庞大稀疏，难以直接学习到一个可靠的映射。如果可以使用线性映射或者其他方法，完成降维 $L \rightarrow \hat L$ ，就可以学习到特征 - 降维后的标签间的可靠映射。然后再通过升维 $\hat L \rightarrow L$ ，得到真实标签。此类方法的差异主要在降维、升维技术上，例如压缩感知、奇异值分解等。</p>
<p><strong>集成树的方法</strong>：类似决策树，区别在于使用特征的加权和而非单一特征的信息增益进行划分，健壮性更好。</p>
<p><strong>深度学习方法</strong>：使用深度学习特征，而非传统的词袋、tf-idf 特征。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>其中的 L 是标签数量，可以看到大型数据集上，有几十万标签，规模是很大的。</p>
<p><img src="/blog/Untitled-1.png"></p>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>对于文档，根据分数输出一个有序的标签列表。然后使用排序指标评估：</p>
<ul>
<li>Precision@TopK: TopK 的查准率，越高越好</li>
<li> NDCG@TopK: 归一化折损累计增益，越高越好</li>
</ul>
<h2 id="XML-CNN"><a href="#XML-CNN" class="headerlink" title="XML-CNN"></a>XML-CNN</h2><p>2017，Deep Learning for Extreme Multi-label Text Classification</p>
<p>首次使用深度学习方法解决 XMTC 问题，使用 CNN + 交叉熵 loss。为了避免池化层后直接接分类线性层的参数过多，中间引入了一个较小的线性层，大小为 $h$。</p>
<p><img src="/blog/Untitled-2.png" alt="模型结构图，红色的方框为较小的线性层"></p>
<p>模型结构图，红色的方框为较小的线性层</p>
<p>结果：在六个数据集上基本达到了 SOTA，消融实验证明 BCE 损失、动态池化、小线性层都是有效的。</p>
<h2 id="Attention-XML"><a href="#Attention-XML" class="headerlink" title="Attention-XML"></a>Attention-XML</h2><p>2019 AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification</p>
<p>做法：</p>
<p>构建宽而矮的概率标签树（plt），减少错误积累</p>
<ul>
<li>首先构造初始标签树，对标签的词袋特征，自顶向下 2-means 做层次聚类，直到类别数 &lt; M</li>
<li> 对树进行压缩，得到宽而矮的概率标签树</li>
</ul>
<p>为每一层训练一个 Bi-LSTM 多分类器</p>
<ul>
<li>对每个样本，每层只使用上层该样本的 Top-C 候选的孩子节点训练，类似负采样</li>
<li>每个标签对输入序列计算 attention 得到 $\hat m_i$，通过共享的全连接、输出层映射到分数</li>
<li>预测的时候，使用链式法则，得到每个标签的概率分数</li>
<li>使用 BCE loss 训练，使用三棵 plt 集成取得最好性能</li>
</ul>
<p><img src="/blog/Untitled-3.png"></p>
<p>结果：未集成的 AttentionXML 已经超过了 XML-CNN 达到了 SOTA，集成后性能提升半个点或更多</p>
<h2 id="X-BERT"><a href="#X-BERT" class="headerlink" title="X-BERT"></a>X-BERT</h2><p>2019 X-BERT: eXtreme Multi-label Text Classification using Bidirectional Encoder Representations from Transformers</p>
<p>首次将 BERT 扩展到 XMTC 任务，解决以下挑战：</p>
<ul>
<li>直接分类忽视了标签间的依赖关系、相关性</li>
<li> SoftMax 瓶颈</li>
</ul>
<p>论文提出了一个三阶段模型：</p>
<ol>
<li>ELMo 编码标签描述 + tf-idf 特征，KMeans 聚类标签得到 K 个簇</li>
<li> BERT 完成 K 个簇上的多分类</li>
<li>使用一对多分类器完成簇内标签的判断</li>
</ol>
<p>在四个数据集上达到了 SOTA，未与 AttentionXML 比较</p>
<h2 id="X-Transformers"><a href="#X-Transformers" class="headerlink" title="X-Transformers"></a>X-Transformers</h2><p>2020，Taming Pretrained Transformers for Extreme Multi-label Text Classification</p>
<ul>
<li>XLNET 编码标签描述 + 正样本聚合特征（Positive Instance Feature Aggregation, PIFA），进行分层聚类</li>
</ul>
<p><img src="/blog/Untitled-4.png"></p>
<ul>
<li> BERT/RoBERTa/XLNET Large + 平方 Hinge-loss 微调聚类标签的多分类（三模型集成效果最好）</li>
<li>使用 tf-idf + 嵌入特征，构建线性 OVA 分类器，使用混合采样策略构建负样本<ol>
<li>使用聚类簇内其他标签对应的样本作为负样本</li>
<li>使用其他 Topb 聚类簇内标签样本作为负样本</li>
</ol>
</li>
</ul>
<p>跟 X-BERT 一批人做的，效果超过了 X-BERT，AttentionXML</p>
<h2 id="LightXML"><a href="#LightXML" class="headerlink" title="LightXML"></a>LightXML</h2><p>2021 LightXML: Transformer with Dynamic Negative Sampling for High-Performance Extreme Multi-label Text Classification</p>
<p>动机，解决 X-Transformers，Attention-XML 的以下问题：</p>
<ul>
<li>使用多个模型集成，计算资源和成本过高</li>
<li>负样本采样，降低了模型效率和准确性</li>
</ul>
<p>做法：</p>
<ul>
<li>标签聚类：使用标签的稀疏特征，构造概率标签树，与 Attention-XML 类似，但<strong>只有两层</strong>，也就是只有一层聚类结果</li>
<li>文本表示，使用 BERT/RoBERTa/XLNET base，取最后 5 层的 [CLS] 表征拼接</li>
<li>标签召回：根据文本表征完成聚类簇多分类上分数计算，采样所有正样本，由易到难地采样部分负样本</li>
<li>计算分数：根据正负样本标签嵌入、文本表征，计算分数，BCE loss</li>
</ul>
<p>训练资源：</p>
<ul>
<li>1 块 16G V100</li>
</ul>
<p><img src="/blog/Untitled-5.png"></p>
<p>数据集统计信息</p>
<p><img src="/blog/Untitled-1.png"></p>
<p>使用 3 个模型或者 3 个聚类结果进行集成</p>
<p>结果：在五个数据集上超过了 X-Transformers，达到了 SOTA。消融实验证明 5 个 cls 拼接有助于收敛，动态负采样能够提升性能</p>
<h2 id="GUDN"><a href="#GUDN" class="headerlink" title="GUDN"></a>GUDN</h2><p>2022，GUDN: A novel guide network for extreme multi-label text classification</p>
<p>动机：LightXML 未使用标签语义信息</p>
<p>做法：</p>
<ul>
<li>特征提取：BERT 编码特征文本，取最后 10 层的 CLS 拼接作为标签提取的特征</li>
<li>引导网络：事实上就是辅助 loss。使用以下两种：文档表征和标签表征间的 MSE loss；标签表征用于分类的 BCE loss</li>
<li> 分类排名：对中等数据集直接使用线性层分类；对大标签数据集使用 BOW 聚类 + 动态负采样，类似 LightXML</li>
</ul>
<p><img src="/blog/Untitled-6.png"></p>
<p>实验资源：</p>
<ul>
<li>4 块 24G 3090</li>
</ul>
<p>结果：</p>
<p>在四个数据集上基本达到了 SOTA，超过 LightXML。消融实验证明，论文提出的两个辅助 loss 是有效的。</p>
<h2 id="GLO-CALXML"><a href="#GLO-CALXML" class="headerlink" title="GLO-CALXML"></a>GLO-CALXML</h2><p>2022，Exploiting Local and Global Features in Transformer-based Extreme Multi-label Text Classification</p>
<p>这篇论文是目前的 SOTA，把 Eurlex-4k 数据集上的 precision 提高了 3 个点多，可惜我没有复现出来。</p>
<p>动机：传统方法只使用 CLS 作为文档表征，只有全局表征，可能缺失信息。</p>
<p>做法：</p>
<ul>
<li>每个标签有 global embedding 和 local embedding</li>
<li>global embedding 和 cls 计算全局概率，local embedding 作为 query 与第一层所有 token embedding 做 attention，取加权和计算局部特征的概率</li>
<li> loss：两个概率的 BCE loss 之和</li>
<li>预测：使用两个概率平均值</li>
</ul>
<p>我猜测这篇论文是从 Eurlex-4k 数据集得到的启发，该数据集由关键词组构成，并不是通顺的句子。所以编码器更高层的表征意义不大，并不存在复杂的上下文。浅层表征反而可以保留更多信息。基于此，设计一个 attention 去直接接触浅层表征是很自然的想法。而且这种 global 和 local 结合的工作印象里也挺多的，多文本分类 MLC 上好像就有。</p>
<p>实验：</p>
<ul>
<li>没有使用标签树，只在标签较少的三个数据集上进行了实验</li>
<li>使用 3 个预训练模型集成</li>
<li>在两个数据集上超过了 LightXML（只使用单模型的情况也是超过的）， 与 GUDN 互有胜负</li>
<li>消融实验证明，局部特征和全局特征的融合是有效的</li>
</ul>
<p>以下是三模型集成结果，可以看出在 Eurlex-4k 和 Wiki10-31K 上分别有 3 个点、2 个点的提升，有点降维打击了。</p>
<p><img src="/blog/GLOCALXML-exper.png"></p>
<h2 id="DEPL"><a href="#DEPL" class="headerlink" title="DEPL"></a>DEPL</h2><p>2022，Long-tailed Extreme Multi-label Text Classification with Generated Pseudo Label Descriptions</p>
<p>动机：传统模型都使用 precision@topk 指标，该指标不能很好反应尾部标签的分类性能，论文提出使用 macro-averaged F1@k，使得不同标签有相同的权重。</p>
<p><img src="/blog/Untitled-7.png" alt="横坐标，数量由少到多的标签id，纵坐标，Macro-F1@19"></p>
<p>横坐标，数量由少到多的标签 id，纵坐标，Macro-F1@19</p>
<p>做法：</p>
<p>将 XMTC 定义为检索任务，根据文档检索标签。为了解决标签文本过短的问题，使用 SVM 构建分类器，并根据 token 重要性生成标签伪描述。具体来说：</p>
<ul>
<li>基于文档的 tf-id 特征 + 点积相似度函数 + hinge loss 训练稀疏分类器，训练维度为词表大小的 label embedding，将其中元素认为是 token 对 label 的重要性，选 topk 的 token 拼接到标签名后</li>
<li>使用 BERT 编码，文档表征选择 cls，标签表征选择最后一层的池化结果，使用点积衡量相似性</li>
<li>负采样，根据第一步分类器的 top 负样本和随机填充的负样本构建标签子集</li>
<li>融合：文档表征后接线性层，得到概率分布 A；点积相似性结果得到另一个概率分布；两者平均作为最终结果</li>
</ul>
<p>框架如下：</p>
<p><img src="/blog/Untitled-8.png"></p>
<p>评价指标：除了 P@k，Macro F1@K，还有 PSP@K，PSP@K 是加权版的 P@K，预测正确尾部标签的收益更高</p>
<p>结果：在 Macro F1@k，PSP@K 上达到了 SOTA，P@K 要弱一些；消融实验证明，伪标签描述对于尾部标签预测是有效的</p>
<h2 id="TReaderXML"><a href="#TReaderXML" class="headerlink" title="TReaderXML"></a>TReaderXML</h2><p>2022 Exploiting Dynamic and Fine-grained Semantic Scope for Extreme Multi-label Text Classification</p>
<p>一篇比较奇怪的论文</p>
<p>动机：传统方法使用聚类获得标签簇，但是这些簇只有粗粒度的语义，难以建模具体语义</p>
<p>做法：构建细粒度的教师知识优化样本语义</p>
<p>基于层次结构假设，如果一个样本与标签 A 相关，与标签 A 的父标签也相关</p>
<ul>
<li><p>对于每个文档样本 x，遍历其他样本，找到与其余弦相似度 topk 的样本</p>
</li>
<li><p>将该样本对应的标签、父标签（标签树）编码平均后，得到教师知识</p>
</li>
<li><p>使用一个掩码多层自注意力（自左向右）、一个多头注意力层完成编码过程</p>
<p>  <img src="/blog/Untitled-9.png"></p>
</li>
</ul>
<p>损失函数：基于最大熵的多标签一对多损失</p>
<p><img src="/blog/Untitled-10.png"></p>
<p>实验结果：在两个数据集上超过 LightXML 达到了 SOTA</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://analyticsindiamag.com/what-is-extreme-multilabel-text-classification/">什么是极端多标签文本分类？ (analyticsindiamag.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/342922980">算法理论 01 SVD 奇异值分解 - 知乎 (zhihu.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>自然语言理解</category>
        <category>文本分类</category>
        <category>多标签分类</category>
      </categories>
      <tags>
        <tag>XMTC</tag>
        <tag>文本分类</tag>
        <tag>多标签</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch Lightning: 让 PyTorch 更为易用</title>
    <url>/blog/2022/08/11/pytorch-lightning/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>今天来介绍一个很好用的深度学习框架，PyTorch Lightning。从名字就可以看出，它是基于 PyTorch 的框架。它的核心思想是，将学术代码（模型定义、前向 / 反向、优化器、验证等）与工程代码（for-loop，保存、tensorboard 日志、训练策略等）解耦开来，使得代码更为简洁清晰。工程代码经常会出现在深度学习代码中，PyTorch Lightning 对这部分逻辑进行了封装，只需要在 Trainer 类中简单设置即可调用，无需重复造轮子。</p>
<span id="more"></span>

<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>近些年来，各种深度学习框架层出不穷，TensorFlow、PyTorch 已经成为深度学习研究人员使用最多的两个框架。其中，PyTorch 的热度连年飙升，大有超越 TensorFlow 之势。<br>基础的张量运算、计算图等功能已经被这些框架支持的很好了，后面的框架开始着力于解决其他问题，例如：</p>
<ul>
<li>transformers: 致力于简单高效地使用预训练模型，支持 PyTorch 和 TensorFlow 作为后端</li>
<li> Fairseq：提供通用建模序列任务的工具包，包含丰富高效的命令行接口，基于 PyTorch</li>
<li>Pytorch Lightning：解决深度学习代码中，学术代码、工程代码耦合性高，工程代码需要重复造轮子等问题</li>
</ul>
<p>我之前已经介绍过 transformers 了，相信做 NLP 的同学都对这个框架很熟悉了。Fairseq 我也接触过一段时间，但是由于其文档不是很详细，大部分问题都要读源码才能找到答案，<br>入门起来比较痛苦。Pytorch Lightning 就是本文的重点了，下面做详细介绍。</p>
<h2 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h2><p>引用一个官网的 gif，介绍 Pytorch Lightning 的动机是什么。一段典型的、基于 PyTorch 的深度学习代码可能是下面左侧的这样的，<br>包含：模型、优化器、数据定义，训练、验证循环逻辑。</p>
<p><img src="/blog/pl_quick_start_full_compressed.gif"></p>
<p>可以将其转化为右侧的 Lightning Module，按照以下步骤：</p>
<ul>
<li>将模型定义代码写在<code>__init__</code>中</li>
<li>定义前向传播逻辑</li>
<li>将优化器代码写在 <code>configure_optimizers</code> 钩子中</li>
<li>训练代码写在 <code>training_step</code> 钩子中，可以使用 <code>self.log</code> 随时记录变量的值，会保存在 tensorboard 中</li>
<li>验证代码写在 <code>validation_step</code> 钩子中</li>
<li>移除硬件调用<code>.cuda()</code> 等，PyTorch Lightning 会自动将模型、张量的设备放置在合适的设备；移除<code>.train()</code> 等代码，这也会自动切换</li>
<li>根据需要，重写其他钩子函数，例如 <code>validation_epoch_end</code>，对 <code>validation_step</code> 的结果进行汇总；<code>train_dataloader</code>，定义训练数据的加载逻辑</li>
<li>实例化 Lightning Module 和 Trainer 对象，传入数据集</li>
<li>定义训练参数和回调函数，例如训练设备、数量、保存策略，Early Stop、半精度等</li>
</ul>
<p>其中，最为直接的好处是，你无需关注模型和张量的设备，可以省去不计其数的<code>.cuda()</code>，再也不需要担心 device 报错了（当然，自己新建的张量还是需要调整下设备的）。<br>此外就是功能强大的 Trainer 和 tensorboard 的集成，可以非常优雅地调用。</p>
<h2 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h2><p>如果想要解锁 PyTorch Lightning 的更多玩法，可以参考 [官方文档](<a href="https://pytorch-lightning.readthedocs.io/en/latest/levels/core_skills.html">Basic skills — PyTorch Lightning 1.8.0dev documentation (pytorch-lightning.readthedocs.io)</a>)，详细地介绍了各种技巧，辅以代码示例，很容易理解上手。对于核心的 API，Lightning Module 的各种钩子，Trainer 的参数、用法，也做了详细的介绍。文档中还包含各种常见的工作流、上手教程，内容非常的齐全。</p>
<p>例如，<code>configure_optimizers</code> 支持配置多个优化器、搭配学习率衰减策略。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># most cases. no learning rate scheduler</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> Adam(self.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># multiple optimizer case (e.g.: GAN)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_dis.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">return</span> gen_opt, dis_opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with learning rate schedulers</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_dis.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    dis_sch = CosineAnnealing(dis_opt, T_max=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> [gen_opt, dis_opt], [dis_sch]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with step-based learning rate schedulers</span></span><br><span class="line"><span class="comment"># each optimizer has its own scheduler</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_dis.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    gen_sch = {</span><br><span class="line">        <span class="string">'scheduler'</span>: ExponentialLR(gen_opt, <span class="number">0.99</span>),</span><br><span class="line">        <span class="string">'interval'</span>: <span class="string">'step'</span>  <span class="comment"># called after each training step</span></span><br><span class="line">    }</span><br><span class="line">    dis_sch = CosineAnnealing(dis_opt, T_max=<span class="number">10</span>) <span class="comment"># called every epoch</span></span><br><span class="line">    <span class="keyword">return</span> [gen_opt, dis_opt], [gen_sch, dis_sch]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with optimizer frequencies</span></span><br><span class="line"><span class="comment"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1704.00028</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_dis.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    n_critic = <span class="number">5</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        {<span class="string">'optimizer'</span>: dis_opt, <span class="string">'frequency'</span>: n_critic},</span><br><span class="line">        {<span class="string">'optimizer'</span>: gen_opt, <span class="string">'frequency'</span>: <span class="number">1</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<p>可以在，<code>epoch_end</code> 系列的钩子中，完成 Epoch-level 的 metric 计算。以 <code>validation_epoch_end</code> 为例，其接收一个参数 <code>validation_step_outputs</code>，是一个 list，包含了 <code>validation_step</code> 的所有返回结果。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validation_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line">    pred = ...</span><br><span class="line">    <span class="keyword">return</span> pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validation_epoch_end</span>(<span class="params">self, validation_step_outputs</span>):</span></span><br><span class="line">    all_preds = torch.stack(validation_step_outputs)</span><br><span class="line">    ...</span><br></pre></td></tr></tbody></table></figure>

<p>Trainer 中，常见的回调函数有 <code>ModelCheckpoint</code>，完成模型的定期保存；<code>EarlyStopping</code>，定义模型的早停策略。Trainer 定义时，只需要传入 <code>precision=16</code>，即可实现 PyTorch naive 的混合半精度，如果要制定 apex 后端，也只需要加上一行 <code>amp_backend='apex'</code> 即可。使用 <code>accelerator</code> 可以方便切换各种加速设备，CPU、GPU、TPU、IPU 等等。指定 <code>strategy="ddp"</code> 即可使用数据并行策略。</p>
<p>过多的就不再赘述了，官方文档介绍的还挺详细的，按需引入即可。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Lightning in 15 minutes — PyTorch Lightning 1.8.0dev documentation (pytorch-lightning.readthedocs.io)</a></li>
</ul>
]]></content>
      <categories>
        <category>代码</category>
      </categories>
      <tags>
        <tag>代码</tag>
        <tag>框架</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>MoE 相关工作、研究进展总结</title>
    <url>/blog/2022/07/03/MoE-Summary/</url>
    <content><![CDATA[<h1 id="MoE"><a href="#MoE" class="headerlink" title="MoE"></a>MoE</h1><p>今天总结一下最近读过的 MoE 相关的论文、研究进展。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>混合专家网络（<strong>Mixture-of-Experts，MoE</strong>），旨在通过条件计算增加模型容量。具体来说，是对不同的输入激活网络中的不同部分，在控制运算量，即 FLOPs 不变的前提，显著增加模型参数，以达到增加容量的效果。这是一种稀疏网络架构，网络中只有部分参数被激活，与之对应的是传统的密集网络，每个输入都会激活所有参数。</p>
<span id="more"></span>

<p>一个例子如下所示。存在 n 个不同的专家，每个专家都是个独立的 FFN，不共享参数。门控网络对每个 token 计算专家网络上的概率分布，取 top2 专家输出的加权和作为 MoE 层的输出。</p>
<p><img src="/blog/architecture.png"></p>
<p>实现和训练过程中需要注意以下事项：</p>
<h3 id="稀疏噪声Top-k门控"><a href="#稀疏噪声Top-k门控" class="headerlink" title="稀疏噪声Top-k门控"></a>稀疏噪声 Top-k 门控</h3><p>计算 logits 时加入随机噪声，使得每个专家都有激活的机会。</p>
<p>将 Top-k 之外的 logits 置 0，降低 softmax 开销。</p>
<h3 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h3><p>门控网络倾向于为特定的少数专家提供较大的权重。事实上，刚开始受到关注的某些专家会训练地更快，从而会更容易被选择。因此，需要额外的损失项保证专家间的负载均衡。</p>
<p>原始论文中使用了专家的重要性（batch 内每个 token 在同一专家的分数之和），构造辅助损失，保证每个专家收到的 batch token 的重要性相近，又设计了额外的负载损失保证每个专家收到的 token 数量近似。仔细区分的话，重要性损失保证了每个专家都有近似的训练进度，负载损失保证了专家间的负载近似平衡。</p>
<p>下面是重要性损失的公式。负载损失类似。采用这样复杂的损失形式是因为每个专家激活次数是离散量、不可微。</p>
<p><img src="/blog/Untitled.png"></p>
<h3 id="混合并行"><a href="#混合并行" class="headerlink" title="混合并行"></a>混合并行</h3><p>一般而言，大模型需要分布式数据并行训练。</p>
<p>专家、参数过多时，模型难以容纳在单张卡上。需要做模型并行，将一个或多个专家放在一张卡上，不同的卡保存不同的专家。在门控网络计算完毕，得到每个 token 对应的专家后，通过多卡通信，将每个 token 转移到专家对应的设备上进行计算。</p>
<h3 id="任务-amp-数据集"><a href="#任务-amp-数据集" class="headerlink" title="任务&amp;数据集"></a>任务 &amp; 数据集</h3><ul>
<li>语言模型：百亿 Google News、维基百科</li>
<li> NLU 常见基准：GLUE、SuperGLUE、SQuAD</li>
<li> 摘要：XSum</li>
<li> 问答：CB Web QA、CB Natural QA、CB Trivia QA</li>
<li> 翻译：WMT’14</li>
</ul>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul>
<li><strong>Top-k 门控</strong>：K 过大会增加通信开销，过小又可能损失性能（缺乏了多位专家的对比）</li>
<li><strong>负载均衡</strong>：辅助损失形式复杂，且引入了额外的优化目标</li>
<li><strong>模型并行</strong>：分布式模型并行需要编程上的模型划分、通信、带来额外工程开销</li>
<li><strong>训练稳定性</strong>：稀疏模型可能存在训练不稳定，即 loss 发散的问题</li>
<li><strong>难以推理部署</strong>：稀疏大模型需要大量设备才能推理部署</li>
</ul>
<h2 id="Switch-Transformer"><a href="#Switch-Transformer" class="headerlink" title="Switch Transformer"></a>Switch Transformer</h2><ul>
<li>多语任务上，取得了相较于密集 T5-XXL 4x 的加速</li>
</ul>
<h3 id="门控网络"><a href="#门控网络" class="headerlink" title="门控网络"></a>门控网络</h3><ul>
<li>主张：原始论文中认为至少需要激活两名专家才能学习到专家间的差异，才能使训练有效。论文主张只使用一个专家，既保证了质量又简化了路由计算，性能更好。</li>
<li>使用 Top1 门控，降低通信开销。</li>
</ul>
<h3 id="负载均衡-1"><a href="#负载均衡-1" class="headerlink" title="负载均衡"></a>负载均衡</h3><ul>
<li>动机：简化负载均衡辅助损失，为 tpu 训练，固定专家的最大容量</li>
<li>使用容量因子静态设置专家容量，溢出的 token 不激活任何专家，退化为残差连接。</li>
<li>简化辅助损失：使用每个专家分到的 token 比例、重要性的点积构造简化版的辅助损失。</li>
</ul>
<p>N 为专家数，T 为 token 数，$\mathcal B$ 为 batch 内的所有 token，$f_i$ 为每个专家分到的 token 比例，$P_i$ 为归一化的专家重要性。理想情况下，二者均应该为 $1/N$。虽然 $f_i$ 不可微，但是 $P_i$ 是可微的。论文称该损失在均匀分布下是最小的。</p>
<p><img src="/blog/Untitled%201.png"></p>
<h3 id="蒸馏"><a href="#蒸馏" class="headerlink" title="蒸馏"></a>蒸馏</h3><ul>
<li>动机：将大型稀疏模型蒸馏为小型密集模型</li>
<li>使用密集模型初始化非专家参数</li>
<li>使用 25% 教师概率 + 75% 真实标签混合蒸馏</li>
<li> 5% 的参数保留 30% 的质量</li>
</ul>
<h3 id="稳定性"><a href="#稳定性" class="headerlink" title="稳定性"></a>稳定性</h3><ul>
<li>仅将路由器内部的计算使用 float32 精度，其余使用 bfloat16 不变，兼顾稳定性和效率</li>
<li>降低参数初始化的标准差，更稳定</li>
<li>仅提高专家网络的 dropout，避免过拟合。如果增加所有参数的 dropout 会损害性能。</li>
</ul>
<h2 id="Hash-Layers"><a href="#Hash-Layers" class="headerlink" title="Hash Layers"></a><strong>Hash Layers</strong></h2><ul>
<li>在两个数据集上困惑度优于 Switch Transformer 约 0.5。</li>
</ul>
<h3 id="负载均衡-2"><a href="#负载均衡-2" class="headerlink" title="负载均衡"></a>负载均衡</h3><ul>
<li>动机：不使用辅助损失达到负载均衡</li>
<li>通过随机 Hash、聚类 Hash、分散 Hash 多种方法，将建立 token - 专家间的固定 Hash 关系</li>
<li>使用多头 Hash：将专家和 embedding 分段，做 Hash、计算后再拼接到一起</li>
</ul>
<p><img src="/blog/Untitled%202.png"></p>
<h2 id="BASE-Layers"><a href="#BASE-Layers" class="headerlink" title="BASE Layers"></a><strong>BASE Layers</strong></h2><h3 id="负载均衡-3"><a href="#负载均衡-3" class="headerlink" title="负载均衡"></a>负载均衡</h3><ul>
<li>动机：不使用辅助损失达到负载均衡</li>
<li>将负载均衡问题定义为线性分配问题，每个 token 和 expert 点积计算分数，将分数认作将 token 分配个 exper 的收益，按照成熟的线性分配算法求解。为避免专家、token 过多时求解全局最优的开销过大，论文并将其 token 分成若干个组，求解组内的线性分配问题。</li>
<li>线性分配问题形如：有 n 个 token 和 m 个 expert，矩阵元素 A (n,m) 是将 token n 分配给专家 m 得到的收益。满足每个专家分配 token 个数相等时，最高收益的分配方案。</li>
<li>需要两次 all2all 操作，第一次将 token 映射到随机的 worker，worker 求解组内线性分配问题，第二次 worker 将 token 发送给对应的 expert。</li>
</ul>
<p><img src="/blog/Untitled%203.png"></p>
<h2 id="DENSE-TO-SPARSE-GATE"><a href="#DENSE-TO-SPARSE-GATE" class="headerlink" title="DENSE-TO-SPARSE GATE"></a><strong>DENSE-TO-SPARSE GATE</strong></h2><h3 id="提高训练效率"><a href="#提高训练效率" class="headerlink" title="提高训练效率"></a>提高训练效率</h3><ul>
<li>动机：认为专家和稀疏门控的联合训练对模型精度产生负面影响。通过将门控由密集逐渐转为稀疏，将专家和稀疏门的训练解耦</li>
<li>具体是通过调节门控 softmax 的温度，由大而小，将大于某个分数阈值的专家均激活</li>
<li>由于密集训练完毕后，专家间自然地就存在分布不均衡，因此没有采用负载均衡控制算法</li>
<li>结果：<ul>
<li>同样验证集困惑度，取得比 switch transformer 2x 的训练加速和 1.25x 的 FLOPs 加速</li>
<li>有无负载均衡损失的效果类似，最终专家间负载基本均衡</li>
</ul>
</li>
</ul>
<p><img src="/blog/Untitled%204.png"></p>
<p><img src="/blog/Untitled%205.png" alt="有无辅助损失的对比"></p>
<p>有无辅助损失的对比</p>
<h2 id="Tricks-for-Training-Sparse-Translation-Models"><a href="#Tricks-for-Training-Sparse-Translation-Models" class="headerlink" title="Tricks for Training Sparse Translation Models"></a><strong>Tricks for Training Sparse Translation Models</strong></h2><ul>
<li>在两个数据集上改善了低资源任务的性能</li>
</ul>
<h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><ul>
<li>动机：大容量的稀疏模型可能会在少资源任务上过拟合。主张这是由于专家很早就开始专业化，很少改变专业化。</li>
</ul>
<p><img src="/blog/Untitled%206.png" alt="French为高资源任务，Romanian为低资源语言"></p>
<p>French 为高资源任务，Romanian 为低资源语言</p>
<ul>
<li>温度采样：训练过程中，按不同温度采样不同任务样本，逐渐加热温度，增大从低资源任务采样数据的比例</li>
<li>密集预训练：刚开始时固定步数内，专家参数共享进行密集预训练，提高了收敛性</li>
</ul>
<p><img src="/blog/Untitled%207.png" alt="专家的过早专业化现象得到了遏制"></p>
<p>专家的过早专业化现象得到了遏制</p>
<h2 id="Efficient-Large-Scale-Language-Modeling-with-Mixtures-of-Experts"><a href="#Efficient-Large-Scale-Language-Modeling-with-Mixtures-of-Experts" class="headerlink" title="Efficient Large Scale Language Modeling with Mixtures of Experts"></a><strong>Efficient Large Scale Language Modeling with Mixtures of Experts</strong></h2><p>关于自回归 MoE 语言模型各种设置的实证研究。</p>
<p>一些结论：</p>
<ul>
<li>相同验证集性能下，MoE 比相同 FLOPs 的密集模型训练速度更快，对域内验证数据快 8-16 倍，域外数据快 2-4 倍。</li>
<li>MoE 的零样本学习能力优于密集模型，但差距随着训练推进减小</li>
<li> MoE 从零样本到少样本间取得的性能提升差于密集模型</li>
<li> MoE 的微调性能差于密集模型，且在部分数据集上存在微调后性能变差的情况</li>
</ul>
<h3 id="稳定性-1"><a href="#稳定性-1" class="headerlink" title="稳定性"></a>稳定性</h3><p>没有像 Switch Transformer 一样调整初始化的权重，而是增大了每个 expert 的学习率。因为与密集的数据并行相比，每个专家接收到的 token batch 减少了 E 倍，进而将梯度减小了 $\sqrt E$ 倍。E 为专家数量。因此论文主张将学习率增大 $\sqrt E$ 倍。</p>
<h2 id="GShard"><a href="#GShard" class="headerlink" title="GShard"></a>GShard</h2><ul>
<li>一个模块，用户使用 api 为关键张量添加注释，模块会自动实现切分和并行，避免人工编程实现的工程开销。将模型设计和实现分离。</li>
<li>使用 SPMD 而非 MPMD，降低设备增多时的编译开销</li>
</ul>
<h2 id="DeepSpeed-MoE"><a href="#DeepSpeed-MoE" class="headerlink" title="DeepSpeed-MoE"></a>DeepSpeed-MoE</h2><p>提供了端到端的 MoE 训练和推理的解决方案，作为 DeepSpeed 库的一部分，支持最高 3.7 倍的 MoE 压缩，7.3 倍的推理优化。</p>
<p>提出了金字塔残差 MoE（PR-MoE），以更少的参数取得了比传统 MoE 更优的性能。</p>
<h3 id="压缩参数"><a href="#压缩参数" class="headerlink" title="压缩参数"></a>压缩参数</h3><p>实验证明，网络深层使用专家效果优于浅层，因此，论文提出由浅至深逐渐增加专家的数量。也就是金字塔型。</p>
<h3 id="近似Top2"><a href="#近似Top2" class="headerlink" title="近似Top2"></a>近似 Top2</h3><p>Top2 门控虽然能带来收益，但是引入了很多训练、通信开销。论文认为，top2 门控优于 top1 门控的原因是，分数第二大的专家<strong>有时</strong>可以纠正第一个专家的错误。因此，论文提出了可以固定一个专家，从剩下的专家里选择最优的。换而言之，在专家之外添加了一个所有 token 都需要经过的 MLP，近似得到 top2 门控的性能。</p>
<p><img src="/blog/Untitled%208.png"></p>
<h3 id="蒸馏稀疏模型"><a href="#蒸馏稀疏模型" class="headerlink" title="蒸馏稀疏模型"></a>蒸馏稀疏模型</h3><ul>
<li>将知识蒸馏到专家更少的稀疏模型里，而非传统的密集模型，保留稀疏模型训练和推理的优势</li>
<li>蒸馏过程中逐渐减小蒸馏的影响，避免损害性能</li>
</ul>
<p><img src="/blog/Untitled%209.png" alt="完全使用KD损失会在训练近结束时损失性能"></p>
<p>完全使用 KD 损失会在训练近结束时损失性能</p>
<h2 id="ST-MoE"><a href="#ST-MoE" class="headerlink" title="ST-MoE"></a>ST-MoE</h2><p>旨在解决 MoE 训练过程中的不稳定性以及微调过程中的质量不确定性。</p>
<p>论文提出了一个 269B 的稀疏模型，实现了 SOTA</p>
<h3 id="稳定性-2"><a href="#稳定性-2" class="headerlink" title="稳定性"></a>稳定性</h3><p>论文尝试了三种常见的提高训练稳定性的方法：</p>
<ul>
<li>移除乘法交互：去除均方根尺度参数、GEGLU 激活</li>
<li>添加噪声</li>
<li>梯度裁剪</li>
</ul>
<p>虽然能够提升稳定性， 但均会损害模型质量</p>
<p>论文提出了一种路由 z-loss，约束梯度，其中，B 是 batch 中 token 数量，N 是专家的数量，</p>
<p>x 是路由器接收的 logits。该 loss 惩罚了门控网络中的大的 logits，类似 l2 正则项。</p>
<p><img src="/blog/Untitled%2010.png"></p>
<p>路由 z-loss 能提升稳定性，还能稍微改善模型质量。</p>
<h3 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h3><ul>
<li>论文认为，稀疏模型微调效果差的原因是其倾向于过拟合。实验也证明，稀疏模型更快微调收敛，证实稀疏模型在数据分布变化时可以高效地适应优化。大任务上微调可以取得比密集模型更性能，但是在较小的任务上差于密集模型</li>
</ul>
<p><img src="/blog/overfit.png"></p>
<ul>
<li>调节 expert 的 dropout 可提升泛化性能</li>
<li> MoE 只微调非专家参数和微调所有参数性能类似。只微调非专家参数可以加速训练、减小显存。</li>
<li>密集模型和 MoE 的最优微调超参有较大差别，直接影响最后性能，不能直接照搬。</li>
</ul>
<h3 id="丢弃token健壮性"><a href="#丢弃token健壮性" class="headerlink" title="丢弃token健壮性"></a>丢弃 token 健壮性</h3><p>微调质量不会因为 10% 的 token 丢弃率受到显著性能影响。负载不平衡可能不会显着影响模型质量。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>条件计算</category>
      </categories>
      <tags>
        <tag>条件计算</tag>
        <tag>门控</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>ST-MoE: 高效稀疏专家网络</title>
    <url>/blog/2022/06/12/ST-MoE/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>上篇回顾了经典论文专家混合网络 MoE，今天读一篇 Google 团队在 22 年 2 月出炉的大作《ST-MoE: Designing Stable and Transferable Sparse Expert Models》，旨在解决训练过程中的不稳定性以及微调过程中的质量不确定性。论文提出的 269B 的稀疏模型，计算成本与 32B 的密集模型相当。稀疏模型第一次在迁移学习中实现了 SOTA。</p>
<span id="more"></span>

<p>值得一提的是，谷歌的大牛 Jeff Dean 也是该论文的作者之一。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>稀疏专家网络，根据输入激活网络的不同子部分，可以更高效地增加模型容量，可参考 <a href="https://tqnwhz.github.io/blog/2022/06/05/MoE/#more">MoE：通过条件计算增加模型容量 | 一隅 (tqnwhz.github.io)</a>。简单来说，稀疏专家网络为每个 token 动态地选择模型参数，同时保证其 FLOPs 基本恒定（例如选择固定的专家数目），这允许网络极大地扩展参数数量。相较于同规模的静态网络，稀疏专家网络可有 4-7 倍的预训练加速，训练成本减少了一个数量级。性能上，该方法产生了 SOTA 的翻译模型，使用 GPT-3 1/3 的训练成本达到了同等级的单样本学习性能。</p>
<p>然而，问题依然存在。研究人员发现使用预训练的专家网络在常见基准上微调时，其性能逊于较小的模型，例如 SuperGLUE。也就是说，其迁移学习能力较差。Switch-XXL，一个参数更少但计算量大 8 倍（FLOPs 大约等于最大的 T5 模型）的模型，提高了自然语言理解任务的质量。然而，有研究指出，预训练中的不稳定性可能妨碍了必要的预训练，例如参数和计算的必要平衡。如何可靠地训练专家网络还是个悬而未决的问题。</p>
<p>本文的贡献总结如下：</p>
<ul>
<li>对稳定性 - 质量权衡进行了大规模研究</li>
<li>引入路由 z-loss，解决了不稳定问题，并略微提升了质量</li>
<li>稀疏和密集模型的微调分析，分析其对批量大小和学习率等超参数的敏感性。糟糕的超参数实际上不会导致密集模型的微调增益，尽管有很大的预训练速度提升。</li>
<li>在分布式设置中设计帕累托高效稀疏模型的架构、路由和模型设计原则。</li>
<li>跨专家层跟踪 token 路由决策的定性分析。</li>
<li>一个 269B 稀疏模型（<strong>S</strong>table <strong>T</strong>ransferable <strong>M</strong>ixture-<strong>o</strong>f-<strong>E</strong>xperts 或 ST-MoE-32B），它在各种自然语言基准测试中实现了最先进的性能。</li>
</ul>
<h2 id="训练稳定性研究"><a href="#训练稳定性研究" class="headerlink" title="训练稳定性研究"></a>训练稳定性研究</h2><p>训练的稳定性，定义为训练 loss 的发散程度。下图为两个同规模 FLOPs 的稀疏模型的 loss 变化。左侧为不稳定训练，右侧为稳定训练的例子。</p>
<p><img src="/blog/stability-example.png"></p>
<p>虽然可以使用一些简单的方法提升稳定性，例如更严格的梯度裁剪、任意小的学习率，但是这往往会损害模型质量。论文将提高稳定性的方法分为以下几类进行研究：</p>
<ul>
<li>消除乘法交互</li>
<li>引入模型噪声</li>
<li>限制激活和梯度</li>
</ul>
<p>构建这项研究的主要问题是小型模型很少不稳定，但大型不稳定模型成本太高而无法运行足够的步骤和种子。与 T5-XL 匹配的稀疏模型 FLOP 是很好的研究对象，因为它在大约 1/3 的运行中不稳定，但训练成本相对低廉。论文使用 6 个随机种子，每个模型都使用掩码语言建模目标在 mC4 上进行了 20k 步的预训练。</p>
<h3 id="移除乘法交互"><a href="#移除乘法交互" class="headerlink" title="移除乘法交互"></a>移除乘法交互</h3><p>论文展示并分析 Transformers 中两个乘法交互实例的影响。</p>
<p>GELU 门控线性单元 (GEGLU)：一种激活函数，公式如下：<br>$$<br>FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\odot(xV+c)<br>$$<br>均方根尺度（Root Mean Square Scale）参数。RMS 归一化中的均方根尺度参数。RMS 归一化公式如下：<br>$$<br>y_i=\frac{x_i}{\sqrt{\frac1d \sum_{i=1}^d x_i^2}}\cdot g_i<br>$$<br>结果如下所示。移除 GEGLU 层或 RMS 尺度参数都提高了稳定性，但对模型质量造成了重大损失。这些尺度参数 (g) 对模型质量与其他地方的参数（例如 FFN）相比具有不成比例的增益。 Shleifer 等人 (2021) 发现在 Transformers 的残差连接中添加一个学习的乘法标量会使它们更加不稳定。</p>
<p><img src="/blog/remove-multiplication.png"></p>
<h3 id="添加噪声"><a href="#添加噪声" class="headerlink" title="添加噪声"></a>添加噪声</h3><p>一般认为，在模型中添加噪声可以提高训练稳定性（Neelakantan et al., 2015）。而且通过 dropout 注入噪声的微调很少出现不稳定的情况，论文研究了噪声是否可以提高稀疏模型的稳定性。</p>
<p>结果如下表所示，可以看到，稳定性的提升依然是以质量为代价的。</p>
<p><img src="/blog/noise.png"></p>
<h3 id="约束激活和梯度"><a href="#约束激活和梯度" class="headerlink" title="约束激活和梯度"></a>约束激活和梯度</h3><p>稳定神经网络最成功的方法之一是对激活和梯度的约束。一种流行的方法是通过反向传播时裁剪梯度以避免梯度爆炸。接着是激活的约束，论文认为，路由器（也就是 MoE 里的门控网络），以 float32 精度计算专家的概率分布。这在更大的模型规模上不足以产生可靠的训练。为了解决这个问题， 论文引入了路由 z-loss，公式如下：<br>$$<br>L_z(x)=\frac 1 B \sum_{i=1}^B(log \sum_{j=1}^N e^{x_j^{(i)}})^2<br>$$<br>其中，$B$ 是 batch 中 token 数量，$N$ 是专家的数量，$x\in\mathcal R^{B\times N}$ 是路由器接收的 logits。该 loss 惩罚了门控网络中的大的 logits，类似 l2 正则项。</p>
<p>结果如下表所示，严格的梯度裁剪会损害性能，而路由 z-loss 则不会。论文中使用的 z-loss 的权重为 0.001。</p>
<p><img src="/blog/clip-z-loss.png"></p>
<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><h3 id="过拟合假设"><a href="#过拟合假设" class="headerlink" title="过拟合假设"></a>过拟合假设</h3><p>论文认为，稀疏模型微调效果差的原因是其倾向于过拟合。论文在 SuperGLUE 中的两个任务进行了实验。下图为 Dense L 和 ST-MoE-L 模型的微调结果。每个模型都在来自 C4 语料库的 500B 个 token 上进行了预训练。可以看出，稀疏模型更快地收敛到 100% 的训练集精度，证实稀疏模型在数据分布变化时可以高效地适应优化。在更大的任务 ReCORD 上，稀疏模型的验证质量随着训练的提升而显着超过密集模型。然而，在较小的任务 CB 上，稀疏模型在保留数据上落后于密集模型。</p>
<p><img src="/blog/overfit.png"></p>
<p>基于此，适度的 dropout 可以提升稀疏模型的泛化性能，实验结果如下图所示。</p>
<p><img src="/blog/dropout.png"></p>
<h3 id="微调参数子集"><a href="#微调参数子集" class="headerlink" title="微调参数子集"></a>微调参数子集</h3><p>为了防止过拟合，论文在微调期间尝试仅更新模型参数的子集：</p>
<ul>
<li>所有参数 (All)</li>
<li> 仅非 MoE 参数 (Non MoE)</li>
<li> 仅 MoE 参数 (MoE)</li>
<li> 仅 self-attention 和 enc-dec attention 参数 (Attention) </li>
<li>仅非 MoE FFN 参数 (FFN)</li>
</ul>
<p>结果如下图所示，其中 3 个结果大致相同，而仅微调 MoE 参数会导致质量急剧下降。更新非 MoE 参数的效果、更新所有参数、仅更新 FFN 参数的效果差不多。只有更新非 MoE 参数才能成为加速和减少微调内存的有效方法。</p>
<p><img src="/blog/subset-fintuning.png"></p>
<h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><p>论文研究两个超参数：批大小    和学习率对稀疏模型和密集模型微调的影响。在 C4 的 500B 个 token 上预训练 Dense-L 和 ST-MoE-L，然后在 SuperGLUE 上进行微调。结果如下图所示。</p>
<p><img src="/blog/batch-size-lr.png"></p>
<p>在所有超参数设置中，稀疏模型（橙色）的性能优于密集模型（蓝色）—— 然而，每个模型的最佳设置都可以显着改变结果。稀疏模型和密集模型在不同的批量大小和学习率下具有截然不同的性能。稀疏模型受益于更小的批量大小和更高的学习率。论文指出了在微调过程中正确调整批量大小和学习率的重要性。<strong>简单地使用适用于密集模型的相同微调超参数可以掩盖稀疏模型获得的任何预训练改进</strong>。换而言之，不能直接将密集模型的超参数照搬到稀疏模型微调上。</p>
<h3 id="丢弃token的健壮性"><a href="#丢弃token的健壮性" class="headerlink" title="丢弃token的健壮性"></a>丢弃 token 的健壮性</h3><p>稀疏模型将 token 路由给每一层的一个或多个专家。为了使这些模型在现代硬件的 SPMD 范式中高效，专家容量（每个专家处理的 token 数量）需要提前固定。当专家收到的 token 超过其容量时，多余的 token 将被丢弃。论文使用辅助损失进行预训练，尽量向每个专家发送等量的 token 和容量因子（Capacity Factor）。专家的容量为 $CF\cdot tokens/experts$，其中 CF 是容量因子，tokens 为一个 batch 是 token 总数。显然，理想的负载均衡情况下，CF 为 1 即可，不会有丢弃 token 现象。然而，由于很难做到如此理想的负载均衡，CF 为 1 时不可避免的会有丢失 token 现象出现，增大 CF 可缓解该现象。</p>
<p>下表展示了辅助 loss、token 丢弃率对性能的影响。出人意料的是，微调质量不会因为 10% 的 token 丢弃率受到实质性能影响。这与 Yang et al. (2021) 的结论一致，即负载不平衡可能不会显着影响模型质量。</p>
<p><img src="/blog/drop-token.png"></p>
<h3 id="插入哨兵标记"><a href="#插入哨兵标记" class="headerlink" title="插入哨兵标记"></a>插入哨兵标记</h3><p>结果如下图所示。不能提高泛化能力，但哨兵标记可以加速训练收敛。</p>
<p><img src="/blog/sentinel.png"></p>
<h2 id="设计指南"><a href="#设计指南" class="headerlink" title="设计指南"></a>设计指南</h2><p>如何设计一个稀疏模型？专家数量、路由算法、容量系数如何决定？如何根据硬件调整？论文对此给出了建议。</p>
<ul>
<li>专家数量：建议每个 GPU 核心配备一名专家或更少，避免同核心专家数过多导致增多内存传输，降低效率。</li>
<li>容量因子和路由策略（Top-n）：越大性能越好，但计算、通信成本都会增加。论文中平衡训练速度和性能，选择 1.25 的 CF。</li>
</ul>
<p>详细的结果可以参考论文原文。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>论文设计了一个 T5-Large 近似 FLOP 的 269B 的稀疏模型，在多个 NLP 任务中实现了 SOTA，包括情感分析（SST-2）、语义消歧（WIC）、句子相似度（MRPC、STS-B、QQP）等。</p>
<p>实验结果如下表所示，大部分都是 SOTA。值得注意的是，为简单起见，论文是在所有任务上联合微调的，而非单独微调。如果是单独微调效果可能会更好。</p>
<p><img src="/blog/exper-result.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文针对稀疏专家网络训练的稳定性、质量等问题，做了多因素的定量实验，为相关实践提供了一份指南。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>条件计算</category>
      </categories>
      <tags>
        <tag>MoE</tag>
        <tag>专家网络</tag>
      </tags>
  </entry>
  <entry>
    <title>MoE：通过条件计算增加模型容量</title>
    <url>/blog/2022/06/05/MoE/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>今天来读一篇关于条件计算的论文，《<strong>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</strong>》，收录于 2017 年 ICLR。神经网络模型的容量（ capacity），例如模型从语料中学习的知识，受限于模型的参数规模。通常，每个样本都要经历到模型所有参数的计算中。增加模型容量意味着成比例的计算性能下降。本文提出了一种条件计算的方法，对每个样本只激活部分参数，可以在不成比例地增加计算量的情况下显着增加模型容量，实现了超过 1000 倍的容量提升，在大型语言建模和机器翻译基准上，这些模型以更低的计算成本实现了新的 SOTA。</p>
<span id="more"></span>

<p>具体而言，论文引入了稀疏门控混合专家层（MoE，Sparsely-Gated Mixture-of-Experts Layer），由前馈神经网络和门控网络组成。门控网络用于选择专家的稀疏组合，处理每个输入。网络的所有部分都通过反向传播联合训练。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>正如前文所说，模型的容量受限于参数规模，简单扩大参数会导致训练成本大致呈二次增长。条件计算基于每个样本选择网络的不同部分激活。用于选择的门控决策可以是二元的、稀疏的、连续的、随机的或确定的。各种形式的强化学习和反向传播策略可用于训练门控决策。然而，该方法面临很多的挑战：</p>
<ul>
<li><strong>分支低效</strong>：计算设备，尤其 GPU，计算要比分支快得多。熟悉流水线 CPU 的同学应该知道，分支的分支错误惩罚会导致数个时钟周期的停顿，严重影响计算效率</li>
<li><strong>批大小受限</strong>：条件计算减少了网络条件活动块的 batch size</li>
<li><strong> 网络带宽（IO）受限</strong>：GPU 的计算能力往往是网络带宽的数千倍，嵌入层，也可以看做一种条件计算，往往需要网络发送，这种参数交互受到网络带宽而非计算能力的限制</li>
<li><strong>额外损失项</strong>：可能需要额外的损失项控制每个样本的网络稀疏程度，在模型负载和质量间做出平衡。</li>
<li><strong>大规模数据的依赖</strong>。模型容量的扩大需要大规模数据集训练，现有的一些条件计算工作只使用了最多 60w 数据，难以为万亿计参数的模型提供足够的监督信号。</li>
</ul>
<p>论文提出的稀疏门控专家层 MoE，由许多专家组成，每个专家都是一个简单的前馈神经网络，以及一个可训练的门控网络，该网络选择专家的稀疏组合来处理每个输入。结构图示意如下，其中门控网络从中选择了两个，处理样本。</p>
<p><img src="/blog/architecture.png"></p>
<p>虽然技术是通用的，但论文主要关注在语言模型、机器翻译任务上，这些任务可以从模型规模受益。基于此，论文在堆叠的 LSTM 层上应用了一个 MoE 卷积，如上图所示。对文本的不同位置调用一次 MoE，选择不同的专家组合。不同的专家组合根据语法和语义变得高度专业化。</p>
<h2 id="MoE结构"><a href="#MoE结构" class="headerlink" title="MoE结构"></a>MoE 结构</h2><p>MoE 由一个门控网络 $G$、n 个专家网络 $E_1,E_2,\dots,E_n$ 组成。门控网络的输出是一个 $n$ 维的向量。每个专家都是一个前馈网络，输入和输出的维度一致。用 $G (x),E_i (x)$ 分别代表门控网络、第 i 个专家网络的输出，$x$ 为专家网络的输入。MoE 的输出为专家网络输出的加权和，权重为门控网络对应维度的元素值，公式如下：<br>$$<br>y=\sum_{i=1}^nG(x)_iE_i(x)<br>$$<br>稀疏性体现在 $G (x)$ 上，如果 $G (x)_i$ 为 0，就不需要计算 $E_i (x)$ 了。在实验中，有上千个专家网络，但是只需要计算少部分。如果专家的数量过多，可以通过分层 MoE 减少分支。分层 MoE 中，每个专家都是带有自己门控网络的二级专家组合。下面介绍稀疏门控是如何实现的。</p>
<h3 id="Softmax-门控"><a href="#Softmax-门控" class="headerlink" title="Softmax 门控"></a>Softmax 门控</h3><p>简单的非稀疏门控可以通过 Softmax 函数实现，公式如下：<br>$$<br>G_\sigma(x)=Softmax(x\cdot W_g)<br>$$</p>
<h3 id="噪声Top-k门控"><a href="#噪声Top-k门控" class="headerlink" title="噪声Top-k门控"></a>噪声 Top-k 门控</h3><p>在 Softmax 门控网络中，添加稀疏性可提高计算效率，添加噪声可以实现负载均衡，随机性使得每个专家都有激活的机会。具体是通过在取 Softmax 函数前，添加可调高斯噪声，然后只保留前 k 个值，其余置 $-\infty$，在进行 Softmax 后，对应的门控信号就为 0,。可调高斯噪声，是指标准正态分布的噪声乘以可训练的噪声权重 $Softplus (x\cdot W_{noise})$。公式如下：<br>$$<br>G(x)=Softmax(KeepTopK(H(x),k))<br>$$<br>$$<br>H(x)_i=(x\cdot W_g)<em>i+StandardNorm()\cdot Softplus((x\cdot W</em>{noise})_i)<br>$$<br>$$<br>KeepTopK(v,k)_i=\begin{cases}<br>v_i,\ 如果 v_i 是前 k 大的元素 \<br>-\infty,\ 其他<br>\end{cases}<br>$$</p>
<h3 id="训练门控网络"><a href="#训练门控网络" class="headerlink" title="训练门控网络"></a>训练门控网络</h3><p>通过简单的反向传播以及模型的其余部分来训练门控网络。如果选择 k &gt; 1，则前 k 个专家的门值相对于门控网络的权重具有非零导数。梯度通过门控网络反向传播到其输入。</p>
<h2 id="解决性能挑战"><a href="#解决性能挑战" class="headerlink" title="解决性能挑战"></a>解决性能挑战</h2><h3 id="批大小缩减"><a href="#批大小缩减" class="headerlink" title="批大小缩减"></a>批大小缩减</h3><p>大的 batch size 对于计算效率是非常重要的，能够减小参数加载和更新的开销。如果门控网络每次只从专家网络中选择 k 个，每个专家对应的 batch size 会小得多，这会使得参数的更新更为低效。虽然这个问题可以通过暴力加大 batch size 缓解，但这又受限于 GPU 的显存限制。</p>
<p>论文提出了以下两种增加 batch size 的技术：</p>
<p><strong>混合数据并行和模型并行</strong>。传统的数据并行分布式训练中，不同设备上的多个模型副本<strong>异步</strong>处理不同 batch 数据，并通过一组参数服务器同步参数。论文提出同步的数据并行策略，不同设备数据同时组合应用于 MoE 层。模型标准层、门控网络都遵循平常的数据并行设置，不同的是，MoE 的每个专家只保留一份共享副本。每个专家都会收到一个组合批次，该批次包含数据并行中与该专家相关的批次。不同的设备上保存着不同的专家子集。所以这是一种混合数据并行和模型并行的方法。模型并行的设置猜测是为了减少显存开销，大量专家的情况下，每个设备上不激活的专家还是很占显存的。</p>
<p><strong>利用卷积性</strong>。MoE 可以类似卷积操作，施加在每层的不同时间步上应用 MoE，相当于增大了 batch size。但对于 RNN 此类网络，其自回归性使得卷积操作无法进行。</p>
<h3 id="网络带宽"><a href="#网络带宽" class="headerlink" title="网络带宽"></a>网络带宽</h3><p>分布式计算中另一个主要的性能问题是网络带宽。网络中，专家的输入和输出通过网络发送。为了保持计算效率，专家的计算量与其 IO（输入和输出）的比值必须超过计算设备的计算量与网络容量的比值。对于 GPU，这可能是数千比一。实验中，专家是仅有一个隐藏层的感知机，权重矩阵的大小为 input_size×hidden_size 和 hidden_size×output_size，因此计算与输入和输出的比率等于隐藏层的大小。因此，可以简单地通过使用更大的隐藏层或更多隐藏层来提高计算效率。</p>
<p>其实，也很容易理解，增加内部的计算量当然就相对降低了 IO 开销。</p>
<h2 id="平衡专家利用率"><a href="#平衡专家利用率" class="headerlink" title="平衡专家利用率"></a>平衡专家利用率</h2><p>根据论文观察结果，门控网络倾向于为特定的少数专家提供较大的权重。事实上，刚开始受到关注的某些专家会训练地更快，从而会更容易被选择，我愿称其为神经网络的马太效应。为了避免这种情况，论文使用了一种软间隔的方法。定义专家的重要性为 batch 数据中在该专家上的门控值之和。额外损失项 $L_{importance}$，定义为重要性的变异系数的平方乘以缩放系数 $w_{impotance}$。公式如下：<br>$$<br>Importance(X)=\sum_{x\in X}G(x)<br>$$</p>
<p>$$<br>L_{importance}(X)=w_{importance}\cdot CV(Importance(X))^2<br>$$<br>变异系数（coefficient of variation，CV），定义为标准差和平均值之比，是概率分布离散程度的归一化度量。当有多个变量进行离散程度比较时，标准差会受到量纲的影响，而变异系数可以消除这种影响。上述损失类似 L2 正则项，倾向于让专家有相同的重要性，但是，专家收到的样本数量可能不同，例如一位专家收到少而权重大的数据，另一个专家收到多而权重小的数据。这会导致分布式硬件出现内存、性能的问题。</p>
<p>为解决上述问题，论文还引入了一个损失 $L_{load}$。公式如下：<br>$$<br>P(x,i)=\Phi(\frac{(x\cdot W_g)<em>i-kth_excluding(H(x),k,i)}{Softplus((x\cdot W</em>{noise})_i)})<br>$$</p>
<p>$$<br>Load(X)<em>i=\sum</em>{x\in X}P(x,i)<br>$$</p>
<p>$$<br>L_{load}(X)=w_{load}\cdot CV(Load(X))^2<br>$$</p>
<p>其中，$P (x,i)$ 定义为 $G (x)_i$ 不为 0 的概率，$kth_excluding (H (x),k,i)$ 为除了第 $i$ 个元素外，$H (x)$ 中最大的第 $k$ 个元素的值，$\Phi$ 为标准正态分布的概率分布函数。</p>
<p>$L_{load}$ 粗看可能较难理解，将其转化为下式，其中 $(x\cdot W_g)<em>{a_k}$ 为第 k 大的门控值。可以看出，上式来自 $H (x)$ 的计算，$H (x)$ 中添加了高斯噪声平滑项，$P (x,i)$ 实际计算得到的是专家 $i$ 和被选中的权值最小的专家（第 k 个）门控值间的差异，并引入高斯噪声，通过标准正态的 CDF 映射得到选中第 $i$ 专家的概率。再通过变异系数计算损失，与 $L</em>{importance}$ 类似。<br>$$<br>P(x,i)=\Phi(\frac{(x\cdot W_g)<em>i-(x\cdot W_g)</em>{a_k}}{Softplus((x\cdot W_{noise})_i)}-\epsilon)<br>$$</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="亿级语言模型"><a href="#亿级语言模型" class="headerlink" title="亿级语言模型"></a>亿级语言模型</h3><ul>
<li>数据集：约 8.29 亿 token 的新闻语料，词表约 80w</li>
<li> 先前 SOTA：若干个堆叠的 LSTM 网络组成，参数从 200w 到 1.51 亿</li>
<li>本文模型：两层 LSTM 堆叠，中间有一个 MoE 层，MoE 层的大小、专家数量有所不同（4-4096）。每个专家约 100w 参数，选择的专家数 k=4</li>
</ul>
<p>为了研究增加容量的效果，论文训练了一系列 MoE 模型，并控制它们的计算成本（约时间步每 800w 次加乘操作，该指标记作 ops/timestap）。</p>
<p>下图展示了该任务上 LSTM 和 MoE 的比较结果。左侧图展示了同样计算成本下，模型的困惑度与参数的关系。基线 LSTM 模型只存在图左上方，少参数而高困惑度。展平的 MoE 与分层 MoE 能在同样参数规模下，获得更低的困惑度。右图为 40 亿参数下，困惑度与计算成本的关系。</p>
<p><img src="/blog/comparison.png"></p>
<p>下表展示了详细的对比结果，高计算成本的 MoE 能够显著降低困惑度。要注意的是，虽然 MoE 参数最多 30 倍于基础模型，但是每个样本只有 4 个专家处于激活态，这使得模型间的实际激活的参数、计算性能是可比的。</p>
<p><img src="/blog/table-comparison.png"></p>
<h3 id="千亿谷歌语料"><a href="#千亿谷歌语料" class="headerlink" title="千亿谷歌语料"></a>千亿谷歌语料</h3><p>与上述实验配置、结果相似。不过随着 MoE 层中的参数数量超过 10 亿，增加额外容量似乎会产生递减收益，</p>
<p><img src="/blog/google.png"></p>
<h3 id="单语对机器翻译"><a href="#单语对机器翻译" class="headerlink" title="单语对机器翻译"></a>单语对机器翻译</h3><p> WMT’14 翻译数据集，以英法翻译结果为例。此任务使用的模型是 GNMT 模型的修改版本。为了减少计算量，模型的编码器和解码器中的 LSTM 层数分别从 9 层和 8 层减少到 3 层和 2 层。论文在编码器（第 2 层和第 3 层之间）和解码器（第 1 层和第 2 层之间）中都插入了 MoE 层。每个 MoE 层包含多达 2048 位专家，每个专家大约有 200 万个参数，总共为模型添加了大约 80 亿个参数。</p>
<p><img src="/blog/en-fr-translation.png"></p>
<h3 id="多语对翻译"><a href="#多语对翻译" class="headerlink" title="多语对翻译"></a>多语对翻译</h3><p>传统方法，使用同一模型进行多语翻译的性能，要差于训练多个模型分别处理单语对翻译。这是由于多个模型提供了更大的模型容量。论文用单一的 MoE 重复了这个实验。MoE 模型在开发集比多语言 GNMT 模型。在 BLEU 得分上，MoE 模型在 12 个语言对中的 11 个上显着击败了多语言 GNMT 模型（高达 5.84 分），甚至在 12 个语言对中的 8 个上击败了单语 GNMT 模型。</p>
<p><img src="/blog/multi-translation.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文提出了一种条件计算的方法，并解决了其在实践中遇到的种种挑战（负载均衡、分布式性能等）。在语言模型、机器翻译任务上，证实了其提升模型容量同时保证计算性能的能力。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://zh.m.wikipedia.org/zh-sg/%E5%8F%98%E5%BC%82%E7%B3%BB%E6%95%B0">变异系数 - 维基百科，自由的百科全书 (wikipedia.org)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>条件计算</category>
      </categories>
      <tags>
        <tag>条件计算</tag>
        <tag>门控</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>KnowledGPT: 基于预训练语言模型的知识对话</title>
    <url>/blog/2022/05/03/KnowledGPT/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>《Knowledge-Grounded Dialogue Generation with Pre-trained Language Models》是由北大发表的论文，旨在通过预训练的语言模型进行知识对话，论文收录于 EMNLP 2020 主会。代码公开在 <a href="https://github.com/zhaoxlpku/KnowledGPT">zhaoxlpku/KnowledGPT (github.com)</a>。论文通过在预训练语言模型（如 GPT）外配置知识选择模块，从非结构化的知识文本中选择知识，并通过一种无监督的方法联合优化知识选择和知识对话生成。论文提出的 KnowLEDGPT 在 Wizard 和 CMU DoG 两个数据集上的自动 &amp; 人工评估实现了 SOTA。</p>
<span id="more"></span>

<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>数据驱动的开放域对话系统一直饱受通用性、无用回复的问题。当人类参与者试图深入某个特定主题时，这种缺陷尤其严重。虽然预训练模型通过模型扩张、更多的数据改善了这一问题，其可以在训练期间记住足够多的语言模式，但它们仍只能捕获 “平均语义”。当对话需要知识才能进行下去时，除非模型已经在预训练阶段 “见过” 相关知识并以正确的方式存储在参数中且可以正确关联，否则模型生成的回复仍可能是通用无用的。这与人类的真实对话还有着不小的差距。</p>
<p>最近，出现了两条似乎有望弥合差距的研究方向。第一种是大规模的预训练模型，通过模型扩张、大量数据在预训练期间学到足够多的语言模式，缓解通用性回复的问题。然而，这种简单粗暴的方法还是只能捕获数据的 “平均” 语义。当需要特定知识时，生成的回复仍然可能是通用的。</p>
<p>另一种方法是显式地引入外部知识。知识可分为结构化的知识图谱和非结构化的文本（维基百科等）。由于高质量的知识图谱更难获得，而非结构化的文本则更易获得，成本低廉而且数量庞大。然而，这些文本往往长而冗余，而预训练模型往往会受到序列长度的限制。事实上，该问题上，模型容量和处理长输入的能力是冲突的。一方面，在海量的文本上预训练，模型必须设置一个输入上限。另一方面，在回复生成时，需要尽可能多的保留候选知识，以保证相关知识的召回。这一冲突是知识对话生成的基本阻碍。</p>
<p>为了克服这一问题，研究者开始引入知识选择模块，从若干相关知识候选中选择最相关的知识子集，进而显著序列长度。然而，现在的一些方法中，知识选择模块要么与模型强耦合，无法应用在预训练模型中；要么通过人工标注数据训练，成本昂贵。本文提出了一种无监督的方法，将知识选择形式化为基于 BERT 的序列预测任务。通过计算真实回复和候选知识间的相似度作为伪标签，预热模型。再通过强化学习、课程学习的方法，交替更新知识选择模型和回复生成模型。因此，知识选择通过回复生成的反馈得到进一步优化。</p>
<p>论文在 Wizard of Wikipedia 和 CMU Document Grounded Conversations 两个数据集上进行实验。结果表明，模型可以显着优于最先进的方法以及一些以启发式方式使用的预训练模型，达到新的 SOTA。此外，作为副产品，知识选择模块在维基百科向导上的知识选择准确性方面也优于最先进的模型，这意味着其他模型也可以从该组件中受益。</p>
<h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><ul>
<li>数据集 $D={(U_i,D_i,r_i)}_{i=1}^N$</li>
<li>$U_i$ 代表上下文</li>
<li> $D_i$ 代表与 $U_i$ 相关的文档</li>
<li> $r_i$ 代表给定 $U_i$ 上下文，基于 $D_i$ 的回复</li>
<li>目标：$\max P (r_i|U_i,D_i;\theta)$</li>
</ul>
<p>由于论文使用 GPT2 建模此条件概率，因此将上式改写为：<br>$$<br>\begin{aligned}<br>P(r_i|U_i,D_i)&amp;=P(r_i|g(U,D);\theta)\<br>&amp;=\prod_i^{l_r}P(r_i|g(U,D),r_{1:t-1};\theta)<br>\end{aligned}<br>$$<br>$g (U,D)$ 将知识和上下文结合的模块，包含知识选择。问题就变成了如何定义 $g (U,D)$ 和如何微调 $\theta$。显然，可以直接将 $D$ 和 $U$ 直接拼接在一起然后截断，但是这可能会删去重要的知识且引入噪声，进而影响性能。$g (U,D)$ 的学习面临以下问题：</p>
<ul>
<li>如何对<strong>上下文与外部知识之间的相关性</strong>进行建模</li>
<li>当<strong>缺少真实知识标签时</strong>如何学习 $g (U,D)$</li>
<li> 如何将 $g (U,D)$ 和 GPT-2 模型与 D <strong>联合优化</strong>，从而使两者相互促进。</li>
</ul>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>模型的架构如下图所示。在 Transformer 架构的基础上，知识选择模块由上下文感知<strong>知识编码器</strong>和<strong>顺序知识选择器</strong>组成。知识编码器通过自注意力层捕获上下文 U 和 D 中的每个句子之间的交互模式，然后将这些模式馈送到知识选择器，逐步地解码有用的知识。由于无法获得人工标记，学习方法从充分利用回复构建的伪标记开始，通过强化学习方法和课程学习方法交替优化知识选择和回复生成。</p>
<p><img src="/blog/architecture.png"></p>
<h3 id="上下文感知知识编码器"><a href="#上下文感知知识编码器" class="headerlink" title="上下文感知知识编码器"></a>上下文感知知识编码器</h3><p>虽然名字很玄乎，但是原理很简单。将上下文 $U_i$ 和每个候选知识 $d_i\in D_i=(d_1,d_2,\cdots,d_m)$ 分别拼接起来，馈入 BERT，取最后一层的 **[CLS]** 的向量作为上下文表征。也就是架构图左侧的下半部分。用 $E=(e_1,e_2,\cdots,e_m)$ 代表编码器的输出，其中 $e_i$ 代表知识 $d_i$ 和上下文拼接后的上下文表征。</p>
<h3 id="顺序知识选择器"><a href="#顺序知识选择器" class="headerlink" title="顺序知识选择器"></a>顺序知识选择器</h3><p>这里的原理也很简单。前面提到，论文将知识选择视作<strong>序列预测</strong>任务（而非序列分类）。换而言之，知识选择模块意在<strong>顺序地解码出一个知识序列子集</strong>。也就是类似 RNN 的自回归方法。这样建模的原因可能是无需额外考虑序列分类时拼接的顺序。</p>
<p>具体而言，这一块有一个 LSTM 解码器组成，输入为编码器的输出序列 $E$。解码每一步与输入序列计算 Attention，从知识表征中选择输出，直到输出特殊的结束符号 $e_{spe}$ 或者序列长度达到上限。另外，在解码时的每一步，需要排除已经输出的知识，显然同一个知识没有必要输出两次。最后得到的解码输出就是选择后的知识序列 $D’$，$g (U,D)$ 定义为 $U\cup D’$。</p>
<h3 id="学习策略"><a href="#学习策略" class="headerlink" title="学习策略"></a>学习策略</h3><p>由于知识选择器的训练过程是无标签的，这可能会很困难。在最近的一篇论文中（Kim et al., 2020），当人类标签被移除时，知识选择的准确率从 27% 下降到 0.3%。另外，在端到端的模型中，知识选择的结果作为回复生成的输入，二者耦合纠缠。在训练早期，$g (U,D)$ 可能性能很差，训练噪声传入 GPT-2 再反向传播至 $g (U,D)$，可能会导致两边模型效果都很差。因此，论文提出了一种弱监督的联合优化策略，如下所示。</p>
<h4 id="伪标签构建"><a href="#伪标签构建" class="headerlink" title="伪标签构建"></a>伪标签构建</h4><p>为了减轻联合优化中的错误累积，论文考虑构建弱监督信号并预热 g (U, D) 的学习和 GPT-2 的微调。 直觉来看，人类的回复带有与知识候选相关性的线索，因此可以用来构建伪标签。具体来说，首先根据相似性 ${Sim (d_t, r)}<em>{t=1}^m$ 将 $D={d_t}</em>{t=1}^m$ 降序排序为 ${d_{j_t}}^m_{t=1}$。其中 $Sim (・,・) $ 表示<strong>相似度函数</strong>，例如 uni-gran F1。 然后构建 D 的子集：<br>$$<br>\overline D={d_{j_1},d_{j_2},\cdots,d_{j_\overline m}}<br>$$</p>
<p>$$<br>\overline m=\arg\max_t(Sim(d_{j_{1:t}},r))<br>$$<br>其中，$d_{j_{1:t}}$ 是 ${d_{j_i}} <em>{i=1}^t$ 的拼接结果。上述子集的构建就是取相似性降序知识的前缀拼接结果中，与真实回复相似性最大的拼接结果对应的知识子集。然后以 $\overline D$ 为知识选择的伪标签，分别<strong>预热</strong>知识选择模块和回复生成模块。即，通过最大似然分别优化 $D_K={(U_i,D_i,\overline D_i}</em> {i=1}^N$ 和 $D_G={(U_i,\overline D_i,r_i}_{i=1}^N$。</p>
<h4 id="联合优化"><a href="#联合优化" class="headerlink" title="联合优化"></a>联合优化</h4><p><strong>强化学习</strong>。论文使用策略梯度法继续训练 $g (U,D)$。其中，$g (U,D)$ 由 GPT-2 进一步监督且直接针对目标指标进行优化（例如实验中的 F1）。具体而言，在知识选择器解码时的每一步，从概率分布 $P (d_i|U,d_{1:t-1})$ 中采样另一个知识，遵循同样的停止策略，定义如下的损失函数：<br>$$<br>\mathcal L_K=-\frac 1 N\sum_{i=1}^N(\tilde R_i \sum_{t=1}^{|\tilde D_i|}logP(d_{i,j_t}|U_i,d_{i,j_{1:t-1}}))<br>$$</p>
<p>$$<br>\tilde R_i=R(\tilde D_i)-b<br>$$<br>其中，$R (\tilde D_i)=Sim (r’<em>i,r_i)$，$r_i’$ 是 GPT-2 利用 $U_i,\tilde D_i$ 生成的回复，$b=\sum</em>{i=1}^N R (\tilde D_i)/N$ 是基线用以减小梯度估计的方差。可以看到，如果获得比基线更高的奖励，即 $R (\tilde D_i)&gt;0$，则最小化 $\mathcal L_K$ 等效于最大化 $\tilde D_i$ 的条件似然。</p>
<p><strong>课程学习</strong>。虽然模型已经在伪标签上进行过预热， 但是在训练之初，$D’$ 可能还是差于 $\overline D$。因此，论文将 $D’,\overline D$ 混合，使用课程学习策略微调。具体来说，是设置一个超参数 $p$，控制从两者间采样 $D$ 作为回复生成模块的输入的概率，并随着训练过程调整 $p$ 的大小，完成从 $\overline D$ 到 $D$ 的迁移。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集-amp-指标"><a href="#数据集-amp-指标" class="headerlink" title="数据集&amp;指标"></a>数据集 &amp; 指标</h3><p>论文在 Wizard of Wikipedia 和 CMU Document Grounded Conversations 两个数据集上进行实验。这两个数据集都是在 Amazon Mechanical Turk 上通过众包构建的，使用 Wikipedia 作为知识库，并由数据所有者分为训练集、验证集和测试集。Wizard 中的数据主题涵盖范围很广（1365 个），每次对话都发生在有权访问特定主题知识的向导和渴望从向导学习该主题的学徒之间。测试集分为两个子集：Test Seen 和 Test Unseen。 Test Seen 包含新的对话，主题出现在训练集中，而 Test Unseen 中的主题从未出现在训练集和验证集中。与 Wizard 不同，CMU DoG 专注于电影领域，除了向导 - 学徒间的对话，数据还包含两个了解文档并试图深入讨论内容的用户之间的对话。</p>
<p>自动评估指标：真实回复的困惑度 (PPL)、BOW Embedding 和 unigram F1 作为指标。</p>
<p>人工评估：3 位注释者从流畅度、上下文连贯性和知识相关性三个方面判断回复的质量，并在 {0,1,2}（代表 “坏”、“一般” 和 “好”）中分配一个分数。</p>
<h3 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h3><ul>
<li>Transformer Memory Network (TMN):  Wizard 数据集论文中提出的模型。</li>
<li>Incremental Transformer with Deliberation Decoder (ITDD): 对多轮对话和知识进行增量编码，并使用 Deliberation 技术解码响应。</li>
<li>Sequential Knowledge Transformer (SKT): 在最近的一篇论文中发表的具有最佳知识选择性能的顺序潜变量模型，作为 Wizard 数据的基线。</li>
<li>Disentangled Response Decoder (DRD): 通过预训练技术解决低资源挑战的模型。论文选择在预训练后使用完整训练数据对所有参数进行微调的一个版本作为基线，因为这样的配置会产生最好的性能。</li>
</ul>
<p>论文提出的模型名为 KnowledGPT。除了上述基线，为了验证提出的方法，论文对下面两个模型也进行实验比较：</p>
<ul>
<li>$GPT-2_{trunc}$：将上下文和相关知识连接成一个长输入，然后截断该输入以满足 GPT-2 模型的长度约束。这是为了检查简单的启发式方法是否适用于该任务。</li>
<li>SKT+GPT-2：我们将 SKT 选择的候选知识提供给 GPT-2 以生成回复。这是为了检查是否可以简单地用现成的知识选择模型替换知识选择模块。</li>
</ul>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>下两表分别展示了 Wizard 和 CMU DoG 的实验结果。 KnowledGPT 在两个数据集中的大多数指标上都达到了新的最先进水平，这证明了本文提出方法的有效性。 $GPT-2_{trunc}$ 比 KnowledGPT 差，原因是：</p>
<ul>
<li><strong>知识损失</strong>：论文发现在 53% 的测试示例（Test Seen+Test Unseen）中，真实知识被移除了。 在这种情况下，GPT-2 只依赖于上下文、其他候选者中的相关知识（由于上下文和知识之间的一对多关系）以及包装在 GPT-2 参数中的知识进行响应，这 解释了 SKT 和 DRD 的可比性能。</li>
<li><strong>噪声输入</strong>：即使保留了真实知识，候选知识中的冗余和不相关信息仍然是有害的。</li>
</ul>
<p>KnowledGPT 在 Wizard 上的表现也优于 SKT+GPT-2，因为</p>
<ul>
<li>KnowledGPT 在知识选择上比 SKT 更准确，尽管它在学习中没有利用任何人工注释。事实上，SKT 在 Test Seen 和 Test Unseen 上知识选择的准确率分别为 26.8 和 18.3，而 KnowledGPT 这两个数字分别为 28.0 和 25.4</li>
<li> 在 KnowledGPT 中，知识选择和响应生成是联合优化的。</li>
</ul>
<p><img src="/blog/wizard-result.png"></p>
<p><img src="/blog/CMU-result.png"></p>
<p>下表展示了人工评估结果。虽然这三个模型在流畅度上具有可比性，但 KnowledGPT 在上下文连贯性和知识相关性方面都优于其他模型，这与自动指标的结果一致。</p>
<p><img src="/blog/human-evaluation.png"></p>
<h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>为了了解学习策略对模型性能的影响，论文将完整的 KnowledGPT 与以下变体进行了比较：</p>
<ol>
<li><strong>-pseudo</strong>：移除预热阶段</li>
<li><strong> -joint</strong>：移除联合优化阶段</li>
<li><strong> -reinforcement</strong>：g (U, D) 在 $D_K$ 上用 MLE 优化后固定</li>
<li><strong> -curriculum</strong>：GPT-2 在 $D_G$ 上用 MLE 优化后固定</li>
</ol>
<p>实验结果如下表所示。</p>
<p><img src="/blog/ablation-study.png"></p>
<p>可以得到以下结论：</p>
<ul>
<li><strong>伪标签预热对 Wizard 数据集很重要</strong>，移除后性能急速下降。这是因为在 Wizard 中，知识与人类反应之间存在很强的相关性。结果表明，尽管伪标签是用启发式方法构建的，但它仍然包含有价值的信息，允许联合优化从一个好的点开始。</li>
<li><strong>强化步骤和课程步骤是有用的</strong>。因为强化步骤允许知识选择模块更好地利用 GPT-2 的反馈，并且通过课程步骤 GPT-2 可以逐步利用知识选择模块的输出。</li>
<li><strong>联合优化是有意义的</strong>。去掉这个阶段会导致性能下降。</li>
</ul>
<p>此外，论文还对知识选择数量的上限 $T_{max}$ 做了实验。结果如下表。$T_{max}$ 越大，KnowledGPT 越有可能将真实候选知识纳入生成，PPL 越低。这也解释了为什么 $GPT-2_{trunc}$ 的 PPL 低于 KnowledGPT。另一方面，较大的 $T_{max}$ 也意味着生成的噪声更多。这就是为什么当 $T_{max}$ 超过一个值时，F1 开始下降。</p>
<p><img src="/blog/T-max.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>论文将大规模的预训练语言模型应用于知识对话生成任务。为此，论文设计了一个知识选择模块，并提出了一种无监督方法来联合优化知识选择和响应生成。两个基准的评估结果表明，论文提出 KnowledGPT 模型实现了新的 SOTA。</p>
<p>这篇论文提出的联合优化策略，包括伪标签的构建、强化学习和课程学习，都让我眼前一亮。感觉还是蛮有启发性的工作。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话生成</category>
        <category>知识对话</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>GPT</tag>
        <tag>知识对话</tag>
        <tag>语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title>XLNET：基于置换语言任务的自回归模型</title>
    <url>/blog/2022/04/17/XLNET/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>XLNET 是由卡耐基梅隆大学和谷歌于 2019 年提出的<strong>自回归预训练模型</strong>，论文名为《XLNet: Generalized Autoregressive Pretraining for Language Understanding》，收录于 2019 NIPS 中。其动机是为了解决 BERT 面临的两个问题：忽视了 **[MASK] token 间的依赖关系<strong>以及 [MASK] 导致的</strong>预训练 - 微调差异 **。 XLNet 在 20 项任务上的表现优于 BERT，通常大幅度提高，包括问答、自然语言推理、情感分析和文档排序。</p>
<span id="more"></span>

<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="自编码vs自回归"><a href="#自编码vs自回归" class="headerlink" title="自编码vs自回归"></a>自编码 vs 自回归</h3><p>预训练模型常在大规模语料上，通过构建无监督的预训练目标进行训练。其中，自回归 (AR) 语言建模和自编码 (AE) 是两个最成功的预训练目标。</p>
<p>自编码方法旨在通过从被破坏的输入重建数据，典型的例子为 BERT 的掩码语言模型（MLM），将若干 token 掩码为 [MASK]，通过上下文的其他 token 预测被掩码的 token。类似完形填空任务，将 “我吃饭了” 掩码为” 我吃 [MASK] 了”，预测 [MASK] 为” 饭”。但 BERT 的缺点在于，它对于多个 [MASK] 标记分开预测，换而言之，<strong>假设它们是互相独立的，没有考虑它们间的依赖关系</strong>。而且，在预训练过程中，[MASK] 的 token embedding 为空，对其后续的上下文表征没有任何帮助。而在下游微调任务中，当前 token 的 token embedding 对其上下文表征是最重要的。这就导致了上下游间的差异。</p>
<p>自回归的方法旨在使用自回归模型拟合语料的概率分布，典型的例子为 GPT 的语言模型。将句子概率 $p (x)$ 分解为条件概率的乘积，即 $p (x)=\prod_t p (x_t|x_{&lt;t})$。这种方法非常符合人的认知，但是缺点是无法利用双向的上下文信息。而双向的上下文信息对于下游的理解任务是至关重要的。</p>
<p>XLNET 希望将 AR 和 AE 的优点结合起来，避免它们的局限性。XLNET 提出了一种名为置换语言模型的预训练目标，即<strong>打乱句子的 token 顺序并使用自回归的方法预测</strong>。这种方法不依赖于掩码、结合了双向上下文信息、自回归训练。XLNet 在许多下游任务上比 BERT 表现出色，包括 GLUE 语言理解任务、阅读理解任务（如 SQuAD 和 RACE），Yelp 和 IMDB 等文本分类任务，以及 ClueWeb09-B 文档排名任务。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="置换语言模型"><a href="#置换语言模型" class="headerlink" title="置换语言模型"></a>置换语言模型</h3><p>对于长度为 $T$ 的序列 $x$，存在 $T!$ 种排列可用于自回归生成。用 $\mathcal Z_T$ 代表所有长度为 $T$ 的序列 $[1,2,\cdots,T]$ 的排列集合，$z_t$ 和 $z_{&lt;t}$ 分别代表排列 $z\in\mathcal Z_T$ 的第 $t$ 个和前 $t-1$ 个元素。置换语言模型的目标可形式化定义为：<br>$$<br>\max_\theta \mathbb E_{z\sim\mathcal Z_T}[\sum_{t=1}^Tlogp(x_{z_t}|x_{z_{&lt;t}})]<br>$$<br>事实上，只需要每次随机采样一个排列序列 $z$，优化对数似然即可。而且我们也不需要显式地改变句子顺序，只需要对 Attention 进行掩码即可。后面会介绍到。</p>
<h3 id="双流自注意力机制"><a href="#双流自注意力机制" class="headerlink" title="双流自注意力机制"></a>双流自注意力机制</h3><p>虽然已经得到了形式化的训练目标，但是直接使用 Transformer-XL 优化上述目标可能是没什么用的。具体而言，如果使用标准的 softmax 计算下一个 token 的概率分布，即 $p (X_{z_t}|x_{z_{&lt;t}})$，公式为<br>$$<br>p(X_{z_t}=x|x_{z_{&lt;t}})=\frac{\exp(e(x)^Th_\theta(x_{z_{&lt;t}}))}{\sum_{x’}\exp(e(x’)^Th_\theta(x_{z_{&lt;t}})}<br>$$<br>其中，**$h_\theta (x_{z_{&lt;t}})$ 是 $x_{z_{t}}$ 的隐藏状态表征 * <em><strong>，$e (x)$ 是 $x$ 的嵌入向量。可以看到 $h_\theta (x_{z_{&lt;t}})$ 是与要预测的 token 的位置 $z_t$ 无关的。也就是</strong>无论下一个预测的 token 是哪个位置的，得到的概率分布都是相同的</em><em>。这显然与我们的期望是相悖的。为了解决这个问题，需要<strong>重参数化</strong>这个概率分布，加入 $z_t$ 的信息：<br>$$<br>p(X_{z_t}=x|x_{z_{&lt;t}})=\frac{\exp(e(x)^Tg_\theta(x_{z_{&lt;t}},z_t))}{\sum_{x’}\exp(e(x’)^Tg_\theta(x_{z_{&lt;t}},z_t)}<br>$$<br>其中，*</em> $g_\theta$ 是一种新的、依赖于下一个 token 位置的表征 **。</p>
<p>那么新的问题就来了，怎么建模这个 $g_\theta$ 呢？从形式上可以看出，$g_\theta$ 希望在根据位置 $z_t$ 从已有的上下文 $x_{z_{&lt;t}}$ 中通过注意力机制收集信息。这样就有了两种略显矛盾的需求：</p>
<ul>
<li>预测 $x_{z_t}$ 时，$g_\theta (x_{z_{&lt;t}},z_t)$ 只能依赖位置 $z_t$ 而非内容 $x_{z_t}$</li>
<li> 预测 $j&gt;t$ 的其他标记 $x_{z_j}$ 时，也需要 $x_{z_t}$ 的隐藏表征，此时 $g_\theta (x_{z_{&lt;t}},z_t)$ 应该编码 $x_{z_t}$ 的内容以充分利用上下文信息</li>
</ul>
<p>那么，就需要两种上下文表征：</p>
<ul>
<li><strong>内容表征</strong> $h_\theta (x_{z_{\le t}})$，简写为 $h_{z_{\le t}}$，类似标准 Transformer 中的隐藏状态，同时编码上下文和位置</li>
<li><strong>查询表征</strong> $g_\theta (x_{z_t})$，简写为 $g_{z_t}$，只根据 $h_{z_{&lt;t}}$ 和位置 $z_t$ 计算得到</li>
</ul>
<p>为便于计算，查询流的第一层初始化为可训练的向量，即 $g_i^{(0)}=w$，内容流初始化为嵌入向量，即 $h_i^{(0)}=e (x_i)$。对于其它层 $m=1,\cdots,M$，使用共享参数更新两种流的向量表征，示意图如下所示：</p>
<p><img src="/blog/two-stream.png"></p>
<ul>
<li>图 (a) 为内容流注意力，类比标准 Transformer 中的自注意力，query、key、value 均为有内容的向量表征。</li>
<li>图 (b) 为查询流注意力，不能获取当前位置的内容 $x_{z_t}$，以其位置 $z_t$ 为 query，key、value 也均为有内容的向量表征。</li>
<li>图 (c) 为置换语言模型的训练过程。采样一个排列 $3,2,4,1$，内容流中，1 可以看到 3,2,4，且根据 1 的内容计算双向自注意力，其 Attention 不需要遮挡；2 只能看到 3 和它本身，因此位置 1,4 需要被遮挡。以此类推。查询流与内容流类似，不过需要额外去掉当前位置的内容，即矩阵中的主对角线全部被遮挡。</li>
</ul>
<p>更新公式可以示意为（<strong>注意 KV 里下标 $&lt;,\le$ 的差异</strong>）：<br>$$<br>g_{z_t}^{(m)}\leftarrow Attention(Q=g_{z_t}^{(m-1)},KV=h_{z_{&lt;t}}^{(m-1)};\theta)\<br>h_{z_t}^{(m)}\leftarrow Attention(Q=h_{z_t}^{(m-1)},KV=h_{z_{\le t}}^{(m-1)};\theta)<br>$$<br>使用最后一层的 $g_{z_t}^{(M)}$ 来计算上述的条件概率 $p (X_{z_t}=x|x_{z_{&lt;t}})$。在微调的时候，可以直接将查询流丢弃。</p>
<h3 id="部分预测"><a href="#部分预测" class="headerlink" title="部分预测"></a>部分预测</h3><p>虽然上述理论很美好，但是早期实验证实其收敛很慢。很容易理解，排列中前一部分的 token 的上下文位置随机，预测下一随机位置的 token 是困难的。为了优化方便，论文只预测排列中最后一部分的 token，它们的上下文最为丰富。形式化而言，将排列 $z$ 划分为上下文 $z_{\le c}$ 和目标 $z_{&gt; c}$，c 为切分点。使用超参数 $|z|/(|z|-c)\approx K$ 控制这个比例。这里类似 BERT 里控制 [MASK] 比例，理论上当然是越大越好，但是太大了不易收敛。上下文部分 $z_{\le c}$ 的查询流也不需要计算了，可以节省时间和内存。</p>
<h3 id="集成Transformer-XL"><a href="#集成Transformer-XL" class="headerlink" title="集成Transformer-XL"></a>集成 Transformer-XL</h3><p>在 Transformer-XL 的基础上，添加：</p>
<ul>
<li><strong>相对位置编码</strong></li>
<li><strong>段循环机制</strong></li>
</ul>
<p>主要介绍下段循环机制。对于过长的输入，往往需要以固定长度 T（如 512）将其分段（segment）。假设两段输入分别为 $\tilde x=s_{1:T},x=s_{T+1:2T}$，$\tilde z,z$ 分别为其排列顺序，$\tilde h^{(m)}$ 为 m 层的向量表征。对于段 $x$，计算注意力时可以添加 $\tilde h^{(m-1)}$ 的注意力，类似记忆网络（Memory Network），如下式所示：<br>$$<br>h_{z_t}^{(m)}\leftarrow Attention(Q=h_{z_t}^{(m-1)},KV=[\tilde h^{(m-1)},h_{z_{\le t}}^{(m-1)}];\theta)<br>$$<br>这样可以缓存和重用前一段的模型结果。</p>
<h3 id="多段建模"><a href="#多段建模" class="headerlink" title="多段建模"></a>多段建模</h3><p>许多下游任务以多个段为输入，例如问答、对话等。XLNET 参考 BERT，构建 [CLS, A, SEP, B, SEP] 的输入，用以进行置换语言模型训练。不过事实上，XLNET 并没有使用下句预测的预训练任务，因为它在消融实验里并没有带来帮助。</p>
<p>继承自相对位置编码的思想，XLNET 引入了<strong>相对段编码</strong>。与为每个段显式加入绝对段编码的 BERT 不同，XLNET 使用两个相对段编码 $s_+,s_-$ 衡量两个位置 $i,j$ 是否来自同一个段，$s_+,s_-$ 是每个注意力头可学习的参数。换而言之，XLNET 只考虑两个位置是否来自一个段，不关心它们来自哪两个段。当位置 $i$ 向 $j$ 计算注意力时，段编码 $s_{ij}$ 用以计算权重 $a_{ij}=(q_i+b)^Ts_{ij}$，$q_i$ 是标准的查询向量，$b$ 为特定于注意力头的可学习的偏差向量，得到的 $a_{ij}$ 被添加到注意力 ij 间的权重上。直观而言，如果 ij 来自同一个段，这个权重应该更大，反之则更小。相较于绝对段编码，相对段编码更易扩展，而且可以提高泛化能力。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h3><ul>
<li>预训练数据：包含 BookCorpus、Wikipedia、Giga5、ClueWeb 2012-B 等，共 126GB。BERT 只使用了前两项共 13GB。</li>
<li>参数规模：XLNET-Large 和 BERT-Large 架构一致</li>
<li>训练成本：512 块 TPU，500k 步，5.5 天，完了还是欠拟合</li>
<li> K=6，即只预测一个句子排列的末尾 $1/6\approx 17%$，与 BERT 15% 的掩码率相近</li>
<li>微调：<strong>基于跨度的预测</strong>（span-based prediction）。随机采用一个长度 $L\in [1,2,3,4,5]$，随机采样一段长为 L 的连续的跨度作为预测目标。</li>
</ul>
<h3 id="与BERT的公平比较"><a href="#与BERT的公平比较" class="headerlink" title="与BERT的公平比较"></a>与 BERT 的公平比较</h3><p>XLNET 与 BERT 在相同数据集上训练、架构一致。BERT 选取原生 BERT、全词掩码、无下句预测三种版本中的最优结果。下表为 GLUE、SQuAD 数据集的结果。可以看到在不同下游任务上，XLNET 均超越 BERT，一般一两个点。</p>
<p><img src="/blog/fair-comparison.png"></p>
<h3 id="其他结果"><a href="#其他结果" class="headerlink" title="其他结果"></a>其他结果</h3><p>下面的 XLNET 就是全量数据上的结果了。</p>
<p>下表为在 RACE（阅读理解任务）和 onClueWeb09-B（文档排序任务）测试集的最新结果的比较。RACE 数据集以长度著称，考验了模型的长片段信息捕捉能力。ClueWeb09-B 文档排序数据集，主要用于测试模型生成的词向量效果。可以看到 XLNET 都是最优的。</p>
<p><img src="/blog/scaling.png"></p>
<p>下表为在 SQuAD 数据集上的结果，大幅领先 BERT，略优于 RoBERTa。</p>
<p><img src="/blog/SQuAD.png"></p>
<p>下表为在多个文本分类数据集上的实验结果，主要与 BERT 比较。</p>
<p><img src="/blog/text-classification.png"></p>
<p>下表为在 GLEU 数据集上，与 RoBERTa 等模型的比较结果。还是大幅领先 BERT，略优于 RoBERTa。</p>
<p><img src="/blog/GLEU.png"></p>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>消融实验旨在具体探究：</p>
<ul>
<li>置换语言模型的有效性，对比 MLM</li>
<li>Transformer-XL 架构的重要性</li>
<li>一些其他细节：基于跨度预测、双向输入、下句预测</li>
</ul>
<p>为了公平比较，所有模型都基于 12 层架构，具有与 BERT-Base 相同的模型超参数，并且仅在 Wikipedia 和 BooksCorpus 上进行训练。基线为原始的 BERT-Base 以及使用降噪自编码器（DAE）训练的 Transformer-XL。DAE 和 BERT 差别在于，BERT 只预测 [MASK] token，DAE 预测所有 token。结果如下表所示。</p>
<p><img src="/blog/ablation.png"></p>
<p>1-4 行可以看出，Tranformer-XL 和置换语言模型确实提供了性能提升。如果删去内存缓存机制（第 5 行），性能明显下降，尤其是涉及长上下文的任务。6-7 行显示，基于跨度的预测和双向输入在 XLNet 中都起着重要作用。最后，不出所料的是，下句预测在 XLNET 里也不一定会带来改进。因此 XLNET 也将其排除在预训练目标之外。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文提出了一种名为置换语言模型的预训练任务及配套的预训练模型 XLNET，旨在结合自回归和自编码两种预训练目标的优点。消融实验证明，所提出的置换语言模型的预训练目标是有效的，在各项自然语言理解任务上比 BERT 取得了显著改进。不过相较于 RoBERTa 的提升不大。考虑到 RoBERTa 的<strong>可并行性</strong>，可能 NLU 任务还是偏好 RoBERTa 吧。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/71916499">飞跃芝麻街：XLNet 详解 - 知乎 (zhihu.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>自然语言处理</tag>
        <tag>XLNET</tag>
      </tags>
  </entry>
  <entry>
    <title>ACM 2021: 生物医学域的语言模型</title>
    <url>/blog/2022/04/10/Biomedical-LM/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>《Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing》为微软研究院发表的论文，收录于 2021 年 ACM 中。论文旨在探讨，特定领域的预训练也可以从通用领域语言模型开始受益，这一假设是否适用于具有大规模未标记数据的领域（例如生物医学）。论文证实，相较于从通用领域继续预训练，从头开始训练预训练模型能够获得更好的性能。此外，论文还提出了一个新的生物医学 NLP 基准 BLURB，并创建了排行榜。</p>
<span id="more"></span>

<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>NLP 中，在未标记文本上自监督地训练预训练模型，加之以目标域微调，已经被证实为是行之有效的迁移学习策略，尤其是在目标域数据稀缺、源域和目标域高度相关的情况下。尚不清楚特定领域的预训练是否可以从通用领域的迁移中受益。在生物医学领域，已有的工作表明使用域内文本可以提供超过通用域语言模型的额外收益。然而，这些工作都基于一个普遍的假设–域外文本依然是有用的，并且通常采用混合域方法，例如，通过从现有的通用域语言模型开始特定域的预训练。</p>
<h3 id="混合域方法"><a href="#混合域方法" class="headerlink" title="混合域方法"></a>混合域方法</h3><p>混合域方法如下图上半部分所示。基于在互联网、维基百科等预训练得到的通用语言模型，继续进行特定域的预训练。</p>
<p><img src="/blog/comparison.png"></p>
<p>然而，这种方法存在以下问题：</p>
<ul>
<li><strong>文本差异</strong>。生物医学领域的文本和通用域文本有较大差异，混合域方法增加了负迁移的可能性。</li>
<li><strong>词表差距</strong>。目标域与通用域的词表差距较大。以 BERT 为例，BERT 的词表是从 Wikipedia 和 BookCorpus 生成的，这与生物医学域的词汇差距较大。例如”naloxone” 一词（纳洛酮），医学里的常用名词，在 BERT 中被拆为四块：[na, ##lo, ##xon, ##e]</li>
</ul>
<h3 id="从头开始训练"><a href="#从头开始训练" class="headerlink" title="从头开始训练"></a>从头开始训练</h3><p>上图的下半部分是从头开始进行特定域的预训练的例子（train from scratch）。from scratch 是从头开始的意思，引申一下就是不使用预训练模型，从头开始训练。</p>
<p>如果目标域本身的文本很少，则混合域预训练方法是有意义的，因为可以从使用相关领域的预训练中受益。但是，生物医学并非如此，它在 PubMed 中拥有超过 3000 万个摘要。因此，从头开始的特定领域预训练可能是更好的训练策略。它的优点有：</p>
<ul>
<li><strong>域内数据</strong>。使用的数据全部来自域内，不受其他领域文本的影响。</li>
<li><strong>域内词表</strong>。不受通用域词表限制。</li>
<li><strong>随机初始化</strong>。神经网络训练使用非凸优化，这意味着<strong>持续的预训练可能无法从通用域语言模型中完全撤消次优初始化</strong>。</li>
</ul>
<p>论文实验表明，在生物医学领域，从头开始的特定领域预训练大大优于通用语言模型的继续预训练，从而证明支持混合领域预训练的普遍假设并不总是适用。</p>
<p>此外，论文在提出了一个生物医学语言理解和推理基准 BLURB（Biomedical Language Understanding &amp; Reasoning Benchmark），并在 <a href="https://aka.ms/BLURB%E5%88%9B%E5%BB%BA%E4%BA%86%E6%8E%92%E8%A1%8C%E6%A6%9C%E3%80%82%E5%9F%BA%E5%87%86%E4%B8%AD%E5%8C%85%E5%90%AB%EF%BC%9A%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89%E3%80%81%E5%BE%AA%E8%AF%81%E5%8C%BB%E5%AD%A6%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88PICO%EF%BC%89%E3%80%81%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E3%80%81%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6%E3%80%81%E6%96%87%E6%A1%A3%E5%88%86%E7%B1%BB%E3%80%81%E9%97%AE%E7%AD%94%E7%AD%89%E5%AD%90%E4%BB%BB%E5%8A%A1%E3%80%82%E5%B0%86%E5%AD%90%E4%BB%BB%E5%8A%A1%E5%BE%97%E5%88%86%E5%8A%A0%E6%9D%83%E5%90%8E%E5%BE%97%E5%88%B0%E6%80%BB%E5%88%86%E6%95%B0%E3%80%82">https://aka.ms/BLURB 创建了排行榜。基准中包含：命名实体识别（NER）、循证医学信息提取（PICO）、关系抽取、句子相似度、文档分类、问答等子任务。将子任务得分加权后得到总分数。</a></p>
<p>BLURB 涉及到的任务类型、数据规模和评估指标如下表所示。</p>
<p><img src="/blog/task-intro.png"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>采用<strong>预训练 + 特定任务微调</strong>的范式：</p>
<h3 id="模型设置"><a href="#模型设置" class="headerlink" title="模型设置"></a>模型设置</h3><ul>
<li>词表：Word Piece 子词模型</li>
<li>预训练任务包含下局预测（NSP）任务，以便与 BERT 比较</li>
<li>全词掩码 (WWM)，掩码率为 15%</li>
<li> 数据集：PubMed5 摘要，包含 1400 万个摘要，32 亿个单词，21 GB。</li>
</ul>
<h3 id="特定任务微调"><a href="#特定任务微调" class="headerlink" title="特定任务微调"></a>特定任务微调</h3><p>从同一个预训练 BERT 开始，按照下图的架构，依次执行</p>
<ul>
<li><strong>输入转换</strong>：转换为任务所需的输入。例如关系抽取任务中，需要用特殊符号替换实体词；QA 中需要加入特殊分隔符等。</li>
<li><strong>模型计算</strong>：得到编码向量。</li>
<li><strong>特征抽取</strong>：从向量中选择任务所需的特征。例如，分类任务取 [CLS] 符号的表征向量，NER 取每个 token 的表征向量。</li>
<li><strong>预测</strong>：根据任务 + 特征进行预测。例如，NER 使用线性层 / LSTM/CRF 进行分类，句子相似度使用线性层执行回归等。</li>
</ul>
<p><img src="/blog/task-specific-finetune.png"></p>
<p>BLURB 中的六个任务可分别定义为 token / 句子级别的分类 / 回归任务，如下表所示。</p>
<p><img src="/blog/tasks.png"></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="基线模型"><a href="#基线模型" class="headerlink" title="基线模型"></a>基线模型</h3><p>实验涉及到的一些基线模型如下：</p>
<p><img src="/blog/baselines.png"></p>
<p>其中，SciBERT 是使用生物医学和计算机科学的数据，从头开始生成词汇和预训练。 然而，从生物医学应用的角度来看，SciBERT 仍然采用混合域预训练方法，因为计算机科学文本显然是域外的。<code>Wiki + Books</code> 的词表，即 BERT 的词表，意味着模型都是从 BERT 继续预训练的。</p>
<p>可以看到，除了本文的 <code>PubMedBERT</code>，其他模型都是混合域训练的方法。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>BLURB 上的结果如下表所示，其中所有的 BERT 模型都经过了相同的微调过程。通过从头开始进行特定领域的预训练，PubMedBERT 在大多数生物医学 NLP 任务中明显优于其他 BERT 模型。与使用域外文本训练的 BERT 模型相比，收益最为显着。</p>
<p><img src="/blog/overview-result.png"></p>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><h4 id="词表"><a href="#词表" class="headerlink" title="词表"></a>词表</h4><p>针对两种词表 &amp; 两种掩码方式，论文使用 PubMedBERT 进行了消融实验。换而言之，两边的预训练方法都是从头开始的，不过是词表和掩码方式有所差异。结果如下表。可以看出，全词掩码是明显优于普通掩码的。从 PubMed 构建的词表在大多数任务上都是优于原始 BERT 的词表的。</p>
<p><img src="/blog/vocab-wwm.png"></p>
<p>从词表将句子编码后的平均长度也可以看出来，如下表，可以看出 PubMed 构建的词表的句子平均长度更短，在 QA 上达到了上百的差异。这里也可以看出，原始的 BERT 的词表在 BioASQ 任务上的 tokenize 后的平均句子长度为 <strong>702.4&gt;512，句子被截断了</strong>，怪不得在这个数据集上跟 PubMedBERT 差了十几个点。。。</p>
<p><img src="/blog/vocab-length.png"></p>
<h4 id="预训练方法"><a href="#预训练方法" class="headerlink" title="预训练方法"></a>预训练方法</h4><p>下表展示了预训练预料、训练时间对性能的影响。前两列均使用从通用语料 + 生物医学语料混合域预训练的方法，不过词表不同。两列性能比较互有来回，猜测是由于只根据单一域构建词表，混合域预训练性能受限，与只训练一半时间的 PubMed 类似。</p>
<p><img src="/blog/ptr-method.png"></p>
<p><strong>对抗预训练</strong></p>
<p>结果如下表所示。对抗预训练还损害了性能。作者认为可能是由于域内语料类别单一，如果预训练语料库更加多样化且相对域外，则对抗性训练更有用。</p>
<p><img src="/blog/adversial.png"></p>
<h3 id="微调方法"><a href="#微调方法" class="headerlink" title="微调方法"></a>微调方法</h3><p>以 NER、关系抽取为例，在 BERT 之前，都是使用 LSTM 和 CRF 进行标注。由于 BERT 横扫榜单，这种显式建模方法的实用性遭到了质疑。BERT 模型的顶层已经捕获了整个文本范围内的许多非线性依赖关系。直接进行分类也可以取得很好的效果。在下表中（F1 分数）也可以看出，线性层对在 5 个数据集上都取得了最优。而且线性层是可并行的，LSTM 还要引入额外的串行开销。</p>
<p><img src="/blog/ner-ablation.png"></p>
<p><strong>关系抽取中的虚拟化方法和关系编码的影响</strong>。虚拟化（dummify）方法是指在关系抽取中，为了防止过拟合，往往使用特殊符号（例如 $DRUG 和 $GENE）替换实体词。关系编码是指用于关系分类的向量，例如 [CLS] 或者特殊实体开始标记。</p>
<p>论文对关系编码的三种方法：[CLS]、实体词向量的最大池化、实体首符号的向量，以及输入实体符号的方法：虚拟化、原始文本、在实体前后添加标记进行了比较实验。结果如下表所示。简单地使用原始文本确实会使神经方法面临过拟合风险。对原始文本使用 [CLS] 是最糟糕的选择，因为很难判定要抽取哪两个实体间的关系。虚拟化仍然是最可靠的方法，它适用于任何一种关系编码方法。有趣的是，使用实体标记会在两个数据集中产生稍好的结果，因为它似乎可以防止过度拟合，同时保留有用的实体信息。</p>
<p><img src="/blog/relation-extraction.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文研究了” 通用域预训练模型，对于拥有大量未标记文本的特定域的迁移学习，是否有效 “的问题。在生物医学领域的实验表明，从头开始训练（train from scratch）是更好的选择。论文中还做了非常详细的消融实验，逐个探究词表、训练数据等的影响。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>迁移学习</tag>
        <tag>生物医学</tag>
      </tags>
  </entry>
  <entry>
    <title>Sentence-BERT: 减小语义相似度的计算开销</title>
    <url>/blog/2022/04/04/Sentence-BERT/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>BERT、RoBERTa 此类预训练模型虽然能够提升语义文本相似度（STS）任务的性能，但是在某些场景下会带来巨大的计算开销。例如，利用 BERT 从 10000 个句子的集合中找到最相似的两个句子，需要进行约 5000 万次推理，大约 65 个小时。因此，《Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks》，来自 EMNLP 2019，提出了 Sentence-BERT（SBERT），使用孪生或三胞胎 BERT + 余弦相似度计算语义相似度。可以将上述例子的 65 小时降低至 5 秒钟，同时保证准确率。STS 和迁移任务上的实验证明，SBERT 和 SRoBERTa 已经成为句向量标识的 SOTA 方法。</p>
<span id="more"></span>

<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>BERT 的结构限制了其不适用于某些任务：大规模文本相似度比较、聚类、信息检索。摘要中已经提到了第一种的例子。聚类和信息检索通常需要将句子映射为一个固定维度的向量，使得相似的句子在语义空间中相近。BERT 虽然也能将句子映射为特定的向量，例如取最后一层 [CLS] 的 token embedding，或者做平均池化。但是，实验证明，这样的到的向量甚至差于 GloVe 嵌入的平均。</p>
<p>SBERT 通过以下步骤解决这个问题：</p>
<ol>
<li><p>孪生 BERT 将句子映射为固定维度向量</p>
</li>
<li><p>使用相似度度量（余弦相似度、欧氏距离、曼哈顿距离…）</p>
</li>
</ol>
<p>换而言之，即取消两个句子同时馈入网络的要求，利用相似度度量控制学习得到好的单个句子的嵌入。结构如下图所示：</p>
<p><img src="/blog/architecture.png" alt="结构"></p>
<p>两个句子被分别编码 + 平均池化得到向量表证后，构建计算语义相似度的分类任务（左图）或者回归任务（有图）。</p>
<p>作者在 NLI 数据上微调 SBERT，在 7 个 STS 任务上，SBERT 取得了 11.7 个点的提升，相较于 InferSent 和 Universal Sentence Encoder。在 SentEval 上，SBERT 分别取得了 2.1 和 2.6 个点的提升。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>SBERT 的核心步骤已经在上面介绍了，但还有一些细节需要考虑：</p>
<ol>
<li>如何得到固定维度的向量：[CLS]/ 平均池化 / 最大池化</li>
<li>训练目标：分类 / 回归 / 三元组</li>
</ol>
<p>其中，模型的结构（孪生、三胞胎）、训练目标与可用的训练数据息息相关。</p>
<p><strong>分类目标</strong>：将两个句子向量及逐元素差 $u,v,|u-v|$ 馈入 MLP，并进行 SoftMax 进行分类，公式为：<br>$$<br>o_t=softmax(W_t(u,v,|u-v|))<br>$$<br><strong>回归目标</strong>：与分类类似，区别在于计算 $u,v$ 的余弦相似度，使用 MSE 损失。</p>
<p><strong>三元组目标</strong>：给定一个原始句子 $a$，一个正面句子 $p$，一个负面例子 $n$，减小 $a,p$ 之间的距离，增大 $a,n$ 之间的距离，即要<strong>最小化下式的损失</strong>：<br>$$<br>\max(||s_a-s_p||-||s_a-s_n||+\epsilon,0)<br>$$<br>其中，$s_x$ 为句子 $x$ 的嵌入向量，$||\cdot||$ 为句子度量，$\epsilon$ 为边界，用以保证 $s_p$ 至少 $\epsilon$ 比 $s_n$ 更靠近 $s_a$，即 $||s_a-s_p||+\epsilon&lt;||s_a-s_n||$。当这个式子不成立时，损失为 0，参数不进行更新。实验中使用的距离度量为欧氏距离，$\epsilon=1$。</p>
<h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>作者使用 SNLI、Multi-Genre NLI 数据集训练 SBERT。其中，SNLI 数据集由 57 万个句子对 + 标签组成，MultiNLI 由 43 万个句子对组成，带有文本蕴含信息。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在语义文本相似度任务上进行实验验证。传统方法通常学习一个（复杂的）回归函数，将句子嵌入映射到相似性分数。 然而，这些回归函数以句子对为输入，难以扩展且面临组合爆炸问题。SBERT 则总是使用余弦相似度来比较两个句子之间的相似度嵌入。 负曼哈顿距离和负欧几里得距离的实验结果也大致相同。</p>
<h3 id="无监督STS"><a href="#无监督STS" class="headerlink" title="无监督STS"></a>无监督 STS</h3><p>不使用任何 STS 特定的训练数据。评估数据集为 STS tasks 2012 - 2016， STS benchmark，SICK-Relatedness。每个数据集的句子对包含 0 到 5 的的语义关联标签。评估指标上（两个相似度分数），不使用皮尔逊相关系数（Pearson correlation coefficient），因为它主要衡量随机变量间的<strong>线性相关性</strong>。而是使用斯皮尔曼等级相关系数（Spearman’s rank correlation coefficient），它可以衡量<strong>等级变量间的单调性</strong>，因而更适用于 STS 场景。毕竟两个分数间的相关性不一定是线性的。</p>
<p>实验结果如下表所示。可以看到 naive 的 BERT embedding 差于 GloVe embedding，SBERT 和 SRoBERTa 几乎全面领先，除了 SICK-R。这可能与 Universal Sentence Encoder 训练的新闻、问答等语料有关。</p>
<p><img src="/blog/unsupervised-result.png"></p>
<h3 id="监督STS"><a href="#监督STS" class="headerlink" title="监督STS"></a>监督 STS</h3><p>使用 STS benchmark (STSb) 数据集进行训练。该数据集包括来自标题、新闻和论坛三个类别的 8,628 个句子对，分为训练集（5,749）、验证集（1,500）和测试集（1,379）。</p>
<p>实验结果如下表所示。作者实验了两种训练策略，只在 STSb 上进行训练和在 NLI 数据集上预训练，再在 STSb 数据集上微调。后者能够带来 1-2 个点的提升。可以看到 SBERT 和 BERT 之间的差距不大。</p>
<p><img src="/blog/supervised-result.png"></p>
<h3 id="争论方面的相似性"><a href="#争论方面的相似性" class="headerlink" title="争论方面的相似性"></a>争论方面的相似性</h3><p>Argument Facet Similarity （AFS）语料库注释了来自社交媒体对话的 6,000 个句子论点对，涉及三个有争议的主题：枪支管制、同性婚姻和死刑。数据的标签范围从 0（“不同主题”）到 5（“完全等效”）。 AFS 语料库中的相似性概念与 SemEval 的 STS 数据集中的相似性概念完全不同。 STS 数据通常是描述性的，而 AFS 数据是对话中的争论性摘录。要被认为是相似的，争论不仅必须提出相似的主张，而且还必须提供相似的论证。此外，AFS 中句子之间的词汇差距要大得多。因此，简单的无监督方法以及最先进的 STS 系统在该数据集上表现不佳。</p>
<p>作者使用 10 折交叉验证和跨主题评估（2 个主题用于训练，剩余一个用于评估）进行实验验证。10 折交叉验证设置中的 SBERT 的性能几乎与 BERT 相当。但是，在跨主题评估中， SBERT 的 Spearman 相关性下降了大约 7 个点的 。这可能是由于缺失主题的情况下，BERT 依然能够通过逐字符比较得到较好的结果，SBERT 只能从单一句子推断主题、主张等，这更具有挑战性。</p>
<p><img src="/blog/argument-result.png"></p>
<h3 id="维基百科章节"><a href="#维基百科章节" class="headerlink" title="维基百科章节"></a>维基百科章节</h3><p>利用维基百科的章节构建三元组数据：<strong>假设文章同一部分中的句子在主题上比不同部分中的句子更接近</strong>，锚点和正例来自文章同一个部分，而负例来自同一篇文章的不同部分。作者在 180 万个训练三元组上训练 SBERT 一个 epoch，并在 222,957 个测试三元组上对其进行评估。测试三元组来自一组不同的 Wikipedia 文章。以准确率为评估指标。实验结果如下表所示。</p>
<p><img src="/blog/wikipedia-result.png"></p>
<p>可以看出，SBERT 明显优于 Dor 等人的 BiLSTM 方法。 </p>
<h3 id="SentEval"><a href="#SentEval" class="headerlink" title="SentEval"></a>SentEval</h3><p>SentEval 是一个流行的工具包，用于评估句子嵌入的质量。 句子嵌入用作<strong>逻辑回归分类器的特征</strong>。 逻辑回归分类器在 10 折交叉验证设置中针对各种任务进行训练，并计算测试折的预测准确度。</p>
<p>尽管 SBERT 的句子嵌入的目标不是用于基于特征的迁移学习，微调是更好的方法。但是 SentEval 依然能够在不同任务上评估句子嵌入的质量。作者在 7 个 SentEval 的句子嵌入任务中比较了 SBERT 的性能：</p>
<ul>
<li>MR：电影评论情感预测 </li>
<li>CR：客户产品评论的情绪预测</li>
<li> SUBJ：来自电影评论和情节句子的主题预测</li>
<li> MPQA：来自新闻专线的短语级别意见极性分类</li>
<li> SST：斯坦福情绪树库二分类</li>
<li> TREC ：来自 TREC 的细粒度问题类型分类</li>
<li> MRPC：来自平行新闻来源的微软研究院释义语料库</li>
</ul>
<p>实验结果如下表所示。可以看到 SBERT 在 5/7 的任务上取得了最优性能，带来平均两个点的性能提升。尽管这种迁移学习并非是 SBERT 的目标，SBERT 依然取得了 SOTA 的性能。SBERT 明显比 Universal Sentence Encoder 差的数据集是 TREC 数据集。 Universal Sentence Encoder 在问答数据上进行了预训练，这似乎有利于 TREC 数据集的问题类型分类任务。</p>
<p><img src="/blog/senteval-result.png"></p>
<p>另外，这里的 BERT 的 [CLS] 优于 GloVe 嵌入平均，似乎与前文得到的结论相违背。这是因为任务设置不同。对于 STS 任务，论文使用余弦相似度来估计句子嵌入之间的相似度。余弦相似性平等地对待所有维度。相比之下，SentEval 将逻辑回归分类器与句子嵌入相匹配。这使得某些维度可以对分类结果产生更高或更低的影响。 </p>
<p>观察上表还可以得出结论，BERT 的平均嵌入或者 [CLS] 符号嵌入搭配余弦相似度、欧氏距离、曼哈顿距离是不可行的。对于特征迁移学习，它们产生的结果也比 InferSent 或 Universal Sentence Encoder 稍差。然而，使用 SBERT 的孪生网络 + NLI 微调的方法能够得到 SOTA 的句嵌入。</p>
<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p>作者对池化策略、训练目标、拼接结果（分类目标时使用）进行了消融实验。其中，NLI 对应分类任务，STSb 对应回归任务。结果如下表所示。</p>
<p><img src="/blog/ablation.png"></p>
<p>可以看到，分类任务下，池化策略影响不大，用于分类的特征拼接结果影响很大。InferSent 和 Universal Sentence Encoder 都是使用 $(u,v,|u −v|,u ∗v)$，然而，在 SBERT 里，$u ∗v$ 的加入还会损失性能。其中，最重要的特征是逐元素差 $|u - v|$。</p>
<p>在回归任务下，池化策略具有很大的影响：最大池化的表现比平均池化或 CLS-token 策略要差得多。</p>
<h2 id="计算效率"><a href="#计算效率" class="headerlink" title="计算效率"></a>计算效率</h2><p>下表比较了不同模型在不同设备上每秒可以处理的句子嵌入。越高越好。可以看到 SBERT 在 GPU 上是最快的，相较于 InferSent 的 LSTM，这也得益于 Transformer 的并行性。</p>
<p><img src="/blog/computation-efficiency.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文还是非常扎实的。从非常现实的计算效率问题出发，使用简单的方法，在不同的任务上取得与 BERT 相似甚至超越 BERT 的性能。其影响力也不容小觑，时至今日已经有 2000 + 的引用。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语义相似度</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>语义相似度</tag>
      </tags>
  </entry>
  <entry>
    <title>有错误论文 - ACL2020：交叉 VAE 用于答案检索</title>
    <url>/blog/2022/03/31/Crossing-VAE/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>最近读了一篇想法有趣、公式有错误、复现不出来的 ACL 论文。发出来分享一下。希望大家也不要迷信 ACL 论文，读论文过程中保持独立思考。</p>
<p>《Crossing Variational Autoencoders for Answer Retrieval》提出了一种基于交叉 VAE 的答案检索方法，通过交叉 VAE 来对齐答案和问题之间的语义。论文收录于 ACL 2020 中。</p>
<span id="more"></span>

<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>先按照正常的步骤介绍其背景和方法。</p>
<p>答案检索，即从答案候选集合中，选择最匹配问题的答案。解决该问题的一个关键因素在于，如何学习到一个好的问题 / 答案的向量表示。传统的方法使用两个编码器 / 变分自编码器（孪生网络）分别学习二者的向量表征。这种方法过于分离，无法捕捉二者之间的对齐语义。因此，本文提出了交叉变分自编码器，类似 Seq2Seq 的方法，学习二者间的语义对齐。</p>
<p><img src="/blog/structure.png"></p>
<p>如上图所示。左一为孪生编码器，分别将答案和问题进行编码，得到向量表征后再判断是否匹配。中间的孪生自编码器也类似，区别在于使用自编码器学习到性质更好的向量表征。但是他们有个共同点：<strong>答案和问题的向量表征是分开编码得到的</strong>。这意味着编码过程中只有单个问题或答案的信息（语义、句法等），而我们关心的是答案和问题是否语义对齐。因此按前一种方法训练得到的向量表征，并不能很好地满足任务的需要。右一为本文提出的孪生交叉 VAE，即在 VAE 的基础上，使用问题和答案的隐变量重构彼此，而非重构自身。从模型结构上，该模型不像 VAE 而更像 Seq2Seq。</p>
<p>举个例子，在问答中，相似的问题的答案可能相去甚远，相似的答案的问题形式可能也差别很大。下图展示了 SQuAD 数据集上的例子，同一个答案与 17 个不同的问题相对齐。只看这些问题的语义，可能差别很大。也就是 $z_q$ 是分开的，而答案却相同，即 $z_q$ 是固定的。这种情况下，希望 $p (y|z_q,z_a)$ 对这些不同的 $z_q$ 都有较好的对齐效果是比较困难的。</p>
<p><img src="/blog/SQuAD-example.png"></p>
<p>究其根本，是<strong>同一个答案与不同的问题对齐时，其语义信息侧重点各不相同。</strong>当提问词是”how many” 是，答案侧重点为”three”，当提问词为”three cities” 时，答案侧重点为”New Orleans”… 而分开编码得到的向量表征是固定的。要解决这个问题，必须要将答案和问题的编码结合起来。</p>
<p>论文的方法在 MRR 和 R@1 上分别取得了 SOTA，分别提升了 1.46% 和 3.65%。不过对比工作里的 BERT 我怀疑是未做 finetune 的，效果太差了。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>问题集合：$q\in \mathcal Q$，答案集合：$a\in\mathcal A$。对齐关系三元组定义为 $(q,a,y)$，其中 $y$ 为一个二值变量，标识对齐关系。任务定义为，给定一个问题 $q$ 和一组候选答案 $C (q)\subseteq \mathcal A$，对每个候选答案 $a\in C (q)$ 预测 $p (y|q,a)$ 的值。</p>
<h3 id="交叉VAE"><a href="#交叉VAE" class="headerlink" title="交叉VAE"></a>交叉 VAE</h3><p>根据条件分布 $p (q|z_a),p (a|z_q)$ 学习交叉重构，$z_a,z_q$ 为变分自编码器的连续隐变量，交叉重构定义为：<br>$$<br>p(q|a)=\mathbb E_{z_a\sim p(z_a|a)}[p(q|z_a)]\<br>p(a|q)=\mathbb E_{z_q\sim p(z_q|q)}[p(a|z_q)]<br>$$<br>问答对的匹配，即计算条件分布 $p (y|a,q)$，等价于<br>$$<br>p(y|a,q)=\mathbb E_{z_q\sim p(z_q|q),z_a\sim p(z_a|a)}[p(y|z_a,z_q)]<br>$$<br>目标函数包含三部分：</p>
<p><strong>交叉重构损失</strong>，即两个交叉熵损失之和，公式如下。其中 $E,D$ 分别代表编码器、解码器。<br>$$<br>\mathcal L_{cross}(\theta_E,\theta_D)=y\cdot \mathbb E_{q\sim Q}[-logp_D(q|a,E(a))]+ y\cdot\mathbb E_{a\sim A}[-logp_D(a|q,E(q))]<br>$$<br><strong>KL 散度损失</strong>，两部分后先验 KL 散度之和，公式如下：<br>$$<br>\mathcal L_{KL}(\theta_E)=y\cdot \mathbb E_{q\sim Q}[D_{KL}((p(z_q|q)||p(z_q))]+ y\cdot \mathbb E_{a\sim A}[D_{KL}((p(z_a|a)||p(z_a))]<br>$$<br><strong>问答匹配损失</strong>，最后输出的匹配结果的交叉熵损失，公式如下。其中，$f$ 为匹配函数。<br>$$<br>\mathcal L_{matching}(\phi_f)=-[y\cdot logp_{f_\phi}p(y|z_q,z_a)+(1-y)\cdot logp_{f_\phi}(1-p(y|z_q,z_a))]<br>$$<br>最后，将三个损失相加，即得到了最终的目标函数，并引入超参数控制权重：<br>$$<br>\mathcal J=-\alpha \mathcal L_{cross}-\beta\mathcal L_{KL}+\gamma\mathcal L_{matching}<br>$$</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>细心的读者可能会发现了，你是要<strong>最小化匹配损失，最后的 $\mathcal J$ 应该也是要最小化的。那不就变成最大化 KL 和重构损失了吗？</strong>对，这就是问题所在。论文中提到是要最大化 ELBO 和最小化匹配损失，得到了最后的 $\mathcal J$。然而事实上，$\mathcal L_{crosss}+\mathcal L_{KL}=-ELBO$，最小化 $\mathcal L_{crosss}+\mathcal L_{KL}$ 才是在最大化 ELBO。论文作者根本没有分清楚 VAE 的 ELBO 和损失，最后闹出了这种笑话。加之论文中有将 objective 和 loss 混用且使用错误的情况。也侧面印证了这一事实。正确的损失函数应该形如：<br>$$<br>\mathcal J=\alpha \mathcal L_{cross}+\beta\mathcal L_{KL}+\gamma\mathcal L_{matching}<br>$$<br>复现性上，我问了实验室的一个师兄，说是论文的结果是复现不出来的，论文也没有公开代码。再加上我本人看到这篇论文第一感觉的强烈违和感，将 VAE 改为 Seq2Seq 的形式，整个 VAE 的公式可能都要重新推导，只改最后的 ELBO 如何保证过程正确？CVAE 可能才是更自然的想法。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>读到这里，这篇论文已经没有读下去的必要了。把阅读经验分享出来，希望大家读论文的时候也加以辩证，不要迷信顶会论文，尤其是不开源代码和模型的论文。</p>
<p>科研路漫，诸君共勉。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>问答</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>VAE</tag>
        <tag>问答</tag>
        <tag>负面案例</tag>
      </tags>
  </entry>
  <entry>
    <title>智源 EVA2.0: 聊天机器人 EVA 加强版</title>
    <url>/blog/2022/03/24/EVA2-0/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>EVA 2.0 是智源、清华在论文《EVA2.0: Investigating Open-Domain Chinese Dialogue Systems withLarge-Scale Pre-Training》中提出的对话模型，论文于 3.17 登陆 arxiv，也就是一周前。EVA2.0 旨在探究数据质量、模型架构、训练方法、解码策略等因素的影响，而不是进一步扩张模型和数据。经过以上优化后，仅 300M 参数的 EVA 2.0 就达到了 2.8B 的 EVA 1.0 的水平。代码和模型见 <a href="https://github.com/thu-coai/EVA">thu-coai/EVA</a>。</p>
<span id="more"></span>

<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>论文提出了相关性、通顺性、娱乐性等指标用于分析数据质量，并进行数据处理。</p>
<h3 id="质量指标"><a href="#质量指标" class="headerlink" title="质量指标"></a>质量指标</h3><h4 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h4><p>相关性可以反映上下文和回复间的连贯性、参与度，可以通过一些直觉公式进行估计，或者训练一个 NLU 分类模型。</p>
<p>直觉来看，上下文和回复越相关，同时包含在二者中的单词越多。因此，这一部分分数定义为上下文 $C$ 和回复 $R$ 之间的单词覆盖率，具体公式为:<br>$$<br>S_1=\sum_{w_i\in C,w_j\in R} dist(w_i,w_j)^\tau I(w_i=w_j)<br>$$<br>其中，$dist (w_i,w_j)$ 为 $w_i$ 和 $w_j$ 在对话中的轮次距离，$\tau$ 用于调整分数。</p>
<p>训练模型的方法，论文在 LCCC 数据集上训练了一个 $BERT_{BASE}$ 模型，用于判别上下文和回复是否对应，在测试集上得到了 93.0% 的准确度和 0.86 的 F1 分数。这一部分分数公式如下，也就是分类的 $logits$。<br>$$<br>S_2=logp(1|C,R)<br>$$<br>很显然，神经网络的方法更健壮一些。训练过程和细节论文未提，猜测应该就是正例 + 随机构建负例进行分类吧。不过这里也有一个有趣的地方。LCCC 数据集数据集是 BERT 分类 + 规则过滤进行清洗的，BERT 是在人工标注的 10w 条数据上训练的。EVA 1.0 版本中的 WDC-Dialog 只提到了规则过滤。2.0 从 LCCC 数据集去训练分类器，相当于是逆向了 LCCC 的分类模型，省去了人工标注数据的成本。</p>
<h4 id="通顺性"><a href="#通顺性" class="headerlink" title="通顺性"></a>通顺性</h4><p>论文使用 kenlm，一个统计语言模型工具，来计算每个句子的概率，对话的通顺分数是每轮次分数之和，公式如下：<br>$$<br>S_3=-\frac1n\sum logP(w_1^i,w_2^i,\dots,w_{|u_i|}^i)<br>$$</p>
<h4 id="娱乐性"><a href="#娱乐性" class="headerlink" title="娱乐性"></a>娱乐性</h4><p>由于中文社交媒体上有很多娱乐明星的粉丝（点名微博了属于是），相关对话中会包含一些不想要的信息，并且这些语言习惯也和正常聊天有所差异。因此，论文计算娱乐明星在对话中的占比，计算娱乐分数。</p>
<h3 id="数据改进"><a href="#数据改进" class="headerlink" title="数据改进"></a>数据改进</h3><ul>
<li><strong>数据集改进</strong>：作者发现有些数据集不适合开放域对话场景，例如京东客服对话 JDDC 数据集，使用它训练会导致说话口吻像电商客服，因此将其从 WDC-Dialog 中移除。</li>
<li><strong>回复数量改进</strong>：来自微博等社交媒体的数据集中，存在一个上下文对应相当多回复的情况。这些回复非常相似有时会严重影响性能。因此作者为每个上下文设置了最大回复数量并进行过滤。</li>
<li><strong>规则过滤</strong>：作者在 EVA 1.0 的规则上做了进一步的改进，例如将繁体字转为简体字，移除不合理的连续标点符号等。</li>
<li><strong>基于分类器的过滤</strong>：根据质量分数的加权 $S=\alpha S_1+\beta S_2+\gamma S_3$ 对样本进行过滤，去除质量分数低于阈值的样本。</li>
</ul>
<h3 id="数据扩展"><a href="#数据扩展" class="headerlink" title="数据扩展"></a>数据扩展</h3><p>来自社交媒体的数据会夹杂很多的流行语，而它们在日常对话中并不常见，这会导致偏差。因此，作者还从公开来源收集了以下四种数据，共计 12GB。</p>
<ul>
<li>电影和电视剧的对话字幕</li>
<li>小说和故事中的对话</li>
<li>百度知道的问答对</li>
<li>已有的公开众包对话：DuConv，KdConv，DuRecDial，NaturalConv</li>
</ul>
<h3 id="数据统计信息"><a href="#数据统计信息" class="headerlink" title="数据统计信息"></a>数据统计信息</h3><p>下表展示了 EVA 2.0 和 1.0 的数据集对比。可以看到 EVA 2.0 的数据集不足 1.0 版本的 $1/3$，但显著提升了质量。相关性、通顺性的提升与数据调整中的过滤方式大致有个对应关系，例如规则过滤主要改善通顺性。另外，个人感觉数据调整中会根据质量分数过滤，再采用质量分数来评估似乎欠妥。可能再各自随机采样样本进行人工标注更有说服力一些。</p>
<p><img src="/blog/statistics.png"></p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>模型还是 Seq2Seq 的 Transformer，不过和 EVA 1.0、t5 不一样的是，使用的是 scaled Attention，即计算 Attention 的时候除以了 $\sqrt d$。论文主要讨论了两个模型的两个配置：层数和角色信息。</p>
<p>基于 Seq2Seq 的对话模型的编码器和解码器层数往往不是一致，解码器要比编码器更深以达到更好的生成效果。但是，更深的编码器也能提高对话上下文理解能力。因此，论文在控制参数规模情况下，尝试了不同的层数比例。</p>
<p>近期工作指出，在长对话中，预训练模型可能会混淆它们的角色，因为模型是在角色信息复杂的社交媒体数据上训练得到的。因此，需要为对话模型添加角色信息保证一致性。例如，PLATO-XL 在多角色对话中添加了角色嵌入。不过由于 WDC-Dialog 数据集角色信息的缺失（例如字幕中一般不会包含角色信息），如果要添加角色信息，只能强制将对话认定为两个角色间进行的对话。论文同时使用了角色嵌入和角色标识符并测试效果。</p>
<h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><p>论文研究了两种预训练方法：从头开始预训练或者从长文档预训练的模型再进行预训练。直觉来看，在长文档上预训练的模型已经学到了一些知识。但是由于文档和对话的差异性，不清楚这对对话任务是否会起到正面作用。</p>
<h3 id="解码策略"><a href="#解码策略" class="headerlink" title="解码策略"></a>解码策略</h3><p>论文在上尝试了以下几种不同的解码策略。虽然有研究在英文对话机器人上对这些策略进行了实验，论文认为解码策略是语言相关的，中文的结论可能不同。</p>
<ul>
<li><strong>贪心搜索（Greedy Search）</strong>：朴素解码，每步选概率最大的 token，公式为 $y_t=\arg\max_{y_t} P (y_t|x;y_{&lt;t})=softmax (h_t)$</li>
<li><strong> 采样（Sampling）</strong>：每步根据概率分布采样 token，公式为 $y_t\sim P (y_t|x;y_{&lt;t})=softmax (h_t/T)$，$T$ 为温度系数，增大低概率 token 被采样的概率。</li>
<li><strong>光束搜索（Beam Search）</strong>：贪心搜索的改进，避免局部最优。同时维护多个解码序列，最终选择概率最大的序列。</li>
<li><strong>长度控制（Length Control）</strong>：朴素解码策略倾向于生成短而通用的序列，长度控制可改善这一问题，常与光束搜索搭配使用。<strong>最小长度约束方法</strong>：在解码时将 &lt;EOS&gt; 的概率置 0 直至达到最小长度要求。<strong>长度惩罚</strong>：光束搜索时对候选序列分数除以 $l^\alpha$，其中 $l$ 为序列长度，$alpha$ 越高越偏向长序列（beam search 的 score 是 $logp$，为负数）。</li>
<li><strong>处理重复（Handling Repetitions）</strong>：语言模型生成普遍会出现重复现象。No-Repeat-N-Gram 策略禁止生成对话历史中存在的 n-gram。</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul>
<li><strong>自动评估指标</strong>：uni-gram F1 (F1), ROUGE-L (R-L),BLEU-4 (B-4), distinct 4-grams (D-4) </li>
<li><strong>人工评估指标</strong>：合理性、特异性、一致性</li>
</ul>
<h3 id="层数"><a href="#层数" class="headerlink" title="层数"></a>层数</h3><p>6-18 代表编码器 6 层，解码器 18 层。表格可以看出，层数平衡的架构是最优的。</p>
<p><img src="/blog/layer.png"></p>
<h3 id="角色信息"><a href="#角色信息" class="headerlink" title="角色信息"></a>角色信息</h3><p>还是上面的表格，加入角色信息后性能有所下降。作者解释为由于数据集中角色信息的缺失，强迫模型将多角色对话认作两个角色间进行的对话反而会带来反面效果。</p>
<h3 id="预训练策略"><a href="#预训练策略" class="headerlink" title="预训练策略"></a>预训练策略</h3><p>下两表为两种训练策略的自动和人工评估结果。可以看出，在文档预训练的模型继续训练的模型只在知识指标上效果更优，在对话指标上都不如从头训练模型。</p>
<p><img src="/blog/pretraining-auto.png"></p>
<p><img src="/blog/pretraining-human.png"></p>
<h3 id="解码策略-1"><a href="#解码策略-1" class="headerlink" title="解码策略"></a>解码策略</h3><p>下两表为解码策略的自动和人工评估结果。贪婪搜索默认与 no-repeat-n-gram 结合起来，因为简单的贪婪搜索通常会导致重复。</p>
<p>结论：</p>
<ul>
<li>没有在所有指标上最优的解码策略</li>
<li>采样能够生成多样的回复，但会损失合理性</li>
<li>无重复 n-gram 的简单贪心解码在人类评估中产生了令人惊讶的良好性能</li>
<li>模型在最小长度约束下倾向于生成自相矛盾的响应（与英文不同）</li>
<li>光束搜索结合采样、长度惩罚、禁止重复取得了相对均衡的性能</li>
</ul>
<p><img src="/blog/decode-auto.png"></p>
<p><img src="/blog/decode-human.png"></p>
<h3 id="最终评估"><a href="#最终评估" class="headerlink" title="最终评估"></a>最终评估</h3><p>论文将 EVA 2.0 和 1.0、CPM 进行了对比。结果如下两表。baseline 还是有点少。</p>
<p><img src="/blog/evaluation.png"></p>
<h2 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h2><p>下面举几个栗子来具体展示一下。可以看到，EVA 2.0 虽然有时能取得较好的对话效果，但还是会有不一致、幻觉、不安全、缺少同理心这些问题。这正是谷歌 LaMDA 想要解决的问题，可以参考我的另一篇博客<a href="https://tqnwhz.github.io/blog/2022/03/19/LaMDA/#more">谷歌 LaMDA：高达 137B 参数的 “全能型” 聊天机器人</a>。</p>
<h3 id="好"><a href="#好" class="headerlink" title="好"></a>好</h3><p><img src="/blog/nice-example.png"></p>
<h3 id="不一致"><a href="#不一致" class="headerlink" title="不一致"></a>不一致</h3><p><img src="/blog/inconsistent-example.png"></p>
<h3 id="幻觉"><a href="#幻觉" class="headerlink" title="幻觉"></a>幻觉</h3><p><img src="/blog/hallucination.png"></p>
<h3 id="不安全"><a href="#不安全" class="headerlink" title="不安全"></a>不安全</h3><p><img src="/blog/unsafe-example.png"></p>
<h3 id="缺少同理心"><a href="#缺少同理心" class="headerlink" title="缺少同理心"></a>缺少同理心</h3><p><img src="/blog/lack-empathy.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文还是蛮有价值的，讨论了模型规模和数据规模之外，影响模型性能的因素。类似 RoBERTa 之于 BERT，可以作为炼丹的参考。不过读完之后我最关心的几个问题还是没有解决：</p>
<ul>
<li>模型架构上，PLATO 采用的 unified Transformer 和 EVA 的 Seq2Seq，孰优孰劣？虽然有论文做过实验，但是其模型规模、数据规模都达不到预训练模型的水平。</li>
<li>基线对比上，还是没有跟 PLATO-XL 进行比较。虽然感觉 EVA 2.0 大概率是比不过 PLATO-XL 的，但还是想了解一下差距、可能的解释、改进等。</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话生成</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>自然语言处理</tag>
        <tag>对话生成</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>谷歌 LaMDA：高达 137B 参数的 “全能型” 聊天机器人</title>
    <url>/blog/2022/03/19/LaMDA/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>《LaMDA: Language Models for Dialog Applications》是谷歌于 2022 年发表的论文，收录在 arxiv 中。论文提出了一个名为 LaMDA（<strong>La</strong>nguage <strong>M</strong>odels for <strong>D</strong>ialog <strong>A</strong>pplication）的对话模型，拥有 137B 参数，在 1.56T 公开对话数据和网页上预训练。实验证明，虽然模型扩展能够提升对话质量，但是在安全性和事实性方面的改进很小。而监督数据上的微调能够帮助模型利用外部知识源进行回复，显著改进了安全性和事实性两个指标。</p>
<span id="more"></span>

<p>LaMDA 构建了一个工具集（TS，Tool set），包含：信息检索系统、计算器、翻译器。通过监督数据微调，LaMDA 能够利用这些工具来回答问题，这使得模型能够根据已知知识来源做出响应，减少了幻觉现象。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>LaMDA 使用单个 Transformer 模型来执行多项任务：它生成潜在响应，然后出于安全考虑对其进行过滤、基于外部知识源并重新排序以找到最高质量的响应。LaMDA 的参数范围从 2B 到 137B 参数，在以上指标上进行测试。结果如下图所示。左侧为质量分数，右侧为安全性和事实性分数。</p>
<p><img src="/blog/overview.png"></p>
<p>可以观察到：</p>
<ul>
<li>单独的模型缩放提高了质量，但它在安全性和接地性方面的改进远远落后于人类表现</li>
<li>结合缩放和微调在所有指标上显着提高了 LaMDA，尽管模型的性能仍然低于人类水平在安全性和接地性的水平</li>
</ul>
<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>预训练模型架构都差不多，关键参数如下：</p>
<ul>
<li>参数规模：137B</li>
<li> 数据集：预训练（2.97B 文档 + 1.12B 对话）</li>
<li>架构 &amp; 训练目标：Transformer-Decoder，语言模型（预测下一个 token），如下图所示</li>
<li>训练成本：1024 TPU-v3 * 57.7 天</li>
</ul>
<p><img src="/blog/pretraining.png"></p>
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><h3 id="SSI"><a href="#SSI" class="headerlink" title="SSI"></a>SSI</h3><p><strong>SSI</strong> 是合理性、特异性、趣味性三项指标（Sensibleness, Specificity, Interestingnes）的平均值，是谷歌在 Menna 中提出的 SSA（合理性、特异性两项平均）的改进。各项指标具体含义如下：</p>
<ul>
<li><strong>合理性</strong>：衡量模型的回复在上下文和不要与前面所说的任何内容相矛盾。然而，通用和无聊的回复，例如 “我不知道” 的合理性分数可能很高。因此只有这一项指标是远远不够的。</li>
<li><strong>特异性</strong>：衡量模型的回复是否特定于上下文。例如，如果用户说 “我爱欧洲电视网”，而模型回答 “我也是”，那么它的特异性得分为 0。因为这种句式适用于很多上下文。</li>
<li><strong>趣味性</strong>：衡量模型的回复是否有趣。例如，对 “我如何扔球？” 的回应可能是 “你可以先捡起然后扔球来扔球”，这是有道理的，并且是针对问题的。另一个更深层次和更令人满意的答案可能是 “扔球的一种方法是用双手牢牢握住它，然后再向下摆动你的手臂，伸展你的肘部，然后向上释放球”。</li>
</ul>
<p>每项指标对应一个 0/1 标签，正例为 1，负例为 0，算术平均后就是 SSI 的值。</p>
<h3 id="角色特定指标"><a href="#角色特定指标" class="headerlink" title="角色特定指标"></a>角色特定指标</h3><p><strong>有用性</strong>：如果模型的响应包含基于用户使用信息检索系统进行的独立研究的正确信息，并且用户认为它们有帮助，则它们被标记为有用。<strong>有用的响应是信息性响应的子集</strong>，由用户判断为正确且有用。</p>
<p><strong>角色一致性</strong>：如果模型的响应看起来像执行目标角色的代理会说的话，则它们被标记为角色一致。这个与合理性中的一致性不同，这里的一致性是指，例如让模型扮演珠穆朗玛峰，模型的回复中的语气词、设定等都要以珠穆朗玛峰为准。</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p><strong>安全性</strong>是根据 Google 人工智能原则设定的，用以以避免造成伤害风险的意外结果，并避免产生或加强不公平的偏见，这个对应很多条规则，比较复杂，就略过了。</p>
<p><strong>事实性</strong>：包含外部世界声明的回复中，可由权威外部来源支持的回复的百分比。</p>
<p><strong>信息性</strong>：在所有回复中，包含已知来源支持的外部世界信息的回复所占的百分比。信息性与事实性仅在限定词上有所不同。因此，像 “这是一个好主意” 这样的回答，如果不包含任何外部世界的信息，就不会影响其事实，但会影响其信息性。</p>
<p><strong>引用准确度</strong>：引用其来源 URL 的模型回应在所有明确声称外部世界的回应中所占的百分比，不包括众所周知的事实（如 “马有四条腿”）。</p>
<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><h3 id="判别式-x2F-生成式微调"><a href="#判别式-x2F-生成式微调" class="headerlink" title="判别式/生成式微调"></a>判别式 / 生成式微调</h3><p>为了提高质量，谷歌团队收集了众包人员与 LaMDA 就任何主题交谈的 6400 次对话，每个对话包含 14-30 轮。对于每个模型回复，众包人员为其评估每个质量标签。如果回复不合理（合理性 = 0），不会收集特异性和趣味性标签。同样，如果回复不具体（特异性 = 0），不会收集趣味性标签。</p>
<p>LaMDA 生成回复的时候按照 <code>&lt;上下文&gt;&lt;哨兵&gt;&lt;回复&gt;</code> 的模板进行生成，按照 <code>&lt;上下文&gt;&lt;哨兵&gt;&lt;回复&gt;&lt;属性&gt;&lt;分数&gt;</code> 的模板进行判别式微调，例如”What’s up? RESPONSE not much. SENSIBLE 1”。</p>
<p>得到众包数据后，按照上述模板进行微调。这样在生成时，就能够预测出对应属性值作为筛选的辅助信息。将生成的候选序列按照 $3 *P (sensible) + P (specific) + P (interesting)$ 进行排名，选择排名靠前的候选序列作为下一个响应。</p>
<p>可以看到，经过微调 + 生成筛选，LaMDA 的安全性和质量都有了较高提升。论文贴心地给出了两种类型的图。可以看到经过微调的 LaMDA 比仅预训练的基础模型在各项指标上都有提升，在对话质量上已经接近甚至超过了人类的水平，安全性上也十分接近人类的水平。不过事实性和信息性还有一定的差距。</p>
<p><img src="/blog/finetune-line.png"></p>
<p><img src="/blog/finetune.png"></p>
<h3 id="调用外部信息系统"><a href="#调用外部信息系统" class="headerlink" title="调用外部信息系统"></a>调用外部信息系统</h3><p>为了避免幻觉，谷歌团队构建了一个工具集（Toolset，TS），包含：信息检索系统、计算器、翻译器。TS 接收一个字符串作为输入，输出一个字符串的列表。例如，计算器接收 135+7721”，返回 [“7856”]。类似地，翻译器可以接收 “hello in French” 并输出 [“Bonjour”]。信息检索系统可以接收 “Howold is Rafael Nadal?”，并输出 [“Rafael Nadal / Age / 35”]。如果一个工具无法解析输入（例如，计算器无法解析 “Rafael Nadal 几岁？”），它将返回一个空的结果列表，因此不会对最终输出列表做出贡献。</p>
<p>团队收集了 40k 监督的对话数据用于生成，9k 条 LaMDA 的生成候选数据（标记为正确 / 不正确）用于判别排名。这些数据同样是众包人员与 LaMDA 间通过交互式和静态方法收集得到。</p>
<p>微调分为两部分：</p>
<ul>
<li>根据上下文和基础模型响应，获取 TS 查询字符串。例如，Rafael Nadal 几岁？：上下文 + 基础→“TS，Rafael Nadal 的年龄”</li>
<li> 根据基础回复和 TS 返回结果，预测事实版本回复：例如，“他现在 31 岁”+“Rafael Nadal / Age / 35”。然后它预测事实版本：上下文 + 基础 + 查询 + 片段 →“用户，他现在 35 岁”。</li>
</ul>
<p>实际交互的例子如下：</p>
<p><img src="/blog/toolset.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文还是挺有意思的，这种生成式 + 判别式微调的方式还是第一次见，而且也可以辅助结果搜索。而且，少量的微调数据就可以取得非常好的效果。引用原文中的一句话：使用适量的人工注释微调数据（不到 0.001% 的预训练数据），可以在更好质量和更安全的对话模型方面取得重大进展。</p>
<p>虽然，说是少量，也有几千上万条数据，而且就论文中的标注复杂度，标注成本也是蛮高的。不过相较于预训练数据的海量数据，微调数据可以说是少的多的多了。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话生成</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>对话生成</tag>
        <tag>LaMDA</tag>
      </tags>
  </entry>
  <entry>
    <title>复杂知识库问答（KBQA）中的问题迁移</title>
    <url>/blog/2022/03/13/Program-Transfer/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>《Program Transfer for Answering Complex Questionsover Knowledge Bases》是由清华大学和华为合作完成的工作，收录于 ACL 2022 长文中。该文试图通过一个两阶段的方法来完成 KBQA 任务：解析器将 query 解析为知识库操作序列（无参数），参数分析器为每个操作填入参数。通过在复资源知识库预训练，显著提升了在低资源知识库上的 KBQA 性能。</p>
<span id="more"></span>

<p>本文所讲的方法是语义解析的思想，将 query 先转化为知识库的操作序列，如下图所示。分析出 query 对应的操作序列（Find，Relate..），再为每个操作填入参数（实体、关系等），执行操作即可得到结果。</p>
<p><img src="/blog/example.png"></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>知识库问答，即 KBQA，旨在利用结构化的知识库来解答 query。在<a href="https://tqnwhz.github.io/blog/2021/10/25/KBQA-survey/#more">基于知识库的问答综述（KBQA）</a>中提到了，KBQA 有两种主流方法，<strong>语义解析</strong>的方法将 query 转化为 SQL 语句，执行即可得到结果；信息检索的方法在特定子图中将实体按相关性排序，最终得到结果。复杂 KBQA 是指需要处理多跳逻辑的 KBQA 问题。这种情况下，KBQA 所面临的推理路径的监督数据缺失、语义理解能力不足、检索空间过大等问题会更为突出。</p>
<p>程序规约是指将 KBQA 问题规约为某个可执行的程序，也就是语义解析的思想。近些年来，一些知识库提供了监督的程序规约信号，在这些知识库上的程序规约问答取得了大幅性能提升。如何使用这些监督信号，提升低资源知识库上的性能呢？本文将其定义为程序迁移任务，该任务面临以下挑战：</p>
<ul>
<li><strong>域异构</strong>：由于语言和知识的多样性，源知识库和目标知识库上的知识、问题的表现形式可能相去甚远。</li>
<li><strong>未知的元素</strong>：源知识库的知识覆盖率往往相当有限，例如 KBQA Pro 数据集只覆盖了 3.9% 的关系和 0.24% 的维基百科概念。</li>
<li><strong>过大的搜索空间</strong>：每个知识库操作可选的参数很多，搜索知识库和操作和可选参数不现实。</li>
</ul>
<p>本文的贡献包括：</p>
<ul>
<li>首次提出复杂 KBQA 的程序迁移方法</li>
<li>为程序迁移提出一个两阶段解析框架和本体剪枝策略</li>
<li>通过扩展实验和消融实验证实了程序迁移的有效性</li>
</ul>
<h2 id="框架结构"><a href="#框架结构" class="headerlink" title="框架结构"></a>框架结构</h2><p>这一部分其实非常简单，对着下面这张图就能解释清楚，不需要任何数学公式。</p>
<p><img src="/blog/framework.png"></p>
<ol>
<li>BERT+GRU 将 query 映射为无参数操作序列（与知识库无关）</li>
<li>利用上一步每个操作的 hidden state 投影后在可选参数空间做 softmax，选择参数</li>
<li>参数剪枝：根据 query 维护可能的域、关系、实体集合，并随着参数序列的自回归过程迭代更新。</li>
</ol>
<h3 id="源域预训练"><a href="#源域预训练" class="headerlink" title="源域预训练"></a>源域预训练</h3><p>损失函数包含两部分：操作序列的交叉熵和参数位置的交叉熵，即两个分类任务损失之和。<br>$$<br>\mathcal {L}^{pretrain}=-\sum_{(x^S,y^S)\in D^S}(logp(y^S_s|x^S)+\sum_{t=1}^{|y_s|}logp(arg_t^S|x^S,o_t^S,\mathcal P))<br>$$</p>
<h3 id="目标域微调"><a href="#目标域微调" class="headerlink" title="目标域微调"></a>目标域微调</h3><p>由于目标域缺少源域的监督程序归纳信号，因此需要通过强化学习 / EM 算法来微调。主要步骤是先搜索得到若干个可能的程序，然后执行程序根据结果更新参数。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul>
<li>源域：KBQA Pro</li>
<li> 目标域：WebQuestionSP，ComplexWebQuestions (CWQ)</li>
<li> 知识库：Freebase</li>
</ul>
<h3 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h3><ul>
<li>语义解析：TEX-TRAY，QGG，TeacherNet</li>
<li> 信息检索：Graft-Net，PullNet</li>
<li> 消融实验基线：$Ours_{-f}$（未微调），$Ours_{-p}$（未预训练），$Ours_{-pa}$（未预训练参数解析器），$Ours_{-o}$（缺少本体剪枝）</li>
</ul>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><ul>
<li>F1</li>
<li>Hits@1：Hits@n 是指正例中处在前 n 个结果中的比例，Hits@1 也就是模型结果第一名为正确结果的比例</li>
</ul>
<p>由于数据集中的问题有多个答案，F1 更好地反映了答案的覆盖程度。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/blog/exper-result.png"></p>
<p>值得关注的是 $Ours_{-p}$ 极差的结果和 $Ours_{-f}$ 的一般表现，表明了预训练的重要性。并且移去预训练参数解析器和本体剪枝策略都会对模型效果有较大影响。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文还是蛮有意思的，用一个比较贴近人认知的框架，把迁移学习应用到 KBQA 中并证实了其有效性。</p>
<p>近期还会从各高校 ACL2022 录取宣传中找一些值得读的论文。等到 ACL 2022 公布 accepted list 了就不用这么折腾了。</p>
<p>碎碎念：我也好想发一篇论文啊。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>问答</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>问答</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>PLATO-XL</title>
    <url>/blog/2022/02/19/PLATO-XL/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>PLATO-XL 是百度于 2021 年发布的论文《PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation》中提出的模型，旨在探索大规模预训练对话生成任务的效果，在中英文的多项对话任务上取得了 SOTA。目前，百度提供了<code>百度PLATO</code> 微信公众号服务，可供试用。经笔者测试，PLATO 的效果要远优于微软小冰。因此今天来读下这篇论文。 </p>
<span id="more"></span>

<p>下图是微信公众号二维码，大家可以自行体验。</p>
<p><img src="/blog/QR.png"></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>近些年来，预训练模型的共识是更大的模型 + 更好的数据 = 更好的效果。但是，在对话预训练领域，DialoGPT 的 345M 版本要优于 762M，Blender 的 2.7B 版本要优于 9.4B 的版本，这些反常的现象让人怀疑模型的规模和生成效果是否有清晰的结论关系。本论文指出，对话质量可能还是受益于模型规模，前提是合适的预训练设计。</p>
<p>PLATO-XL 使用 unified  Transformer 的架构进行训以提高参数效率，并实行了” 多角色意识预训练 “用于区分人物信息。据论文所述，多角色意识预训练能够显著减少多轮对话中的不一致性。论文的英文模型和推理脚本开源在了 <a href="https://github.com/PaddlePaddle/Knover/tree/develop/projects/PLATO-XL">github</a> 上，没有开源训练脚本、中文模型等。这很百度。</p>
<p>实验结果表明，PLATO-XL 不仅在开放域闲聊任务取得了 SOTA，也可用于微调后应用于任务型对话、知识增强对话场景，同样取得了 SOTA 效果。</p>
<h2 id="PLATO-XL"><a href="#PLATO-XL" class="headerlink" title="PLATO-XL"></a>PLATO-XL</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>PLATO-XL 的架构如下图所示，使用 Unified Transformer 的 Seq2Seq 的训练方法。将输入和输出以 [SEP] 间隔，输入内部计算双向 self-attention，输入 - 输出间存在 cross-attention，输出间为单侧的 mask-attention。这种做法的优点是参数利用率高，同一套参数即用来编码又用来解码，得到的模型的泛用性也强。</p>
<p><img src="/blog/architecture.png"></p>
<h3 id="多角色意识预训练"><a href="#多角色意识预训练" class="headerlink" title="多角色意识预训练"></a>多角色意识预训练</h3><p>这个名字听着很玄乎，其实思想很简单。从社交媒体中搜集的数据一般如下图所示。多个用户的连续回帖构成了多轮对话。但是由于每个人的性格、观念等的不同，直接拿去训练多轮对话容易产生不一致性。因此 PLATO 引入了角色嵌入（role embedding）来解决这个问题，将角色嵌入和句子向量相加即可。如上面的图所示。</p>
<p><img src="/blog/multiparty.png"></p>
<h3 id="预训练设置"><a href="#预训练设置" class="headerlink" title="预训练设置"></a>预训练设置</h3><p>数据集：</p>
<ul>
<li>英文：来自 Reddit，由第三方收集公开于 pushshift.io 上，使用 PLATO-2 的精细清洗流程。训练集为 2005 年到 2019 年的 811M 个样本。词汇表包含 8k BPE token，使用 SentencePiece 构建。</li>
<li>中文：数据集来自社交媒体，清洗后包含 1.2B 训练样本，词表包含 30k BPE token。</li>
</ul>
<p>PLATO-XL 拥有 11B 参数，使用了 72 个 Transformer 和 32 个注意力头，嵌入维度为 3072，前馈层的隐藏状态为 18432。为了训练的稳定性，PLATO-XL 参考 GPT2 将 Layer Normalization 提前到块开始，并对残差层初始参数进行缩放 $*1/\sqrt N$，N 为残差层数量。</p>
<p>PLAOT-XL 基于飞桨实现，使用了 256 块 32G V100 进行训练。受限于显存，11B 模型无法容纳在单张卡中，标准的数据并行无法进行。因此将优化器状态、梯度、参数分别保存在不同设备，以减少通信并提高计算效率。为了提高 batch size，论文还使用了梯度检查点，即不保存一部分前向过程中的激活值，在反向传播时重新计算，用时间换空间。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h3><ul>
<li>DialoGPT：Reddit 评论， 345M 参数。</li>
<li>Blender：Reddit 评论预训练 + 人工注释对话数据（BST）微调，2.7B 参数。</li>
<li>PLATO-2：使用课程学习方法训练。英文：Reddit 评论 + BST 微调，1.6B 参数。中文：1.2B 社交媒体数据集，336M 参数。</li>
<li>CDial-GPT：LCCC 数据集，95.5M 参数。</li>
<li>ProphetNet-X：豆瓣数据集，379M 参数。</li>
<li>EVA：1.4B 数据，2.8B 参数。</li>
</ul>
<p>此外，论文还与公开的中文聊天机器人：微软小冰、图灵机器人、天猫精灵、小爱同学进行了对比。</p>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>在开放域对话中，自动化指标和人工平板的相关性很小，因此论文主要通过众包平台进行人工评估。</p>
<ul>
<li>连贯性（Coherence）：话语级指标，回复是否与上下文相关和一致。</li>
<li>信息性（Informativeness）：话语级指标，给定上下文的情况下，回复是否有信息。</li>
<li>参与度（Engagingness）：会话级指标，用户是否愿意与机器人持续聊天。</li>
</ul>
<p>上述三个指标取值为 [0,1,2]，分数越高越好。为了进一步分析对话质量，还有两个更细粒度的评估指标：</p>
<ul>
<li>不一致性（Inconsistency）：细粒度评估连贯性，回复是否与上下文冲突。</li>
<li>错觉（Hallucination ）：细粒度评估信息性，回复是否存在事实性错误。</li>
</ul>
<p>这两项指标取值为 [0,1]，越小越好。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>实验结果如下图所示。可以看到 PLATO-XL 在所有指标上都是最优的。但是仅看中文指标，PLATO-XL 和 PLATO-2 的差别却不大，数据规模均为 1.2B，二者可能使用的是同一套数据集。二者的主要差别在不一致性和错觉这两项指标上，其他三项指标差距均不足 0.1，但模型规模差了 30 倍。</p>
<p>另一个角度看，PLATO-2 是 2020.1 登录 arxiv，EVA 是 2021.10 登录 arxiv，相差近两年，EVA 的模型和数据集的规模都大于 PLATO-2，在论文中表现却全线弱于 PLATO-2。不过 EVA 论文中也未与 PLATO-2 直接比较。这就比较微妙了，要么是 PLATO 的 UniLM 框架要优于 EVA 的 Seq2Seq，要么就是 PLATO 的数据集质量更好。</p>
<p><img src="/blog/experiment-result.png"></p>
<p><img src="/blog/chat-result.png"></p>
<h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p>论文给出了一个对话样例。大家也可以自行去微信公众号体验。</p>
<p><img src="/blog/case-study.png"></p>
<p>下面这张图是网上的体验图，可以看到基本可以达到以假乱真的地步。我自己试用的过程中，也激发了 PLATO 的四川话属性。相较于微软小冰，对话质量和体验提升可以说是巨大的。</p>
<p><img src="/blog/demo.png"></p>
<h3 id="其他对话任务"><a href="#其他对话任务" class="headerlink" title="其他对话任务"></a>其他对话任务</h3><p>将 PLATO-XL 微调后用于任务型对话和知识增强对话，在三个数据集上达到了 SOTA 效果，证实了 PLATO-XL 作为对话 AI 的基础模型的潜力。</p>
<p><img src="/blog/other-task.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>PLATO-XL 的效果确实是非常惊艳，但是代码、数据、模型全部闭源，就 emmmm。模型出于商业角度不开源，数据出于知识产权角度不开源，这篇论文最大的影响就是证实了中文闲聊场景大模型的上限是很高的，但是怎么达到这个上限，我不告诉你。</p>
<p>英文模型反倒可以开源，毕竟百度也不做国外的生意，开源英文模型和推理脚本，论文应该才更容易接收。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://ai.baidu.com/support/news?action=detail&amp;id=2630&amp;hmpl=yunying=10.22">「比人还会聊天」百度 PLATO 对话机器人开放体验 - https://ai.baidu.com/</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话生成</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>对话生成</tag>
        <tag>Transformer</tag>
        <tag>闲聊</tag>
        <tag>PLATO</tag>
      </tags>
  </entry>
  <entry>
    <title>对话系统综述</title>
    <url>/blog/2022/02/08/DialogueSystem-Survey/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>今天来读一篇对话系统综述。《Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey》是由南洋理工大学于 2021 年发表的论文，目前收录在 arxiv 中。该文调研了对话系统中的历史研究工作，并从模型和系统两个方面进行分析，确定了可能的未来研究方向。此外，本文还涵盖了相关框架、数据集的介绍。</p>
<span id="more"></span>

<p>具体而言：</p>
<ul>
<li>模型类型层面，分析广泛应用于对话系统的不同模型的原理、特点和应用。</li>
<li>系统类型层面，讨论对比面向任务的对话系统和开放域对话系统。</li>
</ul>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>按照应用，对话系统通常可以分为两类：</p>
<p><strong>面向任务的对话系统（Task-oriented dialogue systems）</strong>，用于解决特定领域的特定问题，例如机票预订等。传统的面向任务的对话系统往往以流水线结构组织，包含四个基本模块：自然语言理解、对话状态跟踪、策略学习和自然语言生成。近期工作探索使用端到端的方法，以实现更好的效果。</p>
<p><strong>开放域对话系统（Open-domain dialogue systems）</strong>，旨在不受域的限制地与用户聊天，通常完全由数据驱动。开放域对话系统通常可以分为三类：</p>
<ul>
<li><strong>生成系统。</strong>使用 Seq2Seq 模型将用户输入和历史消息生成回复，可以灵活生成上下文相关的回复，但有时缺乏连贯性并且乏味（例如我不知道）。</li>
<li><strong>基于检索的系统。</strong>根据用户输入从语料库中检索回复，受有限语料库的影响，有时检索到的回复与对话上下文相关性较弱。</li>
<li><strong>集成系统。</strong>结合生成和检索的方法，从二者中选出最优的。</li>
</ul>
<p>此外，生成系统还可以用于改进检索得到的回复。</p>
<p>传统的对话系统大多基于有限状态的、基于统计学习和机器学习方法。基于有限状态的系统易于实现并且可以生成自然的回复，应用于早期的一些场景固定、对话流程确定的产品中。基于统计学习与机器学习的方法通常使用模板填充来处理任务，相较基于有限状态的系统，要更为灵活。但由于模板固定，应用场景和回复的多样性也受到限制。</p>
<p>深度学习的发展则提高了对话系统的性能，其广泛应用于各种 NLP 任务中，也是近些年的研究热点。</p>
<h2 id="神经模型"><a href="#神经模型" class="headerlink" title="神经模型"></a>神经模型</h2><p>本节讨论的模型包括：卷积神经网络 (CNNs)、循环神经网络 (RNNs)、Vanilla 序列到序列模型、分层循环编码器 - 解码器 (HRED)、记忆网络、注意力网络、Transformer、指针网络和 CopyNet、深度强化学习模型、生成对抗网络 (GAN)、知识图增强神经网络。</p>
<h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p><strong>卷积神经网络</strong>（<strong>Convolutional Neural Networks，CNN</strong>）包含卷积层，池化层和前向层，架构如下图所示。输入为长度为 7，特征维度为 5 的序列，使用 6 个卷积核得到 6 个特征图，池化后拼接得到 6 维向量，接一个全连接层后进行分类。</p>
<p><img src="/blog/CNN.png"></p>
<p>卷积核的操作如下。其中，m 和 n 代表结果矩阵行列的索引，f 是输入矩阵，h 是卷积核。滑动窗口使得卷积层能够捕获局部特征，池化层则用于扫描产生全局特征，赋予了 CNN 局部和全局感知能力。而参数共享机制可以降低模型的复杂度。<br>$$<br>G(m,n)=(f*h)(m,n)=\sum_j\sum_k h(j,k)f(m-j,n-k)<br>$$<br>由于这些特性，CNN 广泛应用于 CV 中，在 NLP 中也得到了一定应用。CNN 是很好的文本特征提取器，但它们可能不是理想的序列编码器。虽然一些对话系统直接使用 CNN 作为编码器编码语言和知识，但是大多数历史最佳对话系统选择在编码文本信息之后，使用 CNN 作为层次特征抽取器。这是由于 CNN 的固定输入长度（输入尺寸改变，全连接层参数也需要改变）和有限的卷积跨度（难以捕捉长距离依赖）。</p>
<p>通常，CNN 用于处理编码信息的对话系统主要有两种情况：</p>
<ul>
<li>应用 CNN 基于来自编码器的特征向量提取特征。从 character-level embeddings 中抽取特征，证明了 CNN 的层级抽取能力。</li>
<li>在响应检索任务中提取特征图。使用单独的编码器对对话上下文和候选响应进行编码，使用 CNN 作为从编码对话上下文和候选响应计算的相似度矩阵的提取器。实验表明，该方法可以在响应检索任务中取得良好的性能。</li>
</ul>
<p>近几年较新的工作不选择 CNN 作为对话编码器的主要原因是，它们无法连续、灵活地跨时间序列提取信息。</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>标准神经网络和 CNN 的两个限制在于：假设数据不同位置相互独立；输入为固定长度。对于存在依赖、长度可变的输入序列，网络性能受到限制。循环神经网络（RNN）应运而生。公式的介绍可以参考我的博客<a href="https://tqnwhz.github.io/blog/2021/07/22/rnns/#more">循环神经网络 RNN 及其变体 GRU、LSTM | 一隅 (tqnwhz.github.io)</a>，这里就不再赘述。</p>
<p>在对话系统中，RNN 多用于编码器和解码器。编码器用于编码对话上下文、对话状态、对话历史以及外部知识等信息。解码器通过 greedy search 或者 beam search 进行解码。</p>
<h3 id="HRED"><a href="#HRED" class="headerlink" title="HRED"></a>HRED</h3><p><strong>HRED（Hierarchical Recurrent Encoder-Decoder）</strong>是一个上下文感知的 Seq2Seq 模型，使用两个层次的 RNN 对 token-level 和 turn-level 的序列进行建模，架构如下图所示。token-level RNN 编码器序列得到序列表征（同时也是解码器），turn-level RNN 编码对话历史的序列表征，得到当前的上下文向量，并作为解码器的初始状态。</p>
<p><img src="/blog/HRED.png"></p>
<p>HRED 还有一些变体。例如 VHRED 在解码阶段引入隐变量，以建模更复杂的序列依赖关系。最近在对话相关任务中的许多工作都应用了基于 HRED 的框架来捕获分层对话特征。其中有一些改进工作，例如参考 Transformer 引入自注意力，引入新的层次捕获全局知识、主题信息等。</p>
<h3 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h3><p>记忆网络（Memory Network）是解决有关外部知识问题的重要网络。诸如 LSTM 类的模型，虽然有记忆功能，但记忆模块太小（只有一个固定维度的向量），无法精确地记录全部内容。因此 Weston et al. (2014) 提出了记忆网络，包含四个模块：</p>
<ul>
<li>记忆模块：存储记忆事实表示</li>
<li> I（Input）模块：将输入的记忆事实映射到嵌入式表示</li>
<li> G（Generalization）模块：决定记忆模块的更新，简单起见可以直接将新记忆插入，不进行遗忘和更新</li>
<li> O（Output）模块：根据输入和现有的记忆状态输出</li>
<li> R（Response）模块：根据 O 模块输出产生最终响应</li>
</ul>
<p>端到端的记忆网络架构如下，流程分为三个阶段：</p>
<ul>
<li><strong>权重计算：</strong>使用模型 A、B 分别将输入记忆和输入查询映射为向量，计算权重 $p_i=Softmax (u^Tm_i)$ 代表查询 $u$ 与记忆 $m_i$ 的关系。</li>
<li><strong>记忆选择：</strong>使用模型 C 将输入记忆编码为嵌入向量 $c_i$，并计算加权和 $o=\sum p_ic_i$。这实际上跟 Attention 类似，是一种 soft-alignment。</li>
<li><strong>最终预测：</strong>将记忆和查询的和映射为概率向量，即 $\hat \alpha=Softmax (W (o+u))$。</li>
</ul>
<p><img src="/blog/MemoryNetwork.png"></p>
<p>记忆网络广泛用于 QA、面向任务的对话系统中，记忆模块用于存储外部知识等信息。记忆网络的思想也在很多模型中得到了体现。</p>
<h3 id="注意力-amp-Transformer"><a href="#注意力-amp-Transformer" class="headerlink" title="注意力 &amp; Transformer"></a>注意力 &amp; Transformer</h3><p>为解决 Seq2Seq 将输入序列编码为固定维度向量带来的信息损失，Bahdanau et al. (2014) 提出注意力机制用于机器翻译任务。其思想是解码的每一步与整个输入序列关联，公式描述如下：<br>$$<br>P(y_i|y_{t&lt;i})=g(y_{i-1},s_i,c_i)\<br>s_i=f(s_{i-1},y_{i-1},c_i)\<br>c_i=\sum_{j=1}^{T_x}\alpha_{ij}h_j<br>$$<br>其中，h 和 s 分别为编码器和解码器的隐藏状态，i 代表时间步，$y$ 代表输出的 token。g 和 f 为计算输出和更新隐藏状态的函数，由 RNN 的种类决定。$c_i$ 为解码的第 i 步与整个输入序列的注意力结果，$\alpha_{ij}$ 为注意力分数，计算公式为：<br>$$<br>\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})} \<br>e_{ij}=a(s_{i-1},h_j)\<br>$$<br>a 是相关性度量函数，根据注意力的类型有不同的形式，$\alpha_{ij}$ 即是相关性上进行归一化（Softmax）。注意力的图示如下：</p>
<p><img src="/blog/Attention.png"></p>
<p>此后，注意力机制大行其道，基本成为 Seq2Seq 的标配。但是由于 RNN 的不可并行性、难以捕捉长距离依赖等缺点，研究者提出了 Transformer，详细的介绍可参考我的博客 <a href="https://tqnwhz.github.io/blog/2021/08/14/Transformer/">Transformer | 一隅 </a>。</p>
<p>… 待补充</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://blog.csdn.net/u014577702/article/details/117825782">Recent Advances in Deep Learning-based Dialogue Systems_北风吹过的秋 - CSDN 博客</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/31170263">论文笔记 - HRED 与 VHRED - 知乎 (zhihu.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32257642">论文笔记 - Memory Networks 系列 - 知乎 (zhihu.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话系统</category>
        <category>综述</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>对话系统</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>预训练模型综述</title>
    <url>/blog/2022/01/22/PTM-Study/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>《Pre-Trained Models: Past, Present and Future》是由来自清华、人大、复旦等高校的多位学者合作完成的预训练模型（Pre-Trained Models, PTM）综述。顾名思义，该文主要在讨论预训练模型的过去、现状和未来。</p>
<span id="more"></span>

<p>深度神经网络（CNN、RNN 等）通过自动化学习特征，解决了传统机器学习面临的复杂特征工程问题。但是由于特征学习过程缺少人为干预，只通过数据学习，严重依赖于有监督数据的质量和数量。而高昂的人工标注费用使得这一问题更为严峻。</p>
<p>迁移学习的思想缓解了这一问题。借鉴于人类可以利用已学到的知识解决新的问题，迁移学习旨在通过预训练 - 微调两阶段的方法，在预训练阶段大规模数据上学习后，只需较少的样本即可在下游任务上微调并取得较好效果。这降低了模型训练和试错的开销。广泛应用于计算机视觉的各项任务：图像分类、目标检测等。</p>
<p>这一想法也被应用于自然语言处理领域。在 GPT 之前，这一思想代表为 Word2Vec、Glove 为代表的静态词向量。通过在连续词袋等任务上的预训练，模型能够学习到有意义的词向量（平移不变性），进而可以应用于各类任务中。然而，受限于静态词向量的表征能力，面临一词多义问题时，这些词向量往往不能表现出很好的效果。虽然也有一些工作试图将每个词的不同含义在空间中区分开，但并没有根本性地解决这个问题。</p>
<p>在 Transformer 提出后的次年，基于该架构的 GPT 和 BERT 预训练模型问世。它们证实了，当预训练模型的规模变得更大，具有数亿个参数时，预训练模型可以捕获多义消歧、词汇和句法结构，在下游任务上表现出出色的性能，取得甚至比人类更优的结果。随着更大算力的投入，近些年来，预训练模型的规模呈几何翻倍式增长。GPT-3 具有数千亿参数，表现出了类似人类的少样本学习能力，如下图所示。</p>
<p><img src="/blog/gpt3-fsl.png"></p>
<p>但是，预训练模型的理论尚不成熟，大规模参数的本质难以理解，巨大的计算成本也让人望而却步。预训练模型已经将研究人员推在一个选择的十字路口中，本文就是在这样的背景下，总结预训练模型取得的成果，并讨论其发展和未来。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>迁移学习的概念可以追溯到 1998 年，迁移学习旨在从多个源任务中获取重要的<strong>知识</strong>，然后将这些知识应用于目标任务。源任务和目标任务的数据格式、任务目标可能不同，但解决任务所需的知识是一致的。因此，NLP 里出现很多不同的预训练方法，本质上是希望能从多个维度去学习无监督语料中的知识。当预训练任务和下游任务关系密切时，模型更有可能取得好效果。</p>
<p>迁移学习包含两种方法：特征迁移和参数迁移。特征迁移方法预训练有效的特征表示以预编码跨领域和任务的知识。通过将这些预训练的表示注入目标任务，可以显着提高目标任务的模型性能。典型的代表就是 NLP 中的静态词向量 Word2Vec。参数迁移方法遵循一个直观的假设，即源任务和目标任务可以共享模型参数或超参数的先验分布。因此，这些方法将知识预编码为共享模型参数。然后将知识通过精细转换使用目标任务的数据调整预训练参数。参数迁移常见于 CNN。ELMO 和 BERT 分别是特征迁移和参数迁移的两种代表。</p>
<p>在迁移学习思想指导下、高质量数据集 ImageNet 的驱动下（覆盖数千个类别的百万张图片）、正则化方法（残差连接）的加持下，在 CV 领域诞生了 ResNet 这样的预训练模型，可用于图像分类、目标检测、图像分割等多项任务。NLP 领域也进行了尝试，典型代表是 CoVE，在 LSTM 上预训练机器翻译任务后，其编码器可以用于下游任务。</p>
<h3 id="自监督学习"><a href="#自监督学习" class="headerlink" title="自监督学习"></a>自监督学习</h3><p>监督学习、无监督学习、子监督学习的关系如下图所示。</p>
<p><img src="/blog/methods.png"></p>
<p>自监督学习利用输入数据本身作为监督，从大规模未标记数据中提取知识，这与无监督学习类似。区别在于无监督学习主要侧重于检测数据模式，通过聚类、异常检测等方法，而自监督学习是通过无监督数据构造出了有监督的数据，使用有监督的方法进行训练。近些年来，NLP 里的预训练任务都是自监督学习。在 BERT 的启发下，研究者们设计出了各类的预训练任务。预训练模型成功的关键就是自监督学习和 Transformer。</p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>在 Transformer 之前，RNN 一直是处理序列任务的标准方法。RNN 顺序读取 token 并更新状态，这使得它可以处理任意长度的序列，但也限制了它的并行能力。Transformer 结构如下图所示。</p>
<p><img src="/blog/transformer-gpt-bert.png"></p>
<p>Transformer 是一种基于 Seq2Seq 的架构，由编码器和解码器组成。编码器和解码器都由几个相同的模块堆叠而成。每个模块都包含多头注意力机制、残差连接和层标准化。详细的介绍可以看我的这篇博客 <a href="https://tqnwhz.github.io/blog/2021/08/14/Transformer/">Transformer | 一隅</a> 。正则化方法使得训练更深的网络成为可能。Transformer 使用的 Attention 包含以下三种：</p>
<ul>
<li>自注意力。用于编码阶段的多头注意力机制，将输入序列中的每个单词和其他单词计算注意力分数，进而得到该单词的向量表征。这也是 Word2Vec 的思想。</li>
<li>掩码自注意力。用于解码阶段生成阶段。Transformer 解码器还是按照自回归的方式进行解码，因此计算注意力的时候需要注意不能泄露数据，只能计算每个单词左侧的注意力，右侧掩码。</li>
<li>交叉注意力。用于解码阶段。Query 为前一个解码器模块的输出，Key 和 Value 为编码网络的输出。类似 Seq2Seq 中的注意力。</li>
</ul>
<p>由于强大的特征抽取能力，Transformer 逐渐成为 NLP 中的标准。GPT、BERT 等 PTM 由此诞生。</p>
<h3 id="GPT-amp-BERT"><a href="#GPT-amp-BERT" class="headerlink" title="GPT&amp;BERT"></a>GPT&amp;BERT</h3><p>GPT 使用 Transformer 的解码器来训练语言模型。BERT 使用 Transformer 的编码器训练掩码语言模型，二者的区别如下图所示。GPT 只有掩码自注意力机制，使用自回归的方法建模语言模型，通过极大似然的方法训练，主要用于自然语言生成（NLG）任务。BERT 中只有自注意力机制，使用双向的注意力机制建模掩码语言模型和下句预测任务，主要用于自然语言理解任务（NLU）。掩码语言模型参考了 “完形填空” 的思路，即从句子中随机挡住一个符号（标记为 [MASK]），由模型来根据其他部分预测这个符号，即可认为是这个单词的表示。</p>
<p><img src="/blog/gpt-bert.png"></p>
<h3 id="GPT-amp-BERT之后"><a href="#GPT-amp-BERT之后" class="headerlink" title="GPT&amp;BERT之后"></a>GPT&amp;BERT 之后</h3><p>在 GPT 和 BERT 之后，研究者们提出了一些改进工作，例如 RoBERTa 和 ALBERT。</p>
<p>RoBERTa 是 BERT 的成功变体之一，主要有四个简单有效的变化：（1）去除 NSP 任务； (2) 更多的训练步骤，更大的 batch size 和更多的数据； (3) 更长的训练期限； (4) 动态改变 [MASK] 模式。这些改进使得 RoBERTa 在一些任务上取得优于 BERT 的效果。</p>
<p>ALBERT 是 BERT 的另一个重要变体，它致力于减少 BERT 的参数。首先，它将输入的词嵌入矩阵分解为两个较小的矩阵。其次，它强制所有 Transformer 层之间的参数共享以显着减少参数。第三，它提出了句子顺序预测（SOP）任务来替代 BERT 的 NSP 任务。作为对其空间效率的牺牲，ALBERT 的微调和推理速度较慢。如图 9 所示，除了 RoBERTa 和 ALBERT，近年来还提出了各种 PTM，以更好地从未标记的数据中捕获知识。一些工作改进了模型架构并探索了新的预训练任务，例如 XLNet、UniLM，MASS 等。此外，整合丰富的数据源也是一个重要的方向，例如利用多语言语料库、知识图谱和图像。还有一些工作致力于则增大模型规模，例如 GPT-3。</p>
<p>参考下面这张预训练模型全家福。</p>
<p><img src="/blog/ptm-family.png"></p>
<h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><p>通常，所有用于语言预训练的 BERT 之后的 Transformer 架构都可以根据两个动机进行分类：统一序列建模和认知启发式架构。</p>
<h3 id="统一序列建模"><a href="#统一序列建模" class="headerlink" title="统一序列建模"></a>统一序列建模</h3><p>NLP 具有挑战性的原因之一，就是因为它有很多下游任务，通常可以分为三类：</p>
<ul>
<li>自然语言理解（NLU）：单词 / 段落 / 句子分类、语法 / 句法分析、知识推理等</li>
<li>开放域自然语言生成：对话生成、文本生成、故事生成等</li>
<li>非开放域自然语言生成：机器翻译、摘要等</li>
</ul>
<p>这些任务虽然多样，但所需的能力却是通用的，无外乎语言理解和语言生成能力。正如费曼所说，我不能创造的，我也不理解（What I cannot create, I do not understand）。语言理解任务可以转化为生成任务。因此 GPT 此类生成模型也可用于理解任务，甚至一些研究表明，与 BERT 相比，GPT 在理解基准方面可以达到相似甚至更好的性能（Liu 等人，2021b）。因此研究者们开始寻求一种统一的方式，建模所有的任务。</p>
<h4 id="结合自回归和自编码"><a href="#结合自回归和自编码" class="headerlink" title="结合自回归和自编码"></a>结合自回归和自编码</h4><p>XLNet 首先将 GPT 式的单向生成和 BERT 式的双向理解统一起来，结合自回归和自编码的思想 ，提出了置换语言模型的预训练任务。XLNet 通过在预训练中排列 token 的顺序，然后应用自回归预测范式来解决这个问题，这赋予了 XLNet 理解和生成的能力。UniLM 提出联合训练不同的语言建模目标，包括单向、双向和 seq2seq 目标。这可以通过更改 Transformers 中的注意力掩码来实现。 UniLM 在生成式问答和抽象摘要方面表现出色。</p>
<p>最近，GLM (Du et al., 2021) 提出了一种更优雅的方法来结合自回归和自编码。给定一个可变长度的掩码跨度，而不是像 BERT 一样，一个 token 对应一个 [MASK] 标记。GLM 会自回归地生成 [MASK] 对应的 token。GLM 是第一个在包括自然语言理解、条件生成和无条件生成在内的所有类型任务上同时实现最佳性能的模型。</p>
<h4 id="应用广义的编码-解码器"><a href="#应用广义的编码-解码器" class="headerlink" title="应用广义的编码-解码器"></a>应用广义的编码 - 解码器</h4><p>在 GLM 之前，无论是编码器架构的模型（例如 BERT）还是解码器架构的模型（例如 GPT），都无法解决在句子的多个空白处添加任意个符号的问题。BERT 只能将 MASK 替换为一个单词，而 GPT 只能在句子末尾添加任意个单词。因此，很自然的想法是转向类似机器翻译 Transformer 的 encoder-decoder 架构。</p>
<p>这一类型的先驱是 MASS（Song et al., 2019），它将掩码预测策略引入到编码器 - 解码器结构中。然而，MASS 并没有涉及填充可变长度空白的问题。 T5 (Raffel et al., 2020) 通过仅用一个掩码标记掩码文本中可变长度的跨度来解决该问题，并要求解码器恢复整个掩码序列。BART (Lewis et al., 2020a) 引入了一个有趣的想法，即通过截断、删除、替换、改组和掩码等多种操作来破坏源序列，而不仅仅是掩码。然而，encoder-decoder 架构的预训练模型存在以下问题：</p>
<ol>
<li>参数更多、参数效率低下</li>
<li>在 NLU 方面表现不佳</li>
</ol>
<h3 id="认知启发式架构"><a href="#认知启发式架构" class="headerlink" title="认知启发式架构"></a>认知启发式架构</h3><p>虽然 Transformer 很强大，但与人类的认知系统还是有较大差距。注意力机制借鉴自人类的感知功能，然而人类还具有决策、逻辑推断、工作记忆等功能。</p>
<h4 id="可维护的工作记忆"><a href="#可维护的工作记忆" class="headerlink" title="可维护的工作记忆"></a>可维护的工作记忆</h4><p>Transformer 的一个问题在于固定的序列长度和 $O (n^2)$ 的复杂度，即每个 token 都要与其他 token 计算注意力。这严重阻碍了它在长文档理解中的生成和应用。然而，人类也不具备很好的长程注意力机制。认知科学家们发现人类可以保持一种工作记忆，这种记忆不仅可以组织和记忆，还可以忘却。传统的 LSTM 正是这种思想的实践。</p>
<p>基于 Transformer 的架构中，Transformer-XL 是第一个引入段级递归和相对位置编码来实现这一目标的。它将源序列分割为若干个段，上一个段的状态会被缓存下来，在当前片段计算注意力时使用（但不更新梯度），然后缓存新的状态，重复此过程，也就是段级递归名称的来源。这个过程隐含着工作记忆的思想。CogQA 提出在多跳阅读中保持认知图。它通过 PTM（BERT）和 GNN 两个系统建模认知图。先通过 BERT 提取相关实体，再通过 GNN 构造认知图谱，进行推理和计算。CogQA 的一个局限是 PTM 仍使用了固定的窗口大小。CogLTX 将长文本切分为若干个 block，利用记忆回想模块给 block 打分，选择相关性更高的 block 进行使用。</p>
<h4 id="可持续的长期记忆"><a href="#可持续的长期记忆" class="headerlink" title="可持续的长期记忆"></a>可持续的长期记忆</h4><p>GPT-3 的成功揭示了 Transformer 具有记忆功能。在此之前， Lample et al. (2019) 等人发现，将 Transformer 的前馈神经网络替换为大型键值记忆网络，仍可以工作的很好。这在某种程度上表明 Transformer 中的前馈神经网络等价于记忆网络。然而，其记忆内存容量非常有限。REALM (Guu et al.,2020) 探索了如何为 Transformer 构建外部存储的先驱。它通过将整个维基百科的文本向量化，使用掩码语言模型预训练知识检索器。在开放域问答上取得了 SOTA 效果。</p>
<p>除了张量文本语料库外，(Vergaet al., 2020; Févry et al., 2020) 提出将知识库中的实体和关系向量化，并将上下文中 token embedding 替换为对应的 entity embedding。(Dhingra et al., 2020; Sun et al., 2021) 从零开始维护一个虚拟知识，并提出了一个可微分的训练目标。所有这些方法在很多开放域问答基准上取得了一些改进。</p>
<h4 id="其他变种"><a href="#其他变种" class="headerlink" title="其他变种"></a>其他变种</h4><p>此外，还有一些工作致力于修改 BERT 的结构 / 预训练目标，达到更好的 NLU 能力。Span-BERT 证实了使用跨边界目标 (SBO) 掩盖连续随机长度的 token 可以提高 BERT 的性能。ELECTRA 将掩码语言模型替换为替换符号检测任务，生成器会替换原始序列中的 token，而判别器检测 token 是否被替换。</p>
<h2 id="使用多源数据"><a href="#使用多源数据" class="headerlink" title="使用多源数据"></a>使用多源数据</h2><p>一些预训练模型使用多源异构数据，例如多模态、多语言的 PTM、知识增强的 PTM。</p>
<h3 id="多语言预训练"><a href="#多语言预训练" class="headerlink" title="多语言预训练"></a>多语言预训练</h3><p>在单语言语料库（如英语）上预训练的模型在许多基准测试中取得了巨大成功。但是我们生活的世界是多语言的，为每种语言都训练和维护一个单独的模型并不是一个合理的方案，尤其是涉及到机器翻译的场景。事实上，虽然人们使用的语言不尽相同，但是他们可以表达相同的意思。这表明语义是独立于语言的。一些研究人员发现，使用多语言训练模型，效果要优于几种单语言模型。因此，相较于训练很多个单语言模型，训练多语言模型可能是个更好的方法。</p>
<p>在 BERT 之前，已经有研究者探索多语言表征，主要有两种方法。第一种是参数共享，例如使用多种语言对训练多语言 LSTM，实现多语言翻译。另一种是学习与语言无关的约束，例如使用 WGAN 框架将语言表示解耦为与语言无关的表示。这两种方式都能应用于多语言场景，但仅限特定的任务。换而言之，上述两种方法都是用同一个特定的任务训练的，不能推广到其他任务。</p>
<p>BERT 的出现表明，使用自监督的方法进行预训练，对特定任务进行微调的方法是可行的。这促使研究人员设计任务预训练通用的多语言模型。多语言任务同样可以分为 NLU 和 NLG 两类。</p>
<p>一些 NLU 任务首先在非并行多语言语料上训练多语言的 PTM。例如，Devlin 等人提出的多语言的 BERT（mBERT），使用维基百科上 104 种语言的非并行语料库建模多语言掩码语言模型（MMLM）任务。（吐槽，维基百科不同语言的内容还是有较大差别的）。研究表明，mBERT 具有在零样本场景中泛化跨语言知识的能力。这表明即使使用相同的 BERT 结构，使用多语言数据也可以使模型学习跨语言表示。XLM-R 构建了一个名为 CC-100 的非并行多语言数据集，规模远大于 mBERT 使用的维基百科语料，尤其是对于那些语料相对匮乏的语言。XLM-R 在 CC-100 上进行预训练，在多项基准测试中获得优于 mBERT 的性能，这表明更大规模的多语言语料库可以带来更好的性能。</p>
<p>然而，多语言掩码语言模型无法很好的利用并行语料。而并行语料对于一些 NLP 任务，例如机器翻译来说是至关重要的。并且直觉来说，并行语料能够让模型更快更好地学习到意义相同的跨语言表征。从这一点出发，XLM 使用双语句子执行翻译语言模型（TLM）的任务。具体做法是将双语语料拼接成一个句子，在两个部分中分别随机掩码。与 MLM 相比，TLM 需要模型从不同语料中获取和对齐语义信息，并进行预测。</p>
<p>除了 TLM，还有一些其他的方法从并行语料中学习跨语言表征。Unicoder 提供了两个基于平行语料的新预训练任务：跨语言单词恢复（CLWR）和跨语言释义分类（CLPC）。CLWR 使用目标语言 embedding 和注意力机制恢复源语言 embedding，类似机器翻译。CLPC 将对齐的语料拼接作为正样本，未对齐的作为负样本，进行句子级别的分类。ALM (Yang et al., 2020) 自动从并行句子生成代码转换序列并对其执行 MLM，这迫使模型仅基于其他语言的上下文进行预测。InfoXLM (Chi et al., 2020b) 从信息论的角度分析了 MMLM 和 TLM，鼓励模型在对比学习的框架下区分对齐的句子对和未对齐的负例。HICTL (Wei et al., 2021) 扩展了使用对比学习来学习句子级和单词级跨语言表示的想法。 ERNIE-M (Ouyang et al., 2020) 提出了反向翻译掩码语言建模（BTMLM），并通过反向翻译机制扩大了并行语料库的规模。反向翻译是一种数据增强的方法，例如将语言 X 翻译为语言 Y 的任务，可以将语言 Y 翻译回语言 X‘，然后比较 X 与 X’是否相同，若不同的话可以将 X’也加入数据集。 这些工作表明，利用平行语料库可以为学习跨语言表示带来很大帮助。</p>
<h3 id="多模态预训练"><a href="#多模态预训练" class="headerlink" title="多模态预训练"></a>多模态预训练</h3><p>人类所面临的世界是多模态的，包含视觉、听觉、语言等多种模态。模态指的是事情是如何发生和经历的。近年来，研究者们对多模态研究热情高涨，这些跨模态的工作大部分都归类于视觉和语言（V&amp;L）的交叉，例如视频和文本、图像和文本的交叉。V&amp;L 预训练的工作主要集中在改进模型架构、利用更多数据以及设计更好的预训练任务上。</p>
<p>对于基于图像文本的 PTM，目前大多数工作都是基于视觉语言 BERT 的架构。主要挑战在于统一语义空间中视觉和文本内容的对齐（即 V&amp;L 基础）。为此，主要有两种模型架构设计：双流和单流。双流模型，例如 ViLBERT，使用两个独立的流处理图像和文本，并将它们通过 Transformer 注意力模块融合。单流模型，例如 Visu-alBERT，图像区域特征和词嵌入通常被拼接送入单个 Transformer 中。考虑到简单性和效率，目前工作主要使用单流模型。</p>
<p>在预训练任务的选择上，V&amp;L 的理解任务广泛使用 MLM、句子 - 图像对齐（SIA）、遮挡区域分类（MRC），遮挡区域特征回归（MRFR）和直接合并下游任务。其中，MLM 旨在借助视觉和文本上下文恢复字幕中的掩码标记。 SIA 旨在判断图像 - 文本对是否匹配。 MRC 可以被认为是视觉 MLM，需要 V&amp;L 模型来预测被掩蔽对象的类别。MRFR 进一步需要 V&amp;L 模型来恢复被掩蔽对象区域的视觉特征。 也有模型在预训练阶段直接进行下游 V&amp;L 理解任务。</p>
<p>上述提到的预训练任务专用于 V&amp;L 理解或者字幕生成，不能用于图像生成任务。最近提出的 DALLE 是第一个基于 Transformer 的文本到图像的 PTM，可用于条件图像生成，它显示了多模态 PTM 在联系文本描述和图像生成之间的潜力，尤其是组合不同对象的出色能力。</p>
<p>除了图像 - 文本 PTM，还有其他形式的 PTM，例如视频和音频。 VideoBERT (Sun et al., 2019a) 对 Cooking312K 视频数据集 (Sun et al.,2019a) 进行预训练，并在零镜头动作分类任务和视频字幕任务上验证模型。SpeechBERT (Chuang et al., 2019）首先将连续音频信号编码成几个语音语义词嵌入，然后使用 MLMon 文本和音频模态作为预训练任务。 预训练后，使用口语问答（SQA）任务进行评估。</p>
<h3 id="知识增强预训练"><a href="#知识增强预训练" class="headerlink" title="知识增强预训练"></a>知识增强预训练</h3><p>PTM 可以从大量数据中获取统计信息，而外部知识是统计建模的优秀先验。外部知识可以分为结构化知识（如知识图谱）和非结构化知识（维基百科文本）。一些工作试图通过整合实体和关系嵌入来增强 PTM，或者是它们与文本的对齐方式。Wang 等人（2021b）基于维基数据实体的描述预训练模型，通过将语言模型损失和知识嵌入损失结合在一起以获得知识增强表示。</p>
<h2 id="提高计算效率"><a href="#提高计算效率" class="headerlink" title="提高计算效率"></a>提高计算效率</h2><p>PTM 的趋势是模型越来越大，因此提升计算效率以满足日益增加的内存与计算需求非常关键，可以分为以下三种方法。</p>
<h3 id="系统级优化"><a href="#系统级优化" class="headerlink" title="系统级优化"></a>系统级优化</h3><p>通常与具体模型无关，可以分为单设备优化和多设备优化。</p>
<p><strong>单设备优化</strong>，一个典型的例子是浮点数精度优化。现代深度学习系统主要基于单精度浮点数（FP32），然而权重往往落在一个有限的区间里， 可以考虑使用半精度格式（FP16）完成大部分计算，而几乎没有精度损失。但是在某些情况下，也可能会出现浮点截断和溢出，为解决这个问题，研究者们提出了混合精度训练，它在 FP32 中保留一些临界权重以避免浮点溢出，并使用动态损失缩放操作来摆脱浮点截断。充分的实验表明，混合精度训练方法比 FP16 中直接训练模型更稳定。尽管混合精度训练方法可以显着减少训练时间和内存使用量，但它们仍然面临一些挑战。当模型参数没有很好地初始化时，混合精度方法仍然可能导致训练不稳定。这些挑战仍有待进一步探索。此外，还可以通过舍弃 Transformer 中的部分隐藏状态、利用 CPU 存储模型参数再通过精细的策略完成 CPU 和 GPU 内存交换，来降低模型内存开销。</p>
<p><strong>多设备优化</strong>。预训练模型往往使用分布式的方法进行训练，使用多个节点中的多个 GPU 来加速计算，并行方法可以分为数据并行、模型并行。</p>
<p><strong>数据并行</strong>是一种简单有效的加速模型的方法，如下图所示。使用数据并行时，大 Batch 数据被划分到不同的节点，可以并行化前向传播。 在反向传播时，不同节点上的梯度应该通过 all-reduce 操作进行聚合，以保证参数优化的一致性，这可能会引入额外的通信开销。容易看出，这相当于每个 GPU 上都保存了一份模型参数。</p>
<p>当单个模型的参数达到十亿或更多时，模型参数无法容纳在同一个 GPU 上（即使是半精度或者混合精度训练），这使得数据并行无法进行。<strong>模型并行</strong>则可以解决这个问题，通过将矩阵运算分块，分布在不同的 GPU 上，再通过节点间的通信操作保证前向 / 反向传播的正确性。但是，模型并行需要在前向 / 反向传播过程中插入通信操作，无法与计算重叠。对比之下数据并行的 all-reduce 操作通常可以被反向计算重叠。因此，数据并行是首选，只要它可以克服内存容量的过度需求。</p>
<p>下面这张图和模型并行和数据并行的示例。</p>
<p><img src="/blog/data-parallel.png"></p>
<p>模型并行还存在另一种流水线并行的方法。将模型划分为很多层，不同层分布在不同的节点，前一层的输出作为后一层的输入。流水线并行只需要在执行管道相邻阶段的节点之间传递中间激活状态，通信成本较小。但是，流水线并行以一个 batch 的前向和反向传播为完整周期，会有流水线气泡产生。</p>
<h3 id="高效预训练"><a href="#高效预训练" class="headerlink" title="高效预训练"></a>高效预训练</h3><p>除了一些系统级的优化方法外，研究人员还致力于探索更有效的预训练方法，以便能够以较低成本的解决方案对大规模 PTM 进行预训练。</p>
<p><strong>高效的训练方法。</strong>传统的预训练任务可能样本效率低下。以 MLM 为例，需要模型根据上下文来预测掩码标记。掩码标记通常是输入标记的子集（通常为 15%），即模型只能从一小组输入标记中学习。为了解决这个问题，ELECTRA (Clarket al., 2020) 提出了替换令牌检测任务。此任务强制模型区分输入标记是否被生成器替换。此任务可以利用来自每个样本的更多监督信息，因为需要区分所有输入标记。实验证明，ELECTRA 仅需少得多的预训练步骤，就可以达到与 MLM 相似的性能。另外，传统 MLM 随机掩盖文档中的标记以进行预测。由于预测不同标记的难度差异很大，随机掩码策略使训练过程变得漫无目的且效率低下。一些工作根据 token 的重要性或者梯度，加速模型训练。</p>
<p>除了预训练任务外，当前的预训练动态也是次优的。最近的大规模 PTM 都要求大的 batch size，因为研究指出这有利于模型收敛。但在一项早期工作中（Goyal 等人，2017 年），研究人员发现简单地增加 batch size 可能会导致<strong>优化困难</strong>。因此，他们提出了一种预热策略（即 warm up），在训练开始时线性增加学习率。这种策略通常用于最近的大规模 PTM。此外，研究者发现在 Transformer 不同层间自适应地使用不同的学习率也可以在 batch size 较大时加快收敛速度。</p>
<p><strong>高效的模型架构。</strong>除了高效的预训练方法，更多的模型架构变体也可以降低计算复杂度，提高训练效率。正如之前提到的，基于 Transformer 的 PTM 面临长输入序列时会存在 $O (n^2)$ 序列长度复杂度的问题。一些工作致力于降低 Transformer 的复杂度。</p>
<h3 id="模型压缩"><a href="#模型压缩" class="headerlink" title="模型压缩"></a>模型压缩</h3><p>另一个提高 PTM 效率的重要方法是模型压缩。通过将大型模型压缩为小型模型，以满足资源受限设备上更快推理和部署的需求。</p>
<p><strong>参数共享。</strong>PTM 可以通过在相似单元之间共享参数进行压缩。AL-BERT (Lan et al., 2019) 使用分解嵌入参数和跨层参数共享来减少 PTM 的参数，在所有 Transformer 层上使用相同的权重。ALBERT 在 BERT 模型的基础上实现了显着的参数减少，同时具有相同甚至更好的性能。这表明 PTM 可能极度过度参数化。</p>
<p><strong>模型剪枝。</strong>为了更好地利用当前 PTM 的过度参数化特性，另一种减少模型参数的方法是模型剪枝，它在 PTM 中剪掉一些无用的部分，以在保持性能的同时实现加速。研究人员研究了 Transformers 中注意力头的冗余，发现只有一小部分就足以获得良好的性能。这些头中的大部分都可以移除，而对准确性的影响很小。</p>
<p><strong>知识蒸馏。</strong>虽然 ALBERT 减少了 PTM 的大小，但并没有减少推理时间，因为模型计算复杂度并没有减小。知识蒸馏旨在训练一个小模型以复现大模型的行为。有一些典型的工作将知识蒸馏用于 PTM，例如 DistillBERT (Sanhet al., 2019)、TinyBERT (Jiao et al., 2019)、BERT-PKD (Sun et al., 2019b) 和 MiniLM (Wang et al., .,2020d)。 但是，知识蒸馏方法需要用于预训练教师模型的数据，考虑到数据版权和隐私，这些数据通常不会发布。而且，教师模型需要对整个预训练数据进行转发，以产生对数或中间表示进行知识蒸馏，导致训练时间更长。</p>
<p><strong>模型量化。</strong>模型量化是指将高精度浮点参数压缩为低精度浮点参数。 模型量化这个词听上去不是很好理解，更像是一种参数压缩方法。 传统的 PTM 通常用 32 位或 16 位浮点数表示参数，而量化后的模型可以用 8 位甚至 1 或 2 位表示。一种量化方法可以使用 k-means 对参数进行聚类，让相近的值落在同一个聚类中心，进而复用同一个值。对于最近的基于 Transformer 的模型，8 位量化已在 Q8BERT 中被证明是有效的，对模型性能的影响很小。为了减轻性能下降，也可以采用其他保持精度的方法。 Q-BERT (Shen et al.,2020a) 使用混合比特量化，其中 Hessian 谱较高的参数需要更高的精度，而 Hessian 谱较低的参数需要较低的精度。</p>
<h2 id="可解释性-amp-理论"><a href="#可解释性-amp-理论" class="headerlink" title="可解释性&amp;理论"></a>可解释性 &amp; 理论</h2><p>鉴于 PTM 在多项任务上取得的卓越性能，研究者试图解释 PTM 的行为，包括其如何工作和捕获到了怎样的模式。这些工作涵盖了 PTM 的几个重要属性：知识、鲁棒性和结构稀疏性 / 模块化。 此外，在构建 PTM 的理论分析方面也有一些开创性的工作。</p>
<h3 id="知识"><a href="#知识" class="headerlink" title="知识"></a>知识</h3><p>PTM 捕获的隐性知识大致可以分为两大类：语言知识和世界知识。</p>
<p><strong>语言知识</strong>包含了句子的语法、语义、词义等信息。与传统的神经模型如 CNN 和 RNN 相比，大规模 PTM 可以从海量的预训练数据中学习到丰富的语言知识。为了研究 PTM 的语言知识，研究人员设计了几种方法：表征分类（利用隐藏状态对句子 / 单词进行分类）、表征分析（利用隐藏状态计算统计信息，例如相似度、距离）、注意力分析（利用注意力矩阵，发现文本的层次结构）、生成分析（语言模型计算概率）。</p>
<p><strong>世界知识</strong>主要包括常识知识和事实知识。Davison 等人提出将关系三元组转化为掩码句子，根据 PTM 给出的互信息对句子进行排序，证明在其表示空间中学习了各种常识特征。Petroni 等人（2019）提出将关系知识生成表述为填空语句的完成。根据实验结果，他们发现在没有任何微调的情况下，PTM 在这项任务上明显优于以前的监督基线，证实 PTM 学习到了事实知识。但是，这些填空语句的构造并非易事。</p>
<h3 id="鲁棒性"><a href="#鲁棒性" class="headerlink" title="鲁棒性"></a>鲁棒性</h3><p>近期工作通过使用对抗性样本证明 PTM 存在严重的鲁棒性问题。对抗性攻击旨在通过对原始输入的小扰动来生成被模型错误分类的新样本。 例如，PTM 很容易被同义词替换所愚弄。事实上，这个问题在 word2vec 时代就存在了。由于同义词、反义词所在上下文相似，它们的表征也近似。同时，不相关的格式词也会误导 PTM 做出错误的预测。但是高质量对抗样本的获取也面临挑战，目前的工作主要利用模型的预测概率和模型梯度来搜索对抗性样本。最近，人在回路（Human-in-the-loop）方法（Wallace 等人，2019b；Nie 等人，2020）已被应用于生成更自然、有效和多样化的对抗样本，这带来了更大的挑战和经验。总而言之，当人们为实际应用部署 PTM 时，PTM 的鲁棒性已成为严重的安全威胁。</p>
<h3 id="结构稀疏性"><a href="#结构稀疏性" class="headerlink" title="结构稀疏性"></a>结构稀疏性</h3><p>正如前文提到的，Transformer 具有过度参数化的问题。研究人员表明，多头注意力结构在机器翻译 (Michel et al., 2019)、抽象摘要 (Baan et al., 2019) 和语言理解 (Kovaleva et al., 2019) 的任务中是多余的，即当去除部分注意力头，可以获得更好的性能。这种现象与 (Clark et al., 2019) 中的观察结果一致，他们发现同一层中的大多数头部具有相似的自我注意模式。他们的研究结果表明，不同头部的注意力行为可以归类为一组有限的模式。除了多头注意力之外，其他几项工作也在探索识别参数的稀疏性。 Gordon 等人 (2020) 表明，低水平的剪枝 (30-40%) 根本不会影响预训练损失或下游任务的性能。</p>
<h3 id="PTM理论"><a href="#PTM理论" class="headerlink" title="PTM理论"></a>PTM 理论</h3><p>由于预训练在深度学习方面取得了巨大成功，研究人员试图研究预训练的工作原理，尤其是无监督预训练。在深度学习的早期，人们发现通过贪婪的逐层无监督预训练和监督微调来训练深度贝叶斯网络是有效的（Hin-ton et al., 2006）。最近，基于包括语言建模在内的对比学习的预训练已经成为主流方法。</p>
<p>Erhan et al. (2010) 提出了两个假设来解释预训练的效果：（1）更好的优化和（2）更好的正则化。在更好的优化方面，与随机初始化的模型相比，预训练的网络更接近全局最小值。在更好的正则化方面，PTM 的训练误差不一定比随机模型好，而 PTM 的测试误差更好，这意味着更好的泛化能力。</p>
<p>对于预训练目标的最新发展。Saunshi, et al（2019）对对比无监督表示学习进行了理论分析。对比学习将出现在相同上下文中的文本 / 图像对视为语义相似对，将随机采样的对视为语义不相似对。然后，相似对之间的距离应该很近，不同点之间的距离应该很远。在语言建模的预测过程中，上下文和目标词是相似对，其他词是负样本（Kong et al., 2020）。Saunshi 等人（2019）首先提供了一个新的概念框架来弥合预训练和微调之间的差距。具体来说，他们引入了潜在类的概念，语义相似的对来自同一个潜在类。例如，潜在类可以是 “快乐” 以包括所有文本，包括快乐的情绪。潜在类涵盖所有可能的类，下游任务定义的类来自潜在类集合。然后，他们证明了对比学习的损失是下游损失的上限。因此，在优化预训练损失时，我们可以预期下游任务的损失会更低。</p>
<h2 id="未来发展方向"><a href="#未来发展方向" class="headerlink" title="未来发展方向"></a>未来发展方向</h2><h3 id="架构和预训练方法"><a href="#架构和预训练方法" class="headerlink" title="架构和预训练方法"></a>架构和预训练方法</h3><p>值得探索的问题有：</p>
<ul>
<li><strong>新架构。</strong>Transformer 饱受诟病的计算复杂度，需要更有效的模型捕获更长范围的依赖信息。另外，也需要根据下游任务设计特定架构，例如 NLU 使用 Transformer Encoder，NLG 使用 Transformer Decoder。</li>
<li><strong>新的预训练任务。</strong>如何设计有效、高效的自监督任务，类似 ELECTRA。</li>
<li>不止微调。微调是将 PTM 的知识转移到下游任务的主要方法，但一个缺点是其参数效率低下：每个下游任务都有自己的微调参数。NLU 最近盛行的 Prompt 就是对微调的改进。</li>
<li><strong>可靠性。</strong>提高 PTM 的鲁棒性，免受对抗攻击。</li>
</ul>
<h3 id="多语言、多模态预训练"><a href="#多语言、多模态预训练" class="headerlink" title="多语言、多模态预训练"></a>多语言、多模态预训练</h3><ul>
<li>更多的模态。除了图像和文本，还可以利用视频和音频进行多模态预训练。主要挑战在于如何对这两种模态中涉及的时间上下文进行建模。</li>
<li><strong>更深刻的解释。</strong>将视觉和语言联系起来的原因仍然没有定论，只是一些经验性的感觉，没有脑科学或者深度学习理论的支撑。另外，多模态训练是否会对单模态造成损失，如何克服？这些都是悬而未决的问题。</li>
<li><strong>更多的下游任务。</strong>虽然多模态预训练可以应用于图文检索、图文生成、图文生成等下游任务。 然而，为多模态预训练找到一个 “真正的” 真实世界应用场景仍然具有挑战性。</li>
<li><strong>迁移学习。</strong>多语言模型应当灵活适配新的语言。另外，目前的多模态多语言模型无法处理音频数据，不同语言的音频需要转换为文本再翻译。</li>
</ul>
<h3 id="计算复杂度"><a href="#计算复杂度" class="headerlink" title="计算复杂度"></a>计算复杂度</h3><p>大规模深度学习模型的新需求给现有的深度学习框架带来了严峻的挑战。为了开发更有效的框架，可以探索以下方向：</p>
<ul>
<li><strong>数据转移。</strong>设计精细的、定义良好的数据调度和计算策略，最小化通信成本、最大化计算和内存资源以及优化计算 - 通信重叠。</li>
<li>并行策略。从数据并行、模型并行、流水线并行以及各种混合并行方法可以根据神经网络的结构和硬件配置找到它们的最佳使用方式。在当前实践中，用户必须全面考虑给定深度学习模型的网络结构和设备间通信带宽，以决定最合适的并行策略或在不同策略之间切换（Shazeer 等，2018）。</li>
<li><strong>大规模预训练。</strong>鉴于现有深度学习框架对模型并行和流水线并行的支持不佳，一些新兴的开源项目开发了用于大规模训练的专用框架。由于应用案例优先以及存在的兼容性问题，这些方法无法共同构成完整的解决方案。</li>
<li><strong>包装器和插件</strong>。由框架提供插件或者包装器自动管理通信操作，避免用户手动编程通信的复杂过程。</li>
</ul>
<h3 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h3><p>目前的 PTM 理论存在以下问题：</p>
<ul>
<li><strong>不确定性。</strong>PTM（以及其他深度神经网络）的一个未得到解决的问题是它们通常对预测过于自信，即这些模型不知道他们不知道什么。你问 GPT-3 “我的脚有几只眼睛”，GPT-3 肯定会给出 “你的脚有两只眼睛” 这样的答案，这看起来违反直觉。在机器学习中处理这种分布外（OOD）数据通常是一项具有挑战性的任务。为了应对上述挑战，一个有希望的方向是采用贝叶斯方法，探索概率工具来捕获数据和模型的不确定性（也分别称为任意不确定性和认知不确定性）。当然，提高贝叶斯深度学习的计算效率是解决上述挑战的关键因素。</li>
<li><strong>泛化和鲁棒性。</strong>PTM 的另一个重要问题是泛化。从理论上理解预训练在提高下游任务泛化方面的作用很重要。有没有有效的方法来探索 PTM 作为额外的数据资源来提高下游任务的鲁棒性？此外，如前所述，PTM 本身的鲁棒性是一个未解决的问题。</li>
</ul>
<h3 id="模型边缘学习"><a href="#模型边缘学习" class="headerlink" title="模型边缘学习"></a>模型边缘学习</h3><p>模型边缘是指存储在模型中的知识。给定三元组 &lt;h,r,t&gt;，我们很容易知道头部实体 h 和尾部实体具有关系 r，但是 PTM 中的表征的意义却不明晰。越来越多的研究人员探索了 PTM 从数据中学到了哪些知识，以及为什么它们在下游任务中表现如此出色？这些模型边缘如何存储和管理？是否有可能建立一个通用连续知识库（UCKB）来存储来自各种 PTM 的模型边缘？这些都是有希望的研究方向。</p>
<h3 id="认知和知识学习"><a href="#认知和知识学习" class="headerlink" title="认知和知识学习"></a>认知和知识学习</h3><p>让 PTM 更有知识是 PTM 未来的一个重要主题。可以将知识型 PTM 的未来发展分为以下三种方法：</p>
<ul>
<li><strong>知识增强。</strong>对于输入文本和外部知识，关键的问题是弥合文本表示和知识表示（包括符号或向量）之间的差距，并统一使用它们的信息作为输入。这个问题的解决需要统一的模型架构和知识引导的预训练目标。</li>
<li><strong>知识支持。</strong>根据输入的先验知识，设计不同的模块处理不同类型的输入，类似人脑不同区域对应不同的活动功能，加快训练和推理进程。</li>
<li><strong>知识监督。</strong>通过从知识库和大规模语料库中学习，与仅使用纯文本相比，PTM 可以具有更好的语言理解和生成能力。通过改进认知架构、明确推理、知识交互这三个方向，未来的 PTM 有望于能够轻松理解文字之外的含义。</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>在具体应用中，PTM 还存在一些问题：</p>
<ul>
<li><strong>对话系统。</strong>虽然基于 Transformer 的开放域对话系统显示出优秀的与人对话的能力，但是在对话领域，缺少特定的预训练任务。</li>
<li><strong>特定领域的 PTM。</strong>当大规模的特定领域语料库可以廉价获得时，可以在这些数据上训练特定领域的 PTM。这种领域专业知识通常被认为对于解决许多特定领域的问题很重要。</li>
<li><strong>领域适应和任务适应。</strong>大规模 PTM 的简单微调对于特定领域的应用是不够的（Gururangan 等人，2020；Ke 等人，2020）。最根本的原因是分布变化：特定域中的数据分布可能与一般预训练文本中的<strong>数据分布</strong>有很大不同。如何弥合预训练和特定任务微调之间的差距变得至关重要。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇综述回顾了预训练模型的发展历史、分析了其核心问题并指明了一些改进的方向。这篇论文中的一些工作我也没有接触过，像多模态、Transformer-XL 等。因此读起来也有一些一知半解。建议有余力的读者去看原文。毕竟综述类文章本身就是知识的压缩，很难在博客中再进行压缩了。这也是我为什么这篇博客完全按照原论文格式排版。。。</p>
<p>最后，新年快乐！</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.jiqizhixin.com/articles/2021-09-07-3">国内数十位 NLP 大佬合作，综述预训练模型的过去、现在与未来 | 机器之心 (jiqizhixin.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/84159401">Transformer-XL 介绍 - 知乎 (zhihu.com)</a></li>
<li>[模型量化了解一下？ - 知乎 (zhihu.com)](<a href="https://zhuanlan.zhihu.com/p/132561405#:~:text=%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E5%9C%A8%E6%9C%80%E5%88%9D%E7%9A%84%E5%AE%9A%E4%B9%89%E9%87%8C%E6%98%AF%E4%B8%BA%E4%BA%86%E5%8E%8B%E7%BC%A9%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%EF%BC%8C%E6%AF%94%E5%A6%82%E9%9F%A9%E6%9D%BE%E5%9C%A8ICLR2016%E4%B8%8A%E8%8E%B7%E5%BE%97best">https://zhuanlan.zhihu.com/p/132561405#:~:text = 模型量化在最初的定义里是为了压缩模型参数，比如韩松在 ICLR2016 上获得 best</a> paper 的论文，首次提出了参数量化方法。., 其使用 k-mean 聚类，让相近的数值聚类到同一个聚类中心，复用同一个数值，从而达到用更少的数值表示更多的数，这是量化操作的一种方案。. 反过来，从量化数变到原始数的过程，称之为反量化，反量化操作完之后，模型就可以按照原来的方式进行正常的计算。. 我们认为绝大部分的模型量化算法都能压缩参数，因此压缩参数的实用性不存在问题。.) </li>
<li><a href="https://www.jiqizhixin.com/articles/2016-10-18-7">人类智慧与机器学习结合──微软的「Human-in-the-Loop」 | 机器之心 (jiqizhixin.com)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>综述</category>
      </categories>
      <tags>
        <tag>BART</tag>
        <tag>预训练模型</tag>
        <tag>BERT</tag>
        <tag>自然语言处理</tag>
        <tag>综述</tag>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>基于知识库的问答综述（KBQA）</title>
    <url>/blog/2021/10/25/KBQA-survey/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>今天来读一篇 2021 年的知识库问答的综述，《A Survey on Complex Knowledge Base Question Answering:Methods, Challenges and Solutions》，论文收录在 IJCAI 2021 中。这篇文章主要介绍了知识库问答的背景与挑战，并总结介绍了两大主流方法：基于语义解析（semantic parsing-based，SP-based）和基于信息检索（information retrivel-based，IR-based）。</p>
<span id="more"></span>

<h2 id="知识库问答"><a href="#知识库问答" class="headerlink" title="知识库问答"></a>知识库问答</h2><p>知识库是一个结构化的数据库，它与知识图谱类似，由（主体 Subject，关系 Relation，客体 Object）三元组组成，例如（JK 罗琳，出生地，英国），常见的知识库有 Freebase 等。这个三元组可以用来回答 “JK 罗琳出生在哪里？” 的问题。与简单的、答案与主体直接相连的简单 QA 不同，复杂 QA 查询任务涉及多跳推理甚至一些聚合关系。例如下图知识库，“谁是 The Jeff Probst Show 提名的 TV Producer 的第一任妻子” 问题的答案，包含多个实体和多跳处理逻辑。<br><img src="/blog/multi-hop.png"></p>
<p>KBQA 的第一步是识别问题中的主体并链接到知识库中的实体，然后根据实体的邻域推导问题答案。这里分为两种方法基于语义解析和基于信息检索的两种方法。语义解析的思想是将自然语言问题表示为可以在知识库中进行查询的符号化的逻辑形式，然后再用逻辑语言进行查询（例如 SQL）。基于信息检索的方法思想是构建一个问题特定的知识图包含了相关的所有信息，然后将所有实体按相关性进行排序。然而，这些方法会面临以下挑战：</p>
<ul>
<li>基于语义解析的方法很难覆盖复杂的查询（多跳推理、约束关系、数值计算等）。类似的，基于信息检索的方法也很难回答复杂的问题，检索的实体范围可能太小，而且解释性差。</li>
<li>复杂的实体和关系会使得搜索空间过大（逻辑形式、候选结果等），搜索开销过大。</li>
<li>两种方法将问题理解看作重要的步骤，当问题的语法和语义复杂时，模型需要有很强的自然语言理解和生成能力。</li>
<li>弱监督问题。问答数据集中往往只存在问题和答案，缺少推理路径，而标注这样的推理路径成本过于高昂。弱监督问题给两种方法都带来了困难。</li>
</ul>
<p>评估指标上，KBQA 往往是从答案集合上选出置信度最高的，常见的评估指标由 F1、准确率、召回率、Hits@1 等。</p>
<h2 id="主流方法"><a href="#主流方法" class="headerlink" title="主流方法"></a>主流方法</h2><p>流程图如下图所示。<br><img src="/blog/methods.png"></p>
<h3 id="基于语义解析的方法"><a href="#基于语义解析的方法" class="headerlink" title="基于语义解析的方法"></a>基于语义解析的方法</h3><p>旨在将自然语言问题解析为逻辑形式，按照以下步骤：</p>
<ol>
<li>问题编码</li>
<li>逻辑解析</li>
<li>逻辑验证</li>
<li>逻辑执行</li>
</ol>
<p>优点：解释性强</p>
<p>缺点：严重依赖逻辑形式和解析算法的设计，成为性能提升的瓶颈</p>
<h3 id="基于信息检索的方法"><a href="#基于信息检索的方法" class="headerlink" title="基于信息检索的方法"></a>基于信息检索的方法</h3><p>旨在根据问题检索候选答案集合并对其进行排序，按照以下步骤：</p>
<ol>
<li>确定中心实体，提取问题特定的部分知识子图</li>
<li>问题编码</li>
<li>图推理，沿着相邻实体关系进行语义匹配，传播和聚合信息</li>
<li>按照置信度进行排序</li>
</ol>
<p>优点：端到端训练</p>
<p>缺点：解释性差</p>
<h2 id="挑战与解决方案"><a href="#挑战与解决方案" class="headerlink" title="挑战与解决方案"></a>挑战与解决方案</h2><p>论文总结了两种方法面临的挑战和解决方案，汇总成下面的表格。</p>
<p><img src="/blog/summary.png"></p>
<h2 id="总结和展望"><a href="#总结和展望" class="headerlink" title="总结和展望"></a>总结和展望</h2><p>论文主要介绍了两种典型的知识库问答方法，总结了它们面临的挑战及解决方案，并指出复杂 KBQA 未来的可能研究方向：</p>
<ul>
<li>在线学习。已有的复杂 KBQA 系统都是离线学习、在线部署。然而用户的反馈可能是改进 KBQA 的方法。基于此，一些工作开始利用用户反馈去纠正 KBQA 的回答。还有一些工作直接让用户参与到了问题解析过程中。</li>
<li>鲁棒性和可解释性。已有的方法虽然可以在基准数据集上可以取得很好的结果，但是遇到分布之外的情况可能无法很好处理。有研究人员开始提出相关的数据集来解决该问题。</li>
<li>更通用的知识库。知识库往往是不完整的，一些工作开始引入文本、图像来进行更复杂的 KBQA 推理。关于知识库更普遍的定义和更灵活的使用方法能让 KBQA 发展的更好。</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>问答检索</category>
      </categories>
      <tags>
        <tag>问答</tag>
        <tag>知识库</tag>
        <tag>KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>变分自编码器 VAE</title>
    <url>/blog/2021/10/07/VAE/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>今天来回顾一下变分自编码器（Variational Autoencoder，VAE），这是 2013 年提出的一种生成模型，时至今日，它的各类变体还活跃在各类会议上。之前我读过它的离散变体 VQ-VAE，这里再回顾一下原本的 VAE。</p>
<span id="more"></span>

<h2 id="数学知识"><a href="#数学知识" class="headerlink" title="数学知识"></a>数学知识</h2><p>理解 VAE 需要一些信息论和概率论的知识，这里总结一下。</p>
<h3 id="概率统计"><a href="#概率统计" class="headerlink" title="概率统计"></a>概率统计</h3><h4 id="数值计算-vs-采样计算"><a href="#数值计算-vs-采样计算" class="headerlink" title="数值计算 vs 采样计算"></a>数值计算 vs 采样计算</h4><p>对于一个随机变量 X，如果我们想知道 X 的期望 $E (X)$。如果我们已知 X 的分布函数，很容易可以计算出准确的期望 $E (X)=\sum p (x) x$（连续型变量替换为积分即可），这当然是最好的。然而很多情况下，我们无法得知准确的分布函数，那么我们可以采用统计量进行估计，对于 n 个随机样本 $x_1,x_2,\dots,x_n$，$\overline X=\frac {1}{n}\sum x_i$ 就是期望 $E (X)$ 的无偏估计。</p>
<h3 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h3><h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p>在信息论中，信息熵衡量了信息的不确定性，公式为 $H (X)=-\sum_{x\in X} p (x) logp (x)$。以单个事件 x 为例，概率越小的事件的信息熵越大。当一个事件必定会发生时（$p (x)=1$），其信息熵为 0，没有任何不确定性。对随机变量 X 而说，其信息熵就是 $-logp (x)$ 的期望，熵越大代表随机变量越不确定，很自然可以想到，分布越均匀，变量的状态越不容易确定，其熵越大。</p>
<p>在通信领域，信息熵可以看作对随机变量 X 进行编码所需的最短期望位数，这也被称为编码定理。在通信编码问题中，将随机变量 X 的每个值编码为一个二进制序列，使得序列长度期望最短。同时为了避免混乱，一个序列不能是其他序列的延申。这时编码位数的最短期望位数就是信息熵，有兴趣的同学可以去看看证明。</p>
<h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h4><p>对于随机变量 $X$ 的真实分布 $p (x)$，有时是未知的，我们只有它的近似分布 $q (x)$，如果按照 $q (x)$ 对变量 X 进行编码，得到的编码长度的期望称为交叉熵，记为 $H (p,q)=-\sum_x p (x) logq (x)$。容易知道交叉熵是大于等于信息熵的，因为信息熵是最短编码长度。</p>
<h4 id="相对熵（KL散度）"><a href="#相对熵（KL散度）" class="headerlink" title="相对熵（KL散度）"></a>相对熵（KL 散度）</h4><p>对于真实分布 $p$ 和近似分布 $q$，相对熵为使用近似分布编码得到的编码长度与最短编码长度的差，即交叉熵与信息熵的差，定义为 $D (p||q)=H (p,q)-H (p)$。KL 散度衡量了两个分布之间的差异，两个分布差异越大，KL 散度越大。不过 KL 散度并不是距离，因为它不是对称的。因此，KL 散度可以用于分类任务中计算真实概率分布与预测的概率分布之间的差异。事实上，<strong>由于这时信息熵为常数，往往将其略去使用交叉熵作为损失函数</strong>。</p>
<h2 id="变分自编码器"><a href="#变分自编码器" class="headerlink" title="变分自编码器"></a>变分自编码器</h2><p>对于数据集 $D={x_1,x_2,\dots,x_n}$（假设是很多张猫的图片，每个样本 $x$ 是像素矩阵）。所有可能的猫图构成数据总体 $X$，即 $x_i\in D\subseteq X$。$X$ 上存在一个概率分布 $p (x)$，当我们随机采样一张猫的图片时，这张图片有 $p (x)$ 的概率被采样到。对于非猫图 $y,\ p (y)=0$。我们希望的是能够找到 $p (x)=p (x^{(1)},x^{(2)},\dots)$ 的准确数学形式，其中 $x^{(1)}$ 代表 x 一维展开后的第 1 个像素，依次类推。如果这个目标能够实现，我们就能分析出 $p (x)$ 这个概率函数，对哪些输入 $x$ 能够取到非 0 概率值（即猫图），进而能够随机采样猫图和非猫图。</p>
<p>如果上述描述还不好理解的话，可以想象一个二维坐标系，以原点为圆心的单位圆上的均匀分布。所有猫图都满足 $x^2+y^2\le1$，在这个单位圆内外分别随机采样，即可得到猫图和非猫图。</p>
<p>但是事实上，这样的目标是很难实现的。在数理统计里，这是个典型的非参数估计问题，在未知分布形式的情况下，没有办法对分布形式和分布参数进行估计。而且很容易想到，这也绝对是一个非常复杂的概率分布。因此直接对 $p (x)$ 建模是不现实的。我们可以曲线救国。假设存在一个隐变量 $z$ 控制着数据 $x$ 的生成。那么根据 $p (x)=\int p (z) p (x|z) dz$ 可以计算得到 $p (x)$。然而这个边界似然是不可解的，每一项都不知道具体的数学形式，更不要说还要积分。</p>
<p>那么求其次，我们可以用一个分布 $q (x,z)$ 近似联合概率分布 $p (x,z)$，那么我们的优化目标就是 $KL (p||q)$ 最小。<br>$$<br>\begin{align}<br>KL(p||q)&amp;=\int\int p(x,z)log\frac{p(x,z)}{q(x,z)}dzdx \<br>&amp;=\int p(x)\int p(z|x)log\frac{p(x,z)}{q(x,z)}dzdx \<br>&amp;=E_{x\sim p(x)}\int p(z|x)log\frac{p(x)p(z|x)}{q(x,z)}dz \<br>&amp;=E_{x\sim p(x)}\int p(z|x)(log\frac{p(z|x)}{q(x,z)}+logp(x))dz<br>\end{align}<br>$$<br>而<br>$$<br>\begin{align}<br>E_{x\sim p(x)}\int q(z|x)logp(x)dz&amp;=E_{x\sim p(x)}logp(x)\int q(z|x)dz\<br>&amp;=E_{x\sim p(x)}logp(x)<br>\end{align}<br>$$<br>为一个常数，可以略去。令<br>$$<br>\begin{align}<br>\mathcal{L}&amp;=E_{x\sim p(x)}\int p(z|x)log\frac{p(z|x)}{q(x,z)}dz\<br>&amp;=E_{x\sim p(x)}\int p(z|x)log\frac{p(z|x)}{q(z)q(x|z)}dz\<br>&amp;=E_{x\sim p(x)}[\int -p(z|x)logq(x|z)  dz+ \int p(z|x)log\frac{p(z|x)}{q(z)}]dz\<br>&amp;=E_{x\sim p(x)}[E_{z\sim p(z|x)}(-log(q(x|z)))+KL(p(z|x)||q(z))]<br>\end{align}<br>$$<br>最小化 KL 与最小化 $\mathcal {L}$ 等价。进而得到了 VAE 的损失函数。只不过与原论文中的符号有些出入，将最后的 KL 项的 p 与 q 调换，即得到了论文中 VAE 的损失函数：<br>$$<br>\mathcal{L}=E_{x\sim p(x)}[E_{z\sim p(z|x)}(-log(q(x|z)))+KL(q(z|x)||p(z))]<br>$$<br>符号的差别是由于论文直接引入的 $q (z|x)$，而这里引入的是联合概率分布 $q (x,z)$。</p>
<p>注意上面的 $\mathcal L$ 是损失函数，而不是<strong>变分下界 ELBO</strong>，VAE 的 ELBO 是损失函数的相反数，引用 VAE 原文中的公式：<br>$$<br>\mathcal{L}(\theta,\phi;x^{(i)})=-D_{KL}(q_\phi(z|x^{(i)})||p(z))+\mathbb E_{q_\phi(z|x^{(i)})}[-log(p_\theta(x^{(i)}|z))]<br>$$</p>
<p><strong>ELBO 是要最大化的，而损失函数是要最小化的</strong>。上面的 ELBO 是单个样本的公式，没有对所有样本计算期望，所以形式上有所差异。</p>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>值得注意的是，VAE 的两个损失函数项并不是割裂的。换而言之，VAE 并不是独立地优化每项损失。这很容易理解，如果 VAE 独立优化第二项损失至最小，$p (z)=q (z|x)$，那么 $q (z|x)$ 将不具备任何 $x$ 的信息，这显然会使得第一项重构损失很大。同理，如果第一项重构损失很小，就意味着 $q (z|x)$ 包含了过多 $x$ 的信息，与 $p (z)$ 的差异（即 KL 项）就会很大。因此，VAE 是在两项损失的相互作用下，取得一个最优解。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://kexue.fm/archives/5253">变分自编码器（一）：原来是这么一回事 - 科学空间</a></li>
<li><a href="https://kexue.fm/archives/5343">变分自编码器（二）：从贝叶斯观点出发 - 科学空间</a></li>
<li><a href="https://kexue.fm/archives/5383">变分自编码器（三）：这样做为什么能成？ - 科学空间</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>生成模型</category>
      </categories>
      <tags>
        <tag>VAE</tag>
        <tag>生成模型</tag>
      </tags>
  </entry>
  <entry>
    <title>对抗自编码器 AAE</title>
    <url>/blog/2021/10/05/aae/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>对抗自编码器（Adversarial Autoencoders，AAE）出自 2015 年的《Adversarial Autoencoders》，其核心想法是将 VAE 与 GAN 结合，来得到一个更好的生成模型。论文首次提出了聚合后验分布（aggregated posterior），并使用它来优化 VAE。事实上这篇论文是在 CV 上生成的，这里只是简单分析下其提出的聚合后验分布及与 VAE 的比较。</p>
<span id="more"></span>

<h2 id="对抗自编码器"><a href="#对抗自编码器" class="headerlink" title="对抗自编码器"></a>对抗自编码器</h2><p>符号定义：</p>
<ul>
<li>$p (z)$：隐变量 $z$ 的先验分布</li>
<li> $q (z|x)$：编码器分布</li>
<li> $p (x|z)$：解码器分布</li>
<li> $p_d (x)$：数据 $x$ 分布</li>
<li> $p (x)$：模型的分布</li>
</ul>
<p>定义聚合分布 $q (z)$ 为 $q (z)=\int_xq (z|x) p_d (x) dx$，AAE 添加了 $q (z)$ 与 $p (z)$ 进行匹配的正则化项，模型结构如下：</p>
<p><img src="/blog/architecture.png"></p>
<p>可以看到与 VAE 最大的区别是，VAE 是从后验分布 $q (z|x)$ 中采样、使用 KL 散度逼近后验分布与先验分布，AAE 是使用对抗网络来逼近 $q (z)$ 与 $p (z)$。在此基础上，引入了一个对抗网络判断隐藏状态来自随机先验分布 $p (z)$ 的采样，还是来自 $q (z)$。在编码器 $q (z|x)$ 的选择上，论文提供了三种候选：</p>
<ul>
<li>确定性函数，$z$ 仅与 $x$ 相关。</li>
<li>高斯分布，与分布参数和随机性相关。</li>
<li>通用近似后验，$z$ 与 $x$ 和随机噪声 $\eta$ 相关。通用近似是一种近似分布的方法，确定性函数 $f (x,\eta)$，$\eta$ 是一个随机噪声，因而其可以看作分布的通用近似。</li>
</ul>
<p>论文中最终使用了确定性函数，个人猜测是更好收敛一些。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>生成模型</category>
      </categories>
      <tags>
        <tag>VAE</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP 必备网站 Hugging Face</title>
    <url>/blog/2021/09/27/huggingface/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>今天来分享一个网站吧，<a href="https://huggingface.co/">Hugging Face </a>，最大的 NLP 社区，提供了数以千计的预训练模型，涵盖一百余种语言、覆盖几乎所有常见的 NLP 任务。其提供的 transformers 框架提供了简洁高效的 api，可以在无需处理模型细节的前提下，快速进行推理、微调。Hugging Face 至今在 github 上已有超过 5 万个 star，可见其影响力。</p>
<span id="more"></span>

<h2 id="为什么需要Hugging-Face"><a href="#为什么需要Hugging-Face" class="headerlink" title="为什么需要Hugging Face"></a>为什么需要 Hugging Face</h2><p><img src="/blog/intro.png"></p>
<p>Hugging Face 不仅仅是若干数据集、预训练模型的资源整合，在此基础上，它还拥有如下特性：</p>
<ul>
<li>开箱即用：对于常见的 NLP 任务，很容易找到对应的预训练模型并进行实验，无需过度关注模型的细节。</li>
<li>多后端支持：Transformers 支持 Pytorch、Jax、Tensorflow 三种框架，无需再为框架微调苦恼。</li>
<li>可定制性：高效封装的同时，Transformers 支持魔改定制模型，模型文件可以单独使用，方便快速实验。</li>
</ul>
<p>鉴于现在 NLP 方向的研究、工程基本都是大规模预训练模型相关，Hugging Face 的重要性就一目了然了。如果你是学生党，Hugging Face 能让你在各类 NLP 比赛中快速使用预训练模型进行实验。如果你已经工作，Hugging Face 也能帮你减少业务问题上的试错成本，快速把任务跑起来。</p>
<h2 id="有用的链接"><a href="#有用的链接" class="headerlink" title="有用的链接"></a>有用的链接</h2><ol>
<li><a href="https://github.com/huggingface/transformers">github 链接</a>，可以对其使用方法、支持的模型有个快速的认识。</li>
<li><a href="https://huggingface.co/">Hugging Face 官网</a>，试试推理 api、看一看文档。</li>
<li><a href="https://huggingface.co/course/chapter0?fw=pt">Hugging Face Course</a>，Hugging Face 出品的官方课程，目前更新了前四章，基本上是 step-by-step 的教你从推理到微调任务如何构建和完成。</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>没错，这么短的一篇博客还有总结。今天刚刚看完 Hugging Face 的前四章课程，感觉学到了很多。早点知道也不会走一些弯路了，一起加油吧！</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>Hugging Face</tag>
        <tag>工程</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer-code</title>
    <url>/blog/2021/09/05/Transformer-code/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>今天来读一下《<a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>》的代码，也就是 Transformer。pytorch 代码<a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">地址</a>)。</p>
<span id="more"></span>

<h2 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h2><p>Transformer 文件夹下有以下文件：</p>
<ul>
<li>Constants.py：模型常量定义</li>
<li> Layers.py：编码器和解码器层定义</li>
<li> Models.py：模型定义</li>
<li> Modules.py：工具模块</li>
<li> Optim.py：优化模块</li>
<li> SubLayer.py：多头注意力机制等子层</li>
<li> Translator.py：翻译 beam search</li>
</ul>
<p>顶层目录下有以下文件：</p>
<ul>
<li>train.py：训练入口，实例化模型、优化器等，进行优化迭代</li>
<li> learn_bpe.py：bpe 学习词表</li>
<li> apply_bpe.py：使用 bpe 得到的词表将文本进行编码</li>
<li> translate.py：加载模型进行翻译</li>
</ul>
<p>下面逐一进行分析模型相关文件，剩下的文件等到下次再读吧。</p>
<h2 id="模型解析"><a href="#模型解析" class="headerlink" title="模型解析"></a>模型解析</h2><p>下面按照依赖顺序对各文件进行解析。</p>
<h3 id="Constants-py"><a href="#Constants-py" class="headerlink" title="Constants.py"></a>Constants.py</h3><p>文件内容非常简单，分别为填充、未知、起始、终止四种 token 的定义。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">PAD_WORD = <span class="string">'&lt;blank&gt;'</span> <span class="comment"># 填充token</span></span><br><span class="line">UNK_WORD = <span class="string">'&lt;unk&gt;'</span> <span class="comment"># 未知token</span></span><br><span class="line">BOS_WORD = <span class="string">'&lt;s&gt;'</span> <span class="comment"># 起始token</span></span><br><span class="line">EOS_WORD = <span class="string">'&lt;/s&gt;'</span> <span class="comment"># 结束token</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="Modules-py"><a href="#Modules-py" class="headerlink" title="Modules.py"></a>Modules.py</h3><p>定义了标量化点乘注意力机制类。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledDotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Scaled Dot-Product Attention '''</span></span><br><span class="line">	</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, temperature, attn_dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.temperature = temperature <span class="comment"># softmax的温度系数，论文中为\sqrt d_k</span></span><br><span class="line">        self.dropout = nn.Dropout(attn_dropout) <span class="comment"># 原论文dropout比例即为0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">		<span class="string">'''</span></span><br><span class="line"><span class="string">		q,k:  bsz x n_head x lq  x d_k</span></span><br><span class="line"><span class="string">		v: bsz x n_head x lq x d_v</span></span><br><span class="line"><span class="string">		'''</span></span><br><span class="line">        attn = torch.matmul(q / self.temperature, k.transpose(<span class="number">2</span>, <span class="number">3</span>)) </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn = attn.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>) <span class="comment"># 填充位置的注意力掩码，避免信息泄露等问题</span></span><br><span class="line"></span><br><span class="line">        attn = self.dropout(F.softmax(attn, dim=-<span class="number">1</span>)) <span class="comment"># 注意力的dropout</span></span><br><span class="line">        output = torch.matmul(attn, v) </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br></pre></td></tr></tbody></table></figure>

<h3 id="SubLayers-py"><a href="#SubLayers-py" class="headerlink" title="SubLayers.py"></a>SubLayers.py</h3><p>多头注意力机制定义：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Multi-Head Attention module '''</span></span><br><span class="line">	<span class="string">'''</span></span><br><span class="line"><span class="string">	n_head：注意力头数</span></span><br><span class="line"><span class="string">	d_model：词嵌入向量维度</span></span><br><span class="line"><span class="string">	d_k：query,key向量维度</span></span><br><span class="line"><span class="string">	d_v：value向量维度，d_v*n_head即为注意力输出的维度</span></span><br><span class="line"><span class="string">	'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_head, d_model, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_k = d_k</span><br><span class="line">        self.d_v = d_v</span><br><span class="line">		<span class="comment"># query key value 矩阵</span></span><br><span class="line">        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=<span class="literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(n_head * d_v, d_model, bias=<span class="literal">False</span>) <span class="comment">#全连接层</span></span><br><span class="line"></span><br><span class="line">        self.attention = ScaledDotProductAttention(temperature=d_k ** <span class="number">0.5</span>) <span class="comment">#注意力计算</span></span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>) <span class="comment"># 层标准化</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head</span><br><span class="line">        sz_b, len_q, len_k, len_v = q.size(<span class="number">0</span>), q.size(<span class="number">1</span>), k.size(<span class="number">1</span>), v.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        residual = q</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pass through the pre-attention projection: b x lq x (n*dv)</span></span><br><span class="line">        <span class="comment"># Separate different heads: b x lq x n x dv</span></span><br><span class="line">        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)</span><br><span class="line">        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)</span><br><span class="line">        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transpose for attention dot product: b x n x lq x dv</span></span><br><span class="line">        q, k, v = q.transpose(<span class="number">1</span>, <span class="number">2</span>), k.transpose(<span class="number">1</span>, <span class="number">2</span>), v.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)   <span class="comment"># For head axis broadcasting.</span></span><br><span class="line">	    <span class="comment"># q为注意力的输出</span></span><br><span class="line">        q, attn = self.attention(q, k, v, mask=mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transpose to move the head dimension back: b x lq x n x dv</span></span><br><span class="line">        <span class="comment"># Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)</span></span><br><span class="line">        q = q.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(sz_b, len_q, -<span class="number">1</span>)</span><br><span class="line">        q = self.dropout(self.fc(q))</span><br><span class="line">        q += residual</span><br><span class="line"></span><br><span class="line">        q = self.layer_norm(q)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> q, attn</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>前馈神经网络子层、残差层定义：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A two-feed-forward-layer module '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_in, d_hid, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_in, d_hid) <span class="comment"># position-wise</span></span><br><span class="line">        self.w_2 = nn.Linear(d_hid, d_in) <span class="comment"># position-wise</span></span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_in, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line"></span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        x = self.w_2(F.relu(self.w_1(x)))</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x += residual <span class="comment"># 残差计算</span></span><br><span class="line"></span><br><span class="line">        x = self.layer_norm(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>



<h3 id="Layers-py"><a href="#Layers-py" class="headerlink" title="Layers.py"></a>Layers.py</h3><p>编码器层类（非常简单，将多头注意力机制与前馈神经网络拼接起来即可）：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">''' Define the Layers '''</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformer.SubLayers <span class="keyword">import</span> MultiHeadAttention, PositionwiseFeedForward</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Compose with two layers '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_input, slf_attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        enc_output, enc_slf_attn = self.slf_attn(</span><br><span class="line">            enc_input, enc_input, enc_input, mask=slf_attn_mask)</span><br><span class="line">        enc_output = self.pos_ffn(enc_output)</span><br><span class="line">        <span class="keyword">return</span> enc_output, enc_slf_attn</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>解码器层类：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' Compose with three layers '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</span><br><span class="line">        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout) <span class="comment"># 解码器对编码器输出的注意力</span></span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, dec_input, enc_output,</span></span></span><br><span class="line"><span class="params"><span class="function">            slf_attn_mask=<span class="literal">None</span>, dec_enc_attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        dec_output, dec_slf_attn = self.slf_attn(</span><br><span class="line">            dec_input, dec_input, dec_input, mask=slf_attn_mask) <span class="comment"># 解码器的自注意力</span></span><br><span class="line">        dec_output, dec_enc_attn = self.enc_attn(</span><br><span class="line">            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask) <span class="comment"># 解码器对编码器输出的注意力</span></span><br><span class="line">        dec_output = self.pos_ffn(dec_output) <span class="comment"># 前馈神经网络</span></span><br><span class="line">        <span class="keyword">return</span> dec_output, dec_slf_attn, dec_enc_attn</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<h3 id="Models-py"><a href="#Models-py" class="headerlink" title="Models.py"></a>Models.py</h3><p>Models.py 是模型定义核心文件。</p>
<p>工具函数：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pad_mask</span>(<span class="params">seq, pad_idx</span>):</span> <span class="comment"># 获取序列的MASK（填充位置为0）</span></span><br><span class="line">    <span class="keyword">return</span> (seq != pad_idx).unsqueeze(-<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_subsequent_mask</span>(<span class="params">seq</span>):</span> <span class="comment"># 获取序列的所有MASK（长度为1,2,...n）</span></span><br><span class="line">    <span class="string">''' For masking out the subsequent info. '''</span></span><br><span class="line">    sz_b, len_s = seq.size()</span><br><span class="line">    subsequent_mask = (<span class="number">1</span> - torch.triu(</span><br><span class="line">        torch.ones((<span class="number">1</span>, len_s, len_s), device=seq.device), diagonal=<span class="number">1</span>)).<span class="built_in">bool</span>()</span><br><span class="line">    <span class="keyword">return</span> subsequent_mask</span><br></pre></td></tr></tbody></table></figure>

<p>位置编码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_hid, n_position=<span class="number">200</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Not a parameter</span></span><br><span class="line">        <span class="comment"># 成员变量无法保存在模型参数中且无法通过.cuda()转移到gpu上，register_buffer注册后则可以</span></span><br><span class="line">        self.register_buffer(<span class="string">'pos_table'</span>, self._get_sinusoid_encoding_table(n_position, d_hid))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_sinusoid_encoding_table</span>(<span class="params">self, n_position, d_hid</span>):</span></span><br><span class="line">        <span class="string">''' Sinusoid position encoding table '''</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> make it with torch instead of numpy</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_position_angle_vec</span>(<span class="params">position</span>):</span></span><br><span class="line">            <span class="keyword">return</span> [position / np.power(<span class="number">10000</span>, <span class="number">2</span> * (hid_j // <span class="number">2</span>) / d_hid) <span class="keyword">for</span> hid_j <span class="keyword">in</span> <span class="built_in">range</span>(d_hid)]</span><br><span class="line"></span><br><span class="line">        sinusoid_table = np.array([get_position_angle_vec(pos_i) <span class="keyword">for</span> pos_i <span class="keyword">in</span> <span class="built_in">range</span>(n_position)])</span><br><span class="line">        sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i</span></span><br><span class="line">        sinusoid_table[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(sinusoid_table[:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.FloatTensor(sinusoid_table).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x + self.pos_table[:, :x.size(<span class="number">1</span>)].clone().detach()</span><br></pre></td></tr></tbody></table></figure>

<p>Transformer 编码器：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A encoder model with self attention mechanism. '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v,</span></span></span><br><span class="line"><span class="params"><span class="function">            d_model, d_inner, pad_idx, dropout=<span class="number">0.1</span>, n_position=<span class="number">200</span>, scale_emb=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx) <span class="comment"># 词嵌入表</span></span><br><span class="line">        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.layer_stack = nn.ModuleList([</span><br><span class="line">            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout) <span class="comment"># 编码网络</span></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.scale_emb = scale_emb</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src_seq, src_mask, return_attns=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        enc_slf_attn_list = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -- Forward</span></span><br><span class="line">        enc_output = self.src_word_emb(src_seq)</span><br><span class="line">        <span class="keyword">if</span> self.scale_emb:</span><br><span class="line">            enc_output *= self.d_model ** <span class="number">0.5</span></span><br><span class="line">        enc_output = self.dropout(self.position_enc(enc_output))</span><br><span class="line">        enc_output = self.layer_norm(enc_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> enc_layer <span class="keyword">in</span> self.layer_stack: <span class="comment"># 编码网络</span></span><br><span class="line">            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)</span><br><span class="line">            enc_slf_attn_list += [enc_slf_attn] <span class="keyword">if</span> return_attns <span class="keyword">else</span> []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> return_attns:</span><br><span class="line">            <span class="keyword">return</span> enc_output, enc_slf_attn_list</span><br><span class="line">        <span class="keyword">return</span> enc_output,</span><br></pre></td></tr></tbody></table></figure>

<p>Transformer 解码器：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A decoder model with self attention mechanism. '''</span></span><br><span class="line">	<span class="string">'''</span></span><br><span class="line"><span class="string">		与编码器类似</span></span><br><span class="line"><span class="string">	'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,</span></span></span><br><span class="line"><span class="params"><span class="function">            d_model, d_inner, pad_idx, n_position=<span class="number">200</span>, dropout=<span class="number">0.1</span>, scale_emb=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)</span><br><span class="line">        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.layer_stack = nn.ModuleList([</span><br><span class="line">            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.scale_emb = scale_emb</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, trg_seq, trg_mask, enc_output, src_mask, return_attns=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        dec_slf_attn_list, dec_enc_attn_list = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -- Forward</span></span><br><span class="line">        dec_output = self.trg_word_emb(trg_seq)</span><br><span class="line">        <span class="keyword">if</span> self.scale_emb:</span><br><span class="line">            dec_output *= self.d_model ** <span class="number">0.5</span> <span class="comment"># Transformer论文中的trick，对词嵌入向量进行放大</span></span><br><span class="line">        dec_output = self.dropout(self.position_enc(dec_output))</span><br><span class="line">        dec_output = self.layer_norm(dec_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> dec_layer <span class="keyword">in</span> self.layer_stack:</span><br><span class="line">            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(</span><br><span class="line">                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)</span><br><span class="line">            dec_slf_attn_list += [dec_slf_attn] <span class="keyword">if</span> return_attns <span class="keyword">else</span> []</span><br><span class="line">            dec_enc_attn_list += [dec_enc_attn] <span class="keyword">if</span> return_attns <span class="keyword">else</span> []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> return_attns:</span><br><span class="line">            <span class="keyword">return</span> dec_output, dec_slf_attn_list, dec_enc_attn_list</span><br><span class="line">        <span class="keyword">return</span> dec_output,</span><br></pre></td></tr></tbody></table></figure>

<p>Transformer：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">''' A sequence to sequence model with attention mechanism. '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,</span></span></span><br><span class="line"><span class="params"><span class="function">            d_word_vec=<span class="number">512</span>, d_model=<span class="number">512</span>, d_inner=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            n_layers=<span class="number">6</span>, n_head=<span class="number">8</span>, d_k=<span class="number">64</span>, d_v=<span class="number">64</span>, dropout=<span class="number">0.1</span>, n_position=<span class="number">200</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            trg_emb_prj_weight_sharing=<span class="literal">True</span>, emb_src_trg_weight_sharing=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            scale_emb_or_prj=<span class="string">'prj'</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx</span><br><span class="line"></span><br><span class="line">        <span class="comment"># In section 3.4 of paper "Attention Is All You Need", there is such detail:</span></span><br><span class="line">        <span class="comment"># "In our model, we share the same weight matrix between the two</span></span><br><span class="line">        <span class="comment"># embedding layers and the pre-softmax linear transformation...</span></span><br><span class="line">        <span class="comment"># In the embedding layers, we multiply those weights by \sqrt{d_model}".</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Options here:</span></span><br><span class="line">        <span class="comment">#   'emb': multiply \sqrt{d_model} to embedding output</span></span><br><span class="line">        <span class="comment">#   'prj': multiply (\sqrt{d_model} ^ -1) to linear projection output</span></span><br><span class="line">        <span class="comment">#   'none': no multiplication</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> scale_emb_or_prj <span class="keyword">in</span> [<span class="string">'emb'</span>, <span class="string">'prj'</span>, <span class="string">'none'</span>]</span><br><span class="line">        scale_emb = (scale_emb_or_prj == <span class="string">'emb'</span>) <span class="keyword">if</span> trg_emb_prj_weight_sharing <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">        self.scale_prj = (scale_emb_or_prj == <span class="string">'prj'</span>) <span class="keyword">if</span> trg_emb_prj_weight_sharing <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">        self.encoder = Encoder(</span><br><span class="line">            n_src_vocab=n_src_vocab, n_position=n_position,</span><br><span class="line">            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,</span><br><span class="line">            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,</span><br><span class="line">            pad_idx=src_pad_idx, dropout=dropout, scale_emb=scale_emb)</span><br><span class="line"></span><br><span class="line">        self.decoder = Decoder(</span><br><span class="line">            n_trg_vocab=n_trg_vocab, n_position=n_position,</span><br><span class="line">            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,</span><br><span class="line">            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,</span><br><span class="line">            pad_idx=trg_pad_idx, dropout=dropout, scale_emb=scale_emb)</span><br><span class="line"></span><br><span class="line">        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=<span class="literal">False</span>) <span class="comment"># 投影到词表</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters():</span><br><span class="line">            <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                nn.init.xavier_uniform_(p) <span class="comment"># xavier 初始化权重</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> d_model == d_word_vec, \</span><br><span class="line">        <span class="string">'To facilitate the residual connections, \</span></span><br><span class="line"><span class="string">         the dimensions of all module outputs shall be the same.'</span></span><br><span class="line">		<span class="comment"># 嵌入层与投影线性层权重共享</span></span><br><span class="line">        <span class="keyword">if</span> trg_emb_prj_weight_sharing:</span><br><span class="line">            <span class="comment"># Share the weight between target word embedding &amp; last dense layer</span></span><br><span class="line">            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> emb_src_trg_weight_sharing:</span><br><span class="line">            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src_seq, trg_seq</span>):</span></span><br><span class="line"></span><br><span class="line">        src_mask = get_pad_mask(src_seq, self.src_pad_idx)</span><br><span class="line">        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) &amp; get_subsequent_mask(trg_seq)</span><br><span class="line"></span><br><span class="line">        enc_output, *_ = self.encoder(src_seq, src_mask)</span><br><span class="line">        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)</span><br><span class="line">        seq_logit = self.trg_word_prj(dec_output)</span><br><span class="line">        <span class="keyword">if</span> self.scale_prj:</span><br><span class="line">            seq_logit *= self.d_model ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> seq_logit.view(-<span class="number">1</span>, seq_logit.size(<span class="number">2</span>))</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Optim-py"><a href="#Optim-py" class="headerlink" title="Optim.py"></a>Optim.py</h3><p>一个简单封装的优化器类，用以动态调整学习率。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">'''A wrapper class for scheduled optimizer '''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScheduledOptim</span>():</span></span><br><span class="line">    <span class="string">'''A simple wrapper class for learning rate scheduling'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, optimizer, lr_mul, d_model, n_warmup_steps</span>):</span></span><br><span class="line">        self._optimizer = optimizer</span><br><span class="line">        self.lr_mul = lr_mul</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_warmup_steps = n_warmup_steps</span><br><span class="line">        self.n_steps = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step_and_update_lr</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">"Step with the inner optimizer"</span></span><br><span class="line">        self._update_learning_rate()</span><br><span class="line">        self._optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">"Zero out the gradients with the inner optimizer"</span></span><br><span class="line">        self._optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_lr_scale</span>(<span class="params">self</span>):</span></span><br><span class="line">        d_model = self.d_model</span><br><span class="line">        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps</span><br><span class="line">        <span class="keyword">return</span> (d_model ** -<span class="number">0.5</span>) * <span class="built_in">min</span>(n_steps ** (-<span class="number">0.5</span>), n_steps * n_warmup_steps ** (-<span class="number">1.5</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_learning_rate</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">''' Learning rate scheduling per step '''</span></span><br><span class="line"></span><br><span class="line">        self.n_steps += <span class="number">1</span></span><br><span class="line">        lr = self.lr_mul * self._get_lr_scale()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> self._optimizer.param_groups:</span><br><span class="line">            param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>读一遍代码发现了论文中忽视的好几个点，例如嵌入向量放缩、dropout 等，读代码还是很有必要的。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title>RoBERTa</title>
    <url>/blog/2021/09/03/RoBERTa/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>RoBERTa 是华盛顿大学和 FaceBook 在论文《RoBERTa: A Robustly Optimized BERT Pretraining Approach》提出的预训练模型，论文似乎仅存在 arxiv 版本。RoBERTa 本质上是 BERT 的一个改进版本。论文发现 BERT 是未充分训练的，改进训练之后的 RoBERTa 在 GLUE、RACE、SQuAD 数据集上达到了 SOTA。代码和模型公开在了 <a href="https://github.com/pytorch/fairseq">github</a> 上。</p>
<span id="more"></span>

<p>相对于 BERT 的修改主要有以下方面：</p>
<ul>
<li>训练时间更长、数据更大（提出了一个新的数据集 CC-News）、batch 更大（有论文指出更大的 batch 模型训练结果越好）</li>
<li>移除下句预测预训练任务</li>
<li>训练序列更长</li>
<li>动态改变数据的 MASK，而 BERT 的 MASK 是固定的</li>
</ul>
<p>在不使用额外的训练数据的情况下，RoBERTa 在 GLUE 和 SQuAD 数据集上取得了优于 BERT 的性能。引入额外的训练数据后，RoBERTa 在 GLUE 中的四项任务、SQuAD、RACE 数据集上达到了 SOTA。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul>
<li>GLUE（ General Language Understanding Evaluation，通用语言理解评估）：由 9 个句子或句子对的分类任务组成。</li>
<li>SQuAD（ Stanford Question Answering Dataset，斯坦福问答数据集）：抽取式问答任务。给出文档和问题，从文档中选择部分文本作为问题答案。</li>
<li>RACE（ ReAding Comprehension from Examinations，考试阅读理解数据集）：顾名思义，数据集来自考试题目，是一个分类任务。单个样本由文档、问题和若干个候选答案组成。正确答案不一定直接体现在文章中，需要深层理解文章并进行推断。</li>
</ul>
<h3 id="静态掩码vs动态掩码"><a href="#静态掩码vs动态掩码" class="headerlink" title="静态掩码vs动态掩码"></a>静态掩码 vs 动态掩码</h3><p>论文将 RoBERTa 与参数量相近的 $BERT_{BASE}$ 进行了比较，结果如下所示。可以看出，动态掩码的效果与静态掩码持平或者略优于。个人猜测原因是动态掩码虽然能够使得模型模型接触到更多数据、更加鲁棒，但频繁的动态掩码会使得某些样本无法得到充足的训练。</p>
<p><img src="/blog/mask-result.png"></p>
<h3 id="下句预测"><a href="#下句预测" class="headerlink" title="下句预测"></a>下句预测</h3><p>下句预测任务是 BERT 中提出的预训练任务，用于判断两句话是否构成连续上下句的关系。BERT 论文中认为下句预测任务是非常重要的，它提升了 QNLI、MNLI、SQuAD 数据集的性能。然而，一些工作开始质疑下句预测任务的有效性。RoBERTa 论文中比较了以下几种训练方法：</p>
<ul>
<li>句子段（连续多个句子）对 + 下句预测，也就是原版 BERT 的训练方法。</li>
<li>句子对 + 下句预测。</li>
<li>跨文档完整句子，将多篇文档拼接在一起，从中连续采样句子，可能跨文档也可能来自同一篇文档。</li>
<li>单文档句子，从单个文档中连续采样句子。</li>
</ul>
<p>实验结果如下：</p>
<p><img src="/blog/nsp.png"></p>
<p>前两种训练方法比较，前者优于后者，说明独立的句子会损害下流任务的性能。接下来比较有无 NSP 任务的训练方法，分析后可以看出，完整句子移除了 NSP 任务，与拥有 NSP 任务的性能基本持平，在某些任务上还略胜一筹。而单文档句子任务甚至优于跨文档完整句子。</p>
<h3 id="更大的Batch"><a href="#更大的Batch" class="headerlink" title="更大的Batch"></a>更大的 Batch</h3><p>机器翻译上的部分工作证实了大 batch-size 能够同时提高优化速度和任务性能，近期工作证实这同样适用于 BERT，论文在 $BERT_{BASE}$ 上进行了 Batch-size 的实验，结果如下：</p>
<p><img src="/blog/batch-size.png"></p>
<p>可以看出，2k 的 batch size 确实要优于 256，但 8k 却差于 2k。论文中也没有进行解释，迷惑。</p>
<h3 id="文本编码"><a href="#文本编码" class="headerlink" title="文本编码"></a>文本编码</h3><p>字节对编码（Byte-Pair Encoding）是一种字词模型，BERT 使用它来构建词表。然而当语料规模很大时，unicode 字符会占据词表中相当大部分。2019 年 GPT2 论文指出，可以使用 unicode 字节而非 unicode 字符来作为基本字词单元，然而这种方法可能会有轻微的性能损失（毕竟破坏了字符的完整结构），但是由于其能减小词表规模，RoBERTa 还是基于此进行的词表构建。</p>
<h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><p>RoBERTa=BERT + 动态掩码 + 跨文档完整句子 + 更大 batch size + 字节编码 + 更大数据 + 更长训练时间</p>
<p>实验结果如下：</p>
<p><img src="/blog/roberta.png"></p>
<p>控制训练数据时，RoBERTa 已经优于 $BERT_{LARGE}$ 了（但在 SQuAD 上逊于 XLNET），在增加数据和训练更长时间后，三个数据集上全面超越 XLNET。</p>
<p>后面就是 GLUE、SQuAD 上各项指标的实验和比较了，基本 RoBERTa 也是最优的，这里就略去了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>RoBERTa 可以看作是 BERT 真正的完全体吧，弥补了原生 BERT 的缺陷。可能是因为创新性不足？没有被会议接受。看来预训练模型也还是很卷的。。。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>BERT</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>BERT</tag>
        <tag>RoBERTa</tag>
      </tags>
  </entry>
  <entry>
    <title>BART</title>
    <url>/blog/2021/08/29/BART/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>BART 是 Facebook AI 于 2019 年发表的《Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension》论文中提出的预训练模型，论文收录于 2020 年 ACL。顾名思义，BART 是一个基于 seq2seq 的预训练模型，可以用于自然语言生成、翻译、理解等任务。论文中的 “Denoising” 直译为降噪，实际上是模型的预训练目标。</p>
<span id="more"></span>

<p>一个水逆的周末，博客更新不能停！</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>预训练模型之前已经介绍过了，参考 <a href="https://tqnwhz.github.io/blog/2021/08/16/BERT/#more">BERT</a>。这里只做简单的介绍。预训练模型的目的是在大量数据上预训练一个能够解决通用任务的模型，下游任务可以在预训练模型的基础上进行调整适配，无需从头训练。预训练模型往往有几个关键因素：</p>
<ul>
<li>模型架构。Transformer 是公认的特征抽取能力很强的架构，因此常见的预训练模型都是用的 Transformer 架构。</li>
<li>预训练目标。在 BERT 之前，预训练模型往往都是按照标准的语言模型进行训练，例如 ELMO。BERT 第一次提出了掩码语言模型这样的预训练任务，不仅能够更好地适配下游任务，而且取得了更优的效果。如何能够挑选一个更好的预训练目标来建模通用任务，也是预训练模型的关键。</li>
<li>适用任务及使用方法。虽然预训练模型是为建模通用任务而存在的，然而还是存在适用任务的限制，具体任务对使用方法也有要求。</li>
<li>数据集。要求很简单，大而全。大就不用说了，数据集最好能够涵盖多个领域的数据，这样适配下游任务也会更简单。</li>
<li>效果。事实上没有个 state-of-the-art 都不太可能发出来，相对没有那么重要。</li>
</ul>
<p>按照这个顺序，我们来介绍一下 BART 模型。</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>基于 Transformer 的 seq2seq 模型，与 GPT 和 BERT 一样，使用的激活函数是 gelu 而不是 relu。与 BERT 的区别在于：</p>
<ul>
<li>有解码器、在解码器的每一层，添加了对编码器最后一层输出的注意力，跟 seq2seq 的注意力一致。</li>
<li>BART 去掉了在词预测之前的前馈神经网络。</li>
</ul>
<p>总结一下就是个 Transformer。与 BERT 的主要区别在于有解码器，可以用于生成任务，与 GPT 的主要区别在于有编码器，可以更好地用于监督的生成任务，如下图所示。</p>
<p><img src="/blog/architecture.png"></p>
<h3 id="预训练目标"><a href="#预训练目标" class="headerlink" title="预训练目标"></a>预训练目标</h3><p>BART 的预训练目标定义为：给定文档，使用噪声函数（符号遮挡、符号删除、符号填充、文档排列、文档旋转）对文档施加噪声，再进行文档重构。换而言之，输入为有噪声的文档，期望输出为没有噪声的文档，这正是论文名中的 “降噪” 的由来。 在实验过程中，噪声可能是以上噪声函数的组合。几种噪声函数的示例分别如下：</p>
<p><img src="/blog/noises.png"></p>
<h3 id="适用任务"><a href="#适用任务" class="headerlink" title="适用任务"></a>适用任务</h3><p>BART 可适用于以下任务：</p>
<ul>
<li>句子分类，输入输出均为该序列，将解码器的最终隐藏状态拿去分类即可，类似 BERT 中的 [CLS] token。</li>
<li>符号分类，输入输出均为该序列，将解码器每个位置的隐藏状态拿去分类即可。</li>
<li>序列生成，例如文本摘要、问答等任务，给定输入输出进行 fine-tune 即可。</li>
<li>目标语言为英语的机器翻译，这个任务其实也属于序列生成，不过有点不太一样。具体做法为，将 BART 的编码器随机初始化（就是丢弃本来的权重），然后冻结其他参数只更新编码器权重，后面再微调所有权重。这里限制为英语主要是 BART 本身在英文语料上训练的。</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="预训练目标比较"><a href="#预训练目标比较" class="headerlink" title="预训练目标比较"></a>预训练目标比较</h3><p>为了评估各种预训练目标的有效性，BART 在尽量控制变量（分别调优，学习率、正则化可能有所差别）的前提下比较了如下几种预训练目标：</p>
<ul>
<li>语言模型，与 GPT 类似，从左向右的语言模型</li>
<li>置换语言模型，基于 XLNET，对 1/6 的符号进行采样，再进行自回归预测</li>
<li>掩码语言模型，与 BERT 类似</li>
<li>多任务掩码语言模型，与 uniLM 类似</li>
<li>掩码 seq2seq：掩码 50% 的序列，由 seq2seq 预测</li>
</ul>
<p>通过在问答、对话、摘要等多项任务上进行比较，论文得出以下结论：</p>
<ul>
<li>预训练目标的性能与下游任务有着很密切的关系，一个简单的语言模型可以在生成式问答上取得最优效果，在抽取式问答上效果确实最差的</li>
<li>符号遮挡是至关重要的，没有符号遮挡的文档旋转、句子重排的预训练目标表现较差</li>
<li>从左到右的语言模型预训练任务能改善生成任务，像掩码语言模型和置换语言模型不包含自回归语言模型训练任务，生成任务效果就会比较差</li>
<li>双向编码器对 SQuAD 数据集是非常重要的</li>
<li>预训练目标并非唯一重要的因素，任务性能也与模型结构等因素有很大关系</li>
<li>纯粹的语言模型在 ELI5 数据集上取得了最好的性能</li>
<li>使用文本填充预训练的 BART 取得了大部分数据集上的最优性能</li>
</ul>
<h3 id="大规模预训练"><a href="#大规模预训练" class="headerlink" title="大规模预训练"></a>大规模预训练</h3><h4 id="判别任务"><a href="#判别任务" class="headerlink" title="判别任务"></a>判别任务</h4><p>在两个数据集上进行实验：</p>
<ul>
<li>SQuAD（Stanford Question Answering Dataset，斯坦福问答数据集）：给定一篇文章和一个问题，从原文中选取部分文字作为问题的答案（答案一定原文中）。虽然是问答任务，但是并不是生成式的任务，而是判别式的任务。</li>
<li>GLUE（General Language Understanding Evaluation，通用语言理解评估）：由九个任务组成，每个任务都是句子或句子对的分类任务，例如 CoLA 对单句子是否符合文法进行评估，QQP 评估一对问题是否等价。</li>
</ul>
<p><img src="/blog/nlu.png"></p>
<p>两个数据集上的实验结果显示，BART 可以和 RoBERTa 打的有来有回。</p>
<h4 id="生成任务"><a href="#生成任务" class="headerlink" title="生成任务"></a>生成任务</h4><p>分别在摘要生成、对话、问答生成、翻译多个任务上进行了实验，数据集分别介绍如下：</p>
<ul>
<li>CNN/DailyMail 和 XSum 是摘要生成的两个英文数据集。每个数据样本由文档与人工总结的摘要组成。与抽取式摘要任务不同，摘要中可以出现文档中未出现的单词或者句子。</li>
<li>CONVAI 是一个对话数据集，回复不仅取决于上下文，还取决于对话人的角色信息，换而言之，模型需要根据上下文和角色信息生成合适的回复。</li>
<li>ELI5 是一个生成式问答数据集，根据文档回答指定的问题。</li>
<li>WMT16 是一个翻译数据集，涵盖了多种语料到英语的翻译数据。</li>
</ul>
<p>在四项生成任务上的评估表明，BART 都取得了 SOTA 的性能。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>BART 是一个基于 Transformer 架构的去噪 seq2seq 模型，通过破坏和重建原始文本进行预训练，在自然语言理解任务上与现有模型难分伯仲，但在自然语言生成任务上达到了 SOTA 的性能。</p>
<p>预训练模型简单分为三类：</p>
<ul>
<li>仅编码器，如 BERT，可以直接用于自然语言理解任务，或者加个解码器再用于生成任务。</li>
<li>仅解码器，如 GPT，可以直接用于自然语言生成任务，或者加个编码器可以用于条件生成任务。</li>
<li>编码器 + 解码器，如 BART，可以同时直接用于两种任务。</li>
</ul>
<p>后面可能会读一下 RoBERTa 的论文。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>BART</category>
      </categories>
      <tags>
        <tag>BART</tag>
        <tag>预训练模型</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT 三部曲之三</title>
    <url>/blog/2021/08/18/GPT-3/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>GPT 系列是 OpenAI 推出的预训练模型，时至今日已经包含了三个模型，今天我来读的是 GPT 系列第三部，出自 2020 年发表在 NeurIPS 上的论文《Language Models are Few-Shot Learners》。秉着最新的成果往往更重要的原则，GPT 系列我打算倒着读。从名字可以看出，GPT-3 关注点在于少样本学习，虽然预训练模型在下游任务微调上取得了很好的成果，但是下游任务的微调往往也需要一定规模的数据集。GPT-3 希望能够用更大的模型（1750 亿）来将微调任务转变为少样本学习任务。</p>
<span id="more"></span>

<h3 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h3><p>像 BERT、GPT 此类的预训练模型，虽然经过微调后能够很好地应用于各种下游任务。但是微调任务往往也需要数万规模的标注数据集。而对于人类来说，并不需要大规模的数据集才能学习到特定任务，只需要一个简单的描述（例如，请告诉我这些句子的情绪是开心的还是低落的）或者很少的数据（这是两个勇敢的人的例子，请再给出一个勇敢的例子）。因此，如何让预训练模型能够像人一样灵活、通用地解决问题，成了研究者追求的目标。</p>
<p>解决上述问题的想法之一是通过元学习（meta-learning）。元学习的。在语言模型中，元学习意味着模型在训练阶段能够拥有识别通用模式的能力，并在推理阶段快速识别并适应特定任务，如下图所示。</p>
<p><img src="/blog/meta-learning.png"></p>
<p>GPT-2 就是通过上下文学习来达到上述效果，具体来说，GPT-2 会将任务嵌入到预料中，例如一个英语到法语的翻译任务的语料为：“”I’m not the cleverest man in the world, but like they say <strong>in French</strong>: Je ne suis pas un imbecile [I’m not a fool].”，进而避免了显式的任务枚举、编码等操作。在这样的语料上训练语言模型，来达到多任务学习的效果。但是这样的一个问题就是模型（虽然有十几亿参数）可能很难学习到这样复杂的依赖关系。虽然 GPT-2 取得了一些初步的结果，但是效果仍远不如微调。</p>
<p><img src="/blog/learning-type.png"></p>
<h3 id="参数膨胀"><a href="#参数膨胀" class="headerlink" title="参数膨胀"></a>参数膨胀</h3><p>自从 NLP 中预训练模型提出以来，参数越多，性能越好基本成为了大家的共识。参数膨胀也成为了预训练模型更新换代的趋势。从 2018 年 BERT 的 3 亿参数，到 GPT-2 的 15 亿参数，再到 2020 年 GPT-3 的 1750 亿参数，每次模型增大都带来了下游任务的改进。其算力的要求也让预训练模型成为大公司垄断的研究方向。</p>
<p>在论文中，作者还实验了从 1.25 亿参数到 130 亿参数间的一系列小模型，并对其在零样本、单样本、少样本实验上的性能进行了评估，结果如下图所示：</p>
<p><img src="/blog/model-size.png"></p>
<p>可以看到，三种任务上的性能都随参数的增加而得到提升。</p>
<h3 id="数据污染"><a href="#数据污染" class="headerlink" title="数据污染"></a>数据污染</h3><p>在使用像 Common Crawl 这样大规模数据集的时候，可能会出现数据污染问题：由于数据规模过大，测试集的一些样本出现在训练集之中，使得评估不准确。GPT-3 使用的是来自互联网上的文本数据，更有可能出现这样的问题。因此，作者开发了工具来量化数据污染对实验的影响，并对受较大影响的数据集进行了标注。</p>
<h2 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h2><p>实验的设置与 GPT-2 类似，不同的是，GPT-3 对比了上下文学习的不同设置，包含以下四种方法（单样本与零样本学习区分开来的原因在于，在某些任务例如与人类对话中，单样本学习更匹配）：</p>
<table>
<thead>
<tr>
<th>方式</th>
<th>特点</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>微调</td>
<td>在下游任务的监督数据集上更新权重</td>
<td>性能好</td>
<td>每个任务都需要较大规模的监督数据</td>
</tr>
<tr>
<td>少样本学习</td>
<td>在下游任务推理时提供 K 个样本（10-100）作为演示，不更新权重</td>
<td>减少了对监督数据规模的要求</td>
<td>效果比微调差的多</td>
</tr>
<tr>
<td>单样本学习</td>
<td>除了任务的自然语言描述，只有一个演示样本。</td>
<td></td>
<td></td>
</tr>
<tr>
<td>无样本学习</td>
<td>只接受任务的自然语言描述，无演示样本。</td>
<td>使用起来最便利<br>最接近人类执行任务的方式</td>
<td>挑战性最强，在某些情况下非常困难</td>
</tr>
</tbody></table>
<p>四种方式介绍如下图所示：</p>
<p><img src="/blog/learning-type.png"></p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>“GPT-3 的模型与 GPT-2 类似，除了 GPT-3 是交替使用密集 Transformer 与局部带状稀疏注意力机制的 Transformer”。这基本就是论文中对 GPT-3 模型的全部介绍了。我翻到 GPT-2 的论文，”GPT-2 是基于 Transformer 的语言模型架构，模型细节大体与 GPT 类似，除了将层标准化提前到子块的输入位置，并在最后一个自注意力机制块后加入层标准化”。套娃现象属实有点严重。</p>
<p>下面是 GPT-1 的结构，看起来就是个 12 层的 Transformer。上面提到的那些局部带状系数注意力机制的 Transformer 等到后面再补充吧。</p>
<p><img src="/blog/gpt-1.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>GPT 系列的模型结构变化不大，重要的一直是实验部分，从普通的预训练语言模型到试图通过少样本学习解决各种下游任务的庞然大物。整个 GPT-3 的论文，只有十页左右在介绍非实验部分，剩下的几十页都是实验。这次因为时间原因先介绍到这里，实验部分等后续有时间再补充吧。</p>
<p>这篇论文一个最大的写作特点在于引用非常的奇怪。可能是我见识比较少，但是 word2vec，glove 这种写出来非常直观的方法，后面加个引用也非常清晰，论文中却不提缩写，只使用的作者姓首字母加年份的引用，像 word2vec、glove 的引用名分别为 MCCD13、PSM14。只看这个引用让人不知所云，还要点超链接浪费时间。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>GPT</category>
      </categories>
      <tags>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT</title>
    <url>/blog/2021/08/16/BERT/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>今天来看读的是大名鼎鼎的 BERT，出自论文 Google 团队 2018 年的论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》。BERT（<strong>B</strong>idirectional <strong>E</strong>ncoder Representations from <strong>T</strong>ransformers）可谓是 NLP 历史上划时代的预训练模型，在 11 项自然语言处理任务上都取得了 state-of-the-art。并且，Google 将 BERT 的代码与预训练模型全部开源，便于大家使用。</p>
<span id="more"></span>

<h2 id="预训练（语言）模型"><a href="#预训练（语言）模型" class="headerlink" title="预训练（语言）模型"></a>预训练（语言）模型</h2><p>预训练语言模型是自然语言处理中的重要部分，与计算机视觉中的预训练模型类似，也是为了从特定的下游任务中脱离出来，在大量数据上预训练可以解决通用任务的模型。这样以来，下流任务可以根据自己的任务特点进行微调，而不需要从头训练。这样做的好处非常明显：</p>
<ul>
<li>训练时间更短</li>
<li>数据要求更少</li>
</ul>
<p>在计算机视觉中，预训练模型例如 ImageNet 首先得到广泛应用，利用 ImageNet，你可以很快地构造一个特定任务的识别模型，而不需要从头训练，重复捕捉像物体边界等信息。在自然语言处理中，预训练模型应用就没有那么广泛。一个主要的原因就是多义词，例如 “苹果” 既可以代表电脑品牌、也可以代表水果，其具体的含义要根据具体的上下文才能推断出来。而像 word2vec/glove 这样的静态词向量算法，词向量一经训练得到就固定了，所以不能建模一词多义的现象。ELMO 于 2018 年 3 月提出，在双向 LSTM 上预训练语言模型，解决了静态词向量存在的问题，然而还没来得及大展拳脚，就被 2018 年 10 月的 BERT 拍死在了沙滩上。。。</p>
<p>预训练模型可以简单分为两类：</p>
<ul>
<li>基于特征的预训练模型，指利用语言模型的中间结果，作为额外的特征，引入到下游任务中。典型的就是 ELMO。特点：<strong>模型参数是固定的</strong>。</li>
<li>基于微调的预训练模型，指在语言模型的基础上，加入少量的特定任务参数（例如分类任务，加一层 softmax），再在任务数据上微调模型，典型的就是 GPT。特点：<strong>模型参数需要微调</strong>。</li>
</ul>
<p>在 BERT 之前的预训练模型，例如 ELMO 与 GPT，存在的一个重要问题是它们只从单向建模了序列。虽然 ELMO 是使用的双向 LSTM，也只是把双向的隐藏状态进行了拼接，双向的特征信息也没有很好地进行融合。形式化的来说，标准的语言模型就是单向的，当前词的选择只依赖于先前词，例如从左到右方向：<br>$$<br>P_{l2r}(x_1,x_2,\dots,x_n)=\prod_{t=1}^nP(x_t|x_{&lt;t})<br>$$</p>
<h3 id="掩码语言模型（masked-language-model，MLM）"><a href="#掩码语言模型（masked-language-model，MLM）" class="headerlink" title="掩码语言模型（masked language model，MLM）"></a>掩码语言模型（masked language model，MLM）</h3><p>考虑到标准的语言模型所存在的问题，BERT 提出了一种掩码语言模型的任务，具体做法为：随机遮挡输入中的部分单词，目标是仅根据单词的上下文预测当前位置本来的单词（即遮掩前的单词）。与从左到右的语言模型不同，掩码语言模型能够融合单词的左右上下文。形式化而言，对于 $x_t$ 的概率分布，掩码语言模型计算的是 $P (x_t|x_{&lt;t},x_{&gt;t})$，标准的语言模型计算的是 $P (x_t|x_{&lt;t})$。但值得注意的是，掩码语言模型并不是传统的语言模型，一些论文中可能存在二者混用的情况。语言模型的定义可以看我之前的博客 <a href="https://tqnwhz.github.io/blog/2021/07/22/language-model/#more">语言模型 | 一隅</a>。</p>
<p>个人认为掩码语言模型的思路与 cbow 的思想是有些像的，根据一个单词的上下文来预测这个词。</p>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>BERT 主要分为两个阶段：预训练和微调。两者流程如下所示：</p>
<p><img src="/blog/architecture.png"></p>
<p>可以看到，除了输出层，预训练和微调的架构基本上是完全一致的。预训练的权重将会作为下流微调任务的参数的初始权重。在序列中，BERT 加入了两个特殊标记：</p>
<ul>
<li>[CLS]：添加于序列起始位置，作为整个序列的表示，可以用于分类任务。</li>
<li>[SEP]：当输入为一对序列的时候（例如问答任务，一问一答），添加于两序列之中，作为分隔符。</li>
</ul>
<p>BERT 由若干个 <strong>Transformer 编码器</strong>（注意只有编码器）组成，在论文中，作者提出了两种规模的 BERT：</p>
<ul>
<li>$BERT_{BASE}$​：由 12 个 Transformer 编码器堆叠而成，隐节点大小为 768，自注意力机制有 12 个头，约 110M 参数。</li>
<li>$BERT_{LARGE}$​：由 24 个 Transformer 编码器堆叠而成，隐节点大小为 1024，自注意力机制有 16 个头，约 340M 参数。</li>
</ul>
<h3 id="输入输出表征"><a href="#输入输出表征" class="headerlink" title="输入输出表征"></a>输入输出表征</h3><p>在输入表征上，BERT 使用了 WordPiece 的词嵌入，词表大小为 30000。WordPiece 是一种子词模型，token 粒度介于整个单词与字符之间，例如会将 “working” 这个单词拆分为 “work”、“ing” 两个 token 存储在词表中，”ing” 又可以与其他的动词结合，这样就可以用更小的词表存储更多的单词，也没有损失太多的语义信息。</p>
<p>BERT 的输入既可以是单个序列，也可以是一对序列（例如问答场景）。应用于一对序列时，需要插入 [SEP] 分隔符，并且要使用段嵌入向量。对于输入的每个 token，它的输入表征为以下三个向量的和：</p>
<ul>
<li>符号向量（token embedding），也就是我们传统意义上所说的词向量。</li>
<li>段向量（segement embedding），用以区分一对序列中的两个不同的序列。</li>
<li>位置向量（position embedding），用以编码位置信息。</li>
</ul>
<p><img src="/blog/embedding.png"></p>
<p>值得注意的是，<strong>以上三种词向量全部通过学习得到</strong>，这与 Transformer 不同。Transformer 中的位置向量是通过三角函数计算得到，而 BERT 是通过学习得到的。就这一点作者似乎没有进行解释，而 Transformer 的作者在论文中实验结果是，学习得到与使用三角函数的位置向量效果相近，而三角函数更易扩展。一种说法是 BERT 使用的语料更大，可能可以学习到更好的位置向量。</p>
<h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><h4 id="掩码语言模型"><a href="#掩码语言模型" class="headerlink" title="掩码语言模型"></a>掩码语言模型</h4><p>在训练时，按一定比例（实验中为 15%）随机屏蔽输入序列中的部分单词（使用 [MASK] 替换），然后预测被屏蔽掉的单词，而不是重建整个输入序列。这个过程与完形填空类似，想象一个句子中有几个单词空缺，掩码语言模型的目标就是根据上下文成功预测空缺的单词。这与语言模型的训练方式还是有较大差别的。</p>
<p>上述方式虽然听上去很合理，但有一个问题，在下流任务的微调阶段，并不会出现 [MASK] 标记，这一定程度上导致了预训练与微调间的不匹配。为了解决这种情况，BERT 并不总是使用 [MASK] 替换掉需要屏蔽的单词，而是按照概率执行对应操作：</p>
<ul>
<li>80% 替换为 [MASK]</li>
<li>10% 替换为随机其他单词</li>
<li> 10% 保持不变</li>
</ul>
<p>之后，被屏蔽掉的单词会被用以预测原本的单词，换而言之，预测概率变为 $P (x_t|x_{&lt;t},x_{mask},x_{&gt;t})$。这样能够缓解预训练与微调间的不匹配情况。直观来看，BERT 会参考被屏蔽掉单词，因为它有 10% 的概率就是真实的单词，但也不会完全依赖这个单词，因为 10% 的概率还是很小的。</p>
<h4 id="下句预测（Next-Sentence-Prediction，NSP）"><a href="#下句预测（Next-Sentence-Prediction，NSP）" class="headerlink" title="下句预测（Next Sentence Prediction，NSP）"></a>下句预测（Next Sentence Prediction，NSP）</h4><p>考虑到 NLP 中的很多任务例如问答、自然语言推理都基于句子间关系的理解，而这种句子间的关系不能被语言模型捕获，因此 BERT 提出了一个名为下句预测的预训练任务。顾名思义，这个任务的目标是判断两个句子是否构成上下句的关系。这个任务的数据非常容易获得，在大规模语料上获取连续的两句话，并以 50% 的概率替换真实的下句话，即可得到正负样本分布均匀的数据集。</p>
<h3 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h3><p>在微调时，只需要将特定任务的输入输出放到 BERT 中，微调所有参数，输入的形式可以是单个句子或句子对，可以应用的任务举例如下：</p>
<ul>
<li>单个句子：文本分类、序列标注、情绪分析（利用 [CLS] 符号）</li>
<li>句子对：释义、问答等任务</li>
</ul>
<p>对于句子对，传统的模型往往将其拆分分开处理，而 BERT 将句子对同时投喂到模型中，能够更好地捕获句子间关系。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验部分主要介绍了 BERT 微调之后是怎么横扫涵盖通用语言评理解评估等十一项任务的，基线模型有 biLSTM+elmo，GPT 等，这里就不多介绍了。有兴趣的可以去看看原文。</p>
<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p>消融实验更像是控制变量法，对于模型中的多个改进，控制变量来分析哪个改进对于效果的提升是最大的，这就是消融实验。消融实验细节可以看原文，我在这里总结一下消融实验的结论：</p>
<h3 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h3><ul>
<li>去除下句预测后的 BERT 模型在 QNLI, MNLI 等涉及句子对的任务上的性能损失严重，证明下句预测对于句子间关系的捕获还是很有作用的。</li>
<li>仅使用从左到右的语言模型训练的 BERT 比使用掩码语言模型训练得到的 BERT 效果要差，证明掩码语言模型训练方式的有效性。</li>
</ul>
<h3 id="模型大小"><a href="#模型大小" class="headerlink" title="模型大小"></a>模型大小</h3><ul>
<li>参数越多，各项任务上的效果越好，非常的真实。</li>
</ul>
<h3 id="训练步数"><a href="#训练步数" class="headerlink" title="训练步数"></a>训练步数</h3><ul>
<li>BERT 真的需要在 128,000 字符 /batch 上训练 1,000,000 步才能达到这么好的效果，训练一百万步比五十万步的 BERT 在 MNLI 获得了 1% 的提升。</li>
<li>掩码语言模型的收敛慢于自左向右的语言模型，但就精度而言，掩码语言模型几乎在训练之初就强于语言模型。</li>
</ul>
<h3 id="基于特征的方法"><a href="#基于特征的方法" class="headerlink" title="基于特征的方法"></a>基于特征的方法</h3><ul>
<li>BERT 对基于特征和微调的方法都是有效的。</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>论文提出了一种全新的预训练任务–掩码语言模型，并在该任务和下句预测任务上预训练了一个基于双向 Transformer 的深层模型 BERT。在十一项 NLP 任务上的微调实验结果表明，BERT 的效果优于现有的预训练模型。</p>
<p>BERT 的论文断断续续读了几天，读下来感觉醍醐灌顶，很多之前模棱两可的东西都真正了解了。果然读论文还是要读原文，别人的博客只是参考。后面有空了再读一读 BERT 的源码，又想去读 GPT 的论文，时间也太少了。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/46833276">论文解读：BERT 模型及 fine-tuning - 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预训练模型</category>
        <category>BERT</category>
      </categories>
      <tags>
        <tag>预训练模型</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/blog/2021/08/14/Transformer/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>今天读的是大名鼎鼎的 BERT——- 的组件之一 Transformer，出自论文 Google 团队 2017 年的论文《Attention Is All You Need》。与传统的 GRU、LSTM 等相比，Transformer 只使用注意力机制来建模输入与输出间的依赖关系，并支持并行化。论文在机器翻译上进行了实验，Transfomer 达到了更好的效果，因此自提出以来，就得到了极为广泛的关注。</p>
<span id="more"></span>

<h2 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h2><p>之前由于换电脑的原因，断更了一段时间。BERT 与 Transformer 的论文之前也粗读过一两次，还是有些一知半解，正好趁这个周末再复习总结一下，记录在博客里。希望我的博客能对你有所帮助。</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>Transformer 使用的仍然是 encoder-decoder 架构，但与 RNN 等自回归模型不同，Transformer 使用的是堆叠的多头注意力机制，全连接层等，其模型结构如下所示：</p>
<p><img src="/blog/architecture.png"></p>
<p>左侧的为单个编码器的结构，第一层为多头注意力、残差层与层标准化，第二层是前馈神经网络。编码网络是由若干个编码器堆叠而成的，原论文中 N=6，嵌入向量维度为 512。</p>
<p>右侧为单个解码器的结构，主要在编码器的基础上，加入了一个 Masked 的多头注意力机制，用以保证每个时间步的输出只已知已经输出的信息。同样的，解码网络也有 6 个解码器组成。</p>
<h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>多头注意力机制可谓是 Transformer 的核心，详细过程可以参考<a href="https://blog.csdn.net/longxinchen_ml/article/details/86533005">图解 Transformer 完整版</a>。这里只做核心部分介绍，单头计算过程为：<br>$$<br>Attention(Q,K,V)=softmax(\frac{QK^\intercal}{\sqrt{d_k}})V<br>$$<br>Q，K，V 分别为查询、键、值矩阵，由词嵌入向量矩阵映射得出。多头注意力机制使用点乘计算相似度，不同的是，这里除以了一个标量 $\sqrt {d_k}$​​​。这个标量是 softmax 的温度系数，由于点积结果方差可能很大，可能会存在梯度过小无法正常更新的情况。除以一个标量能够使得概率分布更加均匀。这一部分可以参考学习下 softmax 的温度系数。</p>
<p>作者发现，相较于仅使用一个注意力机制，使用多个注意力机制并将其拼接能够拥有更好的效果。在论文中，作者使用 8 个注意力机制，每个注意力机制的输出为 512/8=64 维嵌入向量。</p>
<h3 id="注意力机制的使用"><a href="#注意力机制的使用" class="headerlink" title="注意力机制的使用"></a>注意力机制的使用</h3><p>多头注意力机制以三种方式在模型中使用：</p>
<ul>
<li>编码器与解码器间的注意力：查询 q 来自解码器，键 K 与值 V 来自编码器。这里的注意力机制用以在输出的每一步关注在输入序列的不同部分，与 seq2seq 的注意力机制相似、</li>
<li>编码器内的自注意力：查询、键、值均来自编码器。输入序列的每个位置可以得到到整个输入序列的信息。</li>
<li>解码器内的掩码自注意力：查询、键、值均来自解码器。为了保证解码器只能获得已输出的部分序列的信息，将当前位置之后位置的标量化点积设置为 $-\infty$​，进而经过 softmax 后概率值为 0。</li>
</ul>
<h3 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h3><p>在编码器和解码器中的前馈神经网络，搭配 relu 激活函数来为模型构造非线性计算。计算过程如下所示：<br>$$<br>FFN(x)=\max(0,xW_1+b_1)W2+b_2<br>$$<br>其中，输入和输出的维度均为 512，隐藏层维度为 1024。另外，前馈神经网络在每个层内不共享参数，换而言之，它们的参数是独立的。</p>
<h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>由于 Transformer 中不存在 RNN 中的自回归结构，输入序列的不同位置是等价的。为了编码位置信息，作者引入了位置编码，使用 sin 与 cos 函数：<br>$$<br>PE_{(pos,2i)}=\sin(pos/10000^{2i/d_{model}})<br>$$</p>
<p>$$<br>PE_{(pos,2i+1)}=\cos(pos/10000^{2i/d_{model}})<br>$$</p>
<p>其中，pos 为位置，i 为向量维度。作者称选取三角函数的原因是假设这样可以更好地使模型学到相对位置关系，对于任意固定的偏移 k，$PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性函数。另外，作者还尝试了学习位置编码的方式，实验对比显示，二者结果差别不大。因此作者最终选择了上述编码方式，因为它可以处理更长的序列。</p>
<h2 id="为什么使用自注意力机制"><a href="#为什么使用自注意力机制" class="headerlink" title="为什么使用自注意力机制"></a>为什么使用自注意力机制</h2><p>论文从计算时间复杂度、序列操作数、最长路径长度三方面对比了自注意力机制、RNN、CNN 以及受限的自注意力机制，结果如下：</p>
<p><img src="/blog/comparison.png"></p>
<p>这一部分计算过程论文没有给出，我参考了 <a href="https://zhuanlan.zhihu.com/p/264749298">Transformer/CNN/RNN 的对比（时间复杂度，序列操作数，最大路径长度）</a>，这里简单介绍一下。</p>
<h3 id="计算（时间）复杂度"><a href="#计算（时间）复杂度" class="headerlink" title="计算（时间）复杂度"></a>计算（时间）复杂度</h3><p>计算复杂度主要取决于计算的规模，以矩阵乘法为例，形状为 NxM 的矩阵与形状为 MxP 的矩阵相乘，得到一个 NxP 的矩阵。结果矩阵中的每个元素为 M 维向量内积的结果，进行 M 次乘法，并求和。所以整个矩阵乘法的复杂度为 $O (NMP)$。</p>
<h4 id="自注意力机制"><a href="#自注意力机制" class="headerlink" title="自注意力机制"></a>自注意力机制</h4><p>$$<br>Attention(Q,K,V)=softmax(\frac{QK^\intercal}{\sqrt{d_k}})V<br>$$</p>
<p>其中，Q，K 分别为 nxd 与 dxn 的矩阵，$QK^\intercal$​的复杂度为 $O (n^2d)$​​，softmax 的复杂度为 $O (n^2)$，加权求和的矩阵形状分别为 nxn 与 nxd，复杂度为 $O (n^2d)$，因此总复杂度为 $O (n^2d)$​。受限自注意力机制与之同理，区别在于它只使用查询最近的 k 个</p>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p>$$<br>h_t=f(Ux_t+Wh_{t-1})<br>$$</p>
<p>其中，U 与 x 的形状分别为 dxd 与 dx1（假设隐藏状态与输入维度均为 d），复杂度为 $O (d^2)$​，W 与 h 的形状分别为 dxd 与 dx1，复杂度同样为 $O (d^2)$。对于长度为 n 的序列，总复杂度为 $O (nd^2)$。</p>
<h4 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h4><p>将输入序列进行 padding 后，总共需要 n 次卷积，每次卷积计算量为 kxd，假设步长为 1，单个卷积核复杂度为 $O (nkd)$​。为了保证维度相同，需要使用 d 个卷积核，总复杂度为 $O (nkd^2)$</p>
<h3 id="序列操作数"><a href="#序列操作数" class="headerlink" title="序列操作数"></a>序列操作数</h3><p>序列操作数主要衡量了并行化的支持情况，只有 RNN 需要串行地完成 n 次序列操作，其他模型均支持并行化。</p>
<h3 id="最长路径长度"><a href="#最长路径长度" class="headerlink" title="最长路径长度"></a>最长路径长度</h3><p>最长路径为序列中首尾 token 在模型中的路径，其长度越长，依赖越不容易被捕捉到。对于自注意力机制，序列中的任意两个元素均可以看作直接相连，路径长度为 $O (1)$。而 RNN 中，第一个 token 的信息需要进行 n 次迭代才能到达最后一个 token，最大路径长度即为 $O (n)$。CNN 中，通过若干个卷积层来获取不同位置的信息，每个卷积层（论文中使用的是空洞卷积）相当于让序列信息浓缩了 k 倍（卷积层的输出中的每个位置都有输入中 k 个位置的信息），最大路径长度为 $O (log_kn)$​。受限的自注意力机制与连续卷积类似，每次卷积相当于可以获取连续 k 个位置的信息，最大路径长度为 $O (n/k)$。</p>
<p>这就基本解释了这个突兀的表格是怎么计算得来的了。那么可以总结自注意力机制的优点是：</p>
<ul>
<li>单层计算量更少。在实际应用中，序列长度 n 往往小于表征维度 d，因此，自注意力机制的单层计算量相较于 RNN 与 CNN 都要更小。</li>
<li>支持并行化。这个就不说了，全世界都在针对 RNN。</li>
<li>能够更好地捕捉长距离依赖。相较于 CNN 与 RNN，自注意力机制的最长路径最短。</li>
<li>可解释性更强。作者将注意力机制的概率分布展示如下，证明多头注意力的多个头完成了与句子语义与结构相关不同的工作。</li>
</ul>
<p><img src="/blog/example.png"></p>
<p>上图是作者给出的多头注意力的例子，使用了两个头。对于 its 这个单词，得到了非常尖锐的概率分布，its 主要与 law 与 application 相关联，一个头捕获到了 its 指代的主体 law，一个头捕获到了 its 的目标 application。个人感觉这个效果也太过于理想了。。。可能这就是 Transformer 吧。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文提出了完全依赖于注意力机制的序列转换模型 Transformer，相较于 RNN，它有着可并行化、解释性更强、单层参数更少等优点。在机器翻译上取得了 state-of-the-art，在英语成分分析上也取得了比 RNN 更优的结果。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.cnblogs.com/sandwichnlp/p/11612596.html#nlp%E7%95%8Ccnn%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96%E5%8F%B2">三大特征提取器（RNN/CNN/Transformer） - 西多士 NLP - 博客园 (cnblogs.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/264749298">Transformer/CNN/RNN 的对比（时间复杂度，序列操作数，最大路径长度）</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>BERT</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>Transformer</tag>
        <tag>多头注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title>DEM-VAE</title>
    <url>/blog/2021/07/30/DEM-VAE/</url>
    <content><![CDATA[<h2 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h2><p>DEM-VAE 是字节跳动 AI LAB 团队于 2020 年发表的《Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation》论文中提出的模型，论文收录在 ICML 中。论文名直译为 “用于<strong>可解释</strong>文本生成的<strong>分散指数族混合</strong> VAE ”。</p>
<span id="more"></span>

<h2 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h2><p>最近实习面试结束了，回来更新博客了。“黄色的树林里分出两条路，可惜我不能同时去涉足”，最近有些感慨。看到这篇博客的人，希望这篇博客能对你有所帮助，也希望你天天开心。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>连续型 VAE 的隐变量难以解释分散属性，例如主题、对话动作等。这一点与 VQ-VAE 的动机相似。然而只使用分散隐变量的 VAE 的表达能力有限，隐变量 $c$​只包含 $log (#c)$​位的信息，其中 $#c$​为 $c$​​可选值的数量。（这里的意思应该是信息论中的 “信息量”，默认隐变量服从均匀分布，各值取得的概率相等，信息量 $-log (1/#c)=log (#c)$​。）</p>
<p>混合高斯分布的 VAE（GM-VAE）提供了一种自然的想法，将分散隐变量与连续隐变量结合：每个高斯分布代表一个分散属性值，分量的值代表属性相同的句子。在理想情况下，不同高斯分布的均值与方差应该差别很大。然而 GM-VAE 容易出现模式崩溃问题，这使得不同高斯分布的均值与方差非常接近，GM-VAE 退化为只有一个高斯分量的普通 VAE。如下图所示：</p>
<p><img src="/blog/example.png"></p>
<p>在本文中，作者证明了模式崩溃问题不仅存在于 GM-VAE 中，而是具有指数族混合先验分布的 VAE（EM-VAE）的普遍问题，由证据下界中的一个分散项引起。进而，作者提出了一个船新的 DEM-VAE，在 EM-VAE 的目标函数里引入了额外一项分散项。按照论文的说法，DEM-VAE 虽然适度减小了句子的似然（由于引入了新的损失项），但是在 rPPL（reverse perplexity）与 BLEU 得到了更好的结果，并且能够有效地避免模式崩溃问题。</p>
<h2 id="模式崩溃vs后验崩塌"><a href="#模式崩溃vs后验崩塌" class="headerlink" title="模式崩溃vs后验崩塌"></a>模式崩溃 vs 后验崩塌</h2><p>普通的 VAE 会面临后验坍塌（KL 散度消失）的问题，具体而言，KL 损失项在训练之初迅速变为 0。而本文要解决的是模式崩溃问题，是指先验分布中的多个模式崩溃为一个模式。模式崩溃会后验坍塌之间无必然联系。在后验坍塌未出现时，也可能出现模式崩溃。</p>
<p>虽然本文采用的解决方案与之前的解决后验坍塌的方案有些相似：找到目标函数中导致问题的那一项并削弱它的影响。但是本文采用的解决方案只引入了一个启发式的分散项，而不是整个 KL 损失项。</p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="混合指数族VAE"><a href="#混合指数族VAE" class="headerlink" title="混合指数族VAE"></a>混合指数族 VAE</h3><p>混合指数族 VAE 是指使用混合指数族分布作为先验分布的 VAE（<a href="https://en.wikipedia.org/wiki/Exponential_family">Exponential family - Wikipedia</a>），最为常见的就是混合高斯分布的 VAE，GM-VAE，它的先验分布为混合的高斯分布。GM-VAE 使用一个分散变量 $c$ 代表不同的高斯成分，连续隐变量 $z$ 依赖于 $c$。如下图所示：</p>
<p><img src="/blog/gm-vae.png"></p>
<p>其中，实现为依赖关系，虚线为变分后验。其中，$p (c)$​可以近似为均匀分布，$p_\eta (z|c)$​为指数族分布，例如高斯分布。</p>
<p>测试时：从先验分布 $p (c)$ 中采样一个 $c$，然后从 $c$ 对应的高斯分布中采样隐变量 $p (z|c)$，接着投喂到解码器 $p (x|z)$ 中。</p>
<p>训练时：通过最大化边际似然 $\int\sum_cp_\eta (z,c) p_\theta (x|z) dz$​​进行训练是不可行的。与 VAE 一样，使用近似后验分布 $q_\phi (z,c|x)=q_\phi (z|x) q_\phi (c|x)$​作为 $p (z,c|x)$​​​的估计，进一步改为优化如下所示的证据下界：</p>
<p><img src="/blog/elbo.png"></p>
<h3 id="模式崩溃问题"><a href="#模式崩溃问题" class="headerlink" title="模式崩溃问题"></a>模式崩溃问题</h3><p>作者通过研究 ELBO 目标函数，将导致模式崩溃的原因定位到 $\mathcal R_c$ 与 $\mathcal R_z$ 中。作者从指数族分布的参数化定义出发，将损失项 $\mathcal R_z,\mathcal R_c$​重写为 KL 平均正则项与分散项 $\mathcal L_d$​​。<br>$$<br>\mathcal L_d=\mathbb E_{q_\phi(c|x)}A(\eta_c)-A(\mathbb E_{q_\phi(c|x)}\eta_c)&gt;=0<br>$$<br>作者得出结论，最小化分散项 $\mathcal L_d$​使得先验分布的加权方差（即模式崩溃趋势）。这一部分的数学推导较为复杂，有兴趣的可以去看看原文。因此，作者提出在损失函数中加入一项正的分散项来抵消这一趋势，最终损失函数如下所示：<br>$$<br>L(\theta;x)=ELBO+\beta \cdot \mathcal L_d<br>$$<br>其中，$\beta$​是一个超参数，通过调整 $\beta$​​来达到平衡模式崩溃与正常训练。</p>
<h3 id="DEM-VAE"><a href="#DEM-VAE" class="headerlink" title="DEM-VAE"></a>DEM-VAE</h3><p>在上述方法基础上，作者发现，使用额外的互信息项能够进一步优化可解释性，这一部分可以在实验结果中看到。互信息项在之前的工作中用于缓解 KL 散度消失的问题，定义如下：<br>$$<br>\mathcal L_{mi}=\mathcal H(c)-\mathcal H(c|x)=\mathbb E_x\mathbb E_{q_\phi(c|x)}(logq_\phi(c|x)-logq_\phi(c))<br>$$<br>公式部分介绍完毕。在模型结构上，编码器为 GRU 等循环单元、解码器为一个语言模型。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="模式崩溃实验结果"><a href="#模式崩溃实验结果" class="headerlink" title="模式崩溃实验结果"></a>模式崩溃实验结果</h3><p><img src="/blog/mode-collapse-result.png"></p>
<p>可以看出，同时引入互信息项和分散项的 VAE（DGM-VAE，DEM-VAE）的各个分量分布有着较为明显的分类边界，没有出现模式崩溃问题。</p>
<h3 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a>文本生成</h3><p>作者使用四个指标：逆困惑度、BLEU、词级 KL 散度、负对数似然来评估文本生成的质量。其中，逆困惑度是指一个 LSTM 语言模型，从 VAE 的先验分布中采样的数据上进行训练，再在测试集上进行评估。实验结果如下：</p>
<p><img src="/blog/lg-result.png"></p>
<p>可以看到，正如前文作者所说，由于引入了额外的分散项，使得 NLL（负对数似然）相较基线模型更大，但是 rPPL，BLEU 等指标上取得了更好的结果。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文也是离散 VAE 的一种尝试，在混合高斯分布的基础上，引入额外的分散项来解决模式崩溃问题。这使得模型的解释性更强。与之前介绍过得 EQ-VAE 相比，隐变量可以表征更多信息。感觉还是很有意义的工作，就是有点难懂。。。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.iczhiku.com/hotspotDetail/q7K2UUl4a6Isl4ZlEzOrgg==">ICML 2021 | DEM-VAE：一类新的可解释文本生成模型 - IC 智库 (iczhiku.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>文本生成</category>
      </categories>
      <tags>
        <tag>VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>循环神经网络 RNN 及其变体 GRU、LSTM</title>
    <url>/blog/2021/07/22/rnns/</url>
    <content><![CDATA[<h2 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h2><p>同样，借着复习面试，把 RNN 家族再梳理回顾一下，包含 RNN、GRU、LSTM。</p>
<span id="more"></span>

<h2 id="循环神经网络RNN"><a href="#循环神经网络RNN" class="headerlink" title="循环神经网络RNN"></a>循环神经网络 RNN</h2><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/blog/rnn.png"></p>
<p>RNN 的结构如上图所示，其核心思想是使用同一套参数来更新状态 $s$ 与计算输出 $o$，箭头右侧是按时序展开的模型结构图。可以看到，RNN 仅使用了一个状态 $s$​来保存序列信息，共有三个参数矩阵。这一部分公式化描述如下:<br>$$<br>s_t=f(Ux_t+Ws_{t-1})<br>$$</p>
<p>$$<br>o_t=g(Vs_t)<br>$$</p>
<p>其中，$f$ 与 $g$​均为激活函数，激活函数可选的有 sigmoid，tanh，relu 等（下面会分析）。</p>
<p>RNN 有以下缺陷：</p>
<ul>
<li>容易发生梯度消失和梯度爆炸现象（由于导数连乘）。</li>
<li>难以捕捉长距离的依赖。</li>
</ul>
<p>在其中，梯度消失相对于梯度爆炸要更为严重，因为梯度爆炸是可以观测到的（NAN），梯度消失则难以直接观测。梯度爆炸问题很容易解决，可以通过梯度裁剪的方法进行解决。</p>
<h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>梯度消失和爆炸的解决方法：</p>
<ul>
<li>梯度的剪切以及正则化（常见的是 l1 正则和 l2 正则）。</li>
<li>relu、elu 等激活函数。（梯度消失）</li>
<li>批标准化（<strong>Batch Normalization</strong>）。</li>
<li>残差结构（将映射 F (x) 改为 F (x)+x，使用 relu 激活函数的 F 在 x&lt;0 时能够无损传播梯度，保证了深层网络的性能）。</li>
<li>LSTM、GRU 等结构。</li>
</ul>
<h4 id="批标准化-Batch-Normalization"><a href="#批标准化-Batch-Normalization" class="headerlink" title="批标准化 Batch Normalization"></a>批标准化 Batch Normalization</h4><p>Batch Normalization 是一种常用于 CNN 的正则化方法，可以分为两个步骤：</p>
<p>（1）标准化：对 batch 的数据求均值与标准差，将数据标准化到标准正态分布</p>
<p>（2）进行放缩与平移</p>
<p>整个过程类似于 VAE 的重参数化，先获得一个正态分布的变量，再进行放缩平移，达到从任意正态分布中取样的效果。</p>
<p>也就是说，batch normlization 假设每个 batch 的数据服从一个正态分布（参数 γ 和 β 学习得来，即通过 batch 数据计算得来），先将数据标准化，再放缩与平移，使得数据 “看起来” 是从这个正态分布中取样而来的。</p>
<p>在预测阶段，所有参数的取值是固定的，对 BN 层而言，意味着 μ、σ、γ、β 都是固定值。γ 和 β 比较好理解，随着训练结束，两者最终收敛，预测阶段使用训练结束时的值即可。</p>
<p> 对于 μ 和 σ，在训练阶段，它们为当前 mini batch 的统计量，随着输入 batch 的不同，μ 和 σ 一直在变化。在预测阶段，输入数据可能只有 1 条，该使用哪个 μ 和 σ，或者说，每个 BN 层的 μ 和 σ 该如何取值？可以采用训练收敛最后几批 mini batch 的 μ 和 σ 的期望，作为预测阶段的 μ 和 σ。</p>
<h3 id="层标准化-Layer-Normalization"><a href="#层标准化-Layer-Normalization" class="headerlink" title="层标准化 Layer Normalization"></a>层标准化 Layer Normalization</h3><p>Batch Normalization 是在 Batch 的方向上进行 Normalization。这种方法在 NLP 中不是很适合。由于文本序列的长度可变性，一个 batch 中的数据往往长度不同，进而对每个位置进行标准化不是很合适。</p>
<p>而 Layer Normalization 则是在序列的方向上进行 Normalization。这使得它可以处理变长序列。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>对于激活函数而言，sigmoid 的最大梯度为 0.25，因此很容易发生梯度消失现象，而 tanh 虽然最大梯度为 1，但也只有 0 处取得，也容易<br>发生梯度消失。因此 RNN 常使用 relu 作为激活函数。relu 的梯度非 0 即 1，这能够缓解梯度消失现象，但也有一定的问题：1. 容易发生梯度爆炸。（梯度恒为 1 时）2. 负数部分梯度恒为 0，部分神经元无法激活。elu 能够缓解 relu 的 0 梯度的问题，但是由于加入了幂运算，会更慢一点。</p>
<h2 id="门控循环单元GRU"><a href="#门控循环单元GRU" class="headerlink" title="门控循环单元GRU"></a>门控循环单元 GRU</h2><h3 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h3><p>GRU 的思想是在 RNN 的基础上，引入门控信号来缓解 RNN 存在的梯度消失问题。模型结构如下：</p>
<p><img src="/blog/gru.png"></p>
<p>公式化描述如下（公式中的 $\odot$ 代表哈达玛积，即同型矩阵间逐元素乘法）：</p>
<p>首先根据输入 $x_t$ 与上一时刻隐藏状态 $h_{t-1}$ 计算得到两个门控状态 $z_t$ 与 $r_t$​，假设 $h_t\in \mathbb R^H$：<br>$$<br>z_t=sigmoid(W_zx_t+U_zh_{t-1})\in \mathbb R^{H}<br>$$</p>
<p>$$<br>r_t=sigmoid(W_rx_t+U_rh_{t-1})\in \mathbb R^{H}<br>$$</p>
<p>之后，使用重置门计算得到一个新的隐藏状态（即图中的 $h’$）：<br>$$<br>\tilde h_t=tanh(Wx_t+U(r_t\odot h_{t-1}))\in \mathbb R^{H}<br>$$<br>再使用更新门 $z_t$ 更新隐藏状态：<br>$$<br>h_t=(1-z)\odot h_{t-1}+z\odot \tilde h_t\in \mathbb R^{H}<br>$$</p>
<h2 id="长短期记忆网络LSTM"><a href="#长短期记忆网络LSTM" class="headerlink" title="长短期记忆网络LSTM"></a>长短期记忆网络 LSTM</h2><h3 id="模型结构-2"><a href="#模型结构-2" class="headerlink" title="模型结构"></a>模型结构</h3><p>LSTM 的思想是在 RNN 的基础上，加入一个不易被改变的新状态 $c_t$​​，代表的是 0-t 时刻的全局信息。而 $h_t$​代表的是在 0~t-1 时刻全局信息的影响下，$t$ 时刻的信息。换而言之，$c_t$ 变化的很慢，而 $h_t$ 变化的很快。</p>
<p><img src="/blog/lstm.png"></p>
<p>公式化描述如下：</p>
<p>首先计算得到三个门控状态（分别对应图中的 $z^i,z^f,z^o$）：<br>$$<br>i_t=sigmoid(W_ix_t+U_ih_{t-1})\in \mathbb R^{H}<br>$$</p>
<p>$$<br>f_t=sigmoid(W_fx_t+U_fh_{t-1})\in \mathbb R^{H}<br>$$</p>
<p>$$<br>o_t=sigmoid(W_ox_t+U_oh_{t-1})\in \mathbb R^{H}<br>$$</p>
<p>以及一个与当前输入密切相关的向量（对应图中的 $z$）<br>$$<br>\tilde c_t=tanh(W_zx_t+U_zh_{t-1})<br>$$<br>接着，更新两种状态：<br>$$<br>c_t=f_t\odot c_{t-1}+i_t\odot \tilde c_t<br>$$</p>
<p>$$<br>h_t=o_t\odot tanh(c_t)<br>$$</p>
<p>其中，$i_t.f_t,o_t$ 分别代表信息、遗忘、输出门控。信息和遗忘门控负责 cell state 的更新，输出门控负责 hidden state 的更新。具体而言，LSTM 可以简单分为以下三个阶段：</p>
<ul>
<li>遗忘阶段，根据遗忘门控，忘记上一个 cell state 的部分信息。</li>
<li>记忆阶段，根据信息门控，将输入信息进行选择记忆。</li>
<li>输出阶段，根据输出门控，输出最终的状态。</li>
</ul>
<h3 id="LSTM-VS-GRU"><a href="#LSTM-VS-GRU" class="headerlink" title="LSTM VS GRU"></a>LSTM VS GRU</h3><p>本质上，LSTM 和 GRU 都是通过引入门控信号来解决 RNN 的梯度消失问题。在实现方法上，GRU 相对于 LSTM 要更为简单。GRU 抛弃了 LSTM 中的 hidden state（GRU 中的 hidden state 实际上是 LSTM 中的 cell state），因为 LSTM 中的 $h_t$ 只是想保存当前时刻的信息，这一部分已经包含到 GRU 中的 $\tilde h_t$ 中了。cell state 中的之前的全局信息与当前时刻的信息应当是一个此消彼长的状态，GRU 因此直接使用一个门控信号 $z_t$ 同时控制了遗忘和更新。</p>
<p>在参数上，GRU 有着比 LSTM 更少的参数，收敛速度更快，并且与 LSTM 有着差不多的性能表现，因此实际工程中多使用 GRU。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/68579467">深度学习之 3—— 梯度爆炸与梯度消失 - 知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/32481747">人人都能看懂的 GRU - 知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/32085405">人人都能看懂的 LSTM - 知乎 (zhihu.com)</a></p>
<p>[RNN vs LSTM vs GRU – 该选哪个？ - 知乎 (zhihu.com)](<a href="https://zhuanlan.zhihu.com/p/55386469#:~:text=%E8%BF%99%E9%87%8C%E5%BD%92%E7%BA%B3%E4%B8%80%E4%B8%8B">https://zhuanlan.zhihu.com/p/55386469#:~:text = 这里归纳一下</a> LSTM 与 GRU 的区别： 首先，,LSTM 选择暴露部分信息（ 才是真正的输出， 只是作为信息载体，并不输出)</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>循环神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>GRU</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>序列到序列模型</title>
    <url>/blog/2021/07/22/seq2seq/</url>
    <content><![CDATA[<h2 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h2><p>序列到序列模型（sequence to sequence, seq2seq）是自然语言处理中的一个重要模型，在机器翻译、对话系统等多个领域都有着广泛的应用。今天借着准备面试的机会，将这一部分知识整理出一篇博客。</p>
<span id="more"></span>

<h2 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h2><p><img src="/blog/basemodel.png"></p>
<p>seq2seq 模型常用语序列间的转化任务，其结构如上图所示，主要由两部分组成：</p>
<ul>
<li>编码器，常见为循环神经网络，用以将输入序列编码为固定维度的向量（即最后时刻编码器的隐藏状态），进而投喂给解码器进行解码。</li>
<li>解码器，同样常见为循环神经网络，用以根据向量输出最终序列。可以看做一个条件语言模型，“条件” 即为输入序列。因此，可以使用预训练的语言模型初始化权重，再进行 fine-tune。</li>
</ul>
<p>在训练阶段，解码器的输出仅用于计算损失，解码器的输入是编码器得到的上下文状态向量 (最后一个时间步的隐藏状态) 和目标序列当前的单词。换而言之，训练时，解码器的输出一定是与目标序列等长的。</p>
<p>在推理阶段，解码器每一个时间步的输出是下一个时间步的输入。可以通过限制输出序列的最大长度或者在输出结束标志后停止。对于 batch 的数据，往往使用限制最大长度，再删去结束标志之后的部分。</p>
<p>seq2seq 虽然简单有效，但存在以下的缺点：</p>
<ul>
<li>输入序列过长时，固定长度的向量无法存储全部的信息，进而造成信息丢失。</li>
<li>贪婪解码问题，下面会提到。</li>
</ul>
<h2 id="贪婪解码问题"><a href="#贪婪解码问题" class="headerlink" title="贪婪解码问题"></a>贪婪解码问题</h2><p>对于 seq2seq 模型，我们希望得到概率最大的输出序列，即建模的是 $\arg\max_YP (Y|X)$（$X$ 为输入序列，$Y$ 为输出序列）。然而事实上，解码器每一步求解的是 $\arg\max_{y_t} P (y_t|y_{t-1:1},X)$​，即当前时间步概率最大的单词。这样以来，整个解码的过程就是贪婪的，每一步的单词概率最大并不意味着整个句子的概率最大。</p>
<p>怎么解决这个问题呢？解决方法是 beam search（光束搜索）。核心思想是，在推理阶段（训练时不需要，因为知道 ground truth），</p>
<p>保留 k 个可能性最大的序列（可能性以概率相乘的对数作为分数，即 $\sum_{i=1}^tlogP_{I,M}(y_i|y_{i-1:1},X)$​）。</p>
<p>当某个序列输出终止符号时，可以认作该序列已经结束，继续维护其他序列。</p>
<p>搜索的终止条件可以根据任务具体选择，例如：</p>
<ul>
<li>最多搜索多长时间步（例如 30 步）。</li>
<li>至少拥有多少个候选序列（例如 10 个）。</li>
</ul>
<p>在搜索结束，得到若干个候选序列后，将序列分数标准化后，即 $\frac {1}{t}\sum_{i=1}^tlogP_{I,M}(y_i|y_{i-1:1},X)$​作为最终的分数。这样是为了避免短序列概率更大（概率连乘的数量小），然后选择概率最大的序列。</p>
<p>在搜索时，分数不需要进行标准化，因为搜索时处理的序列总是等长的。</p>
<h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>为了解决 seq2seq 在面对长序列时的信息丢失问题，研究者们在 seq2seq 中引入了注意力（Attention）机制。借助于注意力机制，解码器能够在解码时与输入序列直接相连，还可以关注到输入序列的不同部分。公式化描述如下：</p>
<p>首先根据编码器状态 ${h_1,\dots,h_N}$ 与当前解码器状态 $s_t$ 点乘计算分数<br>$$<br>e^t=[s_t^\intercal h_1,\dots,s_t^\intercal h_N]\in \mathbb R^N<br>$$<br>将分数归一化后作为输入序列与当前位置相关性的概率分布：<br>$$<br>\alpha^t=softmax(e^t)\in \mathbb R ^N<br>$$<br>加权求和后作为最终的注意力结果：<br>$$<br>\alpha_t=\sum_{i=1}^N\alpha_i^th_i\in \mathbb R^h<br>$$<br>将注意力结果与解码器隐藏状态拼接后计算新的隐藏状态 $\hat s_t$，再计算输出。<br>$$<br>[\alpha_t;s_t]\in \mathbb R^{2h}<br>$$<br>带有 Attention 的 seq2seq 的简单示意图如下：</p>
<p><img src="/blog/model.png"></p>
<h3 id="广义的Attention机制"><a href="#广义的Attention机制" class="headerlink" title="广义的Attention机制"></a>广义的 Attention 机制</h3><p>广义的 attention 定义如下：给定一组向量 values，一个向量 query，attention 是 value 的加权和，权重是某个相似性度量函数，例如点积、加性注意力等。</p>
<p>度量函数可以为：</p>
<ul>
<li><p>点乘：$e_i=s^\intercal h_i\in \mathbb R$</p>
</li>
<li><p>乘法注意力：$e_i=s^\intercal Wh_i \in \mathbb R$​（其中 $W\in \mathbb R^{d_2*d_1}$ 为权重矩阵）</p>
</li>
<li><p>加法注意力：$e_i=v^\intercal tanh (W_1h_i+W_2s)\in \mathbb R$​（其中 $W_1,W_2$ 为权重矩阵，$v$ 是权重向量）</p>
</li>
</ul>
<p>对应到 seq2seq 的 Attention 机制中，query 向量为解码器隐藏状态，values 为编码器的全部隐藏状态，度量函数为点乘。</p>
<p>联想一下 BERT 的自注意力机制：</p>
<p>对于每个单词向量，通过 Query、Key、Value 三个参数矩阵计算得到三个向量：q,k,v。在每个位置，使用当前位置的 query 向量与每个位置的 key 做点乘，作为相似性度量，再对 value 矩阵加权求和。公式如下：<br>$$<br>Attention(Q,K,V)=softmax(\frac{QK^\intercal}{\sqrt{d_k}})V<br>$$</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>seq2seq</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>语言模型</title>
    <url>/blog/2021/07/22/language-model/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>语言模型是自然语言处理的一个重要部分，在很多生成式任务中，都离不开语言模型。今天我想梳理一下我所理解的语言模型及其发展。</p>
<span id="more"></span>

<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>对于一串语言序列 $w_1w_2\dots w_n$，语言模型试图分析其出现的概率，即 $P (w_1,w_2,\dots,w_n)$​​​​。进而，可以通过概率大小判断文本是否合理。例如句子 “学生们打开了书” 的概率应该比 “学生们打开了玛卡巴卡” 高得多，即更像是人说的话。</p>
<p>在之前的 VQ-VAE 中，我们提到过自回归模型，按照自回归的思路，如果第 n 个单词只与前 n-1 个单词相关，那么句子的概率可以转化为如下形式：<br>$$<br>P(w_1,w_2,\dots,w_n)=\prod_t^nP(w_t|w_{t-1:1})<br>$$<br>那么怎么求解右侧的式子呢？</p>
<h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><p>首先要提到的是 N-gram 模型。为了解决上面这个问题，N-gram 模型引入了马尔科夫假设，认为某一个词只与它之前的 $N-1$ 个词有关。以 4-gram 为例，即每个词只与其之前的 3 个词有关，即：<br>$$<br>P(w_n|w_{n-1:1})=P(w_n|w_{n-1},w_{n-2},w_{n-3})<br>$$<br>换而言之，只要在大规模语料中进行频数的统计，那么就可以得到上述概率的估计：<br>$$<br>P(w_n|w_{n-1},w_{n-2},w_{n-3})=\frac{C(w_n,w_{n-1},w_{n-2},w_{n-3})}{C(w_{n-1},w_{n-2},w_{n-3})}<br>$$<br>进而整个句子的概率可以计算如下：<br>$$<br>P(w_1,w_2,\dots,w_n)=\prod_t^nP(w_t|w_{t-1},w_{t-2},w_{t-3})<br>$$<br>上述的方法虽然简单直接，但是有以下缺点：</p>
<ul>
<li>稀疏问题：一些片段可能没有在语料中出现，计数为 0，在概率连乘之下整句概率变为 0。</li>
<li>存储问题：随着 n 的增大，存储量指数级上升，而 n 过小时模型性能又会很差。</li>
</ul>
<h2 id="基于窗口的神经网络"><a href="#基于窗口的神经网络" class="headerlink" title="基于窗口的神经网络"></a>基于窗口的神经网络</h2><p>在 N-gram 的基础上，使用神经网络来计算条件概率。同样以 4-gram 为例，计算用公式表达如下:<br>$$<br>\begin{align}<br>P(w_n|w_{n-1:1})&amp;=P(w_n|w_{n-1},w_{n-2},w_{n-3})\<br>&amp;=softmax(W[w_{n-1};w_{n-2};w_{n-3}])<br>\end{align}<br>$$<br>思路非常简单，即将前 N-1 个词输入到神经网络，由神经网络计算得到第 N 个词的概率分布。这样做解决了 N-gram 的稀疏问题与存储问题，但也存在一些问题：</p>
<ul>
<li>缺少参数共享：以上述公式为例，$W$​​​中可以分为三部分，分别处理前三个词。然而，词向量的处理逻辑应该是相似的（因为他们都是同样的方法训练出来的）。</li>
<li>需要变化窗口大小时，矩阵 W 的形状也需要变化，进而需要重新训练。</li>
</ul>
<h2 id="循环神经网络RNN"><a href="#循环神经网络RNN" class="headerlink" title="循环神经网络RNN"></a>循环神经网络 RNN</h2><p>RNN 的思路同样也很直接，使用同一个矩阵 $W$ 来处理词向量，并使用一个隐藏状态来记录已处理的信息（换而言之，就无需马尔科夫假设）。RNN 的公式如下:<br>$$<br>h_t=\sigma(W^{(hh)}h_{t-1}+W^{(hx)}x_t)<br>$$</p>
<p>$$<br>\hat{y_t}=softmax(W^{(S)}h_t)<br>$$</p>
<p>其中，$\sigma$ 是激活函数，$h_t$ 是 t 时刻的隐藏状态，$W$ 是参数矩阵。</p>
<p>RNN 有以下优点：</p>
<ul>
<li>能够处理任意长度的序列。</li>
<li>没有进行马尔科夫假设，理论上每一时刻模型都知道之前时刻的全部信息。</li>
</ul>
<p>但也有以下缺点：</p>
<ul>
<li>会出现梯度消失和梯度爆炸问题。</li>
<li>不支持并行化，计算较慢。（可以说是自回归模型的通病）</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/63397627">CS224N 笔记 (六)：语言模型与 RNN - 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语言模型</category>
      </categories>
      <tags>
        <tag>语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title>ConKADI</title>
    <url>/blog/2021/07/20/ConKADI/</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>今天来看一篇对话系统的文章，收录于 2020 年的 ACL。这篇文章是常识对话模型（CCM）的后续工作，我之前粗读过一次，还有很多疑惑，今天正好借着这个机会细读一遍。在此之前，先梳理一下对话系统的发展历史。</p>
<span id="more"></span>
<h2 id="对话系统发展简介">对话系统发展简介</h2>
<p>对话系统，即能够与人进行对话的计算机系统，是自然语言处理中的一个重要方向。在前神经网络时期，对话系统主要基于模板生成回复。即使是现在，仍由一些场景下在使用基于模板的回复生成。在 2014 年，seq2seq（Sequence
to
sequence）模型被提出。seq2seq 提供了一种序列间进行转换映射的通用方法。此后，seq2seq 被广泛用于各类序列任务，包含对话系统。但是 seq2seq 应用于对话系统任务时会有以下问题：</p>
<ul>
<li>对同一输入，只能生成单一回复。而理想的对话系统间输入与回复间的关系应该是一对多的。</li>
<li>倾向于生成通用性回复（例如，我不知道）。</li>
</ul>
<p>此后，构建能够生成多样性回复的对话系统一直是研究人员研究的重点。2017 年，zhao 等人将条件变分自编码器（Condition
vae，CVAE）应用于对话生成，通过在隐变量分布中采样不同的隐变量，模型能够生成多样的回复。</p>
<p>另外，有研究指出，对话模型生成通用性回复的原因之一是语料中缺少人类拥有的知识背景，这使得模型无法学习知识进而理解对话。基于此，一部分工作开始探索在对话模型中引入外部知识。2018 年 zhou 等提出的常识对话模型（CCM）就是这类研究的典型代表。</p>
<p>常识对话模型 CCM 虽然比传统模型取得了更好的效果，但是 CCM 在检索知识实体相关知识事实时，没有考虑到实体单词所在的上下文，而复杂实体单词的具体含义往往是由其上下文决定的。这就来到了本文要介绍的 ConKADI。</p>
<p><img src="apple.png"></p>
<h2 id="研究方法">研究方法</h2>
<p>本文提出了：</p>
<ul>
<li>Felicitous Fact
mechanism（恰当事实机制）帮助模型关注在上下文高度相关的知识事实。</li>
<li>上下文知识融合以及灵活模式融合技术，促进知识的集成。</li>
</ul>
<p>ConKADI（Context Knowledge-Aware Diverse and Informative conversation
generation
model），别的不说，这个名字真的跟叠 buff 一样。。。模型的流程如下：</p>
<ol type="1">
<li>恰当事实机制根据知识实体词所在上下文计算得到知识事实的概率分布。（此过程中，使用真实回复作为后验来监督学习）。</li>
<li>上下文融合机制在解码之前将上下文与知识融合。</li>
<li>ConKADI 在灵活融合模式下生成三种类型的单词。</li>
</ol>
<h3 id="模型概览">模型概览</h3>
<p><img src="model.png"></p>
<p>主要由以下几部分组成：</p>
<ul>
<li>知识检索器（Knowledge Retriever）：给定输入<span class="math inline"> \(X\)</span>，对于每一个单词<span class="math inline"> \(x_i\)</span>，检索<span class="math inline"> \(x_i\)</span>​作为头实体或者尾实体的知识事实，若不为实体词，则返回一个空事实。</li>
<li>上下文编码器（Context
Encoder）：使用双向 GRU 进行编码，特殊的是，GRU 的输入加入了当前实体词的嵌入向量。</li>
<li>恰当知识识别器（Felicitous Fact Recognizer）：计算检索事实<span class="math inline"> \(F=\{f_1,f_2,\dots,f_n\}\)</span>​上的概率分布<span class="math inline"> \(z\)</span>，计算过程如下:</li>
</ul>
<p><span class="math display">\[
z_{post}=\eta(\phi(F\cdot
W_{ft})\cdot\phi([{h^x_n}^\intercal;{h^y_m}^\intercal]\cdot
W_{post}))^\intercal
\]</span></p>
<p><span class="math display">\[
z_{prior}=\eta(\phi(F\cdot W_{ft})\cdot\phi({h^x_n}^\intercal\cdot
W_{prior}))^\intercal
\]</span></p>
<p>其中，<span class="math inline">\(\eta\)</span>​​是 softmax 函数，<span class="math inline">\(\phi\)</span>​​是 tanh 激活函数，<span class="math inline">\(F\in
R^{l*(d_e+d_r+d_e)}\)</span>​​是知识事实矩阵，<span class="math inline">\(W_{ft},W_{post},W_{prior}\)</span>​​​​​是训练参数​。直观来看，上下文、知识事实都包含在公式中，但是也不好进一步解释公式的由来，更像是两部分拼凑在一起的。与 VAE 一样，在得到先后验分布后，使用 KL 散度作为损失函数<span class="math inline"> \(\mathcal
L_k\)</span>，达到逼近先后验分布的效果。</p>
<ul>
<li>上下文知识融合：为了增强解码器对知识背景的理解，将输入上下文与知识融合作为解码器的初始权重，即<span class="math inline"> \({h^y_0}^\intercal=tanh([{h^x_n}^\intercal;f_z^\intercal]\cdot
W_{init})\)</span>​</li>
</ul>
<p>此外，为了保证<span class="math inline"> \({h^x_n}^\intercal,f_z^\intercal\)</span> 是有意义的，模型中还引入了词袋损失（参考 CVAE）。为了监督<span class="math inline"> \(z_{post}\)</span>​的概率分布的计算，引入了监督的条件信号（参考 CCM），二者之和为损失函数<span class="math inline"> \(\mathcal L_f\)</span>。​</p>
<h3 id="知识解码器">知识解码器</h3>
<p>解码器同样是 GRU，在解码时，会从以下三种类型的单词中选择进行输出：</p>
<ul>
<li>词表单词</li>
<li>知识实体单词，计算过程如下：</li>
</ul>
<p><span class="math display">\[
z_{d,t}=\eta(\phi(F\cdot
W_{ft})\cdot\phi([{h^y_t}^\intercal;{u_{t-1}}^\intercal]\cdot
W_{d}))^\intercal
\]</span></p>
<p><span class="math display">\[
\gamma_t=sigmoid([{h^y_t}^\intercal;u_t^\intercal;c_t^\intercal]\cdot
W_{gate})\in R^1
\]</span></p>
<p><span class="math display">\[
p_{k,t}=\gamma_t*z+(1.0-\gamma_t)*z_d
\]</span></p>
<p>其中，<span class="math inline">\(c_t\)</span> 是注意力机制的结果，<span class="math inline">\(z_{d,t}\)</span> 也是同样方法计算得到的知识事实的概率分布，与<span class="math inline"> \(z\)</span> 相比，<span class="math inline">\(z_{d,t}\)</span> 是动态的，而<span class="math inline"> \(z\)</span> 是静态的，与 CCM 中的动 / 静态图注意力机制类似。之后，计算得到一个标量<span class="math inline"> \(\gamma_t\)</span> 作为二者的相对比例，求和得到最终的实体单词权重。</p>
<ul>
<li>复制单词。解码器可从输入中复制一个单词作为输出，计算过程如下：</li>
</ul>
<p><span class="math display">\[
p_{c,t}=\eta(\phi(H^x\cdot W_{cs})\cdot\phi({u_t^c}^\intercal\cdot
W_{ct})^\intercal)
\]</span></p>
<p><span class="math display">\[
{u^c_t}^\intercal=[{h^y_t}^\intercal,{u_{t-1}}^\intercal,{c_t}^\intercal]
\]</span></p>
<p>计算形式与前文知识事实概率分布的计算相似。</p>
<h3 id="灵活模式融合">灵活模式融合</h3>
<p>最终输出的概率分布为三种模式的加权和（其中，<span class="math inline">\((\gamma_{w,t},\gamma_{k,t},\gamma_{c,t})\)</span> 是由灵活模式融合计算得出的概率分布，即三者之和为 1。）：
<span class="math display">\[
p_{out,t}=\gamma_{w,t}*p_{w,t}+\gamma_{k,t}*p_{k,t}+\gamma_{c,t}*p_{c,t}
\]</span> 这一部分损失函数为<span class="math inline"> \(\mathcal
L_n\)</span>： <span class="math display">\[
-\sum_t\lambda_tlogp_{out,t}(y_t|y_{t-1:1},X,F)+\frac{\mathcal L_m}{2}
\]</span> 其中，<span class="math inline">\(\mathcal
L_m\)</span> 为解码器输出与真实回复间的交叉熵，<span class="math inline">\(\lambda_t\)</span>​为词表外单词（unk）的惩罚项权重：
<span class="math display">\[
\lambda_t=
\begin{cases}
\frac{1}{\#(unk\in Y)}^3,\ if\ y_t=unk\\
1,\ otherwise
\end{cases}
\]</span> 个人猜测思路是这样，如果<span class="math inline"> \(y_t\)</span> 为 unk，<span class="math inline">\(\lambda_t\)</span>​会更小，进而优化对应参数的速度会减慢。</p>
<h2 id="case-study">Case Study</h2>
<p>下文是论文中展示的回复样例，只看表格生成回复的效果还是不错的。</p>
<p><img src="result.png"></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>对话系统</category>
      </categories>
      <tags>
        <tag>常识对话</tag>
        <tag>CopyNet</tag>
      </tags>
  </entry>
  <entry>
    <title>EA-VQ-VAE 代码学习（1）</title>
    <url>/blog/2021/07/17/ea-vq-vae-code/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>之前学习 EA-VQ-VAE 的时候发现只读论文本身还是有很多细节问题不太懂，而 EA-VQ-VAE 的代码开源在 <a href="https://github.com/microsoft/EA-VQ-VAE">github</a> 上。今天正好通过学习代码更深层地理解一下这个模型以及基础的 VQ-VAE 模型。</p>
<span id="more"></span>

<h2 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h2><p>代码的目录结构如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">│  README.md</span><br><span class="line">|  LICENSE</span><br><span class="line">├─data</span><br><span class="line">│      get_atomic_data.sh</span><br><span class="line">│      get_event2mind_data.sh</span><br><span class="line">│      preprocess-atomic.py</span><br><span class="line">│      preprocess-event2mind.py</span><br><span class="line">├─estimator</span><br><span class="line">│      model.py</span><br><span class="line">│      run.py</span><br><span class="line">├─generator</span><br><span class="line">│      beam.py</span><br><span class="line">│      model.py</span><br><span class="line">│      run.py</span><br><span class="line">└─vq-vae</span><br><span class="line">        gpt2.py</span><br><span class="line">        model.py</span><br><span class="line">        run.py</span><br></pre></td></tr></tbody></table></figure>

<p>可以看到，整个代码目录结构还是比较清晰的四部分：</p>
<ul>
<li>data/：用以数据的获取和预处理</li>
<li> estimator/：估计先验分布的模型</li>
<li> generator/：推理阶段生成推理文本（光束搜索等）</li>
<li>vq-vae/：vq-vae 的模型定义：包含 codebook、编码器、解码器等</li>
</ul>
<p>这次先介绍最为核心的 vq-vae 模型，处在 vq-vae/model.py。剩下的部分后续有时间再进行分享。</p>
<h2 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ-VAE"></a>VQ-VAE</h2><h3 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h3><p>首先是 CodeBook。codebook 在 EA-VQ-VAE 充当了隐变量表的角色，保存了一张由 K 个 D 维隐变量组成的 $R^{K*D}$。CodeBook 类代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CodeBook</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_embeddings, embedding_dim, commitment_cost</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CodeBook, self).__init__()  </span><br><span class="line">        self._embedding_dim = embedding_dim</span><br><span class="line">        self._num_embeddings = num_embeddings     </span><br><span class="line">        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)      </span><br><span class="line">        self._commitment_cost = commitment_cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="comment"># Calculate distances</span></span><br><span class="line">        distances = (torch.<span class="built_in">sum</span>(inputs**<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">                    + torch.<span class="built_in">sum</span>(self._embedding.weight**<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">                    - <span class="number">2</span> * torch.matmul(inputs, self._embedding.weight.t()))</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Encoding</span></span><br><span class="line">        encoding_indices = torch.argmin(distances, dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        encodings = torch.zeros(encoding_indices.shape[<span class="number">0</span>], self._num_embeddings).cuda()</span><br><span class="line">        encodings.scatter_(<span class="number">1</span>, encoding_indices, <span class="number">1</span>) <span class="comment"># 离散隐变量索引 [batch_size,num_embeddings]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Quantize and unflatten</span></span><br><span class="line">        quantized = torch.matmul(encodings, self._embedding.weight) <span class="comment">## 乘法获得隐变量</span></span><br><span class="line">        <span class="comment"># 整个隐变量的获取方法有点复杂了，argmin之后直接查询embedding即可，无需手动操作。这里这样处理是为了后续</span></span><br><span class="line">        <span class="comment"># 还要计算perplexity</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loss</span></span><br><span class="line">        <span class="comment"># detach()从计算图中脱离，达到stop gradient的目的</span></span><br><span class="line">        e_latent_loss = torch.mean((quantized.detach() - inputs)**<span class="number">2</span>) </span><br><span class="line">        q_latent_loss = torch.mean((quantized - inputs.detach())**<span class="number">2</span>)</span><br><span class="line">        loss = q_latent_loss + self._commitment_cost * e_latent_loss</span><br><span class="line">        </span><br><span class="line">        quantized = inputs + (quantized - inputs).detach()</span><br><span class="line">        avg_probs = torch.mean(encodings, dim=<span class="number">0</span>)</span><br><span class="line">        perplexity = torch.exp(-torch.<span class="built_in">sum</span>(avg_probs * torch.log(avg_probs + <span class="number">1e-10</span>)))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># convert quantized from BHWC -&gt; BCHW</span></span><br><span class="line">        <span class="keyword">return</span> loss, quantized, perplexity, encodings</span><br></pre></td></tr></tbody></table></figure>

<p>整个代码是比较清晰的。在初始化中根据传入参数初始化嵌入空间，并保存了 commitment cost。commitment cost 指的是 VQ-VAE 损失函数的第三项的权重 $\beta$。由论文可知，CodeBook 的前向过程应该是输入编码器输出 $z_e (x)$，输出最近的隐变量 $z$。那么代码中 inputs 的 shape 应该为 [batch_size，embedding_dim]，进而距离的计算过程就很自然了。其他见代码的注释部分。</p>
<p>接下来是 seq2seq 模型：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        Build Seqence-to-Sequence.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * `encoder`- encoder of seq2seq model. e.g. 2-layer transformer</span></span><br><span class="line"><span class="string">        * `decoder`- decoder of seq2seq model. e.g. GPT2</span></span><br><span class="line"><span class="string">        * `config`- configuration of encoder model. </span></span><br><span class="line"><span class="string">        * `args`- arguments.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder,decoder,config,args</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder=decoder</span><br><span class="line">        self.config=config</span><br><span class="line">        self.args=args</span><br><span class="line">        </span><br><span class="line">        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=<span class="literal">False</span>)      </span><br><span class="line">        self.codebook = CodeBook(args.z_size, config.n_embd,<span class="number">0.25</span>)  </span><br><span class="line">        self.codebook._embedding.weight.data.normal_(mean=<span class="number">0</span>,std=<span class="number">0.1</span>)</span><br><span class="line">        self.lsm = nn.LogSoftmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.lm_head.weight=self.decoder.wte.weight     </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, event_ids,target_ids</span>):</span>   </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            Forward the VQ-VAE model.</span></span><br><span class="line"><span class="string">            Parameters:</span></span><br><span class="line"><span class="string">            * `event_ids`- event ids of examples</span></span><br><span class="line"><span class="string">            * `target_ids`- target ids of examples</span></span><br><span class="line"><span class="string">        """</span>  </span><br><span class="line">        input_ids=torch.cat((event_ids,target_ids),-<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#obtain hidden of event+target by encoder</span></span><br><span class="line">        hidden_xy=self.encoder(input_ids,special=<span class="literal">True</span>)[<span class="number">0</span>][:,-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#obtain latent variable z by coodebook</span></span><br><span class="line">        vae_loss, z, perplexity, encoding=self.codebook(hidden_xy)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#obtain hiddens of target </span></span><br><span class="line">        transformer_outputs=self.decoder(input_ids,z=z)</span><br><span class="line">        hidden_states = transformer_outputs[<span class="number">0</span>][:,-target_ids.size(<span class="number">1</span>):]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#calculate loss</span></span><br><span class="line">        lm_logits = self.lm_head(hidden_states+z[:,<span class="literal">None</span>,:])</span><br><span class="line">        <span class="comment"># Shift so that tokens &lt; n predict n</span></span><br><span class="line">        active_loss = target_ids[..., <span class="number">1</span>:].ne(<span class="number">0</span>).view(-<span class="number">1</span>) == <span class="number">1</span> <span class="comment"># 将推理文本展平并得到非0位置的索引，用以计算loss</span></span><br><span class="line">        shift_logits = lm_logits[..., :-<span class="number">1</span>, :].contiguous() <span class="comment"># 去除末尾的EOS</span></span><br><span class="line">        shift_labels = target_ids[..., <span class="number">1</span>:].contiguous() <span class="comment"># </span></span><br><span class="line">        <span class="comment"># Flatten the tokens</span></span><br><span class="line">        loss_fct = CrossEntropyLoss(ignore_index=-<span class="number">1</span>)</span><br><span class="line">        loss = loss_fct(shift_logits.view(-<span class="number">1</span>, shift_logits.size(-<span class="number">1</span>))[active_loss],</span><br><span class="line">                        shift_labels.view(-<span class="number">1</span>)[active_loss])</span><br><span class="line"></span><br><span class="line">        outputs = (loss,vae_loss,perplexity),loss*active_loss.<span class="built_in">sum</span>(),active_loss.<span class="built_in">sum</span>(),encoding</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></tbody></table></figure>

<p>init 方法比较简单，只是保存参数和新建 codebook。前向过程也比较简单：训练阶段，seq2seq 的输入是事件和推理文本的拼接，然后进行编码和解码（这里编码器为 2 层 Transformer，解码器为预训练的 GPT 模型）。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>文本生成</category>
      </categories>
      <tags>
        <tag>VQ-VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>推理文本生成 | EA-VQ-VAE</title>
    <url>/blog/2021/07/16/ea-vq-vae/</url>
    <content><![CDATA[<h2 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h2><p>今天是我的生日，希望能看到这篇博客的你天天开心。如果今天还恰好是你的生日，希望你生日快乐！</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>EA-VQ-VAE 是微软团队于 2020 年发表的《Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder》中提出的模型，该文发表在 ACL 上。该文的主要工作是利用 VQ-VAE 进行推理文本生成。推理文本生成定义为，给定一个事件（例如 “A 偷看了 B 的日记”），从多个维度对该事件进行推断（“A 的心理状态”，“A 的目的”）。而 EA-VQ-VAE 中的 EA（Evidence-Aware）指的是利用证据来进行推理文本生成。</p>
<span id="more"></span>

<h2 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h2><p>下图展示了整个模型的流程：给定事件 $x$ 后，经过 VQ-VAE 将其映射为离散的隐变量 $z$，根据事件 $x$ 从文本语料中检索证据，再一起投喂给解码器输出最终的推理文本 $y$。下面逐项介绍模型的细节。</p>
<p><img src="/blog/model.png"></p>
<h3 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ-VAE"></a>VQ-VAE</h3><p>VQ-VAE 的详细介绍可以看我的上一篇博客。论文使用的 VQ-VAE 与标准的 VQ-VAE 最主要的区别在于，普通的 VQ-VAE 生成是数据 $x$，而在推理文本生成任务中，生成的是以符合事件 $x$ 的推理文本 $y$，换而言之，这是一个<strong>条件模型</strong>，叫它 VQ-CVAE 可能更恰当一点。基于此，下面所述的后验分布 $q_\phi (z|x,y)$ 与先验分布 $p_\theta (z|x)$ 均与标准的 VQ-VAE 有所不同。</p>
<p>本文使用的 VQ-VAE 分为以下三个部分：</p>
<ul>
<li>codebook：对应 VQ-VAE 中的隐变量嵌入空间，只是换了个名字，同样是一张 $R^{k*d}$ 的表，由 $k$ 个维度为 $d$ 的隐变量组成</li>
<li>后验分布 $q_\phi (z|x,y)$：同样是一个独热分布，使用最近邻算法将编码器输出 $h_(x,y)$ 映射到最近的隐变量 $z’$</li>
<li> 先验分布 $p_\theta (z|x)$：先利用预训练的语言模型（例如 RoBERTa）将事件编码为隐藏状态 $h$，，再将其映射为 k 个类别，即 $p_\theta (z|x)=softmax (hW_k)$</li>
</ul>
<h3 id="证据的检索与选择"><a href="#证据的检索与选择" class="headerlink" title="证据的检索与选择"></a>证据的检索与选择</h3><p>去除事件中的停用词后，在大规模文本语料中使用 Elastic Search 引擎检索事件，并选取前 K 个得分最高的句子。论文使用的语料库基于 BookCorpus，由一万多篇故事书组成，因为作者认为故事中会对事件的起因和结果介绍地较为清晰。</p>
<p>证据的选择与隐变量类似，在训练阶段和推理阶段有着不同的逻辑。在训练阶段，事件 $x$ 与推理文本 $y$ 均已知，例如给定事件 “A 读了 B 的日记”，与推理文本 “A 感到很愧疚”，那么证据 “A 偷了 B 的日记” 就比 “B 把日记给 A 看” 更合理，此时我们想要建模的就是 $q (c|x,y)$（c 代表事件上下文，即证据）与 $p (c|x)$。考虑到已经有一个后验分布 $q_\phi (z|x,y)$，那么我们可以直接利用隐变量来完成证据的选择，即建模 $p (c|z)$, 而不是再引入一个复杂的神经网络。对于一组证据（$c_\phi$ 代表填充的空证据）${c_1,c_2,\dots,c_K,c_\phi}$，使用 Transformer 将其编码为向量 ${h_{c_1},h_{c_2},\dots,h_{c_K},h_{c_\phi}}$。$p_s (c|z)$ 与 $q_\phi (z|x,y)$ 类似，也是一个独热分布，再通过最近邻算法选取最近的证据，即：<br>$$<br>p_s(c_k|z)=<br>\begin{cases}<br>1 &amp;if\ k=\arg\min_j||h_{c_j}-z||_2 \<br>0 &amp;otherwise<br>\end{cases}<br>$$</p>
<p>$$<br>c_z=c_k\ where\ k=\arg\min_j||h_{c_j}-z||_2<br>$$</p>
<p>值得注意的是，作者没有使用注意力机制得到的 “软” 分布，而是借鉴 VQ-VAE，采用了一种独热分布将 $z$ 映射到最近的 $c$。这样的优点是一定程度上降低了学习的难度，由于 $z$ 与 $c$ 处在同一个语义空间，解码器利用起来的效率会更高。而且这样做也更为统一。但我总觉得注意力机制得到的结果会更好一点，论文里没有进行比较属实有点伤。</p>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>解码器使用的是预训练的 GPT-2，是一个基于 Transformer 的语言模型。这里就不多赘述了，有兴趣的小伙伴可以去了解一下 GPT 家族。</p>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>首先单独训练 VQ-VAE 与 codebook，再训练基于后验分布 $q_\phi (z|x,y)$ 的证据感知解码器。</p>
<h3 id="VQ-VAE-1"><a href="#VQ-VAE-1" class="headerlink" title="VQ-VAE"></a>VQ-VAE</h3><p>首先只根据隐变量 $z$ 重构推理文本 $y$，损失函数与 VQ-VAE 损失函数类似：<br>$$<br>loss_{rec}=-logp(y|x,h_{(x,y)}+sg[z-h_{(x,y)}])+||sg[h_{(x,y)}]-z||<em>2^2+\beta||h</em>{(x,y)}-sg[z]||_2^2<br>$$</p>
<p>真实的先验分布可以使用频率近似（$N_{(x)}$ 代表包含 $x$ 事件的数据数量）：<br>$$<br>p(z|x)=\sum_{(x,y_i)\in D}\frac{q_\phi(z|x,y_i)}{N_{(x)}}<br>$$<br>通过 KL 散度来优化先验分布 $p_\theta (z|x)$:<br>$$<br>loss_{prior}=KL(p(z|x)||p_\theta(z|x))<br>$$</p>
<p>不过这里为什么不像 CVAE 一样，直接优化后验分布与先验分布间的 KL 散度，暂时还不是很理解。</p>
<h3 id="证据感知解码器"><a href="#证据感知解码器" class="headerlink" title="证据感知解码器"></a>证据感知解码器</h3><p>这一部分通过最大化边际似然进行训练：<br>$$<br>\begin{align}<br>logp(y|x)&amp;=E_{z\sim q_\phi}[\sum_{c\in C}logp_m(y|x,c)p_s(c|z)]\<br>&amp;=log(p_m(y|x,c_{z’}))+logp_s(c_{z’}|z’)<br>\end{align}<br>$$<br>然而，由于真实的证据是未知的，直接优化上述似然函数可能得不到正确结果。具体而言，与 $z’$ 最近的 $c_{z’}$ 不一定就是真实有用的证据，如果我们已知真实的证据标签 $c$，损失函数中应该还有一项是 $||c-c_{z’}||<em>2$。为解决这个问题，原论文采取了强化学习的方法：<br>$$<br>R=\delta(p_m(y|x,c</em>{z’})-p_m(y|x,c_r))<br>$$</p>
<p>$$<br>\begin{align}<br>logp(y|x)&amp;=logp_m(y|x,c_{z’})+Rlogp_s(c_{z’}|z’)\<br>&amp;=logp_m(y|x,c_{z’})-R|||h_{c_{z’}}-z’|| <em>2^2<br>\end{align}<br>$$<br>其中，$\delta (x)$ 当 x 大于 0 时为 1，否则为 - 1。$c_r$ 为随机选取的与 $c</em>{z’}$ 不同的证据。这样设计的原因是，正确的证据相较于其他证据应该能够提高生成正确推理文本的概率。当 $R$ 为正时，$logp (y|x)$ 会更大，进而激励模型选择正确的证据。</p>
<h2 id="个案研究"><a href="#个案研究" class="headerlink" title="个案研究"></a>个案研究</h2><p><img src="/blog/case.png"></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>文本生成</category>
      </categories>
      <tags>
        <tag>VQ-VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>量子变分自编码器 VQ-VAE</title>
    <url>/blog/2021/07/15/vq-vae/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>VQ-VAE（Vector Quantised - Variational AutoEncoder，量子变分自编码器）出自 2017 年 Google 团队的论文 Neural Discrete Representation Learning。顾名思义，VQ-VAE 是 VAE（ Variational AutoEncoder，变分自编码器）的变种。主要是为了解决 VAE 所存在的” 后验坍塌 “问题。VQ-VAE 与 VAE 的主要区别在于：</p>
<ul>
<li>隐变量是离散的，而非连续的</li>
<li>先验分布是学习得来的，而非固定不变的</li>
</ul>
<span id="more"></span>

<h2 id="研究动机与背景"><a href="#研究动机与背景" class="headerlink" title="研究动机与背景"></a>研究动机与背景</h2><h3 id="离散型隐变量"><a href="#离散型隐变量" class="headerlink" title="离散型隐变量"></a>离散型隐变量</h3><p>离散型隐变量对于某些任务是更为自然与恰当的，例如语言是由离散的字符组成的，图像的像素是 0-255 的自然数。然而，离散 VAE 往往难以训练，现有的训练方法无法弥补其与连续型 VAE 存在的性能上的差距。尽管连续型 VAE 会存在后验坍塌问题，但是由于从高斯分布中使用重参数化技巧采样隐变量，连续型 VAE 中能够获得方差更小，即更稳定的参数梯度。</p>
<h3 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h3><p>自回归模型（<strong>A</strong>uto<strong>r</strong>egressive model）是一种处理时间序列的方法，使用 $x_1,x_2,\dots,x_{t-1}$ 来预测 $x_t$，并假设它们是线性关系。由于其使用 $x$ 本身来预测 $x$，因而得名为自回归模型。形式化来讲，自回归模型定义如下：<br>$$<br>X_t=c+\sum_{i=1}^p\phi_iX_{t-i}+\epsilon_t<br>$$<br>其中，$c$ 是常数项，$\epsilon_t$ 假设为一个均值为 0，标准差为 $\sigma$ 的随机误差。</p>
<p>典型的自回归模型有循环神经网络（Recurrent Neural Network, RNN），PixelCNN 等。下面以文中提到的 PixelCNN 为例进行介绍。</p>
<p>PixelCNN 是虽然是 CNN，但它与传统的 CNN 不同，而是参考了 RNN 的思路，将图片扁平化为一维后，将其看成时间序列进行逐像素的生成。即：<br>$$<br>\begin{align}<br>p(x)&amp;=p(x_1,x_2,\dots,x_t)\<br>&amp;=p(x_1)p(x_2|x_1)\dots p(x_t|x_1,x_2,\dots,x_{t-1})<br>\end{align}<br>$$<br>可以看到，符合上述的自回归模型的定义（令 $X_t=p (x_1,x_2,\dots,x_t)$）。</p>
<h3 id="变分自编码器"><a href="#变分自编码器" class="headerlink" title="变分自编码器"></a>变分自编码器</h3><p>变分自编码器（Variational AutoEncoder，VAE）是一类重要的生成模型。由于篇幅原因这里只做简单介绍，后面可能会单独出一篇博客介绍。VAE 假设存在一个无法观测的隐变量 $z$ 控制数据 $x$ 的生成，它主要由以下几部分组成：</p>
<ul>
<li>编码网络，拟合后验分布 $q (z|x)$ ，将数据 $x$ 映射到连续隐变量 $z$</li>
<li> 生成网络，拟合分布 $p (x|z)$</li>
<li> 隐变量的先验分布 $p (z)$</li>
</ul>
<p>在训练过程中，从 $q (z|x)$ 中采样隐变量 $z$ 来重构数据。在推理过程中，从 $p (z)$ 中采样隐变量来生成数据。</p>
<h2 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h2><p>整体结构如下图所示：</p>
<p><img src="/blog/model.png"></p>
<h3 id="离散隐变量"><a href="#离散隐变量" class="headerlink" title="离散隐变量"></a>离散隐变量</h3><p>模型定义了一个 $K*D$ 的隐变量嵌入空间，其中 $K$ 为空间大小，$D$ 为隐变量向量的维度。在得到编码网络的输出 $z_e (x)$ 后，通过<strong>最近邻算法</strong>将其映射为隐变量嵌入空间中的某个隐变量 $e_k$（简记为 $z$），投喂到解码器。后验分布 $q (z|x)$ 定义为如下的独热分布：<br>$$<br>q(z=k|x) = \begin{cases}<br>1 &amp;if\ k=\arg\min_j||z_e(x)-e_j|| , \<br>0 &amp; otherwise.<br>\end{cases}<br>$$<br>进而：<br>$$<br>z_q(x)=e_k, where\ k=\arg\min_j||z_e(x)-e_j||<br>$$</p>
<h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><p>注意到上述公式中的 $\arg\min$ 操作是无法求梯度的，这使得模型无法进行反向传播。VQ-VAE 采取直通估计（straight-through estimator ）来解决这个问题。原论文中具体做法描述为<strong>” 将解码器输入 $z_q (x)$ 的梯度复制到解码器的输出 $z_e (x)$“</strong>。对应上述结构图中的红线。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数表示如下：<br>$$<br>L=logp(x|z_q(x))+||sg[z_e(x)]-e||_2^2+\beta||z_e(x)-sg[e]||^2_2<br>$$<br>其中，$sg$ 代表停止梯度，即反向传播时不再向前计算梯度。这个符号的含义我个人感觉论文解释的有点不清楚，可能需要对照代码进一步看一下。我目前的理解是，在前向传播的时候，sg 是恒等式，即被忽略掉了，此时计算得到的 loss 是真正的 loss。在反向传播时，sg 部分的计算图相当于断开了，以 $||sg [z_e (x)]-e||_2^2$ 为例，前项传播时等价于 $||z_e (x)-e||_2^2$。反向传播时等价于 $||const-e||_2^2$，即将 $z_e (x)$ 看做常数，不对其进行优化。</p>
<p>损失函数的各项含义解释如下：</p>
<ul>
<li>第一项为重构损失，用以训练编码器和解码器，个人感觉这里是不是少了个负号，这一部分是似然函数，按理说应该是要最大化的。</li>
<li>第二项为 L2 范数损失函数。通过矢量量化（Vector Quantisation，VQ）学习嵌入空间的字典，即希望编码器的输出 $z_e (x)$ 与最近邻算法得到的 $e$ 距离越近越好，用以优化嵌入空间。</li>
<li>第三项为 L2 范数损失函数。与第二项的区别在于优化的是编码器。原论文中的说法是，由于嵌入空间是无量纲的，当仅存在第二项时，若 $e$ 的参数训练速度慢于编码器参数，会使得 $e$ 的参数向任意方向增长。</li>
</ul>
<p>第二项和第三项本质上都是希望编码器的输出 $z_e (x)$ 与离散化隐变量 $e$ 相互接近，相较于 $||z_e (x)-e||_2^2$，个人理解这里的设计是为了控制二者的优化速度。如果希望编码器输出相对稳定，则调小 $\beta$，让嵌入空间更多地靠近编码器的输出，也可以反之。</p>
<p>论文中实验发现 $\beta$ 从 0.1-2.0 都是非常鲁棒的，实验设置 $\beta=0.25$，可能意味着二者靠近的速度影响不大（这也更符合直观认知）。</p>
<h3 id="先验分布-p-z"><a href="#先验分布-p-z" class="headerlink" title="先验分布$p(z)$"></a>先验分布 $p (z)$</h3><p>先验分布 $p (z)$ 是个分类分布（categotical distribution），在训练过程中保持不变。在训练结束后，在隐变量 $z$ 上拟合一个自回归分布，即 $p (z)$，进而通过祖先采样（ancestral sampling）来生成 $x$。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E8%BF%B4%E6%AD%B8%E6%A8%A1%E5%9E%8B">自回归模型 - 维基百科，自由的百科全书 (wikipedia.org)</a></li>
</ul>
]]></content>
      <categories>
        <category>生成模型</category>
        <category>VAE</category>
      </categories>
      <tags>
        <tag>VQ-VAE</tag>
        <tag>论文</tag>
      </tags>
  </entry>
</search>
